,author,claps,title,text,preprocessed_text,language,organizations_mentioned
0,Justin Lee,8300,Chatbots were the next big thing: what happened? – The Startup – Medium,"Oh, how the headlines blared:
Chatbots were The Next Big Thing.
Our hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.
And why wouldn’t they be? All the road signs pointed towards insane success.
At the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an ‘overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots’.
In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:
One year on, we have an answer to that question.
No.
Because there isn’t even an ecosystem for a platform to dominate.
Chatbots weren’t the first technological development to be talked up in grandiose terms and then slump spectacularly.
The age-old hype cycle unfolded in familiar fashion...
Expectations built, built, and then..... It all kind of fizzled out.
The predicted paradim shift didn’t materialize.
And apps are, tellingly, still alive and well.
We look back at our breathless optimism and turn to each other, slightly baffled:
“is that it? THAT was the chatbot revolution we were promised?”
Digit’s Ethan Bloch sums up the general consensus:
According to Dave Feldman, Vice President of Product Design at Heap, chatbots didn’t just take on one difficult problem and fail: they took on several and failed all of them.
Bots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.
Users had to type commands manually into a machine to get anything done.
Then, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!
Meanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.
Another bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:
The next item on the agenda was holding a two-way dialog with a machine. Here’s an example dialog (dating back to the 1990s) with VCR setup system:
Pretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.
It was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.
Modern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.
Basically, we’re still trying to achieve the same innovations we were 30 years ago.
Here’s where I think we’re going wrong:
An oversized assumption has been that apps are ‘over’, and would be replaced by bots.
By pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.
You might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?
It’s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No — not yet, at least.
Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.
Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.
A great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, there’s no competition.
That’s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.
Modern-day apps benefit from decades of research and experimentation. Why would we throw this away?
But, if we swap the word ‘replace’ with ‘extend’, things get much more interesting.
Today’s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.
The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.
Another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.
For plenty of companies, bots just aren’t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they aren’t needed.
Building a bot for the sake of it, letting it loose and hoping for the best will never end well:
The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input.
The advantage of this approach is that it’s pretty easy to list all the cases that they are designed to cover. And that’s precisely their disadvantage, too.
That’s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.
Problems arise when life refuses to fit into those boxes.
According to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.
When we were building GrowthBot, we decided to make it specific to sales and marketers: not an ‘all-rounder’, despite the temptation to get overexcited about potential capabilties.
Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.
A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.
In an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.
Some platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)
As Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.
And conversations are complex. They’re not linear. Topics spin around each other, take random turns, restart or abruptly finish.
Today’s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isn’t available yet.
And in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:
Once upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate information
There’s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, it’s easier and faster to click than it is to type.
Tapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.
We love optical displays of information because we are highly visual creatures. It’s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.
Conversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, we’re swapping something simple for a more-complex alternative.
Sure, there are some concepts that we can only express using language (“show me all the ways of getting to a museum that give me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.
Aiming for a human dimension in business interactions makes sense.
If there’s one thing that’s broken about sales and marketing, it’s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated ‘contact us’ forms.
Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.
A conversation encompasses so much more than just text.
Humans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what they’re talking about, meaning it’s a bit like conversing with someone who has little or no short-term memory.
As HubSpot team pinpointed:
People aren’t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that you’re lying to your users).
And even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.
And here’s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans.
But is that how humans prefer to interact with machines?
Not necessarily.
At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.
In a way, those early-adopters weren’t entirely wrong.
People are yelling at Google Home to play their favorite song, ordering pizza from the Domino’s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots haven’t lived up to the hype generated circa 2015/16.
Not even close.
Computers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.
Computers aren’t good at understanding human emotion. The state of NLP means they still don’t ‘get’ what we’re asking them, never mind how we feel.
That’s why it’s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.
For now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.
But that’s not the whole story.
Yes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.
As Bill Gates once said:
The hype is over. And that’s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.
I believe we’re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.
Messaging will continue to gain traction. Chatbots aren’t going away. NLP and AI are becoming more sophisticated every day.
Developers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.
And I can’t wait to see what happens next.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Head of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
",oh headlines blared chatbots next big thing hopes sky high brighteyed bushytailed industry ripe new era innovation time start socializing machines wouldnt road signs pointed towards insane success mobile world congress 2017 chatbots main headliners conference organizers cited overwhelming acceptance event inevitable shift focus brands corporates chatbots fact significant question around chatbots would monopolize field whether chatbots would take first place one year answer question isnt even ecosystem platform dominate chatbots werent first technological development talked grandiose terms slump spectacularly ageold hype cycle unfolded familiar fashion expectations built built kind fizzled predicted paradim shift didnt materialize apps tellingly still alive well look back breathless optimism turn slightly baffled chatbot revolution promised digits ethan bloch sums general consensus according dave feldman vice president product design heap chatbots didnt take one difficult problem fail took several failed bots interface users different ways big divide text vs speech beginning computer interfaces written word users type commands manually machine get anything done graphical user interfaces guis came along saved day became entranced windows mouse clicks icons hey eventually got color meanwhile bunch research scientists busily developing natural language nl interfaces databases instead learn arcane database query language another bunch scientists developing speechprocessing software could speak computer rather type turned whole lot difficult anyone originally realised next item agenda holding twoway dialog machine heres example dialog dating back 1990s vcr setup system pretty cool right system takes turns collaborative way smart job figuring user wants carefully crafted deal conversations involving vcrs could operate within strict limitations modern day bots whether use typed spoken input face challenges also work efficient scalable way variety platforms basically still trying achieve innovations 30 years ago heres think going wrong oversized assumption apps would replaced bots pitting two disparate concepts one another instead seeing separate entities designed serve different purposes discouraged bot development might remember similar war cry apps first came onto scene ten years ago remember apps replaced internet said new product service needs two following better cheaper faster chatbots cheaper faster apps yet least whether theyre better subjective think fair say todays best bot isnt comparable todays best app plus nobody thinks using lyft complicated hard order food buy dress app complicated trying complete tasks bot bot fail great bot useful average app comes rich sophisticated multilayered apps theres competition thats machines let us access vast complex information systems early graphical information systems revolutionary leap forward helping us locate systems modernday apps benefit decades research experimentation would throw away swap word replace extend things get much interesting todays successful bot experiences take hybrid approach incorporating chat broader strategy encompasses traditional elements next wave multimodal apps say want like siri get back information map text even spoken response another problematic aspect sweeping nature hype tends bypass essential questions like plenty companies bots arent right solution past two years littered cases bots blindly applied problems arent needed building bot sake letting loose hoping best never end well vast majority bots built using decisiontree logic bots canned response relies spotting specific keywords user input advantage approach pretty easy list cases designed cover thats precisely disadvantage thats bots purely reflection capability fastidiousness patience person created many user needs inputs able anticipate problems arise life refuses fit boxes according recent reports 70 100000 bots facebook messenger failing fulfil simple user requests partly result developers failing narrow bot one strong area focus building growthbot decided make specific sales marketers allrounder despite temptation get overexcited potential capabilties remember bot one thing well infinitely helpful bot multiple things poorly competent developer build basic bot minutes one hold conversation thats another story despite constant hype around ai still long way achieving anything remotely humanlike ideal world technology known nlp natural language processing allow chatbot understand messages receives nlp emerging research labs much infancy platforms provide bit nlp even best toddlerlevel capacity example think siri understanding words meaning matt asay outlines results another issue failure capture attention creativity developers conversations complex theyre linear topics spin around take random turns restart abruptly finish todays rulebased dialogue systems brittle deal kind unpredictability statistical approaches using machine learning limited level ai required humanlike conversation isnt available yet meantime highquality examples trailblazing bots lead way dave feldman remarked upon time way interact computers typing arcane commands terminal visual interfaces using windows icons mouse revolution manipulate information theres reasons computing moved textbased graphical user interfaces guis input side easier faster click type tapping selecting obviously preferable typing whole sentence even predictive often errorprone text output side old adage picture worth thousand words usually true love optical displays information highly visual creatures accident kids love touch screens pioneers dreamt graphical interface inspired cognitive psychology study brain deals communication conversational uis meant replicate way humans prefer communicate end requiring extra cognitive effort essentially swapping something simple morecomplex alternative sure concepts express using language show ways getting museum give 2000 steps dont take longer 35 minutes tasks carried efficiently intuitively guis conversational ui aiming human dimension business interactions makes sense theres one thing thats broken sales marketing lack humanity brands hide behind ticket numbers feedback forms donotreplyemails automated responses gated contact us forms facebooks goal bots pass socalled turing test meaning cant tell whether talking bot human bot isnt human never conversation encompasses much text humans read lines leverage contextual information understand double layers like sarcasm bots quickly forget theyre talking meaning bit like conversing someone little shortterm memory hubspot team pinpointed people arent easily fooled pretending bot human guaranteed diminish returns mention fact youre lying users even rare bots powered stateoftheart nlp excel processing producing content fall short comparison heres thing conversational uis built replicate way humans prefer communicate humans humans prefer interact machines necessarily end day amount witty quips humanlike mannerisms save bot conversational failure way earlyadopters werent entirely wrong people yelling google home play favorite song ordering pizza dominos bot getting makeup tips sephora terms consumer response developer involvement chatbots havent lived hype generated circa 201516 even close computers good computers searching data crunching numbers analyzing opinions condensing information computers arent good understanding human emotion state nlp means still dont get asking never mind feel thats still impossible imagine effective customer support sales marketing without essential human touch empathy emotional intelligence bots continue help us automated repetitive lowlevel tasks queries cogs larger complex system disservice expecting much soon thats whole story yes industry massively overestimated initial impact chatbots would emphasis initial bill gates said hype thats good thing start examining middlegrounded grey area instead hyperinflated frantic black white zone believe beginning explosive growth sense anticlimax completely normal transformational technology messaging continue gain traction chatbots arent going away nlp ai becoming sophisticated every day developers apps platforms continue experiment heavily invest conversational marketing cant wait see happens next quick cheer standing ovation clap show much enjoyed story head growth growthbot messaging conversational strategy hubspot mediums largest publication makers subscribe receive top stories httpsgooglzhclji,en,"['the Mobile World Congress 2017', 'Lyft', 'GrowthBot', 'NLP', 'Conversational', 'Google Home', 'Growth for GrowthBot, Messaging & Conversational Strategy']"
1,Conor Dewey,1400,Python for Data Science: 8 Concepts You May Have Forgotten,"If you’ve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you’re not alone.
I find myself doing this constantly.
While it’s not unnatural to look things up on StackOverflow or other resources, it does slow you down a good bit and raise questions as to your complete understanding of the language.
We live in a world where there is a seemingly infinite amount of accessible, free resources looming just one search away at all times. However, this can be both a blessing and a curse. When not managed effectively, an over-reliance on these resources can build poor habits that will set you back long-term.
Personally, I find myself pulling code from similar discussion threads several times, rather than taking the time to learn and solidify the concept so that I can reproduce the code myself the next time.
This approach is lazy and while it may be the path of least resistance in the short-term, it will ultimately hurt your growth, productivity, and ability to recall syntax (cough, interviews) down the line.
Recently, I’ve been working through an online data science course titled Python for Data Science and Machine Learning on Udemy (Oh God, I sound like that guy on Youtube). Over the early lectures in the series, I was reminded of some concepts and syntax that I consistently overlook when performing data analysis in Python.
In the interest of solidifying my understanding of these concepts once and for all and saving you guys a couple of StackOverflow searches, here’s the stuff that I’m always forgetting when working with Python, NumPy, and Pandas.
I’ve included a short description and example for each, however for your benefit, I will also include links to videos and other resources that explore each concept more in-depth as well.
Writing out a for loop every time you need to define some sort of list is tedious, luckily Python has a built-in way to address this problem in just one line of code. The syntax can be a little hard to wrap your head around but once you get familiar with this technique you’ll use it fairly often.
See the example above and below for how you would normally go about list comprehension with a for loop vs. creating your list with in one simple line with no loops necessary.
Ever get tired of creating function after function for limited use cases? Lambda functions to the rescue! Lambda functions are used for creating small, one-time and anonymous function objects in Python. Basically, they let you create a function, without creating a function.
The basic syntax of lambda functions is:
Note that lambda functions can do everything that regular functions can do, as long as there’s just one expression. Check out the simple example below and the upcoming video to get a better feel for the power of lambda functions:
Once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool.
Specifically, map takes in a list and transforms it into a new list by performing some sort of operation on each element. In this example, it goes through each element and maps the result of itself times 2 to a new list. Note that the list function simply converts the output to list type.
The filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule.
For creating quick and easy Numpy arrays, look no further than the arange and linspace functions. Each one has their specific purpose, but the appeal here (instead of using range), is that they output NumPy arrays, which are typically easier to work with for data science.
Arange returns evenly spaced values within a given interval. Along with a starting and stopping point, you can also define a step size or data type if necessary. Note that the stopping point is a ‘cut-off’ value, so it will not be included in the array output.
Linspace is very similar, but with a slight twist. Linspace returns evenly spaced numbers over a specified interval. So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. This is especially helpful for data visualizations and declaring axes when plotting.
You may have ran into this when dropping a column in Pandas or summing values in NumPy matrix. If not, then you surely will at some point. Let’s use the example of dropping a column for now:
I don’t know how many times I wrote this line of code before I actually knew why I was declaring axis what I was. As you can probably deduce from above, set axis to 1 if you want to deal with columns and set it to 0 if you want rows. But why is this? My favorite reasoning, or atleast how I remember this:
Calling the shape attribute from a Pandas dataframe gives us back a tuple with the first value representing the number of rows and the second value representing the number of columns. If you think about how this is indexed in Python, rows are at 0 and columns are at 1, much like how we declare our axis value. Crazy, right?
If you’re familiar with SQL, then these concepts will probably come a lot easier for you. Anyhow, these functions are essentially just ways to combine dataframes in specific ways. It can be difficult to keep track of which is best to use at which time, so let’s review it.
Concat allows the user to append one or more dataframes to each other either below or next to it (depending on how you define the axis).
Merge combines multiple dataframes on specific, common columns that serve as the primary key.
Join, much like merge, combines two dataframes. However, it joins them based on their indices, rather than some specified column.
Check out the excellent Pandas documentation for specific syntax and more concrete examples, as well as some special cases that you may run into.
Think of apply as a map function, but made for Pandas DataFrames or more specifically, for Series. If you’re not as familiar, Series are pretty similar to NumPy arrays for the most part.
Apply sends a function to every element along a column or row depending on what you specify. You might imagine how useful this can be, especially for formatting and manipulating values across a whole DataFrame column, without having to loop at all.
Last but certainly not least is pivot tables. If you’re familiar with Microsoft Excel, then you’ve probably heard of pivot tables in some respect. The Pandas built-in pivot_table function creates a spreadsheet-style pivot table as a DataFrame. Note that the levels in the pivot table are stored in MultiIndex objects on the index and columns of the resulting DataFrame.
That’s it for now. I hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods, functions, and concepts you frequently encounter when using Python for data science. Personally, I know that even the act of writing these out and trying to explain them in simple terms has helped me out a ton.
If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!
If you enjoyed this post, feel free to hit the clap button and if you’re interested in posts to come, make sure to follow me on Medium at the link below — I’ll be writing and shipping every day this month as part of a 30-Day Challenge.
This article was originally published on conordewey.com
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist & Writer | www.conordewey.com
Sharing concepts, ideas, and codes.
",youve ever found looking question concept syntax programming youre alone find constantly unnatural look things stackoverflow resources slow good bit raise questions complete understanding language live world seemingly infinite amount accessible free resources looming one search away times however blessing curse managed effectively overreliance resources build poor habits set back longterm personally find pulling code similar discussion threads several times rather taking time learn solidify concept reproduce code next time approach lazy may path least resistance shortterm ultimately hurt growth productivity ability recall syntax cough interviews line recently ive working online data science course titled python data science machine learning udemy oh god sound like guy youtube early lectures series reminded concepts syntax consistently overlook performing data analysis python interest solidifying understanding concepts saving guys couple stackoverflow searches heres stuff im always forgetting working python numpy pandas ive included short description example however benefit also include links videos resources explore concept indepth well writing loop every time need define sort list tedious luckily python builtin way address problem one line code syntax little hard wrap head around get familiar technique youll use fairly often see example would normally go list comprehension loop vs creating list one simple line loops necessary ever get tired creating function function limited use cases lambda functions rescue lambda functions used creating small onetime anonymous function objects python basically let create function without creating function basic syntax lambda functions note lambda functions everything regular functions long theres one expression check simple example upcoming video get better feel power lambda functions grasp lambda functions learning pair map filter functions powerful tool specifically map takes list transforms new list performing sort operation element example goes element maps result times 2 new list note list function simply converts output list type filter function takes list rule much like map however returns subset original list comparing element boolean filtering rule creating quick easy numpy arrays look arange linspace functions one specific purpose appeal instead using range output numpy arrays typically easier work data science arange returns evenly spaced values within given interval along starting stopping point also define step size data type necessary note stopping point cutoff value included array output linspace similar slight twist linspace returns evenly spaced numbers specified interval given starting stopping point well number values linspace evenly space numpy array especially helpful data visualizations declaring axes plotting may ran dropping column pandas summing values numpy matrix surely point lets use example dropping column dont know many times wrote line code actually knew declaring axis probably deduce set axis 1 want deal columns set 0 want rows favorite reasoning atleast remember calling shape attribute pandas dataframe gives us back tuple first value representing number rows second value representing number columns think indexed python rows 0 columns 1 much like declare axis value crazy right youre familiar sql concepts probably come lot easier anyhow functions essentially ways combine dataframes specific ways difficult keep track best use time lets review concat allows user append one dataframes either next depending define axis merge combines multiple dataframes specific common columns serve primary key join much like merge combines two dataframes however joins based indices rather specified column check excellent pandas documentation specific syntax concrete examples well special cases may run think apply map function made pandas dataframes specifically series youre familiar series pretty similar numpy arrays part apply sends function every element along column row depending specify might imagine useful especially formatting manipulating values across whole dataframe column without loop last certainly least pivot tables youre familiar microsoft excel youve probably heard pivot tables respect pandas builtin pivot_table function creates spreadsheetstyle pivot table dataframe note levels pivot table stored multiindex objects index columns resulting dataframe thats hope couple overviews effectively jogged memory regarding important yet somewhat tricky methods functions concepts frequently encounter using python data science personally know even act writing trying explain simple terms helped ton youre interested receiving weekly rundown interesting articles resources focused data science machine learning artificial intelligence subscribe self driven data science using form enjoyed post feel free hit clap button youre interested posts come make sure follow medium link ill writing shipping every day month part 30day challenge article originally published conordeweycom quick cheer standing ovation clap show much enjoyed story data scientist writer wwwconordeweycom sharing concepts ideas codes,en,"['StackOverflow', 'Pandas', 'SQL', 'Merge', 'DataFrame', 'MultiIndex', 'Python', 'Challenge', 'Data Scientist & Writer |']"
2,William Koehrsen,2800,Automated Feature Engineering in Python – Towards Data Science,"Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as random search, aim to simplify the model selection and tuning parts of machine learning by finding the best model for a dataset with little to no manual intervention. However, feature engineering, an arguably more valuable aspect of the machine learning pipeline, remains almost entirely a human labor.
Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. This step can be more important than the actual model used because a machine learning algorithm only learns from the data we give it, and creating features that are relevant to a task is absolutely crucial (see the excellent paper “A Few Useful Things to Know about Machine Learning”).
Typically, feature engineering is a drawn-out manual process, relying on domain knowledge, intuition, and data manipulation. This process can be extremely tedious and the final features will be limited both by human subjectivity and time. Automated feature engineering aims to help the data scientist by automatically creating many candidate features out of a dataset from which the best can be selected and used for training.
In this article, we will walk through an example of using automated feature engineering with the featuretools Python library. We will use an example dataset to show the basics (stay tuned for future posts using real-world data). The complete code for this article is available on GitHub.
Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model.
The process of constructing features is very time-consuming because each new feature usually requires several steps to build, especially when using information from more than one table. We can group the operations of feature creation into two categories: transformations and aggregations. Let’s look at a few examples to see these concepts in action.
A transformation acts on a single table (thinking in terms of Python, a table is just a Pandas DataFrame ) by creating new features out of one or more of the existing columns. As an example, if we have the table of clients below
we can create features by finding the month of the joined column or taking the natural log of the income column. These are both transformations because they use information from only one table.
On the other hand, aggregations are performed across tables, and use a one-to-many relationship to group observations and then calculate statistics. For example, if we have another table with information on the loans of clients, where each client may have multiple loans, we can calculate statistics such as the average, maximum, and minimum of loans for each client.
This process involves grouping the loans table by the client, calculating the aggregations, and then merging the resulting data into the client data. Here’s how we would do that in Python using the language of Pandas.
These operations are not difficult by themselves, but if we have hundreds of variables spread across dozens of tables, this process is not feasible to do by hand. Ideally, we want a solution that can automatically perform transformations and aggregations across multiple tables and combine the resulting data into a single table. Although Pandas is a great resource, there’s only so much data manipulation we want to do by hand! (For more on manual feature engineering check out the excellent Python Data Science Handbook).
Fortunately, featuretools is exactly the solution we are looking for. This open-source Python library will automatically create many features from a set of related tables. Featuretools is based on a method known as “Deep Feature Synthesis”, which sounds a lot more imposing than it actually is (the name comes from stacking multiple features not because it uses deep learning!).
Deep feature synthesis stacks multiple transformation and aggregation operations (which are called feature primitives in the vocab of featuretools) to create features from data spread across many tables. Like most ideas in machine learning, it’s a complex method built on a foundation of simple concepts. By learning one building block at a time, we can form a good understanding of this powerful method.
First, let’s take a look at our example data. We already saw some of the dataset above, and the complete collection of tables is as follows:
If we have a machine learning task, such as predicting whether a client will repay a future loan, we will want to combine all the information about clients into a single table. The tables are related (through the client_id and the loan_id variables) and we could use a series of transformations and aggregations to do this process by hand. However, we will shortly see that we can instead use featuretools to automate the process.
The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes.
We can create an empty entityset in featuretools using the following:
Now we have to add entities. Each entity must have an index, which is a column with all unique elements. That is, each value in the index must appear in the table only once. The index in the clients dataframe is the client_idbecause each client has only one row in this dataframe. We add an entity with an existing index to an entityset using the following syntax:
The loans dataframe also has a unique index, loan_id and the syntax to add this to the entityset is the same as for clients. However, for the payments dataframe, there is no unique index. When we add this entity to the entityset, we need to pass in the parameter make_index = True and specify the name of the index. Also, although featuretools will automatically infer the data type of each column in an entity, we can override this by passing in a dictionary of column types to the parameter variable_types .
For this dataframe, even though missed is an integer, this is not a numeric variable since it can only take on 2 discrete values, so we tell featuretools to treat is as a categorical variable. After adding the dataframes to the entityset, we inspect any of them:
The column types have been correctly inferred with the modification we specified. Next, we need to specify how the tables in the entityset are related.
The best way to think of a relationship between two tables is the analogy of parent to child. This is a one-to-many relationship: each parent can have multiple children. In the realm of tables, a parent table has one row for every parent, but the child table may have multiple rows corresponding to multiple children of the same parent.
For example, in our dataset, the clients dataframe is a parent of the loans dataframe. Each client has only one row in clients but may have multiple rows in loans. Likewise, loans is the parent of payments because each loan will have multiple payments. The parents are linked to their children by a shared variable. When we perform aggregations, we group the child table by the parent variable and calculate statistics across the children of each parent.
To formalize a relationship in featuretools, we only need to specify the variable that links two tables together. The clients and the loans table are linked via the client_id variable and loans and payments are linked with the loan_id. The syntax for creating a relationship and adding it to the entityset are shown below:
The entityset now contains the three entities (tables) and the relationships that link these entities together. After adding entities and formalizing relationships, our entityset is complete and we are ready to make features.
Before we can quite get to deep feature synthesis, we need to understand feature primitives. We already know what these are, but we have just been calling them by different names! These are simply the basic operations that we use to form new features:
New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives):
These primitives can be used by themselves or combined to create features. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). We pass in the entityset, the target_entity , which is the table where we want to add the features, the selected trans_primitives (transformations), and agg_primitives (aggregations):
The result is a dataframe of new features for each client (because we made clients the target_entity). For example, we have the month each client joined which is a transformation feature primitive:
We also have a number of aggregation primitives such as the average payment amounts for each client:
Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives.
The complete dataframe has 793 columns of new features!
We now have all the pieces in place to understand deep feature synthesis (dfs). In fact, we already performed dfs in the previous function call! A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature.
For example, the MEAN(payments.payment_amount) column is a deep feature with a depth of 1 because it was created using a single aggregation. A feature with a depth of two is LAST(loans(MEAN(payments.payment_amount)) This is made by stacking two aggregations: LAST (most recent) on top of MEAN. This represents the average payment size of the most recent loan for each client.
We can stack features to any depth we want, but in practice, I have never gone beyond a depth of 2. After this point, the features are difficult to interpret, but I encourage anyone interested to try “going deeper”.
We do not have to manually specify the feature primitives, but instead can let featuretools automatically choose features for us. To do this, we use the same ft.dfs function call but do not pass in any feature primitives:
Featuretools has built many new features for us to use. While this process does automatically create new features, it will not replace the data scientist because we still have to figure out what to do with all these features. For example, if our goal is to predict whether or not a client will repay a loan, we could look for the features most correlated with a specified outcome. Moreover, if we have domain knowledge, we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features.
Automated feature engineering has solved one problem, but created another: too many features. Although it’s difficult to say before fitting a model which of these features will be important, it’s likely not all of them will be relevant to a task we want to train our model on. Moreover, having too many features can lead to poor model performance because the less useful features drown out those that are more important.
The problem of too many features is known as the curse of dimensionality. As the number of features increases (the dimension of the data grows) it becomes more and more difficult for a model to learn the mapping between features and targets. In fact, the amount of data needed for the model to perform well scales exponentially with the number of features.
The curse of dimensionality is combated with feature reduction (also known as feature selection): the process of removing irrelevant features. This can take on many forms: Principal Component Analysis (PCA), SelectKBest, using feature importances from a model, or auto-encoding using deep neural networks. However, feature reduction is a different topic for another article. For now, we know that we can use featuretools to create numerous features from many tables with minimal effort!
Like many topics in machine learning, automated feature engineering with featuretools is a complicated concept built on simple ideas. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables.
In future articles, I’ll show how to use this technique on a real world problem, the Home Credit Default Risk competition currently being hosted on Kaggle. Stay tuned for that post, and in the meantime, read this introduction to get started in the competition! I hope that you can now use automated feature engineering as an aid in a data science pipeline. Our models are only as good as the data we give them, and automated feature engineering can help to make the feature creation process more efficient.
For more information on featuretools, including advanced usage, check out the online documentation. To see how featuretools is used in practice, read about the work of Feature Labs, the company behind the open-source library.
As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist and Master Student, Data Science Communicator and Advocate
Sharing concepts, ideas, and codes.
",machine learning increasingly moving handdesigned models automatically optimized pipelines using tools h20 tpot autosklearn libraries along methods random search aim simplify model selection tuning parts machine learning finding best model dataset little manual intervention however feature engineering arguably valuable aspect machine learning pipeline remains almost entirely human labor feature engineering also known feature creation process constructing new features existing data train machine learning model step important actual model used machine learning algorithm learns data give creating features relevant task absolutely crucial see excellent paper useful things know machine learning typically feature engineering drawnout manual process relying domain knowledge intuition data manipulation process extremely tedious final features limited human subjectivity time automated feature engineering aims help data scientist automatically creating many candidate features dataset best selected used training article walk example using automated feature engineering featuretools python library use example dataset show basics stay tuned future posts using realworld data complete code article available github feature engineering means building additional features existing data often spread across multiple related tables feature engineering requires extracting relevant information data getting single table used train machine learning model process constructing features timeconsuming new feature usually requires several steps build especially using information one table group operations feature creation two categories transformations aggregations lets look examples see concepts action transformation acts single table thinking terms python table pandas dataframe creating new features one existing columns example table clients create features finding month joined column taking natural log income column transformations use information one table hand aggregations performed across tables use onetomany relationship group observations calculate statistics example another table information loans clients client may multiple loans calculate statistics average maximum minimum loans client process involves grouping loans table client calculating aggregations merging resulting data client data heres would python using language pandas operations difficult hundreds variables spread across dozens tables process feasible hand ideally want solution automatically perform transformations aggregations across multiple tables combine resulting data single table although pandas great resource theres much data manipulation want hand manual feature engineering check excellent python data science handbook fortunately featuretools exactly solution looking opensource python library automatically create many features set related tables featuretools based method known deep feature synthesis sounds lot imposing actually name comes stacking multiple features uses deep learning deep feature synthesis stacks multiple transformation aggregation operations called feature primitives vocab featuretools create features data spread across many tables like ideas machine learning complex method built foundation simple concepts learning one building block time form good understanding powerful method first lets take look example data already saw dataset complete collection tables follows machine learning task predicting whether client repay future loan want combine information clients single table tables related client_id loan_id variables could use series transformations aggregations process hand however shortly see instead use featuretools automate process first two concepts featuretools entities entitysets entity simply table dataframe think pandas entityset collection tables relationships think entityset another python data structure methods attributes create empty entityset featuretools using following add entities entity must index column unique elements value index must appear table index clients dataframe client_idbecause client one row dataframe add entity existing index entityset using following syntax loans dataframe also unique index loan_id syntax add entityset clients however payments dataframe unique index add entity entityset need pass parameter make_index true specify name index also although featuretools automatically infer data type column entity override passing dictionary column types parameter variable_types dataframe even though missed integer numeric variable since take 2 discrete values tell featuretools treat categorical variable adding dataframes entityset inspect column types correctly inferred modification specified next need specify tables entityset related best way think relationship two tables analogy parent child onetomany relationship parent multiple children realm tables parent table one row every parent child table may multiple rows corresponding multiple children parent example dataset clients dataframe parent loans dataframe client one row clients may multiple rows loans likewise loans parent payments loan multiple payments parents linked children shared variable perform aggregations group child table parent variable calculate statistics across children parent formalize relationship featuretools need specify variable links two tables together clients loans table linked via client_id variable loans payments linked loan_id syntax creating relationship adding entityset shown entityset contains three entities tables relationships link entities together adding entities formalizing relationships entityset complete ready make features quite get deep feature synthesis need understand feature primitives already know calling different names simply basic operations use form new features new features created featuretools using primitives either stacking multiple primitives list feature primitives featuretools also define custom primitives primitives used combined create features make features specified primitives use ftdfs function standing deep feature synthesis pass entityset target_entity table want add features selected trans_primitives transformations agg_primitives aggregations result dataframe new features client made clients target_entity example month client joined transformation feature primitive also number aggregation primitives average payment amounts client even though specified feature primitives featuretools created many new features combining stacking primitives complete dataframe 793 columns new features pieces place understand deep feature synthesis dfs fact already performed dfs previous function call deep feature simply feature made stacking multiple primitives dfs name process makes features depth deep feature number primitives required make feature example meanpaymentspayment_amount column deep feature depth 1 created using single aggregation feature depth two lastloansmeanpaymentspayment_amount made stacking two aggregations last recent top mean represents average payment size recent loan client stack features depth want practice never gone beyond depth 2 point features difficult interpret encourage anyone interested try going deeper manually specify feature primitives instead let featuretools automatically choose features us use ftdfs function call pass feature primitives featuretools built many new features us use process automatically create new features replace data scientist still figure features example goal predict whether client repay loan could look features correlated specified outcome moreover domain knowledge use choose specific feature primitives seed deep feature synthesis candidate features automated feature engineering solved one problem created another many features although difficult say fitting model features important likely relevant task want train model moreover many features lead poor model performance less useful features drown important problem many features known curse dimensionality number features increases dimension data grows becomes difficult model learn mapping features targets fact amount data needed model perform well scales exponentially number features curse dimensionality combated feature reduction also known feature selection process removing irrelevant features take many forms principal component analysis pca selectkbest using feature importances model autoencoding using deep neural networks however feature reduction different topic another article know use featuretools create numerous features many tables minimal effort like many topics machine learning automated feature engineering featuretools complicated concept built simple ideas using concepts entitysets entities relationships featuretools perform deep feature synthesis create new features deep feature synthesis turn stacks feature primitives aggregations act across onetomany relationship tables transformations functions applied one columns single table build new features multiple tables future articles ill show use technique real world problem home credit default risk competition currently hosted kaggle stay tuned post meantime read introduction get started competition hope use automated feature engineering aid data science pipeline models good data give automated feature engineering help make feature creation process efficient information featuretools including advanced usage check online documentation see featuretools used practice read work feature labs company behind opensource library always welcome feedback constructive criticism reached twitter koehrsen_will quick cheer standing ovation clap show much enjoyed story data scientist master student data science communicator advocate sharing concepts ideas codes,en,"['TPOT', 'algorithm', 'Python', 'GitHub', 'Pandas', 'Python Data Science Handbook', 'EntitySet', 'the Home Credit Default Risk', 'Kaggle', 'Feature Labs']"
3,Gant Laborde,1300,Machine Learning: how to go from Zero to Hero – freeCodeCamp,"If your understanding of A.I. and Machine Learning is a big question mark, then this is the blog post for you. Here, I gradually increase your AwesomenessicityTM by gluing inspirational videos together with friendly text.
Sit down and relax. These videos take time, and if they don’t inspire you to continue to the next section, fair enough.
However, if you find yourself at the bottom of this article, you’ve earned your well-rounded knowledge and passion for this new world. Where you go from there is up to you.
A.I. was always cool, from moving a paddle in Pong to lighting you up with combos in Street Fighter.
A.I. has always revolved around a programmer’s functional guess at how something should behave. Fun, but programmers aren’t always gifted in programming A.I. as we often see. Just Google “epic game fails” to see glitches in A.I., physics, and sometimes even experienced human players.
Regardless, A.I. has a new talent. You can teach a computer to play video games, understand language, and even how to identify people or things. This tip-of-the-iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory.
I’m talking about Machine Learning.
You don’t need to come up with advanced algorithms anymore. You just have to teach a computer to come up with its own advanced algorithm.
So how does something like that even work? An algorithm isn’t really written as much as it is sort of... bred. I’m not using breeding as an analogy. Watch this short video, which gives excellent commentary and animations to the high-level concept of creating the A.I.
Wow! Right? That’s a crazy process!
Now how is it that we can’t even understand the algorithm when it’s done? One great visual was when the A.I. was written to beat Mario games. As a human, we all understand how to play a side-scroller, but identifying the predictive strategy of the resulting A.I. is insane.
Impressed? There’s something amazing about this idea, right? The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games.
Fortunately for you, Elon Musk already provided a non-profit company to do the latter. Yes, in a dozen lines of code you can hook up any A.I. you want to countless games/tasks!
I have two good answers on why you should care. Firstly, Machine Learning (ML) is making computers do things that we’ve never made computers do before. If you want to do something new, not just new to you, but to the world, you can do it with ML.
Secondly, if you don’t influence the world, the world will influence you.
Right now significant companies are investing in ML, and we’re already seeing it change the world. Thought-leaders are warning that we can’t let this new age of algorithms exist outside of the public eye. Imagine if a few corporate monoliths controlled the Internet. If we don’t take up arms, the science won’t be ours. I think Christian Heilmann said it best in his talk on ML.
The concept is useful and cool. We understand it at a high level, but what the heck is actually happening? How does this work?
If you want to jump straight in, I suggest you skip this section and move on to the next “How Do I Get Started” section. If you’re motivated to be a DOer in ML, you won’t need these videos.
If you’re still trying to grasp how this could even be a thing, the following video is perfect for walking you through the logic, using the classic ML problem of handwriting.
Pretty cool huh? That video shows that each layer gets simpler rather than more complicated. Like the function is chewing data into smaller pieces that end in an abstract concept. You can get your hands dirty in interacting with this process on this site (by Adam Harley).
It’s cool watching data go through a trained model, but you can even watch your neural network get trained.
One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. In a presentation I attended by JavaFXpert’s overview on Machine Learning, I learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network. You get to watch it train the neural model!
Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above.
These concepts are exciting! Are you ready to be the Einstein of this new era? Breakthroughs are happening every day, so get started now.
There are tons of resources available. I’ll be recommending two approaches.
In this approach, you’ll understand Machine Learning down to the algorithms and the math. I know this way sounds tough, but how cool would it be to really get into the details and code this stuff from scratch!
If you want to be a force in ML, and hold your own in deep conversations, then this is the route for you.
I recommend that you try out Brilliant.org’s app (always great for any science lover) and take the Artificial Neural Network course. This course has no time limits and helps you learn ML while killing time in line on your phone.
This one costs money after Level 1.
Combine the above with simultaneous enrollment in Andrew Ng’s Stanford course on “Machine Learning in 11 weeks”. This is the course that Jim Weaver recommended in his video above. I’ve also had this course independently suggested to me by Jen Looper.
Everyone provides a caveat that this course is tough. For some of you that’s a show stopper, but for others, that’s why you’re going to put yourself through it and collect a certificate saying you did.
This course is 100% free. You only have to pay for a certificate if you want one.
With those two courses, you’ll have a LOT of work to do. Everyone should be impressed if you make it through because that’s not simple.
But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways.
If you’re not interested in writing the algorithms, but you want to use them to create the next breathtaking website/app, you should jump into TensorFlow and the crash course.
TensorFlow is the de facto open-source software library for machine learning. It can be used in countless ways and even with JavaScript. Here’s a crash course.
Plenty more information on available courses and rankings can be found here.
If taking a course is not your style, you’re still in luck. You don’t have to learn the nitty-gritty of ML in order to use it today. You can efficiently utilize ML as a service in many ways with tech giants who have trained models ready.
I would still caution you that there’s no guarantee that your data is safe or even yours, but the offerings of services for ML are quite attractive!
Using an ML service might be the best solution for you if you’re excited and able to upload your data to Amazon/Microsoft/Google. I like to think of these services as a gateway drug to advanced ML. Either way, it’s good to get started now.
I have to say thank you to all the aforementioned people and videos. They were my inspiration to get started, and though I’m still a newb in the ML world, I’m happy to light the path for others as we embrace this awe-inspiring age we find ourselves in.
It’s imperative to reach out and connect with people if you take up learning this craft. Without friendly faces, answers, and sounding boards, anything can be hard. Just being able to ask and get a response is a game changer. Add me, and add the people mentioned above. Friendly people with friendly advice helps!
See?
I hope this article has inspired you and those around you to learn ML!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Software Consultant, Adjunct Professor, Published Author, Award Winning Speaker, Mentor, Organizer and Immature Nerd :D — Lately full of React Native Tech
Our community publishes stories worth reading on development, design, and data science.
",understanding ai machine learning big question mark blog post gradually increase awesomenessicitytm gluing inspirational videos together friendly text sit relax videos take time dont inspire continue next section fair enough however find bottom article youve earned wellrounded knowledge passion new world go ai always cool moving paddle pong lighting combos street fighter ai always revolved around programmers functional guess something behave fun programmers arent always gifted programming ai often see google epic game fails see glitches ai physics sometimes even experienced human players regardless ai new talent teach computer play video games understand language even identify people things tipoftheiceberg new skill comes old concept recently got processing power exist outside theory im talking machine learning dont need come advanced algorithms anymore teach computer come advanced algorithm something like even work algorithm isnt really written much sort bred im using breeding analogy watch short video gives excellent commentary animations highlevel concept creating ai wow right thats crazy process cant even understand algorithm done one great visual ai written beat mario games human understand play sidescroller identifying predictive strategy resulting ai insane impressed theres something amazing idea right problem dont know machine learning dont know hook video games fortunately elon musk already provided nonprofit company latter yes dozen lines code hook ai want countless gamestasks two good answers care firstly machine learning ml making computers things weve never made computers want something new new world ml secondly dont influence world world influence right significant companies investing ml already seeing change world thoughtleaders warning cant let new age algorithms exist outside public eye imagine corporate monoliths controlled internet dont take arms science wont think christian heilmann said best talk ml concept useful cool understand high level heck actually happening work want jump straight suggest skip section move next get started section youre motivated doer ml wont need videos youre still trying grasp could even thing following video perfect walking logic using classic ml problem handwriting pretty cool huh video shows layer gets simpler rather complicated like function chewing data smaller pieces end abstract concept get hands dirty interacting process site adam harley cool watching data go trained model even watch neural network get trained one classic realworld examples machine learning action iris data set 1936 presentation attended javafxperts overview machine learning learned use tool visualize adjustment back propagation weights neurons neural network get watch train neural model even youre java buff presentation jim gives things machine learning pretty cool 15 hour introduction ml concepts includes info many examples concepts exciting ready einstein new era breakthroughs happening every day get started tons resources available ill recommending two approaches approach youll understand machine learning algorithms math know way sounds tough cool would really get details code stuff scratch want force ml hold deep conversations route recommend try brilliantorgs app always great science lover take artificial neural network course course time limits helps learn ml killing time line phone one costs money level 1 combine simultaneous enrollment andrew ngs stanford course machine learning 11 weeks course jim weaver recommended video ive also course independently suggested jen looper everyone provides caveat course tough thats show stopper others thats youre going put collect certificate saying course 100 free pay certificate want one two courses youll lot work everyone impressed make thats simple make youll deep understanding implementation machine learning catapult successfully applying new worldchanging ways youre interested writing algorithms want use create next breathtaking websiteapp jump tensorflow crash course tensorflow de facto opensource software library machine learning used countless ways even javascript heres crash course plenty information available courses rankings found taking course style youre still luck dont learn nittygritty ml order use today efficiently utilize ml service many ways tech giants trained models ready would still caution theres guarantee data safe even offerings services ml quite attractive using ml service might best solution youre excited able upload data amazonmicrosoftgoogle like think services gateway drug advanced ml either way good get started say thank aforementioned people videos inspiration get started though im still newb ml world im happy light path others embrace aweinspiring age find imperative reach connect people take learning craft without friendly faces answers sounding boards anything hard able ask get response game changer add add people mentioned friendly people friendly advice helps see hope article inspired around learn ml quick cheer standing ovation clap show much enjoyed story software consultant adjunct professor published author award winning speaker mentor organizer immature nerd lately full react native tech community publishes stories worth reading development design data science,en,"['A.I.', 'Brilliant.org', 'the Artificial Neural Network', 'Stanford', 'JavaScript', 'Amazon/Microsoft/Google', 'Adjunct Professor', 'Published Author', 'Mentor', 'Organizer']"
4,Emmanuel Ameisen,935,Reinforcement Learning from scratch – Insight Data,"Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
Recently, I gave a talk at the O’Reilly AI conference in Beijing about some of the interesting lessons we’ve learned in the world of NLP. While there, I was lucky enough to attend a tutorial on Deep Reinforcement Learning (Deep RL) from scratch by Unity Technologies. I thought that the session, led by Arthur Juliani, was extremely informative and wanted to share some big takeaways below.
In our conversations with companies, we’ve seen a rise of interesting Deep RL applications, tools and results. In parallel, the inner workings and applications of Deep RL, such as AlphaGo pictured above, can often seem esoteric and hard to understand. In this post, I will give an overview of core aspects of the field that can be understood by anyone.
Many of the visuals are from the slides of the talk, and some are new. The explanations and opinions are mine. If anything is unclear, reach out to me here!
Deep RL is a field that has seen vast amounts of research interest, including learning to play Atari games, beating pro players at Dota 2, and defeating Go champions. Contrary to many classical Deep Learning problems that often focus on perception (does this image contain a stop sign?), Deep RL adds the dimension of actions that influence the environment (what is the goal, and how do I get there?). In dialog systems for example, classical Deep Learning aims to learn the right response for a given query. On the other hand, Deep Reinforcement Learning focuses on the right sequences of sentences that will lead to a positive outcome, for example a happy customer.
This makes Deep RL particularly attractive for tasks that require planning and adaptation, such as manufacturing or self-driving. However, industry applications have trailed behind the rapidly advancing results coming out of the research community. A major reason is that Deep RL often requires an agent to experiment millions of times before learning anything useful. The best way to do this rapidly is by using a simulation environment. This tutorial will be using Unity to create environments to train agents in.
For this workshop led by Arthur Juliani and Leon Chen, their goal was to get every participants to successfully train multiple Deep RL algorithms in 4 hours. A tall order! Below, is a comprehensive overview of many of the main algorithms that power Deep RL today. For a more complete set of tutorials, Arthur Juliani wrote an 8-part series starting here.
Deep RL can be used to best the top human players at Go, but to understand how that’s done, you first need to understand a few simple concepts, starting with much easier problems.
1/It all starts with slot machines
Let’s imagine you are faced with 4 chests that you can pick from at each turn. Each of them have a different average payout, and your goal is to maximize the total payout you receive after a fixed number of turns. This is a classic problem called Multi-armed bandits and is where we will start. The crux of the problem is to balance exploration, which helps us learn about which states are good, and exploitation, where we now use what we know to pick the best slot machine.
Here, we will utilize a value function that maps our actions to an estimated reward, called the Q function. First, we’ll initialize all Q values at equal values. Then, we’ll update the Q value of each action (picking each chest) based on how good the payout was after choosing this action. This allows us to learn a good value function. We will approximate our Q function using a neural network (starting with a very shallow one) that learns a probability distribution (by using a softmax) over the 4 potential chests.
While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. Intuitively, we might want to use a policy that picks the action with the highest Q value. This performs poorly in practice, as our Q estimates will be very wrong at the start before we gather enough experience through trial and error. This is why we need to add a mechanism to our policy to encourage exploration. One way to do that is to use epsilon greedy, which consists of taking a random action with probability epsilon. We start with epsilon being close to 1, always choosing random actions, and lower epsilon as we go along and learn more about which chests are good. Eventually, we learn which chests are best.
In practice, we might want to take a more subtle approach than either taking the action we think is the best, or a random action. A popular method is Boltzmann Exploration, which adjust probabilities based on our current estimate of how good each chest is, adding in a randomness factor.
2/Adding different states
The previous example was a world in which we were always in the same state, waiting to pick from the same 4 chests in front of us. Most real-word problems consist of many different states. That is what we will add to our environment next. Now, the background behind chests alternates between 3 colors at each turn, changing the average values of the chests. This means we need to learn a Q function that depends not only on the action (the chest we pick), but the state (what the color of the background is). This version of the problem is called Contextual Multi-armed Bandits.
Surprisingly, we can use the same approach as before. The only thing we need to add is an extra dense layer to our neural network, that will take in as input a vector representing the current state of the world.
3/Learning about the consequences of our actions
There is another key factor that makes our current problem simpler than mosts. In most environments, such as in the maze depicted above, the actions that we take have an impact on the state of the world. If we move up on this grid, we might receive a reward or we might receive nothing, but the next turn we will be in a different state. This is where we finally introduce a need for planning.
First, we will define our Q function as the immediate reward in our current state, plus the discounted reward we are expecting by taking all of our future actions. This solution works if our Q estimate of states is accurate, so how can we learn a good estimate?
We will use a method called Temporal Difference (TD) learning to learn a good Q function. The idea is to only look at a limited number of steps in the future. TD(1) for example, only uses the next 2 states to evaluate the reward.
Surprisingly, we can use TD(0), which looks at the current state, and our estimate of the reward the next turn, and get great results. The structure of the network is the same, but we need to go through one forward step before receiving the error. We then use this error to back propagate gradients, like in traditional Deep Learning, and update our value estimates.
3+/Introducing Monte Carlo
Another method to estimate the eventual success of our actions is Monte Carlo Estimates. This consists of playing out the entire episode with our current policy until we reach an end (success by reaching a green block or failure by reaching a red block in the image above) and use that result to update our value estimates for each traversed state. This allows us to propagate values efficiently in one batch at the end of an episode, instead of every time we make a move. The cost is that we are introducing noise to our estimates, since we attribute very distant rewards to them.
4/The world is rarely discrete
The previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value. In the maze for example, there were 49 states (squares) and 4 actions (move in each adjacent direction). In this environment, we are trying to learn how to balance a ball on a 2 dimensional paddle, by deciding at each time step whether we want to tilt the paddle left or right. Here, the state space becomes continuous (the angle of the paddle, and the position of the ball). The good news is, we can still use Neural Networks to approximate this function!
A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). This constrains our learning process, as we have to have an exploration strategy that is built in to the policy itself, but allows us to tie results directly to our reasoning, and enables us to learn more efficiently.
The approach we will use here is called Policy Gradients, and is an on-policy method. Previously, we were first learning a value function Q for each action in each state and then building a policy on top. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. Since we are learning on policy, we cannot use methods such as epsilon greedy (which includes random choices), to get our agent to explore the environment. The way that we encourage exploration is by using a method called entropy regularization, which pushes our probability estimates to be wider, and thus will encourage us to make riskier choices to explore the space.
4+/Leveraging deep learning for representations
In practice, many state of the art RL methods require learning both a policy and value estimates. The way we do this with deep learning is by having both be two separate outputs of the same backbone neural network, which will make it easier for our neural network to learn good representations.
One method to do this is Advantage Actor Critic (A2C). We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. Instead of updating our value function based on rewards, we update it based on our advantage, which measures how much better or worse an action was than our previous value function estimated it to be. This helps make learning more stable compared to simple Q Learning and Vanilla Policy Gradients.
5/Learning directly from the screen
There is an additional advantage to using Deep Learning for these methods, which is that Deep Neural Networks excel at perceptive tasks. When a human plays a game, the information received is not a list of states, but an image (usually of a screen, or a board, or the surrounding environment).
Image-based Learning combines a Convolutional Neural Network (CNN) with RL. In this environment, we pass in a raw image instead of features, and add a 2 layer CNN to our architecture without changing anything else! We can even inspect activations to see what the network picks up on to determine value, and policy. In the example below, we can see that the network uses the current score and distant obstacles to estimate the value of the current state, while focusing on nearby obstacles for determining actions. Neat!
As a side note, while toying around with the provided implementation, I’ve found that visual learning is very sensitive to hyperparameters. Changing the discount rate slightly for example, completely prevented the neural network from learning even on a toy application. This is a widely known problem, but it is interesting to see it first hand.
6/Nuanced actions
So far, we’ve played with environments with continuous and discrete state spaces. However, every environment we studied had a discrete action space: we could move in one of four directions, or tilt the paddle to the left or right. Ideally, for applications such as self-driving cars, we would like to learn continuous actions, such as turning the steering wheel between 0 and 360 degrees. In this environment called 3D ball world, we can choose to tilt the paddle to any value on each of its axes. This gives us more control as to how we perform actions, but makes the action space much larger.
We can approach this by approximating our potential choices with Gaussian distributions. We learn a probability distribution over potential actions by learning the mean and variance of a Gaussian distribution, and our policy we sample from that distribution. Simple, in theory :).
7/Next steps for the brave
There are a few concepts that separate the algorithms described above from state of the art approaches. It’s interesting to see that conceptually, the best robotics and game-playing algorithms are not that far away from the ones we just explored:
That’s it for this overview, I hope this has been informative and fun! If you are looking to dive deeper into the theory of RL, give Arthur’s posts a read, or diving deeper by following David Silver’s UCL course. If you are looking to learn more about the projects we do at Insight, or how we work with companies, please check us out below, or reach out to me here.
Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Lead at Insight AI @EmmanuelAmeisen
Insight Fellows Program - Your bridge to a career in data
",want learn applied artificial intelligence leading practitioners silicon valley new york toronto learn insight artificial intelligence fellows program company working ai would like get involved insight ai fellows program feel free get touch recently gave talk oreilly ai conference beijing interesting lessons weve learned world nlp lucky enough attend tutorial deep reinforcement learning deep rl scratch unity technologies thought session led arthur juliani extremely informative wanted share big takeaways conversations companies weve seen rise interesting deep rl applications tools results parallel inner workings applications deep rl alphago pictured often seem esoteric hard understand post give overview core aspects field understood anyone many visuals slides talk new explanations opinions mine anything unclear reach deep rl field seen vast amounts research interest including learning play atari games beating pro players dota 2 defeating go champions contrary many classical deep learning problems often focus perception image contain stop sign deep rl adds dimension actions influence environment goal get dialog systems example classical deep learning aims learn right response given query hand deep reinforcement learning focuses right sequences sentences lead positive outcome example happy customer makes deep rl particularly attractive tasks require planning adaptation manufacturing selfdriving however industry applications trailed behind rapidly advancing results coming research community major reason deep rl often requires agent experiment millions times learning anything useful best way rapidly using simulation environment tutorial using unity create environments train agents workshop led arthur juliani leon chen goal get every participants successfully train multiple deep rl algorithms 4 hours tall order comprehensive overview many main algorithms power deep rl today complete set tutorials arthur juliani wrote 8part series starting deep rl used best top human players go understand thats done first need understand simple concepts starting much easier problems 1it starts slot machines lets imagine faced 4 chests pick turn different average payout goal maximize total payout receive fixed number turns classic problem called multiarmed bandits start crux problem balance exploration helps us learn states good exploitation use know pick best slot machine utilize value function maps actions estimated reward called q function first well initialize q values equal values well update q value action picking chest based good payout choosing action allows us learn good value function approximate q function using neural network starting shallow one learns probability distribution using softmax 4 potential chests value function tells us good estimate action policy function determines actions end taking intuitively might want use policy picks action highest q value performs poorly practice q estimates wrong start gather enough experience trial error need add mechanism policy encourage exploration one way use epsilon greedy consists taking random action probability epsilon start epsilon close 1 always choosing random actions lower epsilon go along learn chests good eventually learn chests best practice might want take subtle approach either taking action think best random action popular method boltzmann exploration adjust probabilities based current estimate good chest adding randomness factor 2adding different states previous example world always state waiting pick 4 chests front us realword problems consist many different states add environment next background behind chests alternates 3 colors turn changing average values chests means need learn q function depends action chest pick state color background version problem called contextual multiarmed bandits surprisingly use approach thing need add extra dense layer neural network take input vector representing current state world 3learning consequences actions another key factor makes current problem simpler mosts environments maze depicted actions take impact state world move grid might receive reward might receive nothing next turn different state finally introduce need planning first define q function immediate reward current state plus discounted reward expecting taking future actions solution works q estimate states accurate learn good estimate use method called temporal difference td learning learn good q function idea look limited number steps future td1 example uses next 2 states evaluate reward surprisingly use td0 looks current state estimate reward next turn get great results structure network need go one forward step receiving error use error back propagate gradients like traditional deep learning update value estimates 3introducing monte carlo another method estimate eventual success actions monte carlo estimates consists playing entire episode current policy reach end success reaching green block failure reaching red block image use result update value estimates traversed state allows us propagate values efficiently one batch end episode instead every time make move cost introducing noise estimates since attribute distant rewards 4the world rarely discrete previous methods using neural networks approximate value estimates mapping discrete number states actions value maze example 49 states squares 4 actions move adjacent direction environment trying learn balance ball 2 dimensional paddle deciding time step whether want tilt paddle left right state space becomes continuous angle paddle position ball good news still use neural networks approximate function note offpolicy vs onpolicy learning methods used previously offpolicy methods meaning generate data strategyusing epsilon greedy example learn onpolicy methods learn actions taken following policy remember policy method use determine actions take constrains learning process exploration strategy built policy allows us tie results directly reasoning enables us learn efficiently approach use called policy gradients onpolicy method previously first learning value function q action state building policy top vanilla policy gradient still use monte carlo estimates learn policy directly loss function increases probability choosing rewarding actions since learning policy cannot use methods epsilon greedy includes random choices get agent explore environment way encourage exploration using method called entropy regularization pushes probability estimates wider thus encourage us make riskier choices explore space 4leveraging deep learning representations practice many state art rl methods require learning policy value estimates way deep learning two separate outputs backbone neural network make easier neural network learn good representations one method advantage actor critic a2c learn policy directly policy gradients defined learn value function using something called advantage instead updating value function based rewards update based advantage measures much better worse action previous value function estimated helps make learning stable compared simple q learning vanilla policy gradients 5learning directly screen additional advantage using deep learning methods deep neural networks excel perceptive tasks human plays game information received list states image usually screen board surrounding environment imagebased learning combines convolutional neural network cnn rl environment pass raw image instead features add 2 layer cnn architecture without changing anything else even inspect activations see network picks determine value policy example see network uses current score distant obstacles estimate value current state focusing nearby obstacles determining actions neat side note toying around provided implementation ive found visual learning sensitive hyperparameters changing discount rate slightly example completely prevented neural network learning even toy application widely known problem interesting see first hand 6nuanced actions far weve played environments continuous discrete state spaces however every environment studied discrete action space could move one four directions tilt paddle left right ideally applications selfdriving cars would like learn continuous actions turning steering wheel 0 360 degrees environment called 3d ball world choose tilt paddle value axes gives us control perform actions makes action space much larger approach approximating potential choices gaussian distributions learn probability distribution potential actions learning mean variance gaussian distribution policy sample distribution simple theory 7next steps brave concepts separate algorithms described state art approaches interesting see conceptually best robotics gameplaying algorithms far away ones explored thats overview hope informative fun looking dive deeper theory rl give arthurs posts read diving deeper following david silvers ucl course looking learn projects insight work companies please check us reach want learn applied artificial intelligence leading practitioners silicon valley new york toronto learn insight artificial intelligence fellows program company working ai would like get involved insight ai fellows program feel free get touch quick cheer standing ovation clap show much enjoyed story ai lead insight ai emmanuelameisen insight fellows program bridge career data,en,"['Artificial Intelligence', 'the Insight Artificial Intelligence Fellows Program', 'the Insight AI Fellows Program', 'NLP', 'Unity Technologies', 'Deep RL', 'AlphaGo', 'Atari', 'Deep Learning', 'Multi', 'Boltzmann Exploration', 'Contextual Multi', 'Vanilla Policy Gradient', 'Advantage Actor Critic', 'Deep Neural Networks', 'Learning', 'Convolutional Neural Network', 'CNN', 'RL', 'Next', 'UCL', 'Insight', 'Insight AI @EmmanuelAmeisen', 'Insight Fellows Program - Your']"
5,Irhum Shafkat,2000,Intuitively Understanding Convolutions for Deep Learning,"The advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task, often achievable in a single line of code.
However, understanding convolutions, especially for the first time can often feel a bit unnerving, with terms like kernels, filters, channels and so on all stacked onto each other. Yet, convolutions as a concept are fascinatingly powerful and highly extensible, and in this post, we’ll break down the mechanics of the convolution operation, step-by-step, relate it to the standard fully connected network, and explore just how they build up a strong visual hierarchy, making them powerful feature extractors for images.
The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.
The kernel repeats this process for every location it slides over, converting a 2D matrix of features into yet another 2D matrix of features. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer.
Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.
This is all in pretty stark contrast to a fully connected layer. In the above example, we have 5×5=25 input features, and 3×3=9 output features. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. Do take note of this, as it’ll be critical to our later discussion.
Before we move on, it’s definitely worth looking into two techniques that are commonplace in convolution layers: Padding and Strides.
Padding does something pretty clever to solve this: pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”). This way, the kernel when sliding can allow the original edge pixels to be at its center, while extending into the fake pixels beyond the edge, producing an output the same size as the input.
The idea of the stride is to skip some of the slide locations of the kernel. A stride of 1 means to pick slides a pixel apart, so basically every single slide, acting as a standard convolution. A stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2, a stride of 3 means skipping every 2 slides, downsizing roughly by factor 3, and so on.
More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, in favor of strided convolutions when needing to reduce their output sizes.
Of course, the diagrams above only deals with the case where the image has a single input channel. In practicality, most input images have 3 channels, and that number only increases the deeper you go into a network. It’s pretty easy to think of channels, in general, as being a “view” of the image as a whole, emphasising some aspects, de-emphasising others.
So this is where a key distinction between terms comes in handy: whereas in the 1 channel case, where the term filter and kernel are interchangeable, in the general case, they’re actually pretty different. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique.
Each filter in a convolution layer produces one and only one output channel, and they do it like so:
Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).
Each of the per-channel processed versions are then summed together to form one channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.
Finally, then there’s the bias term. The way the bias term works here is that each output filter has one bias term. The bias gets added to the output channel so far to produce the final output channel.
And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. They are then concatenated together to produce the overall output, with the number of output channels being the number of filters. A nonlinearity is then usually applied before passing this as input to another convolution layer, which then repeats this process.
Even with the mechanics of the convolution layer down, it can still be hard to relate it back to a standard feed-forward network, and it still doesn’t explain why convolutions scale to, and work so much better for image data.
Suppose we have a 4×4 input, and we want to transform it into a 2×2 grid. If we were using a feedforward network, we’d reshape the 4×4 input into a vector of length 16, and pass it through a densely connected layer with 16 inputs and 4 outputs. One could visualize the weight matrix W for a layer:
And although the convolution kernel operation may seem a bit strange at first, it is still a linear transformation with an equivalent transformation matrix. If we were to use a kernel K of size 3 on the reshaped 4×4 input to get a 2×2 output, the equivalent transformation matrix would be:
(Note: while the above matrix is an equivalent transformation matrix, the actual operation is usually implemented as a very different matrix multiplication[2])
The convolution then, as a whole, is still a linear transformation, but at the same time it’s also a dramatically different kind of transformation. For a matrix with 64 elements, there’s just 9 parameters which themselves are reused several times. Each output node only gets to see a select number of inputs (the ones inside the kernel). There is no interaction with any of the other inputs, as the weights to them are set to 0.
It’s useful to see the convolution operation as a hard prior on the weight matrix. In this context, by prior, I mean predefined network parameters. For example, when you use a pretrained model for image classification, you use the pretrained network parameters as your prior, as a feature extractor to your final densely connected layer.
In that sense, there’s a direct intuition between why both are so efficient (compared to their alternatives). Transfer learning is efficient by orders of magnitude compared to random initialization, because you only really need to optimize the parameters of the final fully connected layer, which means you can have fantastic performance with only a few dozen images per class.
Here, you don’t need to optimize all 64 parameters, because we set most of them to zero (and they’ll stay that way), and the rest we convert to shared parameters, resulting in only 9 actual parameters to optimize. This efficiency matters, because when you move from the 784 inputs of MNIST to real world 224×224×3 images, thats over 150,000 inputs. A dense layer attempting to halve the input to 75,000 inputs would still require over 10 billion parameters. For comparison, the entirety of ResNet-50 has some 25 million parameters.
So fixing some parameters to 0, and tying parameters increases efficiency, but unlike the transfer learning case, where we know the prior is good because it works on a large general set of images, how do we know this is any good?
The answer lies in the feature combinations the prior leads the parameters to learn.
Early on in this article, we discussed that:
So with backpropagation coming in all the way from the classification nodes of the network, the kernels have the interesting task of learning weights to produce features only from a set of local inputs. Additionally, because the kernel itself is applied across the entire image, the features the kernel learns must be general enough to come from any part of the image.
If this were any other kind of data, eg. categorical data of app installs, this would’ve been a disaster, for just because your number of app installs and app type columns are next to each other doesn’t mean they have any “local, shared features” common with app install dates and time used. Sure, the four may have an underlying higher level feature (eg. which apps people want most) that can be found, but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two. The four could’ve been in any (consistent) order and still be valid!
Pixels however, always appear in a consistent order, and nearby pixels influence a pixel e.g. if all nearby pixels are red, it’s pretty likely the pixel is also red. If there are deviations, that’s an interesting anomaly that could be converted into a feature, and all this can be detected from comparing a pixel with its neighbors, with other pixels in its locality.
And this idea is really what a lot of earlier computer vision feature extraction methods were based around. For instance, for edge detection, one can use a Sobel edge detection filter, a kernel with fixed parameters, operating just like the standard one-channel convolution:
For a non-edge containing grid (eg. the background sky), most of the pixels are the same value, so the overall output of the kernel at that point is 0. For a grid with an vertical edge, there is a difference between the pixels to the left and right of the edge, and the kernel computes that difference to be non-zero, activating and revealing the edges. The kernel only works only a 3×3 grids at a time, detecting anomalies on a local scale, yet when applied across the entire image, is enough to detect a certain feature on a global scale, anywhere in the image!
So the key difference we make with deep learning is ask this question: Can useful kernels be learnt? For early layers operating on raw pixels, we could reasonably expect feature detectors of fairly low level features, like edges, lines, etc.
There’s an entire branch of deep learning research focused on making neural network models interpretable. One of the most powerful tools to come out of that is Feature Visualization using optimization[3]. The idea at core is simple: optimize a image (usually initialized with random noise) to activate a filter as strongly as possible. This does make intuitive sense: if the optimized image is completely filled with edges, that’s strong evidence that’s what the filter itself is looking for and is activated by. Using this, we can peek into the learnt filters, and the results are stunning:
One important thing to notice here is that convolved images are still images. The output of a small grid of pixels from the top left of an image will still be on the top left. So you can run another convolution layer on top of another (such as the two on the left) to extract deeper features, which we visualize.
Yet, however deep our feature detectors get, without any further changes they’ll still be operating on very small patches of the image. No matter how deep your detectors are, you can’t detect faces from a 3×3 grid. And this is where the idea of the receptive field comes in.
A essential design choice of any CNN architecture is that the input sizes grow smaller and smaller from the start to the end of the network, while the number of channels grow deeper. This, as mentioned earlier, is often done through strides or pooling layers. Locality determines what inputs from the previous layer the outputs get to see. The receptive field determines what area of the original input to the entire network the output gets to see.
The idea of a strided convolution is that we only process slides a fixed distance apart, and skip the ones in the middle. From a different point of view, we only keep outputs a fixed distance apart, and remove the rest[1].
We then apply a nonlinearity to the output, and per usual, then stack another new convolution layer on top. And this is where things get interesting. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:
This is because the output of the strided layer still does represent the same image. It is not so much cropping as it is resizing, only thing is that each single pixel in the output is a “representative” of a larger area (of whose other pixels were discarded) from the same rough location from the original input. So when the next layer’s kernel operates on the output, it’s operating on pixels collected from a larger area.
(Note: if you’re familiar with dilated convolutions, note that the above is not a dilated convolution. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)
This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer.
Followed by a pooling/strided layer, the network continues to create detectors for even higher level features (parts, patterns), as we see for mixed4a.
The repeated reduction in image size across the network results in, by the 5th block on convolutions, input sizes of just 7×7, compared to inputs of 224×224. At this point, each single pixel represents a grid of 32×32 pixels, which is huge.
Compared to earlier layers, where an activation meant detecting an edge, here, an activation on the tiny 7×7 grid is one for a very high level feature, such as for birds.
The network as a whole progresses from a small number of filters (64 in case of GoogLeNet), detecting low level features, to a very large number of filters(1024 in the final convolution), each looking for an extremely specific high level feature. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image.
Compared to what a standard feedforward network would have done, the output here is really nothing short of awe-inspiring. A standard feedforward network would have produced abstract feature vectors, from combinations of every single pixel in the image, requiring intractable amounts of data to train.
The CNN, with the priors imposed on it, starts by learning very low level feature detectors, and as across the layers as its receptive field is expanded, learns to combine those low-level features into progressively higher level features; not an abstract combination of every single pixel, but rather, a strong visual hierarchy of concepts.
By detecting low level features, and using them to detect higher level features as it progresses up its visual hierarchy, it is eventually able to detect entire visual concepts such as faces, birds, trees, etc, and that’s what makes them such powerful, yet efficient with image data.
With the visual hierarchy CNNs build, it is pretty reasonable to assume that their vision systems are similar to humans. And they’re really great with real world images, but they also fail in ways that strongly suggest their vision systems aren’t entirely human-like. The most major problem: Adversarial Examples[4], examples which have been specifically modified to fool the model.
Adversarial examples would be a non-issue if the only tampered ones that caused the models to fail were ones that even humans would notice. The problem is, the models are susceptible to attacks by samples which have only been tampered with ever so slightly, and would clearly not fool any human. This opens the door for models to silently fail, which can be pretty dangerous for a wide range of applications from self-driving cars to healthcare.
Robustness against adversarial attacks is currently a highly active area of research, the subject of many papers and even competitions, and solutions will certainly improve CNN architectures to become safer and more reliable.
CNNs were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services, ranging from face detection in your photo gallery to making better medical diagnoses. They might be the key method in computer vision going forward, or some other new breakthrough might just be around the corner. Regardless, one thing is for sure: they’re nothing short of amazing, at the heart of many present-day innovative applications, and are most certainly worth deeply understanding.
Hope you enjoyed this article! If you’d like to stay connected, you’ll find me on Twitter here. If you have a question, comments are welcome! — I find them to be useful to my own learning process as well.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Curious programmer, tinkers around in Python and deep learning.
Sharing concepts, ideas, and codes.
",advent powerful versatile deep learning frameworks recent years made possible implement convolution layers deep learning model extremely simple task often achievable single line code however understanding convolutions especially first time often feel bit unnerving terms like kernels filters channels stacked onto yet convolutions concept fascinatingly powerful highly extensible post well break mechanics convolution operation stepbystep relate standard fully connected network explore build strong visual hierarchy making powerful feature extractors images 2d convolution fairly simple operation heart start kernel simply small matrix weights kernel slides 2d input data performing elementwise multiplication part input currently summing results single output pixel kernel repeats process every location slides converting 2d matrix features yet another 2d matrix features output features essentially weighted sums weights values kernel input features located roughly location output pixel input layer whether input feature falls within roughly location gets determined directly whether area kernel produced output means size kernel directly determines many input features get combined production new output feature pretty stark contrast fully connected layer example 5525 input features 339 output features standard fully connected layer youd weight matrix 259 225 parameters every output feature weighted sum every single input feature convolutions allow us transformation 9 parameters output feature instead looking every input feature getting look input features coming roughly location take note itll critical later discussion move definitely worth looking two techniques commonplace convolution layers padding strides padding something pretty clever solve pad edges extra fake pixels usually value 0 hence oftused term zero padding way kernel sliding allow original edge pixels center extending fake pixels beyond edge producing output size input idea stride skip slide locations kernel stride 1 means pick slides pixel apart basically every single slide acting standard convolution stride 2 means picking slides 2 pixels apart skipping every slide process downsizing roughly factor 2 stride 3 means skipping every 2 slides downsizing roughly factor 3 modern networks resnet architectures entirely forgo pooling layers internal layers favor strided convolutions needing reduce output sizes course diagrams deals case image single input channel practicality input images 3 channels number increases deeper go network pretty easy think channels general view image whole emphasising aspects deemphasising others key distinction terms comes handy whereas 1 channel case term filter kernel interchangeable general case theyre actually pretty different filter actually happens collection kernels one kernel every single input channel layer kernel unique filter convolution layer produces one one output channel like kernels filter slides respective input channels producing processed version kernels may stronger weights others give emphasis certain input channels others eg filter may red kernel channel stronger weights others hence respond differences red channel features others perchannel processed versions summed together form one channel kernels filter produce one version channel filter whole produces one overall output channel finally theres bias term way bias term works output filter one bias term bias gets added output channel far produce final output channel single filter case case number filters identical filter processes input different set kernels scalar bias process described producing single output channel concatenated together produce overall output number output channels number filters nonlinearity usually applied passing input another convolution layer repeats process even mechanics convolution layer still hard relate back standard feedforward network still doesnt explain convolutions scale work much better image data suppose 44 input want transform 22 grid using feedforward network wed reshape 44 input vector length 16 pass densely connected layer 16 inputs 4 outputs one could visualize weight matrix w layer although convolution kernel operation may seem bit strange first still linear transformation equivalent transformation matrix use kernel k size 3 reshaped 44 input get 22 output equivalent transformation matrix would note matrix equivalent transformation matrix actual operation usually implemented different matrix multiplication2 convolution whole still linear transformation time also dramatically different kind transformation matrix 64 elements theres 9 parameters reused several times output node gets see select number inputs ones inside kernel interaction inputs weights set 0 useful see convolution operation hard prior weight matrix context prior mean predefined network parameters example use pretrained model image classification use pretrained network parameters prior feature extractor final densely connected layer sense theres direct intuition efficient compared alternatives transfer learning efficient orders magnitude compared random initialization really need optimize parameters final fully connected layer means fantastic performance dozen images per class dont need optimize 64 parameters set zero theyll stay way rest convert shared parameters resulting 9 actual parameters optimize efficiency matters move 784 inputs mnist real world 2242243 images thats 150000 inputs dense layer attempting halve input 75000 inputs would still require 10 billion parameters comparison entirety resnet50 25 million parameters fixing parameters 0 tying parameters increases efficiency unlike transfer learning case know prior good works large general set images know good answer lies feature combinations prior leads parameters learn early article discussed backpropagation coming way classification nodes network kernels interesting task learning weights produce features set local inputs additionally kernel applied across entire image features kernel learns must general enough come part image kind data eg categorical data app installs wouldve disaster number app installs app type columns next doesnt mean local shared features common app install dates time used sure four may underlying higher level feature eg apps people want found gives us reason believe parameters first two exactly parameters latter two four couldve consistent order still valid pixels however always appear consistent order nearby pixels influence pixel eg nearby pixels red pretty likely pixel also red deviations thats interesting anomaly could converted feature detected comparing pixel neighbors pixels locality idea really lot earlier computer vision feature extraction methods based around instance edge detection one use sobel edge detection filter kernel fixed parameters operating like standard onechannel convolution nonedge containing grid eg background sky pixels value overall output kernel point 0 grid vertical edge difference pixels left right edge kernel computes difference nonzero activating revealing edges kernel works 33 grids time detecting anomalies local scale yet applied across entire image enough detect certain feature global scale anywhere image key difference make deep learning ask question useful kernels learnt early layers operating raw pixels could reasonably expect feature detectors fairly low level features like edges lines etc theres entire branch deep learning research focused making neural network models interpretable one powerful tools come feature visualization using optimization3 idea core simple optimize image usually initialized random noise activate filter strongly possible make intuitive sense optimized image completely filled edges thats strong evidence thats filter looking activated using peek learnt filters results stunning one important thing notice convolved images still images output small grid pixels top left image still top left run another convolution layer top another two left extract deeper features visualize yet however deep feature detectors get without changes theyll still operating small patches image matter deep detectors cant detect faces 33 grid idea receptive field comes essential design choice cnn architecture input sizes grow smaller smaller start end network number channels grow deeper mentioned earlier often done strides pooling layers locality determines inputs previous layer outputs get see receptive field determines area original input entire network output gets see idea strided convolution process slides fixed distance apart skip ones middle different point view keep outputs fixed distance apart remove rest1 apply nonlinearity output per usual stack another new convolution layer top things get interesting even apply kernel size 33 local area output strided convolution kernel would larger effective receptive field output strided layer still represent image much cropping resizing thing single pixel output representative larger area whose pixels discarded rough location original input next layers kernel operates output operating pixels collected larger area note youre familiar dilated convolutions note dilated convolution methods increasing receptive field dilated convolutions single layer takes place regular convolution following strided convolution nonlinearity inbetween expansion receptive field allows convolution layers combine low level features lines edges higher level features curves textures see mixed3a layer followed poolingstrided layer network continues create detectors even higher level features parts patterns see mixed4a repeated reduction image size across network results 5th block convolutions input sizes 77 compared inputs 224224 point single pixel represents grid 3232 pixels huge compared earlier layers activation meant detecting edge activation tiny 77 grid one high level feature birds network whole progresses small number filters 64 case googlenet detecting low level features large number filters1024 final convolution looking extremely specific high level feature followed final pooling layer collapses 77 grid single pixel channel feature detector receptive field equivalent entire image compared standard feedforward network would done output really nothing short aweinspiring standard feedforward network would produced abstract feature vectors combinations every single pixel image requiring intractable amounts data train cnn priors imposed starts learning low level feature detectors across layers receptive field expanded learns combine lowlevel features progressively higher level features abstract combination every single pixel rather strong visual hierarchy concepts detecting low level features using detect higher level features progresses visual hierarchy eventually able detect entire visual concepts faces birds trees etc thats makes powerful yet efficient image data visual hierarchy cnns build pretty reasonable assume vision systems similar humans theyre really great real world images also fail ways strongly suggest vision systems arent entirely humanlike major problem adversarial examples4 examples specifically modified fool model adversarial examples would nonissue tampered ones caused models fail ones even humans would notice problem models susceptible attacks samples tampered ever slightly would clearly fool human opens door models silently fail pretty dangerous wide range applications selfdriving cars healthcare robustness adversarial attacks currently highly active area research subject many papers even competitions solutions certainly improve cnn architectures become safer reliable cnns models allowed computer vision scale simple applications powering sophisticated products services ranging face detection photo gallery making better medical diagnoses might key method computer vision going forward new breakthrough might around corner regardless one thing sure theyre nothing short amazing heart many presentday innovative applications certainly worth deeply understanding hope enjoyed article youd like stay connected youll find twitter question comments welcome find useful learning process well quick cheer standing ovation clap show much enjoyed story curious programmer tinkers around python deep learning sharing concepts ideas codes,en,"['Padding and Strides', 'app', 'CNN']"
6,Sam Drozdov,2300,An intro to Machine Learning for designers – UX Collective,"There is an ongoing debate about whether or not designers should write code. Wherever you fall on this issue, most people would agree that designers should know about code. This helps designers understand constraints and empathize with developers. It also allows designers to think outside of the pixel perfect box when problem solving. For the same reasons, designers should know about machine learning.
Put simply, machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Even though Arthur Samuel coined the term over fifty years ago, only recently have we seen the most exciting applications of machine learning — digital assistants, autonomous driving, and spam-free email all exist thanks to machine learning.
Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Only in the past few years companies like Google, Amazon, and Apple have made some of their powerful machine learning tools available to developers. Now is the best time to learn about machine learning and apply it to the products you are building.
Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Designers should be able to talk with software developers about what is possible, how to prepare, and what outcomes to expect. Below are a few example applications that should serve as inspiration for these conversations.
Machine learning can help create user-centric products by personalizing experiences to the individuals who use them. This allows us to improve things like recommendations, search results, notifications, and ads.
Machine learning is effective at finding abnormal content. Credit card companies use this to detect fraud, email providers use this to detect spam, and social media companies use this to detect things like hate speech.
Machine learning has enabled computers to begin to understand the things we say (natural-language processing) and the things we see (computer vision). This allows Siri to understand “Siri, set a reminder...”, Google Photos to create albums of your dog, and Facebook to describe a photo to those visually impaired.
Machine learning is also helpful in understanding how users are grouped. This insight can then be used to look at analytics on a group-by-group basis. From here, different features can be evaluated across groups or be rolled out to only a particular group of users.
Machine learning allows us to make predictions about how a user might behave next. Knowing this, we can help prepare for a user’s next action. For example, if we can predict what content a user is planning on viewing, we can preload that content so it’s immediately ready when they want it.
Depending on the application and what data is available, there are different types of machine learning algorithms to choose from. I’ll briefly cover each of the following.
Supervised learning allows us to make predictions using correctly labeled data. Labeled data is a group of examples that has informative tags or outputs. For example, photos with associated hashtags or a house’s features (eq. number of bedrooms, location) and its price.
By using supervised learning we can fit a line to the labelled data that either splits the data into categories or represents the trend of the data. Using this line we are able to make predictions on new data. For example, we can look at new photos and predict hashtags or look at a new house’s features and predict its price.
If the output we are trying to predict is a list of tags or values we call it classification. If the output we are trying to predict is a number we call it regression.
Unsupervised learning is helpful when we have unlabeled data or we are not exactly sure what outputs (like an image’s hashtags or a house’s price) are meaningful. Instead we can identify patterns among unlabeled data. For example, we can identify related items on an e-commerce website or recommend items to someone based on others who made similar purchases.
If the pattern is a group we call it a cluster. If the pattern is a rule (e.q. if this, then that) we call it an association.
Reinforcement learning doesn’t use an existing data set. Instead we create an agent to collect its own data through trial-and-error in an environment where it is reinforced with a reward. For example, an agent can learn to play Mario by receiving a positive reward for collecting coins and a negative reward for walking into a Goomba.
Reinforcement learning is inspired by the way that humans learn and has turned out to be an effective way to teach computers. Specifically, reinforcement has been effective at training computers to play games like Go and Dota.
Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. identifying objects in an image with supervised learning requires a labeled data set of images). However, constraints are the fruit of creativity. In some cases, you can set out to collect data that is not already available or consider other approaches.
Even though machine learning is a science, it comes with a margin of error. It is important to consider how a user’s experience might be impacted by this margin of error. For example, when an autonomous car fails to recognize its surroundings people can get hurt.
Even though machine learning has never been as accessible as it is today, it still requires additional resources (developers and time) to be integrated into a product. This makes it important to think about whether the resulting impact justifies the amount of resources needed to implement.
We have barely covered the tip of the iceberg, but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product. If you are interested in learning more about machine learning, here are some helpful resources:
Thanks for reading. Chat with me on Twitter @samueldrozdov
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Digital Product Designer samueldrozdov.com
Curated stories on user experience, usability, and product design. By @fabriciot and @caioab.
",ongoing debate whether designers write code wherever fall issue people would agree designers know code helps designers understand constraints empathize developers also allows designers think outside pixel perfect box problem solving reasons designers know machine learning put simply machine learning field study gives computers ability learn without explicitly programmed arthur samuel 1959 even though arthur samuel coined term fifty years ago recently seen exciting applications machine learning digital assistants autonomous driving spamfree email exist thanks machine learning past decade new algorithms better hardware data made machine learning order magnitude effective past years companies like google amazon apple made powerful machine learning tools available developers best time learn machine learning apply products building since machine learning accessible ever designers today opportunity think machine learning applied improve products designers able talk software developers possible prepare outcomes expect example applications serve inspiration conversations machine learning help create usercentric products personalizing experiences individuals use allows us improve things like recommendations search results notifications ads machine learning effective finding abnormal content credit card companies use detect fraud email providers use detect spam social media companies use detect things like hate speech machine learning enabled computers begin understand things say naturallanguage processing things see computer vision allows siri understand siri set reminder google photos create albums dog facebook describe photo visually impaired machine learning also helpful understanding users grouped insight used look analytics groupbygroup basis different features evaluated across groups rolled particular group users machine learning allows us make predictions user might behave next knowing help prepare users next action example predict content user planning viewing preload content immediately ready want depending application data available different types machine learning algorithms choose ill briefly cover following supervised learning allows us make predictions using correctly labeled data labeled data group examples informative tags outputs example photos associated hashtags houses features eq number bedrooms location price using supervised learning fit line labelled data either splits data categories represents trend data using line able make predictions new data example look new photos predict hashtags look new houses features predict price output trying predict list tags values call classification output trying predict number call regression unsupervised learning helpful unlabeled data exactly sure outputs like images hashtags houses price meaningful instead identify patterns among unlabeled data example identify related items ecommerce website recommend items someone based others made similar purchases pattern group call cluster pattern rule eq call association reinforcement learning doesnt use existing data set instead create agent collect data trialanderror environment reinforced reward example agent learn play mario receiving positive reward collecting coins negative reward walking goomba reinforcement learning inspired way humans learn turned effective way teach computers specifically reinforcement effective training computers play games like go dota understanding problem trying solve available data constrain types machine learning use eq identifying objects image supervised learning requires labeled data set images however constraints fruit creativity cases set collect data already available consider approaches even though machine learning science comes margin error important consider users experience might impacted margin error example autonomous car fails recognize surroundings people get hurt even though machine learning never accessible today still requires additional resources developers time integrated product makes important think whether resulting impact justifies amount resources needed implement barely covered tip iceberg hopefully point feel comfortable thinking machine learning applied product interested learning machine learning helpful resources thanks reading chat twitter samueldrozdov quick cheer standing ovation clap show much enjoyed story digital product designer samueldrozdovcom curated stories user experience usability product design fabriciot caioab,en,"['Google', 'Amazon', 'Apple', 'Twitter @samueldrozdov', 'Digital Product Designer', '@fabriciot', '@caioab']"
7,Conor Dewey,252,The Big List of DS/ML Interview Resources – Towards Data Science,"Data science interviews certainly aren’t easy. I know this first hand. I’ve participated in over 50 individual interviews and phone screens while applying for competitive internships over the last calendar year. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews.
Long story short, I’ve decided to sort through all my bookmarks and notes in order to deliver a comprehensive list of data science resources.
With this list by your side, you should have more than enough effective tools at your disposal next time you’re prepping for a big interview.
It’s worth noting that many of these resources are naturally going to geared towards entry-level and intern data science positions, as that’s where my expertise lies. Keep that in mind and enjoy!
Here’s some of the more general resources covering data science as a whole. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. While the ebook itself is a couple bucks out of pocket, the answers themselves are free on Quora. These were some of my favorite full-coverage questions to practice with right before an interview.
Even Data Scientists cannot escape the dreaded algorithmic coding interview. In my experience, this isn’t the case 100% of the time, but chances are you’ll be asked to work through something similar to an easy or medium question on LeetCode or HackerRank.
As far as language goes, most companies will let you use whatever language you want. Personally, I did almost all of my algorithmic coding in Java even though the positions were targeted at Python and R programmers. If I had to recommend one thing, it’s to break out your wallet and invest in Cracking the Coding Interview. It absolutely lives up to the hype. I plan to continue using it for years to come.
Once the interviewer knows that you can think-through problems and code effectively, chances are that you’ll move onto some more data science specific applications. Depending on the interviewer and the position, you will likely be able to choose between Python and R as your tool of choice. Since I’m partial to Python, my resources below will primarily focus on effectively using Pandas and NumPy for data analysis.
A data science interview typically isn’t complete without checking your knowledge of SQL. This can be done over the phone or through a live coding question, more likely the latter. I’ve found that the difficulty level of these questions can vary a good bit, ranging from being painfully easy to requiring complex joins and obscure functions.
Our good friend, statistics is still crucial for Data Scientists and it’s reflected as such in interviews. I had many interviews begin by seeing if I can explain a common statistics or probability concept in simple and concise terms. As positions get more experienced, I suspect this happens less and less as traditional statistical questions begin to take the more practical form of A/B testing scenarios, covered later in the post.
You’ll notice that I’ve compiled a few more resources here than in other sections. This isn’t a mistake. Machine learning is a complex field that is a virtual guarantee in data science interviews today.
The way that you’ll be tested on this is no guarantee however. It may come up as a conceptual question regarding cross validation or bias-variance tradeoff, or it may take the form of a take home assignment with a dataset attached. I’ve seen both several times, so you’ve got to be prepared for anything.
Specifically, check out the Machine Learning Flashcards below, they’re only a couple bucks and were my by far my favorite way to quiz myself on any conceptual ML stuff.
This won’t be covered in every single data science interview, but it’s certainly not uncommon. Most interviews will have atleast one section solely dedicated to product thinking which often lends itself to A/B testing of some sort. Make sure your familiar with the concepts and statistical background necessary in order to be prepared when it comes up. If you have time to spare, I took the free online course by Udacity and overall, I was pretty impressed.
Lastly, I wanted to call out all of the posts related to data science jobs and interviewing that I read over and over again to understand, not only how to prepare, but what to expect as well. If you only check out one section here, this is the one to focus on. This is the layer that sits on top of all the technical skills and application. Don’t overlook it.
I hope you find these resources useful during your next interview or job search. I know I did, truthfully I’m just glad that I saved these links somewhere. Lastly, this post is part of an ongoing initiative to ‘open-source’ my experience applying and interviewing at data science positions, so if you enjoyed this content then be sure to follow me for more stuff like this.
If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!
If you enjoyed this post, feel free to hit the clap button and if you’re interested in posts to come, make sure to follow me on Medium at the link below — I’ll be writing and shipping every day this month as part of a 30-Day Challenge.
This article was originally published on conordewey.com
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist & Writer | www.conordewey.com
Sharing concepts, ideas, and codes.
",data science interviews certainly arent easy know first hand ive participated 50 individual interviews phone screens applying competitive internships last calendar year exciting somewhat times painful process ive accumulated plethora useful resources helped prepare eventually pass data science interviews long story short ive decided sort bookmarks notes order deliver comprehensive list data science resources list side enough effective tools disposal next time youre prepping big interview worth noting many resources naturally going geared towards entrylevel intern data science positions thats expertise lies keep mind enjoy heres general resources covering data science whole specifically highly recommend checking first two links regarding 120 data science interview questions ebook couple bucks pocket answers free quora favorite fullcoverage questions practice right interview even data scientists cannot escape dreaded algorithmic coding interview experience isnt case 100 time chances youll asked work something similar easy medium question leetcode hackerrank far language goes companies let use whatever language want personally almost algorithmic coding java even though positions targeted python r programmers recommend one thing break wallet invest cracking coding interview absolutely lives hype plan continue using years come interviewer knows thinkthrough problems code effectively chances youll move onto data science specific applications depending interviewer position likely able choose python r tool choice since im partial python resources primarily focus effectively using pandas numpy data analysis data science interview typically isnt complete without checking knowledge sql done phone live coding question likely latter ive found difficulty level questions vary good bit ranging painfully easy requiring complex joins obscure functions good friend statistics still crucial data scientists reflected interviews many interviews begin seeing explain common statistics probability concept simple concise terms positions get experienced suspect happens less less traditional statistical questions begin take practical form ab testing scenarios covered later post youll notice ive compiled resources sections isnt mistake machine learning complex field virtual guarantee data science interviews today way youll tested guarantee however may come conceptual question regarding cross validation biasvariance tradeoff may take form take home assignment dataset attached ive seen several times youve got prepared anything specifically check machine learning flashcards theyre couple bucks far favorite way quiz conceptual ml stuff wont covered every single data science interview certainly uncommon interviews atleast one section solely dedicated product thinking often lends ab testing sort make sure familiar concepts statistical background necessary order prepared comes time spare took free online course udacity overall pretty impressed lastly wanted call posts related data science jobs interviewing read understand prepare expect well check one section one focus layer sits top technical skills application dont overlook hope find resources useful next interview job search know truthfully im glad saved links somewhere lastly post part ongoing initiative opensource experience applying interviewing data science positions enjoyed content sure follow stuff like youre interested receiving weekly rundown interesting articles resources focused data science machine learning artificial intelligence subscribe self driven data science using form enjoyed post feel free hit clap button youre interested posts come make sure follow medium link ill writing shipping every day month part 30day challenge article originally published conordeweycom quick cheer standing ovation clap show much enjoyed story data scientist writer wwwconordeweycom sharing concepts ideas codes,en,"['Data Science Interview Questions', 'LeetCode', 'HackerRank', 'Java', 'Cracking the Coding Interview', 'Pandas', 'SQL', 'Data Scientists', 'Udacity', 'Challenge', 'Data Scientist & Writer |']"
8,Abhishek Parbhakar,937,Must know Information Theory concepts in Deep Learning (AI),"Information theory is an important field that has made significant contribution to deep learning and AI, and yet is unknown to many. Information theory can be seen as a sophisticated amalgamation of basic building blocks of deep learning: calculus, probability and statistics. Some examples of concepts in AI that come from Information theory or related fields:
In the early 20th century, scientists and engineers were struggling with the question: “How to quantify the information? Is there a analytical way or a mathematical measure that can tell us about the information content?”. For example, consider below two sentences:
It is not difficult to tell that the second sentence gives us more information since it also tells that Bruno is “big” and “brown” in addition to being a “dog”. How can we quantify the difference between two sentences? Can we have a mathematical measure that tells us how much more information second sentence have as compared to the first?
Scientists were struggling with these questions. Semantics, domain and form of data only added to the complexity of the problem. Then, mathematician and engineer Claude Shannon came up with the idea of “Entropy” that changed our world forever and marked the beginning of “Digital Information Age”.
Shannon proposed that the “semantic aspects of data are irrelevant”, and nature and meaning of data doesn’t matter when it comes to information content. Instead he quantified information in terms of probability distribution and “uncertainty”. Shannon also introduced the term “bit”, that he humbly credited to his colleague John Tukey. This revolutionary idea not only laid the foundation of Information Theory but also opened new avenues for progress in fields like artificial intelligence.
Below we discuss four popular, widely used and must known Information theoretic concepts in deep learning and data sciences:
Also called Information Entropy or Shannon Entropy.
Entropy gives a measure of uncertainty in an experiment. Let’s consider two experiments:
If we compare the two experiments, in exp 2 it is easier to predict the outcome as compared to exp 1. So, we can say that exp 1 is inherently more uncertain/unpredictable than exp 2. This uncertainty in the experiment is measured using entropy.
Therefore, if there is more inherent uncertainty in the experiment then it has higher entropy. Or lesser the experiment is predictable more is the entropy. The probability distribution of experiment is used to calculate the entropy.
A deterministic experiment, which is completely predictable, say tossing a coin with P(H)=1, has entropy zero. An experiment which is completely random, say rolling fair dice, is least predictable, has maximum uncertainty, and has the highest entropy among such experiments.
Another way to look at entropy is the average information gained when we observe outcomes of an random experiment. The information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome. More the rarer is the outcome, more is the information gained from observing it.
For example, in an deterministic experiment, we always know the outcome, so no new information gained is here from observing the outcome and hence entropy is zero.
For a discrete random variable X, with possible outcomes (states) x_1,...,x_n the entropy, in unit of bits, is defined as:
where p(x_i) is the probability of i^th outcome of X.
Cross entropy is used to compare two probability distributions. It tells us how similar two distributions are.
Cross entropy between two probability distributions p and q defined over same set of outcomes is given by:
Mutual information is a measure of mutual dependency between two probability distributions or random variables. It tells us how much information about one variable is carried by the another variable.
Mutual information captures dependency between random variables and is more generalized than vanilla correlation coefficient, which captures only the linear relationship.
Mutual information of two discrete random variables X and Y is defined as:
where p(x,y) is the joint probability distribution of X and Y, and p(x) and p(y) are the marginal probability distribution of X and Y respectively.
Also called Relative Entropy.
KL divergence is another measure to find similarities between two probability distributions. It measures how much one distribution diverges from the other.
Suppose, we have some data and true distribution underlying it is ‘P’. But we don’t know this ‘P’, so we choose a new distribution ‘Q’ to approximate this data. Since ‘Q’ is just an approximation, it won’t be able to approximate the data as good as ‘P’ and some information loss will occur. This information loss is given by KL divergence.
KL divergence between ‘P’ and ‘Q’ tells us how much information we lose when we try to approximate data given by ‘P’ with ‘Q’.
KL divergence of a probability distribution Q from another probability distribution P is defined as:
KL divergence is commonly used in unsupervised machine learning technique Variational Autoencoders.
Information Theory was originally formulated by mathematician and electrical engineer Claude Shannon in his seminal paper “A Mathematical Theory of Communication” in 1948.
Note: Terms experiments, random variable & AI, machine learning, deep learning, data science have been used loosely above but have technically different meanings.
In case you liked the article, do follow me Abhishek Parbhakar for more articles related to AI, philosophy and economics.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Finding equilibria among AI, philosophy, and economics.
Sharing concepts, ideas, and codes.
",information theory important field made significant contribution deep learning ai yet unknown many information theory seen sophisticated amalgamation basic building blocks deep learning calculus probability statistics examples concepts ai come information theory related fields early 20th century scientists engineers struggling question quantify information analytical way mathematical measure tell us information content example consider two sentences difficult tell second sentence gives us information since also tells bruno big brown addition dog quantify difference two sentences mathematical measure tells us much information second sentence compared first scientists struggling questions semantics domain form data added complexity problem mathematician engineer claude shannon came idea entropy changed world forever marked beginning digital information age shannon proposed semantic aspects data irrelevant nature meaning data doesnt matter comes information content instead quantified information terms probability distribution uncertainty shannon also introduced term bit humbly credited colleague john tukey revolutionary idea laid foundation information theory also opened new avenues progress fields like artificial intelligence discuss four popular widely used must known information theoretic concepts deep learning data sciences also called information entropy shannon entropy entropy gives measure uncertainty experiment lets consider two experiments compare two experiments exp 2 easier predict outcome compared exp 1 say exp 1 inherently uncertainunpredictable exp 2 uncertainty experiment measured using entropy therefore inherent uncertainty experiment higher entropy lesser experiment predictable entropy probability distribution experiment used calculate entropy deterministic experiment completely predictable say tossing coin ph1 entropy zero experiment completely random say rolling fair dice least predictable maximum uncertainty highest entropy among experiments another way look entropy average information gained observe outcomes random experiment information gained outcome experiment defined function probability occurrence outcome rarer outcome information gained observing example deterministic experiment always know outcome new information gained observing outcome hence entropy zero discrete random variable x possible outcomes states x_1x_n entropy unit bits defined px_i probability ith outcome x cross entropy used compare two probability distributions tells us similar two distributions cross entropy two probability distributions p q defined set outcomes given mutual information measure mutual dependency two probability distributions random variables tells us much information one variable carried another variable mutual information captures dependency random variables generalized vanilla correlation coefficient captures linear relationship mutual information two discrete random variables x defined pxy joint probability distribution x px py marginal probability distribution x respectively also called relative entropy kl divergence another measure find similarities two probability distributions measures much one distribution diverges suppose data true distribution underlying p dont know p choose new distribution q approximate data since q approximation wont able approximate data good p information loss occur information loss given kl divergence kl divergence p q tells us much information lose try approximate data given p q kl divergence probability distribution q another probability distribution p defined kl divergence commonly used unsupervised machine learning technique variational autoencoders information theory originally formulated mathematician electrical engineer claude shannon seminal paper mathematical theory communication 1948 note terms experiments random variable ai machine learning deep learning data science used loosely technically different meanings case liked article follow abhishek parbhakar articles related ai philosophy economics quick cheer standing ovation clap show much enjoyed story finding equilibria among ai philosophy economics sharing concepts ideas codes,en,"['Information Theory', 'Information Entropy', 'Shannon Entropy', 'exp 1', 'X.\n', 'Cross', 'Relative Entropy', 'Variational Autoencoders']"
9,Aman Dalmia,2300,What I learned from interviewing at multiple AI companies and start-ups,"Over the past 8 months, I’ve been interviewing at various companies like Google’s DeepMind, Wadhwani Institute of AI, Microsoft, Ola, Fractal Analytics, and a few others primarily for the roles — Data Scientist, Software Engineer & Research Engineer. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone. I believe that if I’d had this knowledge before, I could have avoided many mistakes and have prepared in a much better manner, which is what the motivation behind this post is, to be able to help someone bag their dream place of work.
This post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I’m going to mention at the end of the post. I begin with How to get noticed a.k.a. the interview. Then I provide a List of companies and start-ups to apply, which is followed by How to ace that interview. Based on whatever experience I’ve had, I add a section on What we should strive to work for. I conclude with Minimal Resources you need for preparation.
NOTE: For people who are sitting for campus placements, there are two things I’d like to add. Firstly, most of what I’m going to say (except for the last one maybe) is not going to be relevant to you for placements. But, and this is my second point, as I mentioned before, opportunities on campus are mostly in software engineering roles having no intersection with AI. So, this post is specifically meant for people who want to work on solving interesting problems using AI. Also, I want to add that I haven’t cleared all of these interviews but I guess that’s the essence of failure — it’s the greatest teacher! The things that I mention here may not all be useful but these are things that I did and there’s no way for me to know what might have ended up making my case stronger.
To be honest, this step is the most important one. What makes off-campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get. Having a contact inside the organisation place a referral for you would make it quite easy, but, in general, this part can be sub-divided into three keys steps:
a) Do the regulatory preparation and do that well: So, with regulatory preparation, I mean —a LinkedIn profile, a Github profile, a portfolio website and a well-polished CV. Firstly, your CV should be really neat and concise. Follow this guide by Udacity for cleaning up your CV — Resume Revamp. It has everything that I intend to say and I’ve been using it as a reference guide myself. As for the CV template, some of the in-built formats on Overleaf are quite nice. I personally use deedy-resume. Here’s a preview:
As it can be seen, a lot of content can be fit into one page. However, if you really do need more than that, then the format linked above would not work directly. Instead, you can find a modified multi-page format of the same here. The next most important thing to mention is your Github profile. A lot of people underestimate the potential of this, just because unlike LinkedIn, it doesn’t have a “Who Viewed Your Profile” option. People DO go through your Github because that’s the only way they have to validate what you have mentioned in your CV, given that there’s a lot of noise today with people associating all kinds of buzzwords with their profile. Especially for data science, open-source has a big role to play too with majority of the tools, implementations of various algorithms, lists of learning resources, all being open-sourced. I discuss the benefits of getting involved in Open-Source and how one can start from scratch in an earlier post here. The bare minimum for now should be:
• Create a Github account if you don’t already have one.• Create a repository for each of the projects that you have done.• Add documentation with clear instructions on how to run the code• Add documentation for each file mentioning the role of each function, the meaning of each parameter, proper formatting (e.g. PEP8 for Python) along with a script to automate the previous step (Optional).
Moving on, the third step is what most people lack, which is having a portfolio website demonstrating their experience and personal projects. Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor. Also, you generally have space constraints on your CV and tend to miss out on a lot of details. You can use your portfolio to really delve deep into the details if you want to and it’s highly recommended to include some sort of visualisation or demonstration of the project/idea. It’s really easy to create one too as there are a lot of free platforms with drag-and-drop features making the process really painless. I personally use Weebly which is a widely used tool. It’s better to have a reference to begin with. There are a lot of awesome ones out there but I referred to Deshraj Yadav’s personal website to begin with making mine:
Finally, a lot of recruiters and start-ups have nowadays started using LinkedIn as their go-to platform for hiring. A lot of good jobs get posted there. Apart from recruiters, the people working at influential positions are quite active there as well. So, if you can grab their attention, you have a good chance of getting in too. Apart from that, maintaining a clean profile is necessary for people to have the will to connect with you. An important part of LinkedIn is their search tool and for you to show up, you must have the relevant keywords interspersed over your profile. It took me a lot of iterations and re-evaluations to finally have a decent one. Also, you should definitely ask people with or under whom you’ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you. All of this increases your chance of actually getting noticed. I’ll again point towards Udacity’s guide for LinkedIn and Github profiles.
All this might seem like a lot, but remember that you don’t need to do it in a single day or even a week or a month. It’s a process, it never ends. Setting up everything at first would definitely take some effort but once it’s there and you keep updating it regularly as events around you keep happening, you’ll not only find it to be quite easy, but also you’ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself.
b) Stay authentic: I’ve seen a lot of people do this mistake of presenting themselves as per different job profiles. According to me, it’s always better to first decide what actually interests you, what would you be happy doing and then search for relevant opportunities; not the other way round. The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity. Spending time on your regulatory preparation mentioned above would give you an all-around perspective on yourself and help make this decision easier. Also, you won’t need to prepare answers to various kinds of questions that you get asked during an interview. Most of them would come out naturally as you’d be talking about something you really care about.
c) Networking: Once you’re done with a), figured out b), Networking is what will actually help you get there. If you don’t talk to people, you miss out on hearing about many opportunities that you might have a good shot at. It’s important to keep connecting with new people each day, if not physically, then on LinkedIn, so that upon compounding it after many days, you have a large and strong network. Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon this excellent article by Mark Meloon, where he talks about the importance of building a real connection with people by offering our help first. Another important step in networking is to get your content out. For example, if you’re good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well. Once you have a good enough network, your visibility increases multi-fold. You never know how one person from your network liking or commenting on your posts, may help you reach out to a much broader audience including people who might be looking for someone of your expertise.
I’m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference. However, I do place a “*” on the ones that I’d personally recommend. This recommendation is based on either of the following: mission statement, people, personal interaction or scope of learning. More than 1 “*” is purely based on the 2nd and 3rd factors.
Your interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you’re asked to introduce yourself — your body language and the fact that you’re smiling while greeting them plays a big role, especially when you’re interviewing for a start-up as culture-fit is something that they extremely care about. You need to understand that as much as the interviewer is a stranger to you, you’re a stranger to him/her too. So, they’re probably just as nervous as you are.
It’s important to view the interview as more of a conversation between yourself and the interviewer. Both of you are looking for a mutual fit — you are looking for an awesome place to work at and the interviewer is looking for an awesome person (like you) to work with. So, make sure that you’re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them. And the easiest way I know how to make that happen is to smile.
There are mostly two types of interviews — one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. I’ll start with the second one.
This kind of interview generally begins with a “Can you tell me a bit about yourself?”. At this point, 2 things are a big NO — talking about your GPA in college and talking about your projects in detail. An ideal statement should be about a minute or two long, should give a good idea on what have you been doing till now, and it’s not restricted to academics. You can talk about your hobbies like reading books, playing sports, meditation, etc — basically, anything that contributes to defining you. The interviewer will then take something that you talk about here as a cue for his next question, and then the technical part of the interview begins. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:
There would be a lot of questions on what could be done differently or if “X” was used instead of “Y”, what would have happened. At this point, it’s important to know the kind of trade-offs that is usually made during implementation, for e.g. if the interviewer says that using a more complex model would have given better results, then you might say that you actually had less data to work with and that would have lead to overfitting. In one of the interviews, I was given a case-study to work on and it involved designing algorithms for a real-world use case. I’ve noticed that once I’ve been given the green flag to talk about a project, the interviewers really like it when I talk about it in the following flow:
Problem > 1 or 2 previous approaches > Our approach > Result > Intuition
The other kind of interview is really just to test your basic knowledge. Don’t expect those questions to be too hard. But they would definitely scratch every bit of the basics that you should be having, mainly based around Linear Algebra, Probability, Statistics, Optimisation, Machine Learning and/or Deep Learning. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don’t miss out one bit among them. The catch here is the amount of time you take to answer those questions. Since these cover the basics, they expect that you should be answering them almost instantly. So, do your preparation accordingly.
Throughout the process, it’s important to be confident and honest about what you know and what you don’t know. If there’s a question that you’re certain you have no idea about, say it upfront rather than making “Aah”, “Um” sounds. If some concept is really important but you are struggling with answering it, the interviewer would generally (depending on how you did in the initial parts) be happy to give you a hint or guide you towards the right solution. It’s a big plus if you manage to pick their hints and arrive at the correct solution. Try to not get nervous and the best way to avoid that is by, again, smiling.
Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. It’s really easy to think that your interview is done and just say that you have nothing to ask. I know many people who got rejected just because of failing at this last question. As I mentioned before, it’s not only you who is being interviewed. You are also looking for a mutual fit with the company itself. So, it’s quite obvious that if you really want to join a place, you must have many questions regarding the work culture there or what kind of role are they seeing you in. It can be as simple as being curious about the person interviewing you. There’s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you’re truly interested in being a part of their team. A final question that I’ve started asking all my interviewers, is for a feedback on what they might want me to improve on. This has helped me tremendously and I still remember every feedback that I’ve gotten which I’ve incorporated into my daily life.
That’s it. Based on my experience, if you’re just honest about yourself, are competent, truly care about the company you’re interviewing for and have the right mindset, you should have ticked all the right boxes and should be getting a congratulatory mail soon 😄
We live in an era full of opportunities and that applies to anything that you love. You just need to strive to become the best at it and you will find a way to monetise it. As Gary Vaynerchuk (just follow him already) says:
This is a great time to be working in AI and if you’re truly passionate about it, you have so much that you can do with AI. You can empower so many people that have always been under-represented. We keep nagging about the problems surrounding us, but there’s been never such a time where common people like us can actually do something about those problems, rather than just complaining. Jeffrey Hammerbacher (Founder, Cloudera) had famously said:
We can do so much with AI than we can ever imagine. There are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve. You can make many lives better. Time to let go of what is “cool”, or what would “look good”. THINK and CHOOSE wisely.
Any Data Science interview comprises of questions mostly of a subset of the following four categories: Computer Science, Math, Statistics and Machine Learning.
If you’re not familiar with the math behind Deep Learning, then you should consider going over my last post for resources to understand them. However, if you are comfortable, I’ve found that the chapters 2, 3 and 4 of the Deep Learning Book are enough to prepare/revise for theoretical questions during such interviews. I’ve been preparing summaries for a few chapters which you can refer to where I’ve tried to even explain a few concepts that I found challenging to understand at first, in case you are not willing to go through the entire chapters. And if you’ve already done a course on probability, you should be comfortable answering a few numerical as well. For stats, covering these topics should be enough.
Now, the range of questions here can vary depending on the type of position you are applying for. If it’s a more traditional Machine Learning based interview where they want to check your basic knowledge in ML, you can complete any one of the following courses:- Machine Learning by Andrew Ng — CS 229- Machine Learning course by Caltech Professor Yaser Abu-Mostafa
Important topics are: Supervised Learning (Classification, Regression, SVM, Decision Tree, Random Forests, Logistic Regression, Multi-layer Perceptron, Parameter Estimation, Bayes’ Decision Rule), Unsupervised Learning (K-means Clustering, Gaussian Mixture Models), Dimensionality Reduction (PCA).
Now, if you’re applying for a more advanced position, there’s a high chance that you might be questioned on Deep Learning. In that case, you should be very comfortable with Convolutional Neural Networks (CNNs) and/or (depending upon what you’ve worked on) Recurrent Neural Networks (RNNs) and their variants. And by being comfortable, you must know what is the fundamental idea behind Deep Learning, how CNNs/RNNs actually worked, what kind of architectures have been proposed and what has been the motivation behind those architectural changes. Now, there’s no shortcut for this. Either you understand them or you put enough time to understand them. For CNNs, the recommended resource is Stanford’s CS 231N and CS 224N for RNNs. I found this Neural Network class by Hugo Larochelle to be really enlightening too. Refer this for a quick refresher too. Udacity coming to the aid here too. By now, you should have figured out that Udacity is a really important place for an ML practitioner. There are not a lot of places working on Reinforcement Learning (RL) in India and I too am not experienced in RL as of now. So, that’s one thing to add to this post sometime in the future.
Getting placed off-campus is a long journey of self-realisation. I realise that this has been another long post and I’m again extremely grateful to you for valuing my thoughts. I hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next Data Science interview better. If it did, I request you to really think about what I talk about in What we should strive to work for.
I’m very thankful to my friends from IIT Guwahati for their helpful feedback, especially Ameya Godbole, Kothapalli Vignesh and Prabal Jain. A majority of what I mention here, like “viewing an interview as a conversation” and “seeking feedback from our interviewers”, arose from multiple discussions with Prabal who has been advising me constantly on how I can improve my interviewing skills.
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Fanatic • Math Lover • Dreamer
The official Journal blog
",past 8 months ive interviewing various companies like googles deepmind wadhwani institute ai microsoft ola fractal analytics others primarily roles data scientist software engineer research engineer process get opportunity interact many great minds also peek along sense people really look interviewing someone believe id knowledge could avoided many mistakes prepared much better manner motivation behind post able help someone bag dream place work post arose discussion one juniors lack really fulfilling job opportunities offered campus placements people working ai also preparing noticed people using lot resources per experience past months realised one away minimal ones roles ai im going mention end post begin get noticed aka interview provide list companies startups apply followed ace interview based whatever experience ive add section strive work conclude minimal resources need preparation note people sitting campus placements two things id like add firstly im going say except last one maybe going relevant placements second point mentioned opportunities campus mostly software engineering roles intersection ai post specifically meant people want work solving interesting problems using ai also want add havent cleared interviews guess thats essence failure greatest teacher things mention may useful things theres way know might ended making case stronger honest step important one makes offcampus placements tough exhausting getting recruiter actually go profile among plethora applications get contact inside organisation place referral would make quite easy general part subdivided three keys steps regulatory preparation well regulatory preparation mean linkedin profile github profile portfolio website wellpolished cv firstly cv really neat concise follow guide udacity cleaning cv resume revamp everything intend say ive using reference guide cv template inbuilt formats overleaf quite nice personally use deedyresume heres preview seen lot content fit one page however really need format linked would work directly instead find modified multipage format next important thing mention github profile lot people underestimate potential unlike linkedin doesnt viewed profile option people go github thats way validate mentioned cv given theres lot noise today people associating kinds buzzwords profile especially data science opensource big role play majority tools implementations various algorithms lists learning resources opensourced discuss benefits getting involved opensource one start scratch earlier post bare minimum create github account dont already one create repository projects done add documentation clear instructions run code add documentation file mentioning role function meaning parameter proper formatting eg pep8 python along script automate previous step optional moving third step people lack portfolio website demonstrating experience personal projects making portfolio indicates really serious getting field adds lot points authenticity factor also generally space constraints cv tend miss lot details use portfolio really delve deep details want highly recommended include sort visualisation demonstration projectidea really easy create one lot free platforms draganddrop features making process really painless personally use weebly widely used tool better reference begin lot awesome ones referred deshraj yadavs personal website begin making mine finally lot recruiters startups nowadays started using linkedin goto platform hiring lot good jobs get posted apart recruiters people working influential positions quite active well grab attention good chance getting apart maintaining clean profile necessary people connect important part linkedin search tool show must relevant keywords interspersed profile took lot iterations reevaluations finally decent one also definitely ask people youve worked endorse skills add recommendation talking experience working increases chance actually getting noticed ill point towards udacitys guide linkedin github profiles might seem like lot remember dont need single day even week month process never ends setting everything first would definitely take effort keep updating regularly events around keep happening youll find quite easy also youll able talk anywhere anytime without explicitly prepare become aware b stay authentic ive seen lot people mistake presenting per different job profiles according always better first decide actually interests would happy search relevant opportunities way round fact demand ai talent surpasses supply gives opportunity spending time regulatory preparation mentioned would give allaround perspective help make decision easier also wont need prepare answers various kinds questions get asked interview would come naturally youd talking something really care c networking youre done figured b networking actually help get dont talk people miss hearing many opportunities might good shot important keep connecting new people day physically linkedin upon compounding many days large strong network networking messaging people place referral starting mistake way often stumbled upon excellent article mark meloon talks importance building real connection people offering help first another important step networking get content example youre good something blog share blog facebook linkedin help others helps well good enough network visibility increases multifold never know one person network liking commenting posts may help reach much broader audience including people might looking someone expertise im presenting list alphabetical order avoid misinterpretation specific preference however place ones id personally recommend recommendation based either following mission statement people personal interaction scope learning 1 purely based 2nd 3rd factors interview begins moment entered room lot things happen moment time youre asked introduce body language fact youre smiling greeting plays big role especially youre interviewing startup culturefit something extremely care need understand much interviewer stranger youre stranger himher theyre probably nervous important view interview conversation interviewer looking mutual fit looking awesome place work interviewer looking awesome person like work make sure youre feeling good take charge making initial moments conversation pleasant easiest way know make happen smile mostly two types interviews one interviewer come come prepared set questions going ask irrespective profile second interview based cv ill start second one kind interview generally begins tell bit point 2 things big talking gpa college talking projects detail ideal statement minute two long give good idea till restricted academics talk hobbies like reading books playing sports meditation etc basically anything contributes defining interviewer take something talk cue next question technical part interview begins motive kind interview really check whether whatever written cv true would lot questions could done differently x used instead would happened point important know kind tradeoffs usually made implementation eg interviewer says using complex model would given better results might say actually less data work would lead overfitting one interviews given casestudy work involved designing algorithms realworld use case ive noticed ive given green flag talk project interviewers really like talk following flow problem 1 2 previous approaches approach result intuition kind interview really test basic knowledge dont expect questions hard would definitely scratch every bit basics mainly based around linear algebra probability statistics optimisation machine learning andor deep learning resources mentioned minimal resources need preparation section suffice make sure dont miss one bit among catch amount time take answer questions since cover basics expect answering almost instantly preparation accordingly throughout process important confident honest know dont know theres question youre certain idea say upfront rather making aah um sounds concept really important struggling answering interviewer would generally depending initial parts happy give hint guide towards right solution big plus manage pick hints arrive correct solution try get nervous best way avoid smiling come conclusion interview interviewer would ask questions really easy think interview done say nothing ask know many people got rejected failing last question mentioned interviewed also looking mutual fit company quite obvious really want join place must many questions regarding work culture kind role seeing simple curious person interviewing theres always something learn everything around make sure leave interviewer impression youre truly interested part team final question ive started asking interviewers feedback might want improve helped tremendously still remember every feedback ive gotten ive incorporated daily life thats based experience youre honest competent truly care company youre interviewing right mindset ticked right boxes getting congratulatory mail soon live era full opportunities applies anything love need strive become best find way monetise gary vaynerchuk follow already says great time working ai youre truly passionate much ai empower many people always underrepresented keep nagging problems surrounding us theres never time common people like us actually something problems rather complaining jeffrey hammerbacher founder cloudera famously said much ai ever imagine many extremely challenging problems require incredibly smart people like put head solve make many lives better time let go cool would look good think choose wisely data science interview comprises questions mostly subset following four categories computer science math statistics machine learning youre familiar math behind deep learning consider going last post resources understand however comfortable ive found chapters 2 3 4 deep learning book enough preparerevise theoretical questions interviews ive preparing summaries chapters refer ive tried even explain concepts found challenging understand first case willing go entire chapters youve already done course probability comfortable answering numerical well stats covering topics enough range questions vary depending type position applying traditional machine learning based interview want check basic knowledge ml complete one following courses machine learning andrew ng cs 229 machine learning course caltech professor yaser abumostafa important topics supervised learning classification regression svm decision tree random forests logistic regression multilayer perceptron parameter estimation bayes decision rule unsupervised learning kmeans clustering gaussian mixture models dimensionality reduction pca youre applying advanced position theres high chance might questioned deep learning case comfortable convolutional neural networks cnns andor depending upon youve worked recurrent neural networks rnns variants comfortable must know fundamental idea behind deep learning cnnsrnns actually worked kind architectures proposed motivation behind architectural changes theres shortcut either understand put enough time understand cnns recommended resource stanfords cs 231n cs 224n rnns found neural network class hugo larochelle really enlightening refer quick refresher udacity coming aid figured udacity really important place ml practitioner lot places working reinforcement learning rl india experienced rl thats one thing add post sometime future getting placed offcampus long journey selfrealisation realise another long post im extremely grateful valuing thoughts hope post finds way useful helped way prepare next data science interview better request really think talk strive work im thankful friends iit guwahati helpful feedback especially ameya godbole kothapalli vignesh prabal jain majority mention like viewing interview conversation seeking feedback interviewers arose multiple discussions prabal advising constantly improve interviewing skills story published noteworthy thousands come every day learn people ideas shaping products love follow publication see product design stories featured journal team quick cheer standing ovation clap show much enjoyed story ai fanatic math lover dreamer official journal blog,en,"['Google', 'Wadhwani Institute of AI', 'Microsoft', 'Ola', 'Fractal Analytics', 'Software Engineer & Research Engineer', 'Minimal Resources', 'LinkedIn', 'Github', 'Overleaf', 'Udacity', 'Networking', 'Facebook', 'GPA', 'Cloudera', 'CHOOSE', 'CS', 'Caltech', 'Random Forests', 'Logistic Regression', 'Multi', 'Perceptron', 'Parameter Estimation', 'Convolutional Neural Networks', 'Stanford', 'CS 231N', 'Reinforcement Learning (RL', 'IIT', 'Prabal', 'Journal']"
10,Sophia Arakelyan,7,From Ballerina to AI Researcher: Part I – buZZrobot,"Last year, I published the article “From Ballerina to AI writer” where I described how I embraced the technical part of AI without a technical background. But having love and passion for AI, I educated myself and was able to build a neural net classifier and do projects in Deep RL.
Recently, I’ve become a participant in the OpenAI Scholarship Program (OpenAI is a non-profit that gathers top AI researchers to ensure the safety of AI to benefit humanity). Every week for the next three months I’ll publish blog posts sharing my story of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research.
Finding your true calling — the key component of happiness
My primary goal with the series of blog posts “From Ballerina to AI researcher” is to show that it’s never too late to embrace a new field, start over again, and find your true calling. Finding work you love is one of the most important components of happiness - — something that you do every day and invest your time in to grow; that makes you feel fulfilled, gives you energy; something that is a refuge for your soul.
Great things never come easy. We have to be able to fight to make great things happen. But you can’t fight for something you don’t believe in, especially if you don’t feel like it’s really important for you and humanity. Finding that thing is a real challenge. I feel lucky that I found my true passion — AI. To me, the technology itself and the AI community — researchers, scientists, people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us — is a great source of energy.
The structure of the blog post series
Today, I’m giving an overall intro of what I’m going to cover in my “From Ballerina to AI Researcher” series.
I’ll dedicate the sequence of blog posts during the OpenAI Scholars program to several aspects of AI technology. I’ll cover those areas that concern me a lot, like AI and automation, bias in ML, dual use of AI, etc.
Also, the structure of my posts will include some insights on what I’m working on right now (the final technical project will be available by the end of August and will be open-sourced).
I feel very lucky to have Alec Radford, an experienced researcher, as my mentor who guides me in the NLP and NLU research area.
First week of my scholarship
I’ve dedicated my first week within the program to learning about the Transformer architecture that performs much better on sequential data compared to RNNs, LSTMs.
The novelty of the architecture is its multi-head self-attention mechanism. According to the original paper, experiments with the transformer on two machine translation tasks showed the model to be superior in quality while being more parallelizable and requiring significantly less time to train.
More concretely, when RNNs or CNNs take a sequence as an input, it goes through sentences word by word, which is a huge obstacle toward parallelization of the process (takes more time to train models). Moreover, if sequences are too long, the model tends to forget the content of distant positions in sequence or mixes it with the following positions’ content — this is the fundamental problem in dealing with sequential data. The transformer architecture reduced this problem thanks to the multi-head self-attention mechanism.
I digged into RNN, LSTM models to catch up with the background information. To that end, I’ve found Andrew Ng’s course on Deep Learning along with the papers extremely useful. To develop insights regarding the transformer, I went through the following resources: the video by Łukasz Kaiser from Google Brain, one of the model’s creators; a blog post with very well elaborated content re: the model, ran the code tensor2tensor and the code using the PyTorch framework from this paper to “feel” the difference between the TF and PyTorch frameworks.
Overall, the goal within the program is to develop deep comprehension of the NLU research area: challenges, current state of the art; and to formulate and test hypotheses that tackle the most important problems of the field.
I’ll share more on what I’m working on in my future articles. Meanwhile, if you have questions/feedback, please leave a comment.
If you want to learn more about me, here are my Facebook and Twitter accounts.
I’d appreciate your feedback on my posts, such as what topics are most interesting to you that I should consider further coverage on.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Former ballerina turned AI writer. Fan of sci-fi, astrophysics. Consciousness is the key. Founder of buZZrobot.com
The publication aims to cover practical aspects of AI technology, use cases along with interviews with notable people in the AI field.
",last year published article ballerina ai writer described embraced technical part ai without technical background love passion ai educated able build neural net classifier projects deep rl recently ive become participant openai scholarship program openai nonprofit gathers top ai researchers ensure safety ai benefit humanity every week next three months ill publish blog posts sharing story transformation person dedicated 15 years professional dancing writing tech ai actually conducting ai research finding true calling key component happiness primary goal series blog posts ballerina ai researcher show never late embrace new field start find true calling finding work love one important components happiness something every day invest time grow makes feel fulfilled gives energy something refuge soul great things never come easy able fight make great things happen cant fight something dont believe especially dont feel like really important humanity finding thing real challenge feel lucky found true passion ai technology ai community researchers scientists people dedicate lives building powerful technology time mission benefit humanity make safe us great source energy structure blog post series today im giving overall intro im going cover ballerina ai researcher series ill dedicate sequence blog posts openai scholars program several aspects ai technology ill cover areas concern lot like ai automation bias ml dual use ai etc also structure posts include insights im working right final technical project available end august opensourced feel lucky alec radford experienced researcher mentor guides nlp nlu research area first week scholarship ive dedicated first week within program learning transformer architecture performs much better sequential data compared rnns lstms novelty architecture multihead selfattention mechanism according original paper experiments transformer two machine translation tasks showed model superior quality parallelizable requiring significantly less time train concretely rnns cnns take sequence input goes sentences word word huge obstacle toward parallelization process takes time train models moreover sequences long model tends forget content distant positions sequence mixes following positions content fundamental problem dealing sequential data transformer architecture reduced problem thanks multihead selfattention mechanism digged rnn lstm models catch background information end ive found andrew ngs course deep learning along papers extremely useful develop insights regarding transformer went following resources video ukasz kaiser google brain one models creators blog post well elaborated content model ran code tensor2tensor code using pytorch framework paper feel difference tf pytorch frameworks overall goal within program develop deep comprehension nlu research area challenges current state art formulate test hypotheses tackle important problems field ill share im working future articles meanwhile questionsfeedback please leave comment want learn facebook twitter accounts id appreciate feedback posts topics interesting consider coverage quick cheer standing ovation clap show much enjoyed story former ballerina turned ai writer fan scifi astrophysics consciousness key founder buzzrobotcom publication aims cover practical aspects ai technology use cases along interviews notable people ai field,en,"['Deep RL', 'Scholars', 'NLP', 'NLU', 'Transformer', 'TF', 'PyTorch', 'sci-fi', 'Consciousness']"
11,Dr. GP Pulipaka,2,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...","Latent semantic analysis works on large-scale datasets to generate representations to discover the insights through natural language processing. There are different approaches to perform the latent semantic analysis at multiple levels such as document level, phrase level, and sentence level. Primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences. The lexical semantics classifies and decomposes the lexical items. Applying lexical semantic structures has different contexts to identify the differences and similarities between the words. A generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms. Homonyms contain similar syntax or similar spelling with similar structuring with different meanings. Homonyms are not related to each other. Book is an example for homonym. It can mean for someone to read something or an act of making a reservation with similar spelling, form, and syntax. However, the definition is different. Polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings. The word polysemy is a Greek word which means many signs. Python provides NLTK library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings. Processing words through tokenization produce tokens. Word lemmatization converts words from the current inflected form into the base form.
Latent semantic analysis
Applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Many times, latent semantic analysis overtook human scores and subject matter tests conducted by humans. The accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. The matrix A can represent numerous hundred thousands of rows and columns on a typical large-corpus text document. Applying singular value decomposition develops a set of operations dubbed matrix decomposition. Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document.
Brief overview of linear algebra
The A with Z x Y matrix contains the real-valued entries with non-negative values for the term-document matrix. Determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix. The rank of A ≤ {Z,Y}. A square c x c represented as diagonal matrix where off-diagonal entries are zero. Examining the matrix, if all the c diagonal matrices are one, the identity matrix of the dimension c represented by Ic. For the square Z x Z matrix, A with a vector k which contains not all zeroes, for λ. The matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors. This allows to reduce the dimensionality of the words from multi-dimensions to two dimensions to view on the plot. The dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing. The Zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage. Hence, eigen decomposition is a by-product of singular value decomposition as the input of the document is highly asymmetrical. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The resources such as punkt and wordnet have to be downloaded from NLTK.
Deep Learning at scale with Google Colab notebooks
Training machine learning or deep learning models on CPUs could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources. Google built Colab Notebooks environment for research and development purposes. It runs entirely on the cloud without requiring any additional hardware or software setup for each machine. It’s entirely equivalent of a Jupyter notebook that aids the data scientists to share the colab notebooks by storing on Google drive just like any other Google Sheets or documents in a collaborative environment. There are no additional costs associated with enabling GPU at runtime for acceleration on the runtime. There are some challenges of uploading the data into Colab, unlike Jupyter notebook that can access the data directly from the local directory of the machine. In Colab, there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive FUSE wrapper.
Once this step is complete, it shows the following log without errors:
The next step would be generating the authentication tokens to authenticate the Google credentials for the drive and Colab
If it shows successful retrieval of access token, then Colab is all set.
At this stage, the drive is not mounted yet, it will show false when accessing the contents of the text file.
Once the drive is mounted, Colab has access to the datasets from Google drive.
Once the files are accessible, the Python can be executed similar to executing in Jupyter environment. Colab notebook also displays the results similar to what we see on Jupyter notebook.
PyCharm IDE
The program can be run compiled on PyCharm IDE environment and run on PyCharm or can be executed from OSX Terminal.
Results from OSX Terminal
Jupyter Notebook on standalone machine
Jupyter Notebook gives a similar output running the latent semantic analysis on the local machine:
References
Gorrell, G. (2006). Generalized Hebbian Algorithm for Incremental Singular Value Decomposition in Natural Language Processing. Retrieved from https://www.aclweb.org/anthology/E06-1013
Hardeniya, N. (2016). Natural Language Processing: Python and NLTK . Birmingham, England: Packt Publishing.
Landauer, T. K., Foltz, P. W., Laham, D., & University of Colorado at Boulder (1998). An Introduction to Latent Semantic Analysis. Retrieved from http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
Stackoverflow (2018). Mounting Google Drive on Google Colab. Retrieved from https://stackoverflow.com/questions/50168315/mounting-google-drive-on-google-colab
Stanford University (2009). Matrix decompositions and latent semantic indexing. Retrieved from https://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-and-latent-semantic-indexing-1.html
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Ganapathi Pulipaka | Founder and CEO @deepsingularity | Bestselling Author | Big data | IoT | Startups | SAP | MachineLearning | DeepLearning | DataScience
",latent semantic analysis works largescale datasets generate representations discover insights natural language processing different approaches perform latent semantic analysis multiple levels document level phrase level sentence level primarily semantic analysis summarized lexical semantics study combining individual words paragraphs sentences lexical semantics classifies decomposes lexical items applying lexical semantic structures different contexts identify differences similarities words generic term paragraph sentence hypernym hyponymy provides meaning relationship instances hyponyms homonyms contain similar syntax similar spelling similar structuring different meanings homonyms related book example homonym mean someone read something act making reservation similar spelling form syntax however definition different polysemy another phenomenon words single word could associated multiple related senses distinct meanings word polysemy greek word means many signs python provides nltk library perform tokenization words chopping words larger chunks phrases meaningful strings processing words tokenization produce tokens word lemmatization converts words current inflected form base form latent semantic analysis applying latent semantic analysis large datasets text documents represents contextual meaning mathematical statistical computation methods large corpus text many times latent semantic analysis overtook human scores subject matter tests conducted humans accuracy latent semantic analysis high reads machine readable documents texts web scale latent semantic analysis technique applies singular value decomposition principal component analysis pca document represented z x matrix rows matrix represent document collection matrix represent numerous hundred thousands rows columns typical largecorpus text document applying singular value decomposition develops set operations dubbed matrix decomposition natural language processing python nltk library applies lowrank approximation termdocument matrix later lowrank approximation aids indexing retrieving document known latent semantic indexing clustering number words document brief overview linear algebra z x matrix contains realvalued entries nonnegative values termdocument matrix determining rank matrix comes number linearly independent columns rows matrix rank zy square c x c represented diagonal matrix offdiagonal entries zero examining matrix c diagonal matrices one identity matrix dimension c represented ic square z x z matrix vector k contains zeroes matrix decomposition applies square matrix factored product matrices eigenvectors allows reduce dimensionality words multidimensions two dimensions view plot dimensionality reduction techniques principal component analysis singular value decomposition holds critical relevance natural language processing zipfian nature frequency words document makes difficult determine similarity words static stage hence eigen decomposition byproduct singular value decomposition input document highly asymmetrical latent semantic analysis particular technique semantic space parse document identify words polysemy nlkt library resources punkt wordnet downloaded nltk deep learning scale google colab notebooks training machine learning deep learning models cpus could take hours could pretty expensive terms programming language efficiency time energy computer resources google built colab notebooks environment research development purposes runs entirely cloud without requiring additional hardware software setup machine entirely equivalent jupyter notebook aids data scientists share colab notebooks storing google drive like google sheets documents collaborative environment additional costs associated enabling gpu runtime acceleration runtime challenges uploading data colab unlike jupyter notebook access data directly local directory machine colab multiple options upload files local file system drive mounted load data drive fuse wrapper step complete shows following log without errors next step would generating authentication tokens authenticate google credentials drive colab shows successful retrieval access token colab set stage drive mounted yet show false accessing contents text file drive mounted colab access datasets google drive files accessible python executed similar executing jupyter environment colab notebook also displays results similar see jupyter notebook pycharm ide program run compiled pycharm ide environment run pycharm executed osx terminal results osx terminal jupyter notebook standalone machine jupyter notebook gives similar output running latent semantic analysis local machine references gorrell g 2006 generalized hebbian algorithm incremental singular value decomposition natural language processing retrieved httpswwwaclweborganthologye061013 hardeniya n 2016 natural language processing python nltk birmingham england packt publishing landauer k foltz p w laham university colorado boulder 1998 introduction latent semantic analysis retrieved httplsacoloradoedupapersdp1lsaintropdf stackoverflow 2018 mounting google drive google colab retrieved httpsstackoverflowcomquestions50168315mountinggoogledriveongooglecolab stanford university 2009 matrix decompositions latent semantic indexing retrieved httpsnlpstanfordeduirbookhtmlhtmleditionmatrixdecompositionsandlatentsemanticindexing1html quick cheer standing ovation clap show much enjoyed story ganapathi pulipaka founder ceo deepsingularity bestselling author big data iot startups sap machinelearning deeplearning datascience,en,"['Polysemy', 'Python', 'NLTK', 'NLKT', 'Google Colab', 'Google', 'Colab Notebooks', 'GPU', 'Colab', 'PyCharm IDE', 'OSX Terminal', 'References\n', 'Incremental Singular Value Decomposition', 'Packt Publishing', 'Foltz', 'University of Colorado', 'Stanford University (2009']"
12,Scott Santens,7300,Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,"(An alternate version of this article was originally published in the Boston Globe)
On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as Chicago Pile-1, it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words.
Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it’s vital we understand this new language, and what it’s increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it.
The language is a new class of machine learning known as deep learning, and the “whispered word” was a computer’s use of it to seemingly out of nowhere defeat three-time European Go champion Fan Hui, not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world’s best living Go players, if not the best. Imagining such a grand duel of man versus machine, China’s top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most.
What actually ended up happening when they faced off? Lee went on to lose all but one of their match’s five games. An AI named AlphaGo is now a better Go player than any human and has been granted the “divine” rank of 9 dan. In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue.
So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what’s happening is no game.
AlphaGo’s historic victory is a clear signal that we’ve gone from linear to parabolic. Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income.
This may all sound like exaggeration, so let’s take a few decade steps back, and look at what computer technology has been actively doing to human employment so far:
Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It’s already here. Computer technology is already eating jobs and has been since 1990.
All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn’t change, and that work can be better handled by machines.
Distressingly, it’s exactly routine work that once formed the basis of the American middle class. It’s routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it’s routine cognitive work that once filled US office spaces. Such jobs are now increasingly unavailable, leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them.
If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That’s what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to learn.
I’m a writer at heart, but my educational background happens to be in psychology and physics. I’m fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as cognitive neuroscience. I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me.
As a quick primer in the way our brains function, they’re a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It’s all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we’ve been learning about how we work, we’ve now begun applying to the way machines work.
One of these applications is the creation of deep neural networks - kind of like pared-down virtual brains. They provide an avenue to machine learning that’s made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It’s not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka big data.
Big data isn’t just some buzzword. It’s information, and when it comes to information, we’re creating more and more of it every day. In fact we’re creating so much that a 2013 report by SINTEF estimated that 90% of all information in the world had been created in the prior two years. This incredible rate of data creation is even doubling every 1.5 years thanks to the Internet, where in 2015 every minute we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. Why?
Imagine programming a computer to recognize a chair. You’d need to enter a ton of instructions, and the result would still be a program detecting chairs that aren’t, and not detecting chairs that are. So how did we learn to detect chairs? Our parents pointed at a chair and said, “chair.” Then we thought we had that whole chair thing all figured out, so we pointed at a table and said “chair”, which is when our parents told us that was “table.” This is called reinforcement learning. The label “chair” gets connected to every chair we see, such that certain neural pathways are weighted and others aren’t. For “chair” to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains.
The power of deep learning is that it’s a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing “chairness” to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of “chairness.” Next we test it with even more images. Where it’s wrong, we correct it, which further improves its “chairness” detection. Repetition of this process results in a computer that knows what a chair is when it sees it, for the most part as well as we can. The important difference though is that unlike us, it can then sort through millions of images within a matter of seconds.
This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo, Google’s DeepMind AI learned how to read and comprehend what it read through hundreds of thousands of annotated news articles. DeepMind also taught itself to play dozens of Atari 2600 video games better than humans, just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions, attaining International Master level status in just 72 hours by repeatedly playing itself. In 2015, an AI even passed a visual Turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all major milestones in AI.
However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo’s victory, was by experts essentially, “Maybe in another ten years.” A decade was considered a fair guess because Go is a game so complex I’ll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:
Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what feels like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and playing itself millions of times. So the answer to when the game of Go would fall to machines wasn’t even close to ten years. The correct answer ended up being, “Any time now.”
Any time now. That’s the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it.
We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean nothing humans do as a job is uniquely safe anymore. From hamburgers to healthcare, machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans.
Amelia is just one AI out there currently being beta-tested in companies right now. Created by IPsoft over the past 16 years, she’s learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she’s able to learn, she’s able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it’s been estimated that she can put 250 million people out of a job, worldwide.
Viv is an AI coming soon from the creators of Siri who’ll be our own personal assistant. She’ll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she’ll know we’ll like best. In doing all of this for us, we’ll see far fewer ads, and that means the entire advertising industry — that industry the entire Internet is built upon — stands to be hugely disrupted.
A world with Amelia and Viv — and the countless other AI counterparts coming online soon — in combination with robots like Boston Dynamics’ next generation Atlas portends, is a world where machines can do all four types of jobs and that means serious societal reconsiderations. If a machine can do a job instead of a human, should any human be forced at the threat of destitution to perform that job? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when jobs for many are entirely unobtainable? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them, where does that money go instead? And what does it no longer buy? Is it even possible that many of the jobs we’re creating don’t need to exist at all, and only do because of the incomes they provide? These are questions we need to start asking, and fast.
Fortunately, people are beginning to ask these questions, and there’s an answer that’s building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is universal basic income. By adopting UBI, aside from immunizing against the negative effects of automation, we’d also be decreasing the risks inherent in entrepreneurship, and the sizes of bureaucracies necessary to boost incomes. It’s for these reasons, it has cross-partisan support, and is even now in the beginning stages of possible implementation in countries like Switzerland, Finland, the Netherlands, and Canada.
The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will. The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million. That’s a net loss, not a net gain of 5 million jobs. In a frequently cited paper, an Oxford study estimated the automation of about half of all existing jobs by 2033. Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies — especially the US economy as I wrote last year about automating truck driving — by eliminating millions of jobs within a short span of time.
And now even the White House, in a stunning report to Congress, has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable “duck and cover” strategies for avoiding nuclear blasts during the Cold War.
All of this is why it’s those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked “Do you want half of people to starve because they literally can’t add economic value, or not?” before going on to suggest, ”If the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.”
AI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, “AI is already having a big impact on our economies... My suspicion is that more countries will have to follow Finland’s lead in exploring basic income guarantees for people.”
Moshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, “we need to rethink the very basic structure of our economic system... we may have to consider instituting a basic income guarantee.”
Even Baidu’s chief scientist and founder of Google’s “Google Brain” deep learning project, Andrew Ng, during an onstage interview at this year’s Deep Learning Summit, expressed the shared notion that basic income must be “seriously considered” by governments, citing “a high chance that AI will create massive labor displacement.”
When those building the tools begin warning about the implications of their use, shouldn’t those wishing to use those tools listen with the utmost attention, especially when it’s the very livelihoods of millions of people at stake? If not then, what about when Nobel prize winning economists begin agreeing with them in increasing numbers?
No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let’s ask ourselves, what’s the purpose of the technologies we’re creating? What’s the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose how we work, and to decline any pay/hours we deem insufficient because we’re already earning the incomes that machines aren’t?
What’s the big lesson to learn, in a century when machines can learn?
I offer it’s that jobs are for machines, and life is for people.
This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with a monthly patron pledge of $1+.
Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O’Sullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinimäki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith.
Scott Santens writes about basic income on his blog. You can also follow him here on Medium, on Twitter, on Facebook, or on Reddit where he is a moderator for the /r/BasicIncome community of over 30,000 subscribers.
If you feel others would appreciate this article, please click the green heart.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
New Orleans writer focused on the potential for human civilization to gets its act together in the 21st century. Moderator of /r/BasicIncome on Reddit.
Articles discussing the concept of the universal basic income
",alternate version article originally published boston globe december 2nd 1942 team scientists led enrico fermi came back lunch watched humanity created first selfsustaining nuclear reaction inside pile bricks wood underneath football field university chicago known history chicago pile1 celebrated silence single bottle chianti understood exactly meant humankind without need words something new occurred quietly changed world forever like whispered word foreign language quiet may heard full meaning may comprehended however vital understand new language increasingly telling us ramifications set alter everything take granted way globalized economy functions ways humans exist within language new class machine learning known deep learning whispered word computers use seemingly nowhere defeat threetime european go champion fan hui five times row without defeat many read news considered impressive way comparable match lee sedol instead many consider one worlds best living go players best imagining grand duel man versus machine chinas top go player predicted lee would lose single game lee confidently expected possibly lose one actually ended happening faced lee went lose one matchs five games ai named alphago better go player human granted divine rank 9 dan words level play borders godlike go officially fallen machine jeopardy watson chess deep blue go simply think go super ultra mega chess may still sound like small accomplishment another feather cap machines continue prove superior fun games play small accomplishment whats happening game alphagos historic victory clear signal weve gone linear parabolic advances technology visibly exponential nature expect see lot milestones crossed long would otherwise expect exponential advances notably forms artificial intelligence limited specific tasks entirely unprepared long continue insist upon employment primary source income may sound like exaggeration lets take decade steps back look computer technology actively human employment far let chart sink fooled thinking conversation automation labor set future already computer technology already eating jobs since 1990 work divided four types routine nonroutine cognitive manual routine work stuff day day nonroutine work varies within two varieties work requires mostly brains cognitive work requires mostly bodies manual four types saw growth stuff routine stagnated back 1990 happened routine labor easiest technology shoulder rules written work doesnt change work better handled machines distressingly exactly routine work formed basis american middle class routine manual work henry ford transformed paying people middle class wages perform routine cognitive work filled us office spaces jobs increasingly unavailable leaving two kinds jobs rosy outlooks jobs require little thought pay people little jobs require much thought pay people well imagine economy plane four engines still fly two long keep roaring avoid concerning crashing happens two remaining engines also fail thats advancing fields robotics ai represent final two engines first time successfully teaching machines learn im writer heart educational background happens psychology physics im fascinated undergraduate focus ended physics human brain otherwise known cognitive neuroscience think start look human brain works mass interconnected neurons somehow results describe mind everything changes least quick primer way brains function theyre giant network interconnected cells connections short long cells connected one connected many electrical signals pass connections various rates subsequent neural firings happen turn kind like falling dominoes far faster larger complex result amazingly us weve learning work weve begun applying way machines work one applications creation deep neural networks kind like pareddown virtual brains provide avenue machine learning thats made incredible leaps previously thought much road even possible obvious growing capability computers expanding knowledge neurosciences vastly growing expanse collective data aka big data big data isnt buzzword information comes information creating every day fact creating much 2013 report sintef estimated 90 information world created prior two years incredible rate data creation even doubling every 15 years thanks internet 2015 every minute liking 42 million things facebook uploading 300 hours video youtube sending 350000 tweets everything generating data like never lots data exactly machines need order learn learn imagine programming computer recognize chair youd need enter ton instructions result would still program detecting chairs arent detecting chairs learn detect chairs parents pointed chair said chair thought whole chair thing figured pointed table said chair parents told us table called reinforcement learning label chair gets connected every chair see certain neural pathways weighted others arent chair fire brains perceive close enough previous chair encounters essentially lives big data filtered brains power deep learning way using massive amounts data get machines operate like without giving explicit instructions instead describing chairness computer instead plug internet feed millions pictures chairs general idea chairness next test even images wrong correct improves chairness detection repetition process results computer knows chair sees part well important difference though unlike us sort millions images within matter seconds combination deep learning big data resulted astounding accomplishments past year aside incredible accomplishment alphago googles deepmind ai learned read comprehend read hundreds thousands annotated news articles deepmind also taught play dozens atari 2600 video games better humans looking screen score playing games repeatedly ai named giraffe taught play chess similar manner using dataset 175 million chess positions attaining international master level status 72 hours repeatedly playing 2015 ai even passed visual turing test learning learn way enabled shown unknown character fictional alphabet instantly reproduce letter way entirely indistinguishable human given task major milestones ai however despite milestones asked estimate computer would defeat prominent go player answer even months prior announcement google alphagos victory experts essentially maybe another ten years decade considered fair guess go game complex ill let ken jennings jeopardy fame another former champion human defeated ai describe confounding complexity makes impossible bruteforce approach scan every possible move determine next best move deep neural networks get around barrier way minds learning estimate feels like best move observation practice alphago analyzing millions professional games playing millions times answer game go would fall machines wasnt even close ten years correct answer ended time time thats new goto response 21st century question involving something new machines better humans need try wrap heads around need recognize means exponential technological change entering labor market space nonroutine jobs first time ever machines learn mean nothing humans job uniquely safe anymore hamburgers healthcare machines created successfully perform tasks need less need humans lower costs humans amelia one ai currently betatested companies right created ipsoft past 16 years shes learned perform work call center employees learn seconds takes us months 20 languages shes able learn shes able time one company putting paces successfully handled one every ten calls first week end second month could resolve six ten calls estimated put 250 million people job worldwide viv ai coming soon creators siri wholl personal assistant shell perform tasks online us even function facebook news feed steroids suggesting consume media shell know well like best us well see far fewer ads means entire advertising industry industry entire internet built upon stands hugely disrupted world amelia viv countless ai counterparts coming online soon combination robots like boston dynamics next generation atlas portends world machines four types jobs means serious societal reconsiderations machine job instead human human forced threat destitution perform job income remain coupled employment job way obtain income jobs many entirely unobtainable machines performing increasing percentage jobs us getting paid money go instead longer buy even possible many jobs creating dont need exist incomes provide questions need start asking fast fortunately people beginning ask questions theres answer thats building momentum idea put machines work us empower seek forms remaining work humans find valuable simply providing everyone monthly paycheck independent work paycheck would granted citizens unconditionally name universal basic income adopting ubi aside immunizing negative effects automation wed also decreasing risks inherent entrepreneurship sizes bureaucracies necessary boost incomes reasons crosspartisan support even beginning stages possible implementation countries like switzerland finland netherlands canada future place accelerating changes seems unwise continue looking future past new jobs historically appeared always wef started 2016 estimating creation 2020 2 million new jobs alongside elimination 7 million thats net loss net gain 5 million jobs frequently cited paper oxford study estimated automation half existing jobs 2033 meanwhile selfdriving vehicles thanks machine learning capability drastically impacting economies especially us economy wrote last year automating truck driving eliminating millions jobs within short span time even white house stunning report congress put probability 83 percent worker making less 20 hour 2010 eventually lose job machine even workers making much 40 hour face odds 31 percent ignore odds like tantamount laughable duck cover strategies avoiding nuclear blasts cold war knowledgeable ai field actively sounding alarm basic income panel discussion end 2015 singularity university prominent data scientist jeremy howard asked want half people starve literally cant add economic value going suggest answer smartest way distribute wealth implementing universal basic income ai pioneer chris eliasmith director centre theoretical neuroscience warned immediate impacts ai society interview futurism ai already big impact economies suspicion countries follow finlands lead exploring basic income guarantees people moshe vardi expressed sentiment speaking 2016 annual meeting american association advancement science emergence intelligent machines need rethink basic structure economic system may consider instituting basic income guarantee even baidus chief scientist founder googles google brain deep learning project andrew ng onstage interview years deep learning summit expressed shared notion basic income must seriously considered governments citing high chance ai create massive labor displacement building tools begin warning implications use shouldnt wishing use tools listen utmost attention especially livelihoods millions people stake nobel prize winning economists begin agreeing increasing numbers nation yet ready changes ahead high labor force nonparticipation leads social instability lack consumers within consumer economies leads economic instability lets ask whats purpose technologies creating whats purpose car drive us artificial intelligence shoulder 60 workload allow us work hours even less pay enable us choose work decline payhours deem insufficient already earning incomes machines arent whats big lesson learn century machines learn offer jobs machines life people article written crowdfunded monthly basic income found value article support along advocacy basic income monthly patron pledge 1 special thanks arjun banker steven grimm larry cohen topher hunt aaron marcuskubitza andrew stern keith davis albert wenger richard chris smothers mark witham david ihnen danielle texeira katie doemland paul wicks jan smole joe esposito jack wagner joe ballou stuart matthews natalie foster chris mccoy michael honey gary aranovich kai wong john david hodge louise whitmore dan osullivan harish venkatesan michiel dral gerald huff susanne berg cameron ottens kian alavi gray scott kirk israel robert solovay jeff schulman andrew henderson robert f greene martin jordo victor lau shane gordon paolo narciso johan grahn tony destefano erhan altay bryan herdliska stephane boisvert dave shelton rise shine pac luke sampson lee irving kris roadruck amy shaffer thomas welsh olli niinimaki casey young elizabeth balcar masud shah allen bauer funders support amazing partner katie smith scott santens writes basic income blog also follow medium twitter facebook reddit moderator rbasicincome community 30000 subscribers feel others would appreciate article please click green heart quick cheer standing ovation clap show much enjoyed story new orleans writer focused potential human civilization gets act together 21st century moderator rbasicincome reddit articles discussing concept universal basic income,en,"['the Boston Globe', 'the University of Chicago', 'Jeopardy', 'Deep Blue', 'AlphaGo', 'SINTEF', 'Facebook', 'Google', 'Atari', 'Google of', 'IPsoft', 'Boston Dynamics’', 'UBI', 'WEF', 'Oxford', 'the White House', 'Congress', 'Singularity University', 'the Centre for Theoretical Neuroscience', 'the American Association for the Advancement of Science', 'Rise & Shine PAC', 'the /r/BasicIncome']"
13,Adam Geitgey,14200,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt or Italiano.
Are you tired of reading endless news stories about deep learning and not really knowing what that means? Let’s change that!
This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we’re going to explain the black magic that allows Google Photos to search your photos based on what is in the picture:
Just like Part 1 and Part 2, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished!
(If you haven’t already read part 1 and part 2, read them now!)
You might have seen this famous xkcd comic before.
The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.
In the last few years, we’ve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.
So let’s do it — let’s write a program that can recognize birds!
Before we learn how to recognize pictures of birds, let’s learn how to recognize something much simpler — the handwritten number “8”.
In Part 2, we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let’s modify this same neural network to recognize handwritten text. But to make the job really simple, we’ll only try to recognize one letter — the numeral “8”.
Machine learning only works when you have data — preferably a lot of data. So we need lots and lots of handwritten “8”s to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some “8”s from the data set:
The neural network we made in Part 2 only took in a three numbers as the input (“3” bedrooms, “2000” sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?
The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an “8” and thee second output will predict the likelihood it isn’t an “8”. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.
Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.
All that’s left is to train the neural network with images of “8”s and not-“8""s so it learns to tell them apart. When we feed in an “8”, we’ll tell it the probability the image is an “8” is 100% and the probability it’s not an “8” is 0%. Vice versa for the counter-example images.
Here’s some of our training data:
We can train this kind of neural network in a few minutes on a modern laptop. When it’s done, we’ll have a neural network that can recognize pictures of “8”s with a pretty high accuracy. Welcome to the world of (late 1980’s-era) image recognition!
It’s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! ...right?
Well, of course it’s not that simple.
First, the good news is that our “8” recognizer really does work well on simple images where the letter is right in the middle of the image:
But now the really bad news:
Our “8” recognizer totally fails to work when the letter isn’t perfectly centered in the image. Just the slightest position change ruins everything:
This is because our network only learned the pattern of a perfectly-centered “8”. It has absolutely no idea what an off-center “8” is. It knows exactly one pattern and one pattern only.
That’s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the “8” isn’t perfectly centered.
We already created a really good program for finding an “8” centered in an image. What if we just scan all around the image for possible “8”s in smaller sections, one section at a time, until we find one?
This approach called a sliding window. It’s the brute force solution. It works well in some limited cases, but it’s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!
When we trained our network, we only showed it “8”s that were perfectly centered. What if we train it with more data, including “8”s in all different positions and sizes all around the image?
We don’t even need to collect new training data. We can just write a script to generate new images with the “8”s in all kinds of different positions in the image:
Using this technique, we can easily create an endless supply of training data.
More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.
To make the network bigger, we just stack up layer upon layer of nodes:
We call this a “deep neural network” because it has more layers than a traditional neural network.
This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.
But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn’t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.
Think about it. It doesn’t make sense to train a network to recognize an “8” at the top of a picture separately from training it to recognize an “8” at the bottom of a picture as if those were two totally different objects.
There should be some way to make the neural network smart enough to know that an “8” anywhere in the picture is the same thing without all that extra training. Luckily... there is!
As a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:
As a human, you instantly recognize the hierarchy in this picture:
Most importantly, we recognize the idea of a child no matter what surface the child is on. We don’t have to re-learn the idea of child for every possible surface it could appear on.
But right now, our neural network can’t do this. It thinks that an “8” in a different part of the image is an entirely different thing. It doesn’t understand that moving an object around in the picture doesn’t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.
We need to give our neural network understanding of translation invariance — an “8” is an “8” no matter where in the picture it shows up.
We’ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).
Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.
Here’s how it’s going to work, step by step —
Similar to our sliding window search above, let’s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:
By doing this, we turned our original image into 77 equally-sized tiny image tiles.
Earlier, we fed a single image into a neural network to see if it was an “8”. We’ll do the exact same thing here, but we’ll do it for each individual image tile:
However, there’s one big twist: We’ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we’ll mark that tile as interesting.
We don’t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:
In other words, we’ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.
The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:
To reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn’t at all!
We’ll just look at each 2x2 square of the array and keep the biggest number:
The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we’ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.
So far, we’ve reduced a giant image down into a fairly small array.
Guess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn’t a match. To differentiate it from the convolution step, we call it a “fully connected” network.
So from start to finish, our whole five-step pipeline looks like this:
Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.
When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.
The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.
For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc.
Here’s what a more realistic deep convolutional network (like you would find in a research paper) looks like:
In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!
So how do you know which steps you need to combine to make your image classifier work?
Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!
Now finally we know enough to write a program that can decide if a picture is a bird or not.
As always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we’ll also add in the Caltech-UCSD Birds-200–2011 data set that has another 12,000 bird pics.
Here’s a few of the birds from our combined data set:
And here’s some of the 52,000 non-bird images:
This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need millions of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data!
To build our classifier, we’ll use TFLearn. TFlearn is a wrapper around Google’s TensorFlow deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network.
Here’s the code to define and train the network:
If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer.
As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn’t help, so I stopped it there.
Congrats! Our program can now recognize birds in images!
Now that we have a trained neural network, we can use it! Here’s a simple script that takes in a single image file and predicts if it is a bird or not.
But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time.
That seems pretty good, right? Well... it depends!
Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things.
For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed “not a bird” every single time would be 95% accurate! But it would also be 100% useless.
We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at how it failed, not just the percentage of the time that it failed.
Instead of thinking about our predictions as “right” and “wrong”, let’s break them down into four separate categories —
Using our validation set of 15,000 images, here’s how many times our predictions fell into each category:
Why do we break our results down like this? Because not all mistakes are created equal.
Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we’d rather have false positives than false negatives. False negatives would be the worse possible case — that’s when the program told someone they definitely didn’t have cancer but they actually did.
Instead of just looking at overall accuracy, we calculate Precision and Recall metrics. Precision and Recall metrics give us a clearer picture of how well we did:
This tells us that 97% of the time we guessed “Bird”, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one!
Now that you know the basics of deep convolutional networks, you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don’t even have to find your own images.
You also know enough now to start branching and learning about other areas of machine learning. Why not learn how to use algorithms to train computers how to play Atari games next?
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 4, Part 5 and Part 6!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
",update article part series check full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 part 8 also read article portugues tieng viet italiano tired reading endless news stories deep learning really knowing means lets change time going learn write programs recognize objects images using deep learning words going explain black magic allows google photos search photos based picture like part 1 part 2 guide anyone curious machine learning idea start goal accessible anyone means theres lot generalizations skip lots details cares gets anyone interested ml mission accomplished havent already read part 1 part 2 read might seen famous xkcd comic goof based idea 3yearold child recognize photo bird figuring make computer recognize objects puzzled best computer scientists 50 years last years weve finally found good approach object recognition using deep convolutional neural networks sounds like bunch made words william gibson scifi novel ideas totally understandable break one one lets lets write program recognize birds learn recognize pictures birds lets learn recognize something much simpler handwritten number 8 part 2 learned neural networks solve complex problems chaining together lots simple neurons created small neural network estimate price house based many bedrooms big neighborhood also know idea machine learning generic algorithms reused different data solve different problems lets modify neural network recognize handwritten text make job really simple well try recognize one letter numeral 8 machine learning works data preferably lot data need lots lots handwritten 8s get started luckily researchers created mnist data set handwritten numbers purpose mnist provides 60000 images handwritten digits 18x18 image 8s data set neural network made part 2 took three numbers input 3 bedrooms 2000 sq feet etc want process images neural network world feed images neural network instead numbers answer incredible simple neural network takes numbers input computer image really grid numbers represent dark pixel feed image neural network simply treat 18x18 pixel image array 324 numbers handle 324 inputs well enlarge neural network 324 input nodes notice neural network also two outputs instead one first output predict likelihood image 8 thee second output predict likelihood isnt 8 separate output type object want recognize use neural network classify objects groups neural network lot bigger last time 324 inputs instead 3 modern computer handle neural network hundred nodes without blinking would even work fine cell phone thats left train neural network images 8s not8s learns tell apart feed 8 well tell probability image 8 100 probability 8 0 vice versa counterexample images heres training data train kind neural network minutes modern laptop done well neural network recognize pictures 8s pretty high accuracy welcome world late 1980sera image recognition really neat simply feeding pixels neural network actually worked build image recognition machine learning magic right well course simple first good news 8 recognizer really work well simple images letter right middle image really bad news 8 recognizer totally fails work letter isnt perfectly centered image slightest position change ruins everything network learned pattern perfectlycentered 8 absolutely idea offcenter 8 knows exactly one pattern one pattern thats useful real world real world problems never clean simple need figure make neural network work cases 8 isnt perfectly centered already created really good program finding 8 centered image scan around image possible 8s smaller sections one section time find one approach called sliding window brute force solution works well limited cases really inefficient check image looking objects different sizes better trained network showed 8s perfectly centered train data including 8s different positions sizes around image dont even need collect new training data write script generate new images 8s kinds different positions image using technique easily create endless supply training data data makes problem harder neural network solve compensate making network bigger thus able learn complicated patterns make network bigger stack layer upon layer nodes call deep neural network layers traditional neural network idea around since late 1960s recently training large neural network slow useful figured use 3d graphics cards designed matrix multiplication really fast instead normal computer processors working large neural networks suddenly became practical fact exact nvidia geforce gtx 1080 video card use play overwatch used train neural networks incredibly quickly even though make neural network really big train quickly 3d graphics card still isnt going get us way solution need smarter process images neural network think doesnt make sense train network recognize 8 top picture separately training recognize 8 bottom picture two totally different objects way make neural network smart enough know 8 anywhere picture thing without extra training luckily human intuitively know pictures hierarchy conceptual structure consider picture human instantly recognize hierarchy picture importantly recognize idea child matter surface child dont relearn idea child every possible surface could appear right neural network cant thinks 8 different part image entirely different thing doesnt understand moving object around picture doesnt make something different means relearn identify object every possible position sucks need give neural network understanding translation invariance 8 8 matter picture shows well using process called convolution idea convolution inspired partly computer science partly biology ie mad scientists literally poking cat brains weird probes figure cats process images instead feeding entire images neural network one grid numbers going something lot smarter takes advantage idea object matter appears picture heres going work step step similar sliding window search lets pass sliding window entire original image save result separate tiny picture tile turned original image 77 equallysized tiny image tiles earlier fed single image neural network see 8 well exact thing well individual image tile however theres one big twist well keep neural network weights every single tile original image words treating every image tile equally something interesting appears given tile well mark tile interesting dont want lose track arrangement original tiles save result processing tile grid arrangement original image looks like words weve started large image ended slightly smaller array records sections original image interesting result step 3 array maps parts original image interesting array still pretty big reduce size array downsample using algorithm called max pooling sounds fancy isnt well look 2x2 square array keep biggest number idea found something interesting four input tiles makes 2x2 grid square well keep interesting bit reduces size array keeping important bits far weve reduced giant image fairly small array guess array bunch numbers use small array input another neural network final neural network decide image isnt match differentiate convolution step call fully connected network start finish whole fivestep pipeline looks like image processing pipeline series steps convolution maxpooling finally fullyconnected network solving problems real world steps combined stacked many times want two three even ten convolution layers throw max pooling wherever want reduce size data basic idea start large image continually boil stepbystep finally single result convolution steps complicated features network able learn recognize example first convolution step might learn recognize sharp edges second convolution step might recognize beaks using knowledge sharp edges third step might recognize entire birds using knowledge beaks etc heres realistic deep convolutional network like would find research paper looks like case start 224 x 224 pixel image apply convolution max pooling twice apply convolution 3 times apply max pooling two fullyconnected layers end result image classified one 1000 categories know steps need combine make image classifier work honestly answer lot experimentation testing might train 100 networks find optimal structure parameters problem solving machine learning involves lot trial error finally know enough write program decide picture bird always need data get started free cifar10 data set contains 6000 pictures birds 52000 pictures things birds get even data well also add caltechucsd birds2002011 data set another 12000 bird pics heres birds combined data set heres 52000 nonbird images data set work fine purposes 72000 lowres images still pretty small realworld applications want googlelevel performance need millions large images machine learning data almost always important better algorithms know google happy offer unlimited photo storage want sweet sweet data build classifier well use tflearn tflearn wrapper around googles tensorflow deep learning library exposes simplified api makes building convolutional neural networks easy writing lines code define layers network heres code define train network training good video card enough ram like nvidia geforce gtx 980 ti better done less hour training normal cpu might take lot longer trains accuracy increase first pass got 754 accuracy 10 passes already 917 50 passes capped around 955 accuracy additional training didnt help stopped congrats program recognize birds images trained neural network use heres simple script takes single image file predicts bird really see effective network need test lots images data set created held back 15000 images validation ran 15000 images network predicted correct answer 95 time seems pretty good right well depends network claims 95 accurate devil details could mean sorts different things example 5 training images birds 95 birds program guessed bird every single time would 95 accurate would also 100 useless need look closely numbers overall accuracy judge good classification system really need look closely failed percentage time failed instead thinking predictions right wrong lets break four separate categories using validation set 15000 images heres many times predictions fell category break results like mistakes created equal imagine writing program detect cancer mri image detecting cancer wed rather false positives false negatives false negatives would worse possible case thats program told someone definitely didnt cancer actually instead looking overall accuracy calculate precision recall metrics precision recall metrics give us clearer picture well tells us 97 time guessed bird right also tells us found 90 actual birds data set words might find every bird pretty sure find one know basics deep convolutional networks try examples come tflearn get hands dirty different neural network architectures even comes builtin data sets dont even find images also know enough start branching learning areas machine learning learn use algorithms train computers play atari games next liked article please consider signing machine learning fun email list ill email something new awesome share best way find write articles like also follow twitter ageitgey email directly find linkedin id love hear help team machine learning continue machine learning fun part 4 part 5 part 6 quick cheer standing ovation clap show much enjoyed story interested computers machine learning likes write,en,"['Português', 'MNIST', 'NVIDIA GeForce GTX', 'max', 'Caltech', 'Google', 'TFLearn', 'API', 'RAM', 'Nvidia GeForce GTX', 'Congrats', 'Precision and Recall', 'Recall', 'Atari']"
14,Adam Geitgey,15200,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt or Italiano.
Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic:
This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing technology — Facebook can recognize faces with 98% accuracy which is pretty much as good as humans can do!
Let’s learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem — telling Will Ferrell (famous actor) apart from Chad Smith (famous rock musician)!
So far in Part 1, 2 and 3, we’ve used machine learning to solve isolated problems that have only one step — estimating the price of a house, generating new data based on existing data and telling if an image contains a certain object. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result.
But face recognition is really a series of several related problems:
As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are too good at recognizing faces and end up seeing faces in everyday objects:
Computers are not capable of this kind of high-level generalization (at least not yet...), so we have to teach them how to do each step in this process separately.
We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms:
Let’s tackle this problem one step at a time. For each step, we’ll learn about a different machine learning algorithm. I’m not going to explain every single algorithm completely to keep this from turning into a book, but you’ll learn the main ideas behind each one and you’ll learn how you can build your own facial recognition system in Python using OpenFace and dlib.
The first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!
If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action:
Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we’ll use it for a different purpose — finding the areas of the image we want to pass on to the next step in our pipeline.
Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We’re going to use a method invented in 2005 called Histogram of Oriented Gradients — or just HOG for short.
To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:
Then we’ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:
Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:
If you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:
This might seem like a random thing to do, but there’s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!
But saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.
To do this, we’ll break up the image into small squares of 16x16 pixels each. In each square, we’ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc...). Then we’ll replace that square in the image with the arrow directions that were the strongest.
The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images.
Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:
To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.
To do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.
The basic idea is we will come up with 68 specific points (called landmarks) that exist on every face — the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:
Here’s the result of locating the 68 face landmarks on our test image:
Now that we know were the eyes and mouth are, we’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won’t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):
Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.
If you want to try this step out yourself using Python and dlib, here’s the code for finding face landmarks and here’s the code for transforming the image using those landmarks.
Now we are to the meat of the problem — actually telling faces apart. This is where things get really interesting!
The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?
There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.
What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you’ve ever watched a bad crime show like CSI, you know what I am talking about:
Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?
It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.
The solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.
The training process works by looking at 3 face images at a time:
Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:
After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.
Machine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.
This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.
But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!
So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image:
So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.
If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.
This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.
You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We’ll use a simple linear SVM classifier, but lots of classification algorithms could work.
All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!
So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:
Then I ran the classifier on every frame of the famous youtube video of Will Ferrell and Chad Smith pretending to be each other on the Jimmy Fallon show:
It works! And look how well it works for faces in different poses — even sideways faces!
Let’s review the steps we followed:
Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:
UPDATE 4/9/2017: You can still follow the steps below to use OpenFace. However, I’ve released a new Python-based face recognition library called face_recognition that is much easier to install and use. So I’d recommend trying out face_recognition first instead of continuing below!
I even put together a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed. You can download and run it on your computer very easily. Give the virtual machine a shot if you don’t want to install all these libraries yourself!
Original OpenFace instructions:
If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter:
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 5!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
",update article part series check full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 part 8 also read article portugues tieng viet italiano noticed facebook developed uncanny ability recognize friends photographs old days facebook used make tag friends photos clicking typing name soon upload photo facebook tags everyone like magic technology called face recognition facebooks algorithms able recognize friends faces tagged times pretty amazing technology facebook recognize faces 98 accuracy pretty much good humans lets learn modern face recognition works recognizing friends would easy push tech limit solve challenging problem telling ferrell famous actor apart chad smith famous rock musician far part 1 2 3 weve used machine learning solve isolated problems one step estimating price house generating new data based existing data telling image contains certain object problems solved choosing one machine learning algorithm feeding data getting result face recognition really series several related problems human brain wired automatically instantly fact humans good recognizing faces end seeing faces everyday objects computers capable kind highlevel generalization least yet teach step process separately need build pipeline solve step face recognition separately pass result current step next step words chain together several machine learning algorithms lets tackle problem one step time step well learn different machine learning algorithm im going explain every single algorithm completely keep turning book youll learn main ideas behind one youll learn build facial recognition system python using openface dlib first step pipeline face detection obviously need locate faces photograph try tell apart youve used camera last 10 years youve probably seen face detection action face detection great feature cameras camera automatically pick faces make sure faces focus takes picture well use different purpose finding areas image want pass next step pipeline face detection went mainstream early 2000s paul viola michael jones invented way detect faces fast enough run cheap cameras however much reliable solutions exist going use method invented 2005 called histogram oriented gradients hog short find faces image well start making image black white dont need color data find faces well look every single pixel image one time every single pixel want look pixels directly surrounding goal figure dark current pixel compared pixels directly surrounding want draw arrow showing direction image getting darker repeat process every single pixel image end every pixel replaced arrow arrows called gradients show flow light dark across entire image might seem like random thing theres really good reason replacing pixels gradients analyze pixels directly really dark images really light images person totally different pixel values considering direction brightness changes really dark images really bright images end exact representation makes problem lot easier solve saving gradient every single pixel gives us way much detail end missing forest trees would better could see basic flow lightnessdarkness higher level could see basic pattern image well break image small squares 16x16 pixels square well count many gradients point major direction many point point upright point right etc well replace square image arrow directions strongest end result turn original image simple representation captures basic structure face simple way find faces hog image find part image looks similar known hog pattern extracted bunch training faces using technique easily find faces image want try step using python dlib heres code showing generate view hog representations images whew isolated faces image deal problem faces turned different directions look totally different computer account try warp picture eyes lips always sample place image make lot easier us compare faces next steps going use algorithm called face landmark estimation lots ways going use approach invented 2014 vahid kazemi josephine sullivan basic idea come 68 specific points called landmarks exist every face top chin outside edge eye inner edge eyebrow etc train machine learning algorithm able find 68 specific points face heres result locating 68 face landmarks test image know eyes mouth well simply rotate scale shear image eyes mouth centered best possible wont fancy 3d warps would introduce distortions image going use basic image transformations like rotation scale preserve parallel lines called affine transformations matter face turned able center eyes mouth roughly position image make next step lot accurate want try step using python dlib heres code finding face landmarks heres code transforming image using landmarks meat problem actually telling faces apart things get really interesting simplest approach face recognition directly compare unknown face found step 2 pictures people already tagged find previously tagged face looks similar unknown face must person seems like pretty good idea right theres actually huge problem approach site like facebook billions users trillion photos cant possibly loop every previoustagged face compare every newly uploaded picture would take way long need able recognize faces milliseconds hours need way extract basic measurements face could measure unknown face way find known face closest measurements example might measure size ear spacing eyes length nose etc youve ever watched bad crime show like csi know talking ok measurements collect face build known face database ear size nose length eye color something else turns measurements seem obvious us humans like eye color dont really make sense computer looking individual pixels image researchers discovered accurate approach let computer figure measurements collect deep learning better job humans figuring parts face important measure solution train deep convolutional neural network like part 3 instead training network recognize pictures objects like last time going train generate 128 measurements face training process works looking 3 face images time algorithm looks measurements currently generating three images tweaks neural network slightly makes sure measurements generates 1 2 slightly closer making sure measurements 2 3 slightly apart repeating step millions times millions images thousands different people neural network learns reliably generate 128 measurements person ten different pictures person give roughly measurements machine learning people call 128 measurements face embedding idea reducing complicated raw data like picture list computergenerated numbers comes lot machine learning especially language translation exact approach faces using invented 2015 researchers google many similar approaches exist process training convolutional neural network output face embeddings requires lot data computer power even expensive nvidia telsa video card takes 24 hours continuous training get good accuracy network trained generate measurements face even ones never seen step needs done lucky us fine folks openface already published several trained networks directly use thanks brandon amos team need run face images pretrained network get 128 measurements face heres measurements test image parts face 128 numbers measuring exactly turns idea doesnt really matter us care network generates nearly numbers looking two different pictures person want try step openface provides lua script generate embeddings images folder write csv file run like last step actually easiest step whole process find person database known people closest measurements test image using basic machine learning classification algorithm fancy deep learning tricks needed well use simple linear svm classifier lots classification algorithms could work need train classifier take measurements new test image tells known person closest match running classifier takes milliseconds result classifier name person lets try system first trained classifier embeddings 20 pictures ferrell chad smith jimmy falon ran classifier every frame famous youtube video ferrell chad smith pretending jimmy fallon show works look well works faces different poses even sideways faces lets review steps followed know works heres instructions starttofinish run entire face recognition pipeline computer update 492017 still follow steps use openface however ive released new pythonbased face recognition library called face_recognition much easier install use id recommend trying face_recognition first instead continuing even put together preconfigured virtual machine face_recognition opencv tensorflow lots deep learning tools preinstalled download run computer easily give virtual machine shot dont want install libraries original openface instructions liked article please consider signing machine learning fun newsletter also follow twitter ageitgey email directly find linkedin id love hear help team machine learning continue machine learning fun part 5 quick cheer standing ovation clap show much enjoyed story interested computers machine learning likes write,en,"['Português', 'Facebook', 'algorithm', 'Histogram of Oriented Gradients', 'HOG', 'CSI', 'Deep Convolutional Neural Network', 'Google', 'OpenFace', 'Jimmy Falon']"
15,Gil Fewster,3300,The mind-blowing AI announcement from Google that you probably missed.,"Disclaimer: I’m not an expert in neural networks or machine learning. Since originally writing this article, many people with far more expertise in these fields than myself have indicated that, while impressive, what Google have achieved is evolutionary, not revolutionary. In the very least, it’s fair to say that I’m guilty of anthropomorphising in parts of the text.
I’ve left the article’s content unchanged, because I think it’s interesting to compare the gut reaction I had with the subsequent comments of experts in the field. I strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own.
In the closing weeks of 2016, Google published an article that quietly sailed under most people’s radars. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year.
Don’t feel bad if you missed it. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System.
This doesn’t exactly scream must read, does it? Especially when you’ve got projects to wind up, gifts to buy, and family feuds to be resolved — all while the advent calendar relentlessly counts down the days until Christmas like some kind of chocolate-filled Yuletide doomsday clock.
Luckily, I’m here to bring you up to speed. Here’s the deal.
Up until September of last year, Google Translate used phrase-based translation. It basically did the same thing you and I do when we look up key words and phrases in our Lonely Planet language guides. It’s effective enough, and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the French equivalent of “please bring me all of your cheese and don’t stop until I fall over.” But it lacks nuance.
Phrase-based translation is a blunt instrument. It does the job well enough to get by. But mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results.
This approach is also limited by the extent of an available vocabulary. Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input.
All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). This new engine comes fully loaded with all the hot 2016 buzzwords, like neural network and machine learning.
The short version is that Google Translate got smart. It developed the ability to learn from the people who used it. It learned how to make educated guesses about the content, tone, and meaning of phrases based on the context of other words and phrases around them. And — here’s the bit that should make your brain explode — it got creative.
Google Translate invented its own language to help it translate more effectively.
What’s more, nobody told it to. It didn’t develop a language (or interlingua, as Google call it) because it was coded to. It developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation.
Stop and think about that for a moment. Let it sink in. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Without being told to do so. In a matter of weeks. (I’ve added a correction/retraction of this paragraph in the notes)
To understand what’s going on, we need to understand what zero-shot translation capability is. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. The GMNT is able to learn how to translate between two languages without being explicitly taught. This wouldn’t be possible in a phrase-based model, where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated.
And this leads the Google engineers onto that truly astonishing discovery of creation:
So there you have it. In the last weeks of 2016, as journos around the world started penning their “was this the worst year in living memory” thinkpieces, Google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics.
I just thought maybe you’d want to know.
Ok, to really understand what’s going on we probably need multiple computer science and linguistics degrees. I’m just barely scraping the surface here. If you’ve got time to get a few degrees (or if you’ve already got them) please drop me a line and explain it all me to. Slowly.
Update 1: in my excitement, it’s fair to say that I’ve exaggerated the idea of this as an ‘intelligent’ system — at least so far as we would think about human intelligence and decision making. Make sure you read Chris McDonald’s comment after the article for a more sober perspective.
Update 2: Nafrondel’s excellent, detailed reply is also a must read for an expert explanation of how neural networks function.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
A tinkerer
Our community publishes stories worth reading on development, design, and data science.
",disclaimer im expert neural networks machine learning since originally writing article many people far expertise fields indicated impressive google achieved evolutionary revolutionary least fair say im guilty anthropomorphising parts text ive left articles content unchanged think interesting compare gut reaction subsequent comments experts field strongly encourage readers browse comments reading article perspectives sober informed closing weeks 2016 google published article quietly sailed peoples radars shame may astonishing article machine learning read last year dont feel bad missed article competing prechristmas rush us navigating also tucked away googles research blog beneath geektastic headline zeroshot translation googles multilingual neural machine translation system doesnt exactly scream must read especially youve got projects wind gifts buy family feuds resolved advent calendar relentlessly counts days christmas like kind chocolatefilled yuletide doomsday clock luckily im bring speed heres deal september last year google translate used phrasebased translation basically thing look key words phrases lonely planet language guides effective enough blisteringly fast compared awkwardly thumbing way bunch pages looking french equivalent please bring cheese dont stop fall lacks nuance phrasebased translation blunt instrument job well enough get mapping roughly equivalent words phrases without understanding linguistic structures produce crude results approach also limited extent available vocabulary phrasebased translation capacity make educated guesses words doesnt recognize cant learn new input changed september google gave translation tool new engine google neural machine translation system gnmt new engine comes fully loaded hot 2016 buzzwords like neural network machine learning short version google translate got smart developed ability learn people used learned make educated guesses content tone meaning phrases based context words phrases around heres bit make brain explode got creative google translate invented language help translate effectively whats nobody told didnt develop language interlingua google call coded developed new language software determined time efficient way solve problem translation stop think moment let sink neural computing system designed translate content one human language another developed internal language make task efficient without told matter weeks ive added correctionretraction paragraph notes understand whats going need understand zeroshot translation capability heres googles mike schuster nikhil thorat melvin johnson original blog post see advantage googles new neural machine old phrasebased approach gmnt able learn translate two languages without explicitly taught wouldnt possible phrasebased model translation dependent upon explicit dictionary map words phrases pair languages translated leads google engineers onto truly astonishing discovery creation last weeks 2016 journos around world started penning worst year living memory thinkpieces google engineers quietly documenting genuinely astonishing breakthrough software engineering linguistics thought maybe youd want know ok really understand whats going probably need multiple computer science linguistics degrees im barely scraping surface youve got time get degrees youve already got please drop line explain slowly update 1 excitement fair say ive exaggerated idea intelligent system least far would think human intelligence decision making make sure read chris mcdonalds comment article sober perspective update 2 nafrondels excellent detailed reply also must read expert explanation neural networks function quick cheer standing ovation clap show much enjoyed story tinkerer community publishes stories worth reading development design data science,en,"['Google', 'Multilingual Neural Machine Translation System', 'Google Translate', 'the Google Neural Machine Translation', 'GMNT', 'Nafrondel']"
16,Adam Geitgey,10400,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in Italiano, Español, Français, Türkçe, Русский, 한국어 Português, فارسی, Tiếng Việt or 普通话.
In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven’t already read part 1, read it now!).
This time, we are going to see one of these generic algorithms do something really cool — create video game levels that look like they were made by humans. We’ll build a neural network, feed it existing Super Mario levels and watch new ones pop out!
Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:
We ended up with this simple estimation function:
In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house’s value.
Instead of using code, let’s represent that same function as a simple diagram:
However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn’t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn’t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?
To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:
Now we have four different price estimates. Let’s combine those four price estimates into one final estimate. We’ll run them through the same algorithm again (but using another set of weights)!
Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.
Let’s combine our four attempts to guess into one big diagram:
This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.
There’s a lot that I’m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:
It’s just like LEGO! We can’t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:
The neural network we’ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it’s a stateless algorithm.
In many cases (like estimating the price of house), that’s exactly what you want. But the one thing this kind of model can’t do is respond to patterns in data over time.
Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?
I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.
Our model might look like this:
But let’s make the problem harder. Let’s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.
Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next?
You probably guessed ’n’ — the word is probably going to be boxing. We know this based on the letters we’ve already seen in the sentence and our knowledge of common words in English. Also, the word ‘middleweight’ gives us an extra clue that we are talking about boxing.
In other words, it’s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.
To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.
Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.
This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.
Predicting the next letter in a story might seem pretty useless. What’s the point?
One cool use might be auto-predict for a mobile phone keyboard:
But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over — forever? We’d be asking it to write a complete story for us!
We saw how we could guess the next letter in Hemingway’s sentence. Let’s try generating a whole story in the style of Hemingway.
To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.
We’ll create our model from the complete text of The Sun Also Rises — 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway’s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.
As we just start to train the RNN, it’s not very good at predicting letters. Here’s what it generates after a 100 loops of training:
You can see that it has figured out that sometimes words have spaces between them, but that’s about it.
After about 1000 iterations, things are looking more promising:
The model has started to identify the patterns in basic sentence structure. It’s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there’s also still a lot of nonsense.
But after several thousand more training iterations, it looks pretty good:
At this point, the algorithm has captured the basic pattern of Hemingway’s short, direct dialog. A few sentences even sort of make sense.
Compare that with some real text from the book:
Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!
We don’t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.
For fun, let’s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of “Er”, “He”, and “The S”:
Not bad!
But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.
In 2015, Nintendo released Super Mario MakerTM for the Wii U gaming system.
This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It’s like a virtual LEGO set for people who grew up playing Super Mario Brothers.
Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?
First, we need a data set for training our model. Let’s take all the outdoor levels from the original Super Mario Brothers game released in 1985:
This game has 32 levels and about 70% of them have the same outdoor style. So we’ll stick to those.
To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game’s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game’s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.
Here’s the first level from the game (which you probably remember if you ever played it):
If we look closely, we can see the level is made of a simple grid of objects:
We could just as easily represent this grid as a sequence of characters with one character representing each object:
We’ve replaced each object in the level with a letter:
...and so on, using a different letter for each different kind of object in the level.
I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well.
To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up:
Just like we saw when creating the model of Hemingway’s prose, a model improves as we train it.
After a little training, our model is generating junk:
It sort of has an idea that ‘-’s and ‘=’s should show up a lot, but that’s about it. It hasn’t figured out the pattern yet.
After several thousand iterations, it’s starting to look like something:
The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the “P”s in the data should appear in 2x2 clusters. That’s pretty cool!
With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! There are several awesome things to notice:
Finally, let’s take this level and recreate it in Super Mario Maker:
Play it yourself!
If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9–0000–0157-F3C3.
The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‘toy’ instead of cutting-edge is that our model is generated from very little data. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model.
If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can’t — because Nintendo won’t let us have them. Big companies don’t give away their data for free.
As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That’s why companies like Google and Facebook need your data so badly!
For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate.
But without Google’s massive trove of data in every language, you can’t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you’ve ever been.
In machine learning, there’s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach.
Readers have sent me links to other interesting approaches to generating Super Mario levels:
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 3!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
",update article part series check full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 part 8 also read article italiano espanol francais turkce portugues tieng viet part 1 said machine learning using generic algorithms tell something interesting data without writing code specific problem solving havent already read part 1 read time going see one generic algorithms something really cool create video game levels look like made humans well build neural network feed existing super mario levels watch new ones pop like part 1 guide anyone curious machine learning idea start goal accessible anyone means theres lot generalizations skip lots details cares gets anyone interested ml mission accomplished back part 1 created simple algorithm estimated value house based attributes given data house like ended simple estimation function words estimated value house multiplying attributes weight added numbers get houses value instead using code lets represent function simple diagram however algorithm works simple problems result linear relationship input truth behind house prices isnt simple example maybe neighborhood matters lot big houses small houses doesnt matter mediumsized houses could capture kind complicated detail model clever could run algorithm multiple times different weights capture different edge cases four different price estimates lets combine four price estimates one final estimate well run algorithm using another set weights new super answer combines estimates four different attempts solve problem model cases could capture one simple model lets combine four attempts guess one big diagram neural network node knows take set inputs apply weights calculate output value chaining together lots nodes model complex functions theres lot im skipping keep brief including feature scaling activation function important part basic ideas click like lego cant model much one single lego block model anything enough basic lego blocks stick together neural network weve seen always returns answer give inputs memory programming terms stateless algorithm many cases like estimating price house thats exactly want one thing kind model cant respond patterns data time imagine handed keyboard asked write story start job guess first letter type letter guess use knowledge english increase odds guessing right letter example probably type letter common beginning words looked stories wrote past could narrow based words usually use beginning stories data could use build neural network model likely would start given letter model might look like lets make problem harder lets say need guess next letter going type point story much interesting problem lets use first words ernest hemingways sun also rises example letter going come next probably guessed n word probably going boxing know based letters weve already seen sentence knowledge common words english also word middleweight gives us extra clue talking boxing words easy guess next letter take account sequence letters came right combine knowledge rules english solve problem neural network need add state model time ask neural network answer also save set intermediate calculations reuse next time part input way model adjust predictions based input seen recently keeping track state model makes possible predict likely first letter story predict likely next letter given previous letters basic idea recurrent neural network updating network time use allows update predictions based saw recently even model patterns time long give enough memory predicting next letter story might seem pretty useless whats point one cool use might autopredict mobile phone keyboard took idea extreme asked model predict next likely character forever wed asking write complete story us saw could guess next letter hemingways sentence lets try generating whole story style hemingway going use recurrent neural network implementation andrej karpathy wrote andrej deeplearning researcher stanford wrote excellent introduction generating text rnns view code model github well create model complete text sun also rises 362239 characters using 84 unique letters including punctuation uppercaselowercase etc data set actually really small compared typical realworld applications generate really good model hemingways style would much better several times much sample text good enough play around example start train rnn good predicting letters heres generates 100 loops training see figured sometimes words spaces thats 1000 iterations things looking promising model started identify patterns basic sentence structure adding periods ends sentences even quoting dialog words recognizable theres also still lot nonsense several thousand training iterations looks pretty good point algorithm captured basic pattern hemingways short direct dialog sentences even sort make sense compare real text book even looking patterns one character time algorithm reproduced plausiblelooking prose proper formatting kind amazing dont generate text completely scratch either seed algorithm supplying first letters let find next letters fun lets make fake book cover imaginary book generating new author name new title using seed text er bad really mindblowing part algorithm figure patterns sequence data easily generate reallooking recipes fake obama speeches limit human language apply idea kind sequential data pattern 2015 nintendo released super mario makertm wii u gaming system game lets draw super mario brothers levels gamepad upload internet friends play include classic powerups enemies original mario games levels like virtual lego set people grew playing super mario brothers use model generated fake hemingway text generate fake super mario brothers levels first need data set training model lets take outdoor levels original super mario brothers game released 1985 game 32 levels 70 outdoor style well stick get designs level took original copy game wrote program pull level designs games memory super mario bros 30yearold game lots resources online help figure levels stored games memory extracting level data old video game fun programming exercise try sometime heres first level game probably remember ever played look closely see level made simple grid objects could easily represent grid sequence characters one character representing object weve replaced object level letter using different letter different kind object level ended text files looked like looking text file see mario levels dont really much pattern read linebyline patterns level really emerge think level series columns order algorithm find patterns data need feed data columnbycolumn figuring effective representation input data called feature selection one keys using machine learning algorithms well train model needed rotate text files 90 degrees made sure characters fed model order pattern would easily show like saw creating model hemingways prose model improves train little training model generating junk sort idea show lot thats hasnt figured pattern yet several thousand iterations starting look like something model almost figured line length even started figure logic mario pipes mario always two blocks wide least two blocks high ps data appear 2x2 clusters thats pretty cool lot training model gets point generates perfectly valid data lets sample entire levels worth data model rotate back horizontal data looks great several awesome things notice finally lets take level recreate super mario maker play super mario maker play level bookmarking online looking using level code 4ac900000157f3c3 recurrent neural network algorithm used train model kind algorithm used realworld companies solve hard problems like speech detection language translation makes model toy instead cuttingedge model generated little data arent enough levels original super mario brothers game provide enough data really good model could get access hundreds thousands usercreated super mario maker levels nintendo could make amazing model cant nintendo wont let us big companies dont give away data free machine learning becomes important industries difference good program bad program much data train models thats companies like google facebook need data badly example google recently open sourced tensorflow software toolkit building largescale machine learning applications pretty big deal google gave away important capable technology free stuff powers google translate without googles massive trove data every language cant create competitor google translate data gives google edge think next time open google maps location history facebook location history notice stores every place youve ever machine learning theres never single way solve problem limitless options deciding preprocess data algorithms use often combining multiple approaches give better results single approach readers sent links interesting approaches generating super mario levels liked article please consider signing machine learning fun email list ill email something new awesome share best way find write articles like also follow twitter ageitgey email directly find linkedin id love hear help team machine learning continue machine learning fun part 3 quick cheer standing ovation clap show much enjoyed story interested computers machine learning likes write,en,"['algorithm', 'Super Answer', 'LEGO', 'Stanford', 'Nintendo', 'Super', 'Super Mario Brothers', 'Super Mario Bros.', 'Google', 'Facebook', 'Google Translate']"
17,David Venturi,10600,"Every single Machine Learning course on the internet, ranked by your reviews","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master’s program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.
I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.
For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.
For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.
For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.
Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.
Each course must fit three criteria:
We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.
There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.
We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.
We made subjective syllabus judgment calls based on three factors:
A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives “computers the ability to learn without being explicitly programmed.” In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.
A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you’ll find helpful visualization of these core steps:
The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.
First off, let’s define deep learning. Here is a succinct description:
As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we’ve got you covered with the following article:
My top three recommendations from that list would be:
Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.
Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.
Stanford University’s Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.
Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.
Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.
Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:
Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn’t stop you from taking the course.
A few prominent reviewers noted the following:
Columbia University’s Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn’t have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.
The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia’s is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).
Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course’s total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.
Below are a few of the aforementioned sparkling reviews:
Machine Learning A-ZTM on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.
It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an “intuition” video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.
As a “bonus,” the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren’t the strong points of the course.
Eremenko and the SuperDataScience team are revered for their ability to “make the complex simple.” Also, the prerequisites listed are “just some high school mathematics,” so this course might be a better option for those daunted by the Stanford and Columbia offerings.
A few prominent reviewers noted the following:
Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let’s look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide — you can find those here.
The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.
Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.
Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla’s above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.
Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course’s description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.
Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Bite-sized videos, as is Udacity’s style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.
Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.
Data Science and Machine Learning with Python — Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.
Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): “Big data” focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.
Machine Learning Engineer Nanodegree (Udacity): Udacity’s flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.
Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech’s independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.
Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): “A real Caltech course, not a watered-down version.” Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn’t as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.
Mining Massive Datasets (Stanford University): Machine learning with a focus on “big data.” Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.
AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.
Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.
StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, “An Introduction to Statistical Learning, with Applications in R” and taught by the professors who wrote it. Reviewers note that the MOOC isn’t as good as the book, citing “thin” exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.
Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford’s or Caltech’s). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.
From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): “A down-to-earth, shy but confident take on machine learning techniques.” Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.
Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.
Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.
Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD’s Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.
Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity’s Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.
Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan’s Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.
Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.
Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.
Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU’s Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.
Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.
Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning — recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.
Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD’s Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.
Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW’s Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.
The following courses had one or no reviews as of May 2017.
Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.
Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.
Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.
Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.
Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.
Machine Learning for Data Science (University of California, San Diego/edX): Doesn’t launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.
Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.
Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise’s Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.
Introducción al Machine Learning (Universitas Telefónica/Miríada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.
Machine Learning Path Step (Dataquest): Taught in Python using Dataquest’s interactive in-browser platform. Multiple guided projects and a “plus” project where you build your own machine learning system using your own data. Subscription required.
The following six courses are offered by DataCamp. DataCamp’s hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.
Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.
Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.
Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.
Machine Learning Toolbox (DataCamp): Teaches the “big ideas” in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.
Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school’s budget. DataCamp’s “Supervised Learning with scikit-learn” is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.
Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.
Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon’s graduate introductory machine learning course. A prerequisite to their second graduate level course, “Statistical Machine Learning.” Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.
Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon’s Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.
Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).
Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas’ undergraduate course (above) apply here as well.
This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.
The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.
If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s Data Science and Big Data subject page.
If you enjoyed reading this, check out some of Class Central’s other pieces:
If you have suggestions for courses I missed, let me know in the responses!
If you found this helpful, click the 💚 so more people will see it here on Medium.
This is a condensed version of my original article published on Class Central, where I’ve included detailed course syllabi.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Curriculum Lead, Projects @ DataCamp. I created my own data science master’s program.
Our community publishes stories worth reading on development, design, and data science.
",year half ago dropped one best computer science programs canada started creating data science masters program using online resources realized could learn everything needed edx coursera udacity instead could learn faster efficiently fraction cost im almost finished ive taken many data sciencerelated courses audited portions many know options skills needed learners preparing data analyst data scientist role started creating reviewdriven guide recommends best courses subject within data science first guide series recommended coding classes beginner data scientist statistics probability classes introductions data science also data visualization guide spent dozen hours trying identify every online machine learning course offered may 2017 extracting key bits information syllabi reviews compiling ratings end goal identify three best courses available present task turned none open source class central community database thousands course ratings reviews since 2011 class central founder dhawal shah kept closer eye online courses arguably anyone else world dhawal personally helped assemble list resources course must fit three criteria believe covered every notable course fits criteria since seemingly hundreds courses udemy chose consider mostreviewed highestrated ones theres always chance missed something though please let us know comments section left good course compiled average ratings number reviews class central review sites calculate weighted average rating course read text reviews used feedback supplement numerical ratings made subjective syllabus judgment calls based three factors popular definition originates arthur samuel 1959 machine learning subfield computer science gives computers ability learn without explicitly programmed practice means developing computer programs make predictions based data humans learn experience computers data experience machine learning workflow process required carrying machine learning project though individual projects differ workflows share several common tasks problem evaluation data exploration data preprocessing model trainingtestingdeployment etc youll find helpful visualization core steps ideal course introduces entire process provides interactive examples assignments andor quizzes students perform task first lets define deep learning succinct description would expected portions machine learning courses contain deep learning content chose include deep learningonly courses however interested deep learning specifically weve got covered following article top three recommendations list would several courses listed ask students prior programming calculus linear algebra statistics experience prerequisites understandable given machine learning advanced discipline missing subjects good news experience acquired recommendations first two articles programming statistics data science career guide several topranked courses also provide gentle calculus linear algebra refreshers highlight aspects relevant machine learning less familiar stanford universitys machine learning coursera clear current winner terms ratings reviews syllabus fit taught famous andrew ng google brain founder former chief scientist baidu class sparked founding coursera 47star weighted average rating 422 reviews released 2011 covers aspects machine learning workflow though smaller scope original stanford class upon based still manages cover large number techniques algorithms estimated timeline eleven weeks two weeks dedicated neural networks deep learning free paid options available ng dynamic yet gentle instructor palpable experience inspires confidence especially sharing practical implementation tips warnings common pitfalls linear algebra refresher provided ng highlights aspects calculus relevant machine learning evaluation automatic done via multiple choice quizzes follow lesson programming assignments assignments eight completed matlab octave opensource version matlab ng explains language choice though python r likely compelling choices 2017 increased popularity languages reviewers note shouldnt stop taking course prominent reviewers noted following columbia universitys machine learning relatively new offering part artificial intelligence micromasters edx though newer doesnt large number reviews ones exceptionally strong professor john paisley noted brilliant clear clever 48star weighted average rating 10 reviews course also covers aspects machine learning workflow algorithms stanford offering columbias advanced introduction reviewers noting students comfortable recommended prerequisites calculus linear algebra statistics probability coding quizzes 11 programming assignments 4 final exam modes evaluation students use either python octave matlab complete assignments courses total estimated timeline eight ten hours per week twelve weeks free verified certificate available purchase aforementioned sparkling reviews machine learning aztm udemy impressively detailed offering provides instruction python r rare cant said top courses 45star weighted average rating 8119 reviews makes reviewed course ones considered covers entire machine learning workflow almost ridiculous good way number algorithms 405 hours ondemand video course takes applied approach lighter mathwise two courses section starts intuition video eremenko summarizes underlying theory concept taught de ponteves walks implementation separate videos python r bonus course includes python r code templates students download use projects quizzes homework challenges though arent strong points course eremenko superdatascience team revered ability make complex simple also prerequisites listed high school mathematics course might better option daunted stanford columbia offerings prominent reviewers noted following 1 pick weighted average rating 47 5 stars 422 reviews lets look alternatives sorted descending rating reminder deep learningonly courses included guide find analytics edge massachusetts institute technologyedx focused analytics general though cover several machine learning topics uses r strong narrative leverages familiar realworld examples challenging ten fifteen hours per week twelve weeks free verified certificate available purchase 49star weighted average rating 214 reviews python data science machine learning bootcamp jose portillaudemy large chunks machine learning content covers whole data science process detailed intro python amazing course though ideal scope guide 215 hours ondemand video cost varies depending udemy discounts frequent 46star weighted average rating 3316 reviews data science machine learning bootcamp r jose portillaudemy comments portillas course apply well except r 175 hours ondemand video cost varies depending udemy discounts frequent 46star weighted average rating 1317 reviews machine learning series lazy programmer incudemy taught data scientistbig data engineerfull stack software engineer impressive resume lazy programmer currently series 16 machine learningfocused courses udemy total courses 5000 ratings almost 46 stars useful course ordering provided individual courses description uses python cost varies depending udemy discounts frequent machine learning georgia techudacity compilation three separate courses supervised unsupervised reinforcement learning part udacitys machine learning engineer nanodegree georgia techs online masters degree oms bitesized videos udacitys style friendly professors estimated timeline four months free 456star weighted average rating 9 reviews implementing predictive analytics spark azure hdinsight microsoftedx introduces core concepts machine learning variety algorithms leverages several big datafriendly tools including apache spark scala hadoop uses python r four hours per week six weeks free verified certificate available purchase 45star weighted average rating 6 reviews data science machine learning python hands frank kaneudemy uses python kane nine years experience amazon imdb nine hours ondemand video cost varies depending udemy discounts frequent 45star weighted average rating 4139 reviews scala spark big data machine learning jose portillaudemy big data focus specifically implementation scala spark ten hours ondemand video cost varies depending udemy discounts frequent 45star weighted average rating 607 reviews machine learning engineer nanodegree udacity udacitys flagship machine learning program features bestinclass project review system career support program compilation several individual udacity courses free cocreated kaggle estimated timeline six months currently costs 199 usd per month 50 tuition refund available graduate within 12 months 45star weighted average rating 2 reviews learning data introductory machine learning california institute technologyedx enrollment currently closed edx also available via caltechs independent platform see 449star weighted average rating 42 reviews learning data introductory machine learning yaser abumostafacalifornia institute technology real caltech course watereddown version reviews note excellent understanding machine learning theory professor yaser abumostafa popular among students also wrote textbook upon course based videos taped lectures lectures slides pictureinpicture uploaded youtube homework assignments pdf files course experience online students isnt polished top three recommendations 443star weighted average rating 7 reviews mining massive datasets stanford university machine learning focus big data introduces modern distributed file systems mapreduce ten hours per week seven weeks free 44star weighted average rating 30 reviews aws machine learning complete guide python chandra lingamudemy unique focus cloudbased machine learning specifically amazon web services uses python nine hours ondemand video cost varies depending udemy discounts frequent 44star weighted average rating 62 reviews introduction machine learning face detection python holczer balazsudemy uses python eight hours ondemand video cost varies depending udemy discounts frequent 44star weighted average rating 162 reviews statlearning statistical learning stanford university based excellent textbook introduction statistical learning applications r taught professors wrote reviewers note mooc isnt good book citing thin exercises mediocre videos five hours per week nine weeks free 435star weighted average rating 84 reviews machine learning specialization university washingtoncoursera great courses last two classes including capstone project canceled reviewers note series digestable read easier without strong technical backgrounds top machine learning courses eg stanfords caltechs aware series incomplete recommender systems deep learning summary missing free paid options available 431star weighted average rating 80 reviews 0 1 machine learning nlp pythoncut chase loony cornudemy downtoearth shy confident take machine learning techniques taught fourperson team decades industry experience together uses python cost varies depending udemy discounts frequent 42star weighted average rating 494 reviews principles machine learning microsoftedx uses r python microsoft azure machine learning part microsoft professional program certificate data science three four hours per week six weeks free verified certificate available purchase 409star weighted average rating 11 reviews big data statistical inference machine learning queensland university technologyfuturelearn nice brief exploratory machine learning course focus big data covers tools like r h2o flow weka three weeks duration recommended two hours per week one reviewer noted six hours per week would appropriate free paid options available 4star weighted average rating 4 reviews genomic data science clustering bioinformatics v university california san diegocoursera interested intersection computer science biology represents important frontier modern science focuses clustering dimensionality reduction part ucsds bioinformatics specialization free paid options available 4star weighted average rating 3 reviews intro machine learning udacity prioritizes topic breadth practical tools python depth theory instructors sebastian thrun katie malone make class fun consists bitesized videos quizzes followed miniproject lesson currently part udacitys data analyst nanodegree estimated timeline ten weeks free 395star weighted average rating 19 reviews machine learning data analysis wesleyan universitycoursera brief intro machine learning select algorithms covers decision trees random forests lasso regression kmeans clustering part wesleyans data analysis interpretation specialization estimated timeline four weeks free paid options available 36star weighted average rating 5 reviews programming python data science microsoftedx produced microsoft partnership coding dojo uses python eight hours per week six weeks free paid options available 346star weighted average rating 37 reviews machine learning trading georgia techudacity focuses applying probabilistic machine learning approaches trading decisions uses python part udacitys machine learning engineer nanodegree georgia techs online masters degree oms estimated timeline four months free 329star weighted average rating 14 reviews practical machine learning johns hopkins universitycoursera brief practical introduction number machine learning algorithms several onetwostar reviews expressing variety concerns part jhus data science specialization four nine hours per week four weeks free paid options available 311star weighted average rating 37 reviews machine learning data science analytics columbia universityedx introduces wide range machine learning topics passionate negative reviews concerns including content choices lack programming assignments uninspiring presentation seven ten hours per week five weeks free verified certificate available purchase 274star weighted average rating 36 reviews recommender systems specialization university minnesotacoursera strong focus one specific type machine learning recommender systems four course specialization plus capstone project case study taught using lenskit opensource toolkit recommender systems free paid options available 2star weighted average rating 2 reviews machine learning big data university california san diegocoursera terrible reviews highlight poor instruction evaluation noted took mere hours complete whole course part ucsds big data specialization free paid options available 186star weighted average rating 14 reviews practical predictive analytics models methods university washingtoncoursera brief intro core machine learning concepts one reviewer noted lack quizzes assignments challenging part uws data science scale specialization six eight hours per week four weeks free paid options available 175star weighted average rating 4 reviews following courses one reviews may 2017 machine learning musicians artists goldsmiths university londonkadenze unique students learn algorithms software tools machine learning best practices make sense human gesture musical audio realtime data seven sessions length audit free premium 10 usd per month options available one 5star review applied machine learning python university michigancoursera taught using python scikit learn toolkit part applied data science python specialization scheduled start may 29th free paid options available applied machine learning microsoftedx taught using various tools including python r microsoft azure machine learning note microsoft produces course includes handson labs reinforce lecture content three four hours per week six weeks free verified certificate available purchase machine learning python big data university taught using python targeted towards beginners estimated completion time four hours big data university affiliated ibm free machine learning apache systemml big data university taught using apache systemml declarative style language designed largescale machine learning estimated completion time eight hours big data university affiliated ibm free machine learning data science university california san diegoedx doesnt launch january 2018 programming examples assignments python using jupyter notebooks eight hours per week ten weeks free verified certificate available purchase introduction analytics modeling georgia techedx course advertises r primary programming tool five ten hours per week ten weeks free verified certificate available purchase predictive analytics gaining insights big data queensland university technologyfuturelearn brief overview algorithms uses hewlett packard enterprises vertica analytics platform applied tool start date announced two hours per week four weeks free certificate achievement available purchase introduccion al machine learning universitas telefonicamiriada x taught spanish introduction machine learning covers supervised unsupervised learning total twenty estimated hours four weeks machine learning path step dataquest taught python using dataquests interactive inbrowser platform multiple guided projects plus project build machine learning system using data subscription required following six courses offered datacamp datacamps hybrid teaching style leverages video textbased instruction lots examples inbrowser code editor subscription required full access course introduction machine learning datacamp covers classification regression clustering algorithms uses r fifteen videos 81 exercises estimated timeline six hours supervised learning scikitlearn datacamp uses python scikitlearn covers classification regression algorithms seventeen videos 54 exercises estimated timeline four hours unsupervised learning r datacamp provides basic introduction clustering dimensionality reduction r sixteen videos 49 exercises estimated timeline four hours machine learning toolbox datacamp teaches big ideas machine learning uses r 24 videos 88 exercises estimated timeline four hours machine learning experts school budgets datacamp case study machine learning competition drivendata involves building model automatically classify items schools budget datacamps supervised learning scikitlearn prerequisite fifteen videos 51 exercises estimated timeline four hours unsupervised learning python datacamp covers variety unsupervised learning algorithms using python scikitlearn scipy course ends students building recommender system recommend popular musical artists thirteen videos 52 exercises estimated timeline four hours machine learning tom mitchellcarnegie mellon university carnegie mellons graduate introductory machine learning course prerequisite second graduate level course statistical machine learning taped university lectures practice problems homework assignments midterm solutions posted online 2011 version course also exists cmu one best graduate schools studying machine learning whole department dedicated ml free statistical machine learning larry wassermancarnegie mellon university likely advanced course guide followup carnegie mellons machine learning course taped university lectures practice problems homework assignments midterm solutions posted online free undergraduate machine learning nando de freitasuniversity british columbia undergraduate machine learning course lectures filmed put youtube slides posted course website course assignments posted well solutions though de freitas fulltime professor university oxford receives praise teaching abilities various forums graduate version available see machine learning nando de freitasuniversity british columbia graduate machine learning course comments de freitas undergraduate course apply well fifth sixpiece series covers best online courses launching data science field covered programming first article statistics probability second article intros data science third article data visualization fourth final piece summary articles plus best online courses key topics data wrangling databases even software engineering youre looking complete list data science online courses find class centrals data science big data subject page enjoyed reading check class centrals pieces suggestions courses missed let know responses found helpful click people see medium condensed version original article published class central ive included detailed course syllabi quick cheer standing ovation clap show much enjoyed story curriculum lead projects datacamp created data science masters program community publishes stories worth reading development design data science,en,"['Coursera', 'Udacity', 'Stanford University’s Machine Learning', 'Google Brain', 'Baidu', 'Stanford', 'MATLAB', 'Columbia University’s Machine Learning', 'Artificial Intelligence MicroMasters', 'Columbia', 'R', 'Python', 'The Analytics Edge', 'Massachusetts Institute of Technology/edX', 'Data Science and Machine Learning Bootcamp', 'Jose Portilla/Udemy', 'Udemy', 'Portilla', 'Georgia Tech/Udacity', 'Georgia Tech', 'Microsoft', 'Amazon', 'Spark for Big Data and Machine Learning', 'Kaggle', 'California Institute of Technology/edX', 'edX', 'CalTech', 'California Institute of Technology', 'Caltech', 'Stanford University', 'MapReduce', 'Chandra Lingam/Udemy', 'Amazon Web Services', 'StatLearning', 'University of Washington/Coursera', 'NLP & Python-Cut', 'Chase', 'Microsoft/edX', 'the Microsoft Professional Program Certificate', 'Queensland University of Technology/FutureLearn', 'WEKA', 'Genomic Data Science and Clustering', 'University of California, San Diego/Coursera', 'UCSD', 'Bioinformatics Specialization', 'Sebastian Thrun', 'Wesleyan University/Coursera', 'intro', 'Data Analysis and Interpretation Specialization', 'Johns Hopkins University/Coursera', 'JHU', 'Data Science Specialization', 'Columbia University/edX', 'Recommender Systems Specialization (University of Minnesota/Coursera', 'University of California', 'Big Data Specialization', 'Practical Predictive Analytics', 'UW', 'University of London/Kadenze', 'University of Michigan/Coursera', 'Python Specialization', 'Big Data University', 'IBM', 'Jupyter', 'Predictive Analytics', 'Big Data', 'Hewlett Packard Enterprise’s', 'Vertica Analytics', 'Certificate of Achievement', 'DataCamp', 'Carnegie Mellon University', 'Carnegie Mellon', 'CMU', 'Carnegie Mellon’s', 'Undergraduate Machine Learning', 'Nando de Freitas/University of British Columbia', 'the University of Oxford and receives', 'Data Science', 'Projects @ DataCamp']"
18,Eran Kampf,57,Data Mining — Handling Missing Values the Database – DeveloperZen,"I’ve recently answered Predicting missing data values in a database on StackOverflow and thought it deserved a mention on DeveloperZen.
One of the important stages of data mining is preprocessing, where we prepare the data for mining. Real-world data tends to be incomplete, noisy, and inconsistent and an important task when preprocessing the data is to fill in missing values, smooth out noise and correct inconsistencies.
If we specifically look at dealing with missing data, there are several techniques that can be used. Choosing the right technique is a choice that depends on the problem domain — the data’s domain (sales data? CRM data? ...) and our goal for the data mining process.
So how can you handle missing values in your database?
This is usually done when the class label is missing (assuming your data mining goal is classification), or many attributes are missing from the row (not just one). However, you’ll obviously get poor performance if the percentage of such rows is high.
For example, let’s say we have a database of students enrolment data (age, SAT score, state of residence, etc.) and a column classifying their success in college to “Low”, “Medium” and “High”. Let’s say our goal is to build a model predicting a student’s success in college. Data rows who are missing the success column are not useful in predicting success so they could very well be ignored and removed before running the algorithm.
Decide on a new global constant value, like “unknown“, “N/A” or minus infinity, that will be used to fill all the missing values. This technique is used because sometimes it just doesn’t make sense to try and predict the missing value.
For example, let’s look at the students enrollment database again. Assuming the state of residence attribute data is missing for some students. Filling it up with some state doesn’t really makes sense as opposed to using something like “N/A”.
Replace missing values of an attribute with the mean (or median if its discrete) value for that attribute in the database.
For example, in a database of US family incomes, if the average income of a US family is X you can use that value to replace missing income values.
Instead of using the mean (or median) of a certain attribute calculated by looking at all the rows in a database, we can limit the calculations to the relevant class to make the value more relevant to the row we’re looking at.
Let’s say you have a cars pricing database that, among other things, classifies cars to “Luxury” and “Low budget” and you’re dealing with missing values in the cost field. Replacing missing cost of a luxury car with the average cost of all luxury cars is probably more accurate than the value you’d get if you factor in the low budget cars.
The value can be determined using regression, inference based tools using Bayesian formalism, decision trees, clustering algorithms (K-Mean\Median etc.).
For example, we could use clustering algorithms to create clusters of rows which will then be used for calculating an attribute mean or median as specified in technique #3. Another example could be using a decision tree to try and predict the probable value in the missing attribute, according to other attributes in the data.
I’d suggest looking into regression and decision trees first (ID3 tree generation) as they’re relatively easy and there are plenty of examples on the net...
Additional Notes
Originally published at www.developerzen.com on August 14, 2009.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Maker of things. Big data geek. Food Lover.
The essence of Software Development ...
",ive recently answered predicting missing data values database stackoverflow thought deserved mention developerzen one important stages data mining preprocessing prepare data mining realworld data tends incomplete noisy inconsistent important task preprocessing data fill missing values smooth noise correct inconsistencies specifically look dealing missing data several techniques used choosing right technique choice depends problem domain datas domain sales data crm data goal data mining process handle missing values database usually done class label missing assuming data mining goal classification many attributes missing row one however youll obviously get poor performance percentage rows high example lets say database students enrolment data age sat score state residence etc column classifying success college low medium high lets say goal build model predicting students success college data rows missing success column useful predicting success could well ignored removed running algorithm decide new global constant value like unknown na minus infinity used fill missing values technique used sometimes doesnt make sense try predict missing value example lets look students enrollment database assuming state residence attribute data missing students filling state doesnt really makes sense opposed using something like na replace missing values attribute mean median discrete value attribute database example database us family incomes average income us family x use value replace missing income values instead using mean median certain attribute calculated looking rows database limit calculations relevant class make value relevant row looking lets say cars pricing database among things classifies cars luxury low budget youre dealing missing values cost field replacing missing cost luxury car average cost luxury cars probably accurate value youd get factor low budget cars value determined using regression inference based tools using bayesian formalism decision trees clustering algorithms kmeanmedian etc example could use clustering algorithms create clusters rows used calculating attribute mean median specified technique 3 another example could using decision tree try predict probable value missing attribute according attributes data id suggest looking regression decision trees first id3 tree generation theyre relatively easy plenty examples net additional notes originally published wwwdeveloperzencom august 14 2009 quick cheer standing ovation clap show much enjoyed story maker things big data geek food lover essence software development,en,['StackOverflow']
19,Oliver Lindberg,1,"Interview with Google’s Alfred Spector on voice search, hybrid intelligence and more","Google’s a pretty good search engine, right? Well, you ain’t seen nothing yet. VP of research Alfred Spector talks to Oliver Lindberg about the technologies emerging from Google Labs — from voice search to hybrid intelligence and beyond
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com.
Google has always been tight-lipped about products that haven’t launched yet. It’s no secret, however, that thanks to the company’s bottom-up culture, its engineers are working on tons of new projects at the same time. Following the mantra of ‘release early, release often’, the speed at which the search engine giant is churning out tools is staggering. At the heart of it all is Alfred Spector, Google’s Vice President of Research and Special Initiatives.
One of the areas Google is making significant advances in is voice search. Spector is astounded by how rapidly it’s come along. The Google Mobile App features ‘search by voice’ capabilities that are available for the iPhone, BlackBerry, Windows Mobile and Android. All versions understand English (including US, UK, Australian and Indian-English accents) but the latest addition, for Nokia S60 phones, even introduces Mandarin speech recognition, which — because of its many different accents and tonal characteristics — posed a huge engineering challenge. It’s the most spoken language in the world, but as it isn’t exactly keyboard-friendly, voice search could become immensely popular in China.
“Voice is one of these grand technology challenges in computer science,” Spector explains. “Can a computer understand the human voice? It’s been worked on for many decades and what we’ve realised over the last couple of years is that search, particularly on handheld devices, is amenable to voice as an import mechanism. “It’s very valuable to be able to use voice. All of us know that no matter how good the keyboard, it’s tricky to type exactly the right thing into a searchbar, while holding your backpack and everything else.”
To get a computer to take account of your voice is no mean feat, of course. “One idea is to take all of the voices that the system hears over time into one huge pan-human voice model. So, on the one hand we have a voice that’s higher and with an English accent, and on the other hand my voice, which is deeper and with an American accent. They both go into one model, or it just becomes personalised to the individual; voice scientists are a little unclear as to which is the best approach.”
The research department is also making progress in machine translation. Google Translate already features 51 languages, including Swahili and Yiddish. The latest version introduces instant, real-time translation, phonetic input and text-to-speech support (in English). “We’re able to go from any language to any of the others, and there are 51 times 50, so 2,550 possibilities,” Spector explains. “We’re focusing on increasing the number of languages because we’d like to handle even those languages where there’s not an enormous volume of usage. It will make the web far more valuable to more people if they can access the English-or Chinese language web, for example.
“But we also continue to focus on quality because almost always the translations are valuable but imperfect. Sometimes it comes from training our translation system over more raw data, so we have, say, EU documents in English and French and can compare them and learn rules for translation. The other approach is to bring more knowledge into translation. For example, we’re using more syntactic knowledge today and doing automated parsing with language. It’s been a grand challenge of the field since the late 1950s. Now it’s finally achieved mass usage.”
The team, led by scientist Franz Josef Och, has been collecting data for more than 100 languages, and the Google Translator Toolkit, which makes use of the ‘wisdom of the crowds’, now even supports 345 languages, many of which are minority languages. The editor enables users to translate text, correct the automatic translation and publish it.
Spector thinks that this approach is the future. As computers become even faster, handling more and more data — a lot of it in the cloud — machines learn from users and thus become smarter. He calls this concept ‘hybrid intelligence’. “It’s very difficult to solve these technological problems without human input,” he says. “It’s hard to create a robot that’s as clever, smart and knowledgeable of the world as we humans are. But it’s not as tough to build a computational system like Google, which extends what we do greatly and gradually learns something about the world from us, but that requires our interpretation to make it really successful. “We need to get computers and people communicating in both directions, so the computer learns from the human and makes the human more effective.”
Examples of ‘hybrid intelligence’ are Google Suggest, which instantly offers popular searches as you type a search query, and the ‘did you mean?’ feature in Google search, which corrects you when you misspell a query in the search bar. The more you use it, the better the system gets.
Training computers to become seemingly more intelligent poses major hurdles for Google’s engineers. “Computers don’t train as efficiently as people do,” Spector explains. “Let’s take the chess example. If a Kasparov was the educator, we could count on almost anything he says as being accurate. But if you tried to learn from a million chess players, you learn from my children as well, who play chess but they’re 10 and eight. They’ll be right sometimes and not right other times. There’s noise in that, and some of the noise is spam. One also has to have careful regard for privacy issues.”
By collecting enormous amounts of data, Google hopes to create a powerful database that eventually will understand the relationship between words (for example, ‘a dog is an animal’ and ‘a dog has four legs’). The challenge is to try to establish these relationships automatically, using tons of information, instead of having experts teach the system. This database would then improve search results and language translations because it would have a better understanding of the meaning of the words.
There’s also a lot of research around ‘conceptual search’. “Let’s take a video of a couple in front of the Empire State Building. We watch the video and it’s clear they’re on their honeymoon. But what is the video about? Is it about love or honeymoons, or is it about renting office space? It’s a fundamentally challenging problem.”
One example of conceptual search is Google Image Swirl, which was added to Labs in November. Enter a keyword and you get a list of 12 images; clicking on each one brings up a cluster of related pictures. Click on any of them to expand the ‘wonder wheel’ further. Google notes that they’re not just the most relevant images; the algorithm determines the most relevant group of images with similar appearance and meaning.
To improve the world’s data, Google continues to focus on the importance of the open internet. Another Labs project, Google Fusion Tables facilitates data management in the cloud. It enables users to create tables, filter and aggregate data, merge it with other data sources and visualise it with Google Maps or the Google Visualisation API. The data sets can then be published, shared or kept private and commented on by people around the world. “It’s an example of open collaboration,” Spector says. “If it’s public, we can crawl it to make it searchable and easily visible to people. We hired one of the best database researchers in the world, Alon Halevy, to lead it.”
Google is aiming to make more information available more easily across multiple devices, whether it’s images, videos, speech or maps, no matter which language we’re using. Spector calls the impact “totally transparent processing — it revolutionises the role of computation in day-today life. The computer can break down all these barriers to communication and knowledge. No matter what device we’re using, we have access to things. We can do translations, there are books or government documents, and some day we hope to have medical records. Whatever you want, no matter where you are, you can find it.”
Spector retired in early 2015 and now serves as the CTO of Two Sigma Investments
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com. Photography by Andy Short
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Independent editor and content consultant. Founder and captain of @pixelpioneers. Co-founder and curator of www.GenerateConf.com. Former editor of @netmag.
Interviews with leading tech entrepreneurs and web designers, conducted by @oliverlindberg at @netmag.
",googles pretty good search engine right well aint seen nothing yet vp research alfred spector talks oliver lindberg technologies emerging google labs voice search hybrid intelligence beyond article originally appeared issue 198 net magazine 2010 republished wwwtechradarcom google always tightlipped products havent launched yet secret however thanks companys bottomup culture engineers working tons new projects time following mantra release early release often speed search engine giant churning tools staggering heart alfred spector googles vice president research special initiatives one areas google making significant advances voice search spector astounded rapidly come along google mobile app features search voice capabilities available iphone blackberry windows mobile android versions understand english including us uk australian indianenglish accents latest addition nokia s60 phones even introduces mandarin speech recognition many different accents tonal characteristics posed huge engineering challenge spoken language world isnt exactly keyboardfriendly voice search could become immensely popular china voice one grand technology challenges computer science spector explains computer understand human voice worked many decades weve realised last couple years search particularly handheld devices amenable voice import mechanism valuable able use voice us know matter good keyboard tricky type exactly right thing searchbar holding backpack everything else get computer take account voice mean feat course one idea take voices system hears time one huge panhuman voice model one hand voice thats higher english accent hand voice deeper american accent go one model becomes personalised individual voice scientists little unclear best approach research department also making progress machine translation google translate already features 51 languages including swahili yiddish latest version introduces instant realtime translation phonetic input texttospeech support english able go language others 51 times 50 2550 possibilities spector explains focusing increasing number languages wed like handle even languages theres enormous volume usage make web far valuable people access englishor chinese language web example also continue focus quality almost always translations valuable imperfect sometimes comes training translation system raw data say eu documents english french compare learn rules translation approach bring knowledge translation example using syntactic knowledge today automated parsing language grand challenge field since late 1950s finally achieved mass usage team led scientist franz josef och collecting data 100 languages google translator toolkit makes use wisdom crowds even supports 345 languages many minority languages editor enables users translate text correct automatic translation publish spector thinks approach future computers become even faster handling data lot cloud machines learn users thus become smarter calls concept hybrid intelligence difficult solve technological problems without human input says hard create robot thats clever smart knowledgeable world humans tough build computational system like google extends greatly gradually learns something world us requires interpretation make really successful need get computers people communicating directions computer learns human makes human effective examples hybrid intelligence google suggest instantly offers popular searches type search query mean feature google search corrects misspell query search bar use better system gets training computers become seemingly intelligent poses major hurdles googles engineers computers dont train efficiently people spector explains lets take chess example kasparov educator could count almost anything says accurate tried learn million chess players learn children well play chess theyre 10 eight theyll right sometimes right times theres noise noise spam one also careful regard privacy issues collecting enormous amounts data google hopes create powerful database eventually understand relationship words example dog animal dog four legs challenge try establish relationships automatically using tons information instead experts teach system database would improve search results language translations would better understanding meaning words theres also lot research around conceptual search lets take video couple front empire state building watch video clear theyre honeymoon video love honeymoons renting office space fundamentally challenging problem one example conceptual search google image swirl added labs november enter keyword get list 12 images clicking one brings cluster related pictures click expand wonder wheel google notes theyre relevant images algorithm determines relevant group images similar appearance meaning improve worlds data google continues focus importance open internet another labs project google fusion tables facilitates data management cloud enables users create tables filter aggregate data merge data sources visualise google maps google visualisation api data sets published shared kept private commented people around world example open collaboration spector says public crawl make searchable easily visible people hired one best database researchers world alon halevy lead google aiming make information available easily across multiple devices whether images videos speech maps matter language using spector calls impact totally transparent processing revolutionises role computation daytoday life computer break barriers communication knowledge matter device using access things translations books government documents day hope medical records whatever want matter find spector retired early 2015 serves cto two sigma investments article originally appeared issue 198 net magazine 2010 republished wwwtechradarcom photography andy short quick cheer standing ovation clap show much enjoyed story independent editor content consultant founder captain pixelpioneers cofounder curator wwwgenerateconfcom former editor netmag interviews leading tech entrepreneurs web designers conducted oliverlindberg netmag,en,"['Google', 'Google Labs', 'Research and Special Initiatives', 'The Google Mobile App', 'iPhone', 'BlackBerry', 'Android', 'Nokia S60', 'pan-human', 'Google Translate', 'EU', 'Google Suggest', 'Kasparov', 'Labs', 'Alon Halevy', '@pixelpioneers', 'www', '@netmag', '@oliverlindberg']"
20,Netflix Technology Blog,439,Netflix Recommendations: Beyond the 5 stars (Part 1),"by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)
In this two-part blog post, we will open the doors of one of the most valued Netflix assets: our recommendation system. In Part 1, we will relate the Netflix Prize to the broader recommendation challenge, outline the external components of our personalized service, and highlight how our task has evolved with the business. In Part 2, we will describe some of the data and models that we use and discuss our approach to algorithmic innovation that combines offline machine learning experimentation with online AB testing. Enjoy... and remember that we are always looking for more star talent to add to our great team, so please take a look at our jobs page.
In 2006 we announced the Netflix Prize, a machine learning and data mining competition for movie rating prediction. We offered $1 million to whoever improved the accuracy of our existing system called Cinematch by 10%. We conducted this competition to find new ways to improve the recommendations we provide to our members, which is a key part of our business. However, we had to come up with a proxy question that was easier to evaluate and quantify: the root mean squared error (RMSE) of the predicted rating. The race was on to beat our RMSE of 0.9525 with the finish line of reducing it to 0.8572 or less.
A year into the competition, the Korbell team won the first Progress Prize with an 8.43% improvement. They reported more than 2000 hours of work in order to come up with the final combination of 107 algorithms that gave them this prize. And, they gave us the source code. We looked at the two underlying algorithms with the best performance in the ensemble: Matrix Factorization (which the community generally called SVD, Singular Value Decomposition) and Restricted Boltzmann Machines (RBM). SVD by itself provided a 0.8914 RMSE, while RBM alone provided a competitive but slightly worse 0.8990 RMSE. A linear blend of these two reduced the error to 0.88. To put these algorithms to use, we had to work to overcome some limitations, for instance that they were built to handle 100 million ratings, instead of the more than 5 billion that we have, and that they were not built to adapt as members added more ratings. But once we overcame those challenges, we put the two algorithms into production, where they are still used as part of our recommendation engine.
If you followed the Prize competition, you might be wondering what happened with the final Grand Prize ensemble that won the $1M two years later. This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line. We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. Also, our focus on improving Netflix personalization had shifted to the next level by then. In the remainder of this post we will explain how and why it has shifted.
One of the reasons our focus in the recommendation algorithms has changed is because Netflix as a whole has changed dramatically in the last few years. Netflix launched an instant streaming service in 2007, one year after the Netflix Prize began. Streaming has not only changed the way our members interact with the service, but also the type of data available to use in our algorithms. For DVDs our goal is to help people fill their queue with titles to receive in the mail over the coming days and weeks; selection is distant in time from viewing, people select carefully because exchanging a DVD for another takes more than a day, and we get no feedback during viewing. For streaming members are looking for something great to watch right now; they can sample a few videos before settling on one, they can consume several in one session, and we can observe viewing statistics such as whether a video was watched fully or only partially.
Another big change was the move from a single website into hundreds of devices. The integration with the Roku player and the Xbox were announced in 2008, two years into the Netflix competition. Just a year later, Netflix streaming made it into the iPhone. Now it is available on a multitude of devices that go from a myriad of Android devices to the latest AppleTV.
Two years ago, we went international with the launch in Canada. In 2011, we added 43 Latin-American countries and territories to the list. And just recently, we launched in UK and Ireland. Today, Netflix has more than 23 million subscribers in 47 countries. Those subscribers streamed 2 billion hours from hundreds of different devices in the last quarter of 2011. Every day they add 2 million movies and TV shows to the queue and generate 4 million ratings.
We have adapted our personalization algorithms to this new scenario in such a way that now 75% of what people watch is from some sort of recommendation. We reached this point by continuously optimizing the member experience and have measured significant gains in member satisfaction whenever we improved the personalization for our members. Let us now walk you through some of the techniques and approaches that we use to produce these recommendations.
We have discovered through the years that there is tremendous value to our subscribers in incorporating recommendations to personalize as much of Netflix as possible. Personalization starts on our homepage, which consists of groups of videos arranged in horizontal rows. Each row has a title that conveys the intended meaningful connection between the videos in that group. Most of our personalization is based on the way we select rows, how we determine what items to include in them, and in what order to place those items.
Take as a first example the Top 10 row: this is our best guess at the ten titles you are most likely to enjoy. Of course, when we say “you”, we really mean everyone in your household. It is important to keep in mind that Netflix’ personalization is intended to handle a household that is likely to have different people with different tastes. That is why when you see your Top10, you are likely to discover items for dad, mom, the kids, or the whole family. Even for a single person household we want to appeal to your range of interests and moods. To achieve this, in many parts of our system we are not only optimizing for accuracy, but also for diversity.
Another important element in Netflix’ personalization is awareness. We want members to be aware of how we are adapting to their tastes. This not only promotes trust in the system, but encourages members to give feedback that will result in better recommendations. A different way of promoting trust with the personalization component is to provide explanations as to why we decide to recommend a given movie or show. We are not recommending it because it suits our business needs, but because it matches the information we have from you: your explicit taste preferences and ratings, your viewing history, or even your friends’ recommendations.
On the topic of friends, we recently released our Facebook connect feature in 46 out of the 47 countries we operate — all but the US because of concerns with the VPPA law. Knowing about your friends not only gives us another signal to use in our personalization algorithms, but it also allows for different rows that rely mostly on your social circle to generate recommendations.
Some of the most recognizable personalization in our service is the collection of “genre” rows. These range from familiar high-level categories like “Comedies” and “Dramas” to highly tailored slices such as “Imaginative Time Travel Movies from the 1980s”. Each row represents 3 layers of personalization: the choice of genre itself, the subset of titles selected within that genre, and the ranking of those titles. Members connect with these rows so well that we measure an increase in member retention by placing the most tailored rows higher on the page instead of lower. As with other personalization elements, freshness and diversity is taken into account when deciding what genres to show from the thousands possible.
We present an explanation for the choice of rows using a member’s implicit genre preferences — recent plays, ratings, and other interactions — , or explicit feedback provided through our taste preferences survey. We will also invite members to focus a row with additional explicit preference feedback when this is lacking.
Similarity is also an important source of personalization in our service. We think of similarity in a very broad sense; it can be between movies or between members, and can be in multiple dimensions such as metadata, ratings, or viewing data. Furthermore, these similarities can be blended and used as features in other models. Similarity is used in multiple contexts, for example in response to a member’s action such as searching or adding a title to the queue. It is also used to generate rows of “adhoc genres” based on similarity to titles that a member has interacted with recently. If you are interested in a more in-depth description of the architecture of the similarity system, you can read about it in this past post on the blog.
In most of the previous contexts — be it in the Top10 row, the genres, or the similars — ranking, the choice of what order to place the items in a row, is critical in providing an effective personalized experience. The goal of our ranking system is to find the best possible ordering of a set of items for a member, within a specific context, in real-time. We decompose ranking into scoring, sorting, and filtering sets of movies for presentation to a member. Our business objective is to maximize member satisfaction and month-to-month subscription retention, which correlates well with maximizing consumption of video content. We therefore optimize our algorithms to give the highest scores to titles that a member is most likely to play and enjoy.
Now it is clear that the Netflix Prize objective, accurate prediction of a movie’s rating, is just one of the many components of an effective recommendation system that optimizes our members enjoyment. We also need to take into account factors such as context, title popularity, interest, evidence, novelty, diversity, and freshness. Supporting all the different contexts in which we want to make recommendations requires a range of algorithms that are tuned to the needs of those contexts. In the next part of this post, we will talk in more detail about the ranking problem. We will also dive into the data and models that make all the above possible and discuss our approach to innovating in this space.
On to part 2:
Originally published at techblog.netflix.com on April 6, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
",xavier amatriain justin basilico personalization science engineering twopart blog post open doors one valued netflix assets recommendation system part 1 relate netflix prize broader recommendation challenge outline external components personalized service highlight task evolved business part 2 describe data models use discuss approach algorithmic innovation combines offline machine learning experimentation online ab testing enjoy remember always looking star talent add great team please take look jobs page 2006 announced netflix prize machine learning data mining competition movie rating prediction offered 1 million whoever improved accuracy existing system called cinematch 10 conducted competition find new ways improve recommendations provide members key part business however come proxy question easier evaluate quantify root mean squared error rmse predicted rating race beat rmse 09525 finish line reducing 08572 less year competition korbell team first progress prize 843 improvement reported 2000 hours work order come final combination 107 algorithms gave prize gave us source code looked two underlying algorithms best performance ensemble matrix factorization community generally called svd singular value decomposition restricted boltzmann machines rbm svd provided 08914 rmse rbm alone provided competitive slightly worse 08990 rmse linear blend two reduced error 088 put algorithms use work overcome limitations instance built handle 100 million ratings instead 5 billion built adapt members added ratings overcame challenges put two algorithms production still used part recommendation engine followed prize competition might wondering happened final grand prize ensemble 1m two years later truly impressive compilation culmination years work blending hundreds predictive models finally cross finish line evaluated new methods offline additional accuracy gains measured seem justify engineering effort needed bring production environment also focus improving netflix personalization shifted next level remainder post explain shifted one reasons focus recommendation algorithms changed netflix whole changed dramatically last years netflix launched instant streaming service 2007 one year netflix prize began streaming changed way members interact service also type data available use algorithms dvds goal help people fill queue titles receive mail coming days weeks selection distant time viewing people select carefully exchanging dvd another takes day get feedback viewing streaming members looking something great watch right sample videos settling one consume several one session observe viewing statistics whether video watched fully partially another big change move single website hundreds devices integration roku player xbox announced 2008 two years netflix competition year later netflix streaming made iphone available multitude devices go myriad android devices latest appletv two years ago went international launch canada 2011 added 43 latinamerican countries territories list recently launched uk ireland today netflix 23 million subscribers 47 countries subscribers streamed 2 billion hours hundreds different devices last quarter 2011 every day add 2 million movies tv shows queue generate 4 million ratings adapted personalization algorithms new scenario way 75 people watch sort recommendation reached point continuously optimizing member experience measured significant gains member satisfaction whenever improved personalization members let us walk techniques approaches use produce recommendations discovered years tremendous value subscribers incorporating recommendations personalize much netflix possible personalization starts homepage consists groups videos arranged horizontal rows row title conveys intended meaningful connection videos group personalization based way select rows determine items include order place items take first example top 10 row best guess ten titles likely enjoy course say really mean everyone household important keep mind netflix personalization intended handle household likely different people different tastes see top10 likely discover items dad mom kids whole family even single person household want appeal range interests moods achieve many parts system optimizing accuracy also diversity another important element netflix personalization awareness want members aware adapting tastes promotes trust system encourages members give feedback result better recommendations different way promoting trust personalization component provide explanations decide recommend given movie show recommending suits business needs matches information explicit taste preferences ratings viewing history even friends recommendations topic friends recently released facebook connect feature 46 47 countries operate us concerns vppa law knowing friends gives us another signal use personalization algorithms also allows different rows rely mostly social circle generate recommendations recognizable personalization service collection genre rows range familiar highlevel categories like comedies dramas highly tailored slices imaginative time travel movies 1980s row represents 3 layers personalization choice genre subset titles selected within genre ranking titles members connect rows well measure increase member retention placing tailored rows higher page instead lower personalization elements freshness diversity taken account deciding genres show thousands possible present explanation choice rows using members implicit genre preferences recent plays ratings interactions explicit feedback provided taste preferences survey also invite members focus row additional explicit preference feedback lacking similarity also important source personalization service think similarity broad sense movies members multiple dimensions metadata ratings viewing data furthermore similarities blended used features models similarity used multiple contexts example response members action searching adding title queue also used generate rows adhoc genres based similarity titles member interacted recently interested indepth description architecture similarity system read past post blog previous contexts top10 row genres similars ranking choice order place items row critical providing effective personalized experience goal ranking system find best possible ordering set items member within specific context realtime decompose ranking scoring sorting filtering sets movies presentation member business objective maximize member satisfaction monthtomonth subscription retention correlates well maximizing consumption video content therefore optimize algorithms give highest scores titles member likely play enjoy clear netflix prize objective accurate prediction movies rating one many components effective recommendation system optimizes members enjoyment also need take account factors context title popularity interest evidence novelty diversity freshness supporting different contexts want make recommendations requires range algorithms tuned needs contexts next part post talk detail ranking problem also dive data models make possible discuss approach innovating space part 2 originally published techblognetflixcom april 6 2012 quick cheer standing ovation clap show much enjoyed story learn netflix designs builds operates systems engineering organizations learn netflixs world class engineering efforts company culture product developments,en,"['AB', 'quantify', 'RMSE', 'Korbell', 'Matrix Factorization', 'SVD', 'Restricted Boltzmann Machines', 'Netflix', 'iPhone', 'Android', 'techblog.netflix.com']"
21,Netflix Technology Blog,365,Netflix Recommendations: Beyond the 5 stars (Part 2),"by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)
In part one of this blog post, we detailed the different components of Netflix personalization. We also explained how Netflix personalization, and the service as a whole, have changed from the time we announced the Netflix Prize.
The $1M Prize delivered a great return on investment for us, not only in algorithmic innovation, but also in brand awareness and attracting stars (no pun intended) to join our team. Predicting movie ratings accurately is just one aspect of our world-class recommender system. In this second part of the blog post, we will give more insight into our broader personalization technology. We will discuss some of our current models, data, and the approaches we follow to lead innovation and research in this space.
The goal of recommender systems is to present a number of attractive items for a person to choose from. This is usually accomplished by selecting some items and sorting them in the order of expected enjoyment (or utility). Since the most common way of presenting recommended items is in some form of list, such as the various rows on Netflix, we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the items for each of our members.
If you are looking for a ranking function that optimizes consumption, an obvious baseline is item popularity. The reason is clear: on average, a member is most likely to watch what most others are watching. However, popularity is the opposite of personalization: it will produce the same ordering of items for every member. Thus, the goal becomes to find a personalized ranking function that is better than item popularity, so we can better satisfy members with varying tastes.
Recall that our goal is to recommend the titles that each member is most likely to play and enjoy. One obvious way to approach this is to use the member’s predicted rating of each item as an adjunct to item popularity. Using predicted ratings on their own as a ranking function can lead to items that are too niche or unfamiliar being recommended, and can exclude items that the member would want to watch even though they may not rate them highly. To compensate for this, rather than using either popularity or predicted rating on their own, we would like to produce rankings that balance both of these aspects. At this point, we are ready to build a ranking prediction model using these two features.
There are many ways one could construct a ranking function ranging from simple scoring methods, to pairwise preferences, to optimization over the entire ranking. For the purposes of illustration, let us start with a very simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating. This gives an equation of the form frank(u,v) = w1 p(v) + w2 r(u,v) + b, where u=user, v=video item, p=popularity and r=predicted rating. This equation defines a two-dimensional space like the one depicted below.
Once we have such a function, we can pass a set of videos through our function and sort them in descending order according to the score. You might be wondering how we can set the weights w1 and w2 in our model (the bias b is constant and thus ends up not affecting the final ordering). In other words, in our simple two-dimensional model, how do we determine whether popularity is more or less important than predicted rating? There are at least two possible approaches to this. You could sample the space of possible weights and let the members decide what makes sense after many A/B tests. This procedure might be time consuming and not very cost effective. Another possible answer involves formulating this as a machine learning problem: select positive and negative examples from your historical data and let a machine learning algorithm learn the weights that optimize your goal. This family of machine learning problems is known as “Learning to rank” and is central to application scenarios such as search engines or ad targeting. Note though that a crucial difference in the case of ranked recommendations is the importance of personalization: we do not expect a global notion of relevance, but rather look for ways of optimizing a personalized model.
As you might guess, apart from popularity and rating prediction, we have tried many other features at Netflix. Some have shown no positive effect while others have improved our ranking accuracy tremendously. The graph below shows the ranking improvement we have obtained by adding different features and optimizing the machine learning algorithm.
Many supervised classification methods can be used for ranking. Typical choices include Logistic Regression, Support Vector Machines, Neural Networks, or Decision Tree-based methods such as Gradient Boosted Decision Trees (GBDT). On the other hand, a great number of algorithms specifically designed for learning to rank have appeared in recent years such as RankSVM or RankBoost. There is no easy answer to choose which model will perform best in a given ranking problem. The simpler your feature space is, the simpler your model can be. But it is easy to get trapped in a situation where a new feature does not show value because the model cannot learn it. Or, the other way around, to conclude that a more powerful model is not useful simply because you don’t have the feature space that exploits its benefits.
The previous discussion on the ranking algorithms highlights the importance of both data and models in creating an optimal personalized experience for our members. At Netflix, we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn data into product features. Here are some of the data sources we can use to optimize our recommendations:
So, what about the models? One thing we have found at Netflix is that with the great availability of data, both in quantity and types, a thoughtful approach is required to model selection, training, and testing. We use all sorts of machine learning approaches: From unsupervised methods such as clustering algorithms to a number of supervised classifiers that have shown optimal results in various contexts. This is an incomplete list of methods you should probably know about if you are working in machine learning for personalization:
Consumer Data Science
The abundance of source data, measurements and associated experiments allow us to operate a data-driven organization. Netflix has embedded this approach into its culture since the company was founded, and we have come to call it Consumer (Data) Science. Broadly speaking, the main goal of our Consumer Science approach is to innovate for members effectively. The only real failure is the failure to innovate; or as Thomas Watson Sr, founder of IBM, put it: “If you want to increase your success rate, double your failure rate.” We strive for an innovation culture that allows us to evaluate ideas rapidly, inexpensively, and objectively. And, once we test something we want to understand why it failed or succeeded. This lets us focus on the central goal of improving our service for our members.
So, how does this work in practice? It is a slight variation over the traditional scientific process called A/B testing (or bucket testing):
When we execute A/B tests, we track many different metrics. But we ultimately trust member engagement (e.g. hours of play) and retention. Tests usually have thousands of members and anywhere from 2 to 20 cells exploring variations of a base idea. We typically have scores of A/B tests running in parallel. A/B tests let us try radical ideas or test many approaches at the same time, but the key advantage is that they allow our decisions to be data-driven. You can read more about our approach to A/B Testing in this previous tech blog post or in some of the Quora answers by our Chief Product Officer Neil Hunt.
An interesting follow-up question that we have faced is how to integrate our machine learning approaches into this data-driven A/B test culture at Netflix. We have done this with an offline-online testing process that tries to combine the best of both worlds. The offline testing cycle is a step where we test and optimize our algorithms prior to performing online A/B testing. To measure model performance offline we track multiple metrics used in the machine learning community: from ranking measures such as normalized discounted cumulative gain, mean reciprocal rank, or fraction of concordant pairs, to classification metrics such as accuracy, precision, recall, or F-score. We also use the famous RMSE from the Netflix Prize or other more exotic metrics to track different aspects like diversity. We keep track of how well those metrics correlate to measurable online gains in our A/B tests. However, since the mapping is not perfect, offline performance is used only as an indication to make informed decisions on follow up tests.
Once offline testing has validated a hypothesis, we are ready to design and launch the A/B test that will prove the new feature valid from a member perspective. If it does, we will be ready to roll out in our continuous pursuit of the better product for our members. The diagram below illustrates the details of this process.
An extreme example of this innovation cycle is what we called the Top10 Marathon. This was a focused, 10-week effort to quickly test dozens of algorithmic ideas related to improving our Top10 row. Think of it as a 2-month hackathon with metrics. Different teams and individuals were invited to contribute ideas and code in this effort. We rolled out 6 different ideas as A/B tests each week and kept track of the offline and online metrics. The winning results are already part of our production system.
The Netflix Prize abstracted the recommendation problem to a proxy question of predicting ratings. But member ratings are only one of the many data sources we have and rating predictions are only part of our solution. Over time we have reformulated the recommendation problem to the question of optimizing the probability a member chooses to watch a title and enjoys it enough to come back to the service. More data availability enables better results. But in order to get those results, we need to have optimized approaches, appropriate metrics and rapid experimentation.
To excel at innovating personalization, it is insufficient to be methodical in our research; the space to explore is virtually infinite. At Netflix, we love choosing and watching movies and TV shows. We focus our research by translating this passion into strong intuitions about fruitful directions to pursue; under-utilized data sources, better feature representations, more appropriate models and metrics, and missed opportunities to personalize. We use data mining and other experimental approaches to incrementally inform our intuition, and so prioritize investment of effort. As with any scientific pursuit, there’s always a contribution from Lady Luck, but as the adage goes, luck favors the prepared mind. Finally, above all, we look to our members as the final judges of the quality of our recommendation approach, because this is all ultimately about increasing our members’ enjoyment in their own Netflix experience. We are always looking for more people to join our team of “prepared minds”. Make sure you take a look at our jobs page.
Originally published at techblog.netflix.com on June 20, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
",xavier amatriain justin basilico personalization science engineering part one blog post detailed different components netflix personalization also explained netflix personalization service whole changed time announced netflix prize 1m prize delivered great return investment us algorithmic innovation also brand awareness attracting stars pun intended join team predicting movie ratings accurately one aspect worldclass recommender system second part blog post give insight broader personalization technology discuss current models data approaches follow lead innovation research space goal recommender systems present number attractive items person choose usually accomplished selecting items sorting order expected enjoyment utility since common way presenting recommended items form list various rows netflix need appropriate ranking model use wide variety information come optimal ranking items members looking ranking function optimizes consumption obvious baseline item popularity reason clear average member likely watch others watching however popularity opposite personalization produce ordering items every member thus goal becomes find personalized ranking function better item popularity better satisfy members varying tastes recall goal recommend titles member likely play enjoy one obvious way approach use members predicted rating item adjunct item popularity using predicted ratings ranking function lead items niche unfamiliar recommended exclude items member would want watch even though may rate highly compensate rather using either popularity predicted rating would like produce rankings balance aspects point ready build ranking prediction model using two features many ways one could construct ranking function ranging simple scoring methods pairwise preferences optimization entire ranking purposes illustration let us start simple scoring approach choosing ranking function linear combination popularity predicted rating gives equation form frankuv w1 pv w2 ruv b uuser vvideo item ppopularity rpredicted rating equation defines twodimensional space like one depicted function pass set videos function sort descending order according score might wondering set weights w1 w2 model bias b constant thus ends affecting final ordering words simple twodimensional model determine whether popularity less important predicted rating least two possible approaches could sample space possible weights let members decide makes sense many ab tests procedure might time consuming cost effective another possible answer involves formulating machine learning problem select positive negative examples historical data let machine learning algorithm learn weights optimize goal family machine learning problems known learning rank central application scenarios search engines ad targeting note though crucial difference case ranked recommendations importance personalization expect global notion relevance rather look ways optimizing personalized model might guess apart popularity rating prediction tried many features netflix shown positive effect others improved ranking accuracy tremendously graph shows ranking improvement obtained adding different features optimizing machine learning algorithm many supervised classification methods used ranking typical choices include logistic regression support vector machines neural networks decision treebased methods gradient boosted decision trees gbdt hand great number algorithms specifically designed learning rank appeared recent years ranksvm rankboost easy answer choose model perform best given ranking problem simpler feature space simpler model easy get trapped situation new feature show value model cannot learn way around conclude powerful model useful simply dont feature space exploits benefits previous discussion ranking algorithms highlights importance data models creating optimal personalized experience members netflix fortunate many relevant data sources smart people select optimal algorithms turn data product features data sources use optimize recommendations models one thing found netflix great availability data quantity types thoughtful approach required model selection training testing use sorts machine learning approaches unsupervised methods clustering algorithms number supervised classifiers shown optimal results various contexts incomplete list methods probably know working machine learning personalization consumer data science abundance source data measurements associated experiments allow us operate datadriven organization netflix embedded approach culture since company founded come call consumer data science broadly speaking main goal consumer science approach innovate members effectively real failure failure innovate thomas watson sr founder ibm put want increase success rate double failure rate strive innovation culture allows us evaluate ideas rapidly inexpensively objectively test something want understand failed succeeded lets us focus central goal improving service members work practice slight variation traditional scientific process called ab testing bucket testing execute ab tests track many different metrics ultimately trust member engagement eg hours play retention tests usually thousands members anywhere 2 20 cells exploring variations base idea typically scores ab tests running parallel ab tests let us try radical ideas test many approaches time key advantage allow decisions datadriven read approach ab testing previous tech blog post quora answers chief product officer neil hunt interesting followup question faced integrate machine learning approaches datadriven ab test culture netflix done offlineonline testing process tries combine best worlds offline testing cycle step test optimize algorithms prior performing online ab testing measure model performance offline track multiple metrics used machine learning community ranking measures normalized discounted cumulative gain mean reciprocal rank fraction concordant pairs classification metrics accuracy precision recall fscore also use famous rmse netflix prize exotic metrics track different aspects like diversity keep track well metrics correlate measurable online gains ab tests however since mapping perfect offline performance used indication make informed decisions follow tests offline testing validated hypothesis ready design launch ab test prove new feature valid member perspective ready roll continuous pursuit better product members diagram illustrates details process extreme example innovation cycle called top10 marathon focused 10week effort quickly test dozens algorithmic ideas related improving top10 row think 2month hackathon metrics different teams individuals invited contribute ideas code effort rolled 6 different ideas ab tests week kept track offline online metrics winning results already part production system netflix prize abstracted recommendation problem proxy question predicting ratings member ratings one many data sources rating predictions part solution time reformulated recommendation problem question optimizing probability member chooses watch title enjoys enough come back service data availability enables better results order get results need optimized approaches appropriate metrics rapid experimentation excel innovating personalization insufficient methodical research space explore virtually infinite netflix love choosing watching movies tv shows focus research translating passion strong intuitions fruitful directions pursue underutilized data sources better feature representations appropriate models metrics missed opportunities personalize use data mining experimental approaches incrementally inform intuition prioritize investment effort scientific pursuit theres always contribution lady luck adage goes luck favors prepared mind finally look members final judges quality recommendation approach ultimately increasing members enjoyment netflix experience always looking people join team prepared minds make sure take look jobs page originally published techblognetflixcom june 20 2012 quick cheer standing ovation clap show much enjoyed story learn netflix designs builds operates systems engineering organizations learn netflixs world class engineering efforts company culture product developments,en,"['Netflix', 'algorithm', 'Logistic Regression', 'Support Vector Machines', 'Neural Networks', 'Gradient Boosted Decision Trees', 'GBDT', 'RankBoost', 'Consumer Science', 'IBM', 'techblog.netflix.com']"
22,Wolf Garbe,6,1000x Faster Spelling Correction algorithm (2012) – Wolf Garbe – Medium,"Update1: An improved SymSpell implementation is now 1,000,000x faster.Update2: SymSpellCompound with Compound aware spelling correction. Update3: Benchmark of SymSpell, BK-Tree und Norvig’s spell-correct.
Recently I answered a question on Quora about spelling correction for search engines. When I described our SymSpell algorithm I was pointed to Peter Norvig’s page where he outlined his approach.
Both algorithms are based on Edit distance (Damerau-Levenshtein distance). Both try to find the dictionary entries with smallest edit distance from the query term.
If the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But SymSpell uses a different way to search the dictionary, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary:
1. Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.
Source: Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze: Introduction to Information Retrieval.
The performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of 2 or 3 has been reached.
2. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.
Source: Peter Norvig: How to Write a Spelling Corrector.
This is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)
3. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step. Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.
This is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes). The cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.
The number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.
Remark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn. While we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)
Remark 2: There are four different comparison pair types:
The last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).
Remark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:
Remark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.
ReasoningThe SymSpell algorithm exploits the fact that the edit distance between two terms is symmetrical:
We are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.
Where does the speed come from?
Computational Complexity The SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).
Comparison to other approaches BK-Trees have a search time of O(log dictionary_size), whereas the SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size. Tries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.
Application Possible application fields of the SymSpell algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.
Source codeThe C# implementation of the Symmetric Delete Spelling Correction algorithm is released on GitHub as Open Source under the MIT License:https://github.com/wolfgarbe/symspell
PortsThere are ports in C++, Crystal, Go, Java, Javascript, Python, Ruby, Rust, Scala, Swift available.
Originally published at blog.faroo.com on June 7, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder SeekStorm (Search-as-a-Service), FAROO (P2P Search) http://www.seekstorm.com https://github.com/wolfgarbe https://www.quora.com/profile/Wolf-Garbe
",update1 improved symspell implementation 1000000x fasterupdate2 symspellcompound compound aware spelling correction update3 benchmark symspell bktree und norvigs spellcorrect recently answered question quora spelling correction search engines described symspell algorithm pointed peter norvigs page outlined approach algorithms based edit distance dameraulevenshtein distance try find dictionary entries smallest edit distance query term edit distance 0 term spelled correctly edit distance 2 dictionary term used spelling suggestion symspell uses different way search dictionary resulting significant performance gain language independence three ways search minimum edit distance dictionary 1 naive approachthe obvious way compute edit distance query term dictionary term selecting strings minimum edit distance spelling suggestion exhaustive search inordinately expensive source christopher manning prabhakar raghavan hinrich schutze introduction information retrieval performance significantly improved terminating edit distance calculation soon threshold 2 3 reached 2 peter norviggenerate possible terms edit distance deletes transposes replaces inserts query term search dictionary word length n alphabet size edit distance d1 n deletions n1 transpositions alterations an1 insertions total 2n2ana1 terms search time source peter norvig write spelling corrector much better naive approach still expensive search time 114324 terms n9 a36 d2 language dependent alphabet used generate terms different many languages huge chinese a70000 unicode han characters 3 symmetric delete spelling correction symspell generate terms edit distance deletes dictionary term add together original term dictionary done precalculation step generate terms edit distance deletes input term search dictionary word length n alphabet size edit distance 1 n deletions total n terms search time three orders magnitude less expensive 36 terms n9 d2 language independent alphabet required generate deletes cost approach precalculation time storage space x deletes every original dictionary entry acceptable cases number x deletes single dictionary entry depends maximum edit distance xn edit distance1 xnn12 edit distance2 xndnd edit distanced combinatorics k n combinations without repetitions knd eg maximum edit distance 2 average word length 5 100000 dictionary entries need additionally store 1500000 deletes remark 1 precalculation different words dictionary might lead delete term deletesun1deletesin1sn generate one new dictionary entry sn inside need store original terms spelling correction suggestion sunsin remark 2 four different comparison pair types last comparison type required replaces transposes need check whether suggested dictionary term really replace adjacent transpose input term prevent false positives higher edit distance bankbnak bankbink bankkanb bankxban bankbaxn remark 3 instead dedicated spelling dictionary using search engine index several benefits remark 4 implemented query suggestionscompletion similar fashion good way prevent spelling errors first place every newly indexed word whose frequency certain threshold stored suggestion prefixes created index yet exist anyway provide instant search feature lookup suggestions comes also almost extra cost multiple terms sorted number results stored index reasoningthe symspell algorithm exploits fact edit distance two terms symmetrical using variant 3 deleteonlytransformation language independent three orders magnitude less expensive speed come computational complexity symspell algorithm constant time o1 time ie independent dictionary size depending average term length maximum edit distance index based hash table average search time complexity o1 comparison approaches bktrees search time olog dictionary_size whereas symspell algorithm constant time o1 time ie independent dictionary size tries comparable search performance approach trie prefix tree requires common prefix makes suitable autocomplete search suggestions applicable spell checking typing error eg first letter common prefix hence trie work spelling correction application possible application fields symspell algorithm fast approximate dictionary string matching spell checkers word processors search engines correction systems optical character recognition natural language translation based translation memory record linkage deduplication matching dna sequences fuzzy string searching fraud detection source codethe c implementation symmetric delete spelling correction algorithm released github open source mit licensehttpsgithubcomwolfgarbesymspell portsthere ports c crystal go java javascript python ruby rust scala swift available originally published blogfaroocom june 7 2012 quick cheer standing ovation clap show much enjoyed story founder seekstorm searchasaservice faroo p2p search httpwwwseekstormcom httpsgithubcomwolfgarbe httpswwwquoracomprofilewolfgarbe,en,"['SymSpell', 'BK', 'Damerau', 'Prabhakar Raghavan & Hinrich', 'Introduction to Information Retrieval', 'Symmetric Delete Spelling Correction', 'ReasoningThe SymSpell', 'codeThe C', 'the Symmetric Delete Spelling Correction', 'GitHub', 'MIT', 'PortsThere', 'Ruby', 'blog.faroo.com']"
23,Paul Christiano,43,Formalizing indirect normativity – AI Alignment,"This post outlines a formalization of what Nick Bostrom calls “indirect normativity.” I don’t think it’s an adequate solution to the AI control problem; but to my knowledge it was the first precise specification of a goal that meets the “not terrible” bar, i.e. which does not lead to terrible consequences if pursued without any caveats or restrictions. The proposal outlined here was sketched in early 2012 while I was visiting FHI, and was my first serious foray into AI control.
When faced with the challenge of writing down precise moral principles, adhering to the standards demanded in mathematics, moral philosophers encounter two serious difficulties:
In light of these difficulties, a moral philosopher might simply declare: “It is not my place to aspire to mathematical standards of precision. Ethics as a project inherently requires shared language, understanding, and experience; it becomes impossible or meaningless without them.”
This may be a defensible philosophical position, but unfortunately the issue is not entirely philosophical. In the interest of building institutions or machines which reliably pursue what we value, we may one day be forced to describe precisely “what we value” in a way that does not depend on charitable or “common sense” interpretation (in the same way that we today must describe “what we want done” precisely to computers, often with considerable effort). If some aspects of our values cannot be described formally, then it may be more difficult to use institutions or machines to reliably satisfy them. This is not to say that describing our values formally is necessary to satisfying them, merely that it might make it easier.
Since we are focusing on finding any precise and satisfactory moral theory, rather than resolving disputes in moral philosophy, we will adopt a consequentialist approach without justification and focus on axiology. Moreover, we will begin from the standpoint of expected utility maximization, and leave aside questions about how or over what space the maximization is performed.
We aim to mathematically define a utility function U such that we would be willing to build a hypothetical machine which exceptionlessly maximized U, possibly at the catastrophic expense of any other values. We will assume that the machine has an ability to reason which at least rivals that of humans, and is willing to tolerate arbitrarily complex definitions of U (within its ability to reason about them).
We adopt an indirect approach. Rather than specifying what exactly we want, we specify a process for determining what we want. This process is extremely complex, so that any computationally limited agent will always be uncertain about the process’ output. However, by reasoning about the process it is possible to make judgments about which action has the highest expected utility in light of this uncertainty.
For example, I might adopt the principle: “a state of affairs is valuable to the extent that I would judge it valuable after a century of reflection.” In general I will be uncertain about what I would say after a century, but I can act on the basis of my best guesses: after a century I will probably prefer worlds with more happiness, and so today I should prefer worlds with more happiness. After a century I have only a small probability of valuing trees’ feelings, and so today I should go out of my way to avoid hurting them if it is either instrumentally useful or extremely easy. As I spend more time thinking, my beliefs about what I would say after a century may change, and I will start to pursue different states of affairs even though the formal definition of my values is static. Similarly, I might desire to think about the value of trees’ feelings, if I expect that my opinions are unstable: if I spend a month thinking about trees, my current views will then be a much better predictor of my views after a hundred years, and if I know better whether or not trees’ feelings are valuable, I can make better decisions.
This example is quite informal, but it communicates the main idea of the approach. We stress that the value of our contribution, if any, is in the possibility of a precise formulation. (Our proposal itself will be relatively informal; instead it is a description of how you would arrive at a precise formulation.) The use of indirection seems to be necessary to achieve the desired level of precision.
Our proposal contains only two explicit steps:
Each of these steps requires substantial elaboration, but we must also specify what we expect the human to do with these tools.
This proposal is best understood in the context of other fantastic-seeming proposals, such as “my utility is whatever I would write down if I reflected for a thousand years without interruption or biological decay.” The counterfactual events which take place within the definition are far beyond the realm our intuition recognizes as “realistic,” and have no place except in thought experiments. But to the extent that we can reason about these counterfactuals and change our behavior on the basis of that reasoning (if so motivated), we can already see how such fantastic situations could affect our more prosaic reality.
The remainder of this document consists of brief elaboration of some of these steps, and a few arguments about why this is a desirable process.
The first step of our proposal is a high-fidelity mathematical model of human cognition. We will set aside philosophical troubles, and assume that the human brain is a purely physical system which may be characterized mathematically. Even granting this, it is not clear how we can realistically obtain such a characterization.
The most obvious approach to characterizing a brain is to combine measurements of its behavior or architecture with an understanding of biology, chemistry, and physics. This project represents a massive engineering effort which is currently just beginning. Most pessimistically, our proposal could be postponed until this project’s completion. This could still be long before the mathematical characterization of the brain becomes useful for running experiments or automating human activities: because we are interested only in a definition, we do not care about having the computational resources necessary to simulate the brain.
An impractical mathematical definition, however, may be much easier to obtain. We can define a model of a brain in terms of exhaustive searches which could never be practically carried out. For example, given some observations of a neuron, we can formally define a brute force search for a model of that neuron. Similarly, given models of individual neurons we may be able to specify a brute force search over all ways of connecting those neurons which account for our observations of the brain (say, some data acquired through functional neuroimaging).
It may be possible to carry out this definition without exploiting any structural knowledge about the brain, beyond what is necessary to measure it effectively. By collecting imaging data for a human exposed to a wide variety of stimuli, we can recover a large corpus of data which must be explained by any model of a human brain. Moreover, by using our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation, by probing responses to questions or performance on games or puzzles.
In fact, this project may be possible using existing resources. The complexity of the human brain is not as unapproachable as it may at first appear: though it may contain 1014synapses, each described by many parameters, it can be specified much more compactly. A newborn’s brain can be specified by about 109bits of genetic information, together with a recipe for a physical simulation of development. The human brain appears to form new long-term memories at a rate of 1–2 bits per second, suggesting that it may be possible to specify an adult brain using 109additional bits of experiential information. This suggests that it may require only about 1010bits of information to specify a human brain, which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging.
This discussion has glossed over at least one question: what do we mean by ‘brain emulation’? Human cognition does not reside in a physical system with sharp boundaries, and it is not clear how you would define or use a simulation of the “input-output” behavior of such an object.
We will focus on some system which does have precisely defined input-output behavior, and which captures the important aspects of human cognition. Consider a system containing a human, a keyboard, a monitor, and some auxiliary instruments, well-insulated from the environment except for some wires carrying inputs to the monitor and outputs from the keyboard and auxiliary instruments (and wires carrying power). The inputs to this system are simply screens to be displayed on the monitor (say delivered as a sequence to be displayed one after another at 30 frames per second), while the outputs are the information conveyed from the keyboard and the other measuring apparatuses (also delivered as a sequence of data dumps, each recording activity from the last 30th of a second).
This “human in a box” system can be easily formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available. Alternatively, the input-output behavior of the human in a box can be directly observed, and a computational model constructed for the entire system. Let H be a mathematical definition of the resulting (randomized) function from input sequences (In(1), In(2), ..., In(K)) to the next output Out(K). H is, by design, a good approximation to what the human “would output” if presented with any particular input sequence.
Using H, we can mathematically define what “would happen” if the human interacted with a wide variety of systems. For example, if we deliver Out(K) as the input to an abstract computer running some arbitrary software, and then define In(K+1) as what the screen would next display, we can mathematically define the distribution over transcripts which would have arisen if the human had interacted with the abstract computer. This computer could be running an interactive shell, a video game, or a messaging client.
Note that H reflects the behavior of a particular human, in a particular mental state. This state is determined by the process used to design H, or the data used to learn it. In general, we can control H by choosing an appropriate human and providing appropriate instructions / training. More emulations could be produced by similar measures if necessary. Using only a single human may seem problematic, but we will not rely on this lone individual to make all relevant ethical judgments. Instead, we will try to select a human with the motivational stability to carry out the subsequent steps faithfully, which will define U using the judgment of a community consisting of many humans.
This discussion has been brief and has necessarily glossed over several important difficulties. One difficulty is the danger of using computationally unbounded brute force search, given the possibility of short programs which exhibit goal-oriented behavior. Another difficulty is that, unless the emulation project is extremely conservative, the models it produces are not likely to be fully-functional humans. Their thoughts may be blurred in various ways, they may be missing many memories or skills, and they may lack important functionalities such as long-term memory formation or emotional expression. The scope of these issues depends on the availability of data from which to learn the relevant aspects of human cognition. Realistic proposals along these lines will need to accommodate these shortcomings, relying on distorted emulations as a tool to construct increasingly accurate models.
For any idealized “software”, with a distinguished instruction return, we can use H to mathematically define the distribution over return values which would result, if the human were to interact with that software. We will informally define a particular program T which provides a rich environment, in which the remainder of our proposal can be implemented. From a technical perspective this will be the last step of our proposal. The remaining steps will be reflected only in the intentions and behavior of the human being simulated in H.
Fix a convenient and adequately expressive language (say a dialect of Python designed to run on an abstract machine). T implements a standard interface for an interactive shell in this language: the user can look through all of the past instructions that have been executed and their return values (rendered as strings) or execute a new instruction. We also provide symbols representing H and T themselves (as functions from sequences of K inputs to a value for the Kth output). We also provide some useful information (such as a snapshot of the Internet, and some information about the process used to create H and T), which we encode as a bit string and store in a single environment variable data. We assume that our language of choice has a return instruction, and we have T return whenever the user executes this instruction. Some care needs to be taken to define the behavior if T enters an infinite loop–we want to minimize the probability that the human accidentally hangs the terminal, with catastrophic consequences, but we cannot provide a complete safety-net without running into unresolvable issues with self-reference.
We define U to be the value returned by H interacting with T. If H represented an unfortunate mental state, then this interaction could be short and unproductive: the simulated human could just decide to type ‘return 0’ and be done with it. However, by choosing an appropriate human to simulate and inculcating an appropriate mental state, we can direct the process further.
We intend for H to use the resources in T to initiate a larger deliberative process. For example, the first step of this process may be to instantiate many copies of H, interacting with variants of messaging clients which are in contact with each other. The return value from the original process could then be defined as the value returned by a designated ‘leader’ from this community, or as a majority vote amongst the copies of H, or so on. Another step might be to create appropriate realistic virtual environments for simulated brains, rather than confining them to boxes. For motivational stability, it may be helpful to design various coordination mechanisms, involving frameworks for interaction, “cached” mental states which are frequently re-instantiated, or sanity checks whereby one copy of H monitors the behavior of another.
The resulting communities of simulated brains then engage in a protracted planning process, ensuring that subsequent steps can be carried out safely or developing alternative approaches. The main priority of this community is to reduce the probability of errors as far as possible (exactly what constitutes an ‘error’ will be discussed at more length later). At the end of this process, we obtain a formal definition of a new protocol H+, which submits its inputs for consideration to a large community and then produces its outputs using some deliberation mechanism (democratic vote, one leader using the rest of the community as advisors, etc.)
The next step requires our community of simulated brains to construct a detailed simulation of Earth which they can observe and manipulate. Once they have such a simulation, they have access to all of the data which would have been available on Earth. In particular, they can now explore many possible futures and construct simulations for each living human.
In order to locate Earth, we will again leverage an exhaustive search. First, H+ decides on informal desiderata for an “Earth simulation.” These are likely to be as follows:
Once H+ has decided on the desiderata, it uses a brute force search to find a simulation satisfying them: for each possible program it instantiates a new copy of H+ tasked with evaluating whether that program is an acceptable simulation. We then define E to be a uniform distribution over programs which pass this evaluation.
We might have doubts about whether this process produces the “real” Earth–perhaps even once we have verified that it is identical according to a laundry list of measures, it may still be different in other important ways. There are two reasons why we might care about such differences. First, if the simulated Earth has a substantially different set of people than the real Earth, then a different set of people will be involved in the subsequent decision making. If we care particularly about the opinions of the people who actually exist (which the reader might well, being amongst such people!) then this may be unsatisfactory. Second, if events transpire significantly differently on the simulated Earth than the real Earth, value judgments designed to guide behavior appropriately in the simulated Earth may lead to less appropriate behaviors in the real Earth. (This will not be a problem if our ultimate definition of U consists of universalizable ethical principles, but we will see that U might take other forms.)
These concerns are addressed by a few broad arguments. First, checking a detailed but arbitrary ‘laundry list’ actually provides a very strong guarantee. For example, if this laundry list includes verifying a snapshot of the Internet, then every event or person documented on the Internet must exist unchanged, and every keystroke of every person composing a document on the Internet must not be disturbed. If the world is well interconnected, then it may be very difficult to modify parts of the world without having substantial effects elsewhere, and so if a long enough arbitrary list of properties is fixed, we expect nearly all of the world to be the same as well. Second, if the essential character of the world is fixed but detailed are varied, we should expect the sort of moral judgments reached by consensus to be relatively constant. Finally, if the system whose behavior depends on these moral judgments is identical between the real and simulated worlds, then outputting a U which causes that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world.
Once H+ has defined a simulation of the world which permits inspection and intervention, by careful trial and error H+ can inspect a variety of possible futures. In particular, they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high-fidelity brain scans for all living humans.
Once these scans have been obtained, H+ can use them to define U as the output of a new community, H++, which draws on the expertise of all living humans operating under ideal conditions. There are two important degrees of flexibility: how to arrange the community for efficient communication and deliberation, and how to delegate the authority to define U. In terms of organization, the distinction between different approaches is probably not very important. For example, it would probably be perfectly satisfactory to start from a community of humans interacting with each other over something like the existing Internet (but on abstract, secure infrastructure). More important are the safety measures which would be in place, and the mechanism for resolving differences of value between different simulated humans.
The basic approach to resolving disputes is to allow each human to independently create a utility function U, each bounded in the interval [0, 1], and then to return their average. This average can either be unweighted, or can be weighted by a measure of each individual’s influence in the real world, in accordance with a game-theoretic notion like the Shapley value applied to abstract games or simulations of the original world. More sophisticated mechanisms are also possible, and may be desirable. Of course these questions can and should be addressed in part by H+ during its deliberation in the previous step. After all, H+ has access to an unlimited length of time to deliberate and has infinitely powerful computational aids. The role of our reasoning at this stage is simply to suggest that we can reasonably expect H+ to discover effective solutions.
As when discussing discovering a brain simulation by brute force, we have skipped over some critical issues in this section. In general, brute force searches (particularly over programs which we would like to run) are quite dangerous, because such searches will discover many programs with destructive goal-oriented behaviors. To deal with these issues, in both cases, we must rely on patience and powerful safety measures.
Once we have a formal description of a community of interacting humans, given as much time as necessary to deliberate and equipped with infinitely powerful computational aids, it becomes increasingly difficult to make coherent predictions about their behavior. Critically, though, we can also become increasingly confident that the outcome of their behavior will reflect their intentions. We sketch some possibilities, to illustrate the degree of flexibility available.
Perhaps the most natural possibility is for this community to solve some outstanding philosophical problems and to produce a utility function which directly captures their preferences. However, even if they quickly discovered a formulation which appeared to be attractive, they would still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory.
Another natural possibility is to eschew a comprehensive theory of ethics, and define value in terms of the community’s judgment. We can define a utility function in terms of the hypothetical judgments of astronomical numbers of simulated humans, collaboratively evaluating the goodness of a state of affairs by examining its history at the atomic level, understanding the relevant higher-order structure, and applying human intuitions.
It seems quite likely that the community will gradually engage in self-modifications, enlarging their cognitive capacity along various dimensions as they come to understand the relevant aspects of cognition and judge such modifications to preserve their essential character. Either independently or as an outgrowth of this process, they may (gradually or abruptly) pass control to machine intelligences which they are suitably confident expresses their values. This process could be used to acquire the power necessary to define a utility function in one of the above frameworks, or understanding value-preserving self-modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value. Any of these operations would be performed only after considerable analysis, when the original simulated humans were extremely confident in the desirability of the results.
Whatever path they take and whatever coordination mechanisms they use, eventually they will output a utility function U’. We then define U = 0 if U’ < 0, U = 1 if U’ > 1, and U = U’ otherwise.
At this point we have offered a proposal for formally defining a function U. We have made some general observations about what this definition entails. But now we may wonder to what extent U reflects our values, or more relevantly, to what extent our values are served by the creation of U-maximizers. Concerns may be divided into a few natural categories:
We respond to each of these objections in turn.
If the process works as intended, we will reach a stage in which a large community of humans reflects on their values, undergoes a process of discovery and potentially self-modification, and then outputs its result. We may be concerned that this dynamic does not adequately capture what we value.
For example, we may believe that some other extrapolation dynamic captures our values, or that it is morally desirable to act on the basis of our current beliefs without further reflection, or that the presence of realistic disruptions, such as the threat of catastrophe, has an important role in shaping our moral deliberation.
The important observation, in the defense of our proposal, is that whatever objections we could think of today, we could think of within the simulation. If, upon reflection, we decide that too much reflection is undesirable, we can simply change our plans appropriately. If we decide that realistic interference is important for moral deliberation, we can construct a simulation in which such interference occurs, or determine our moral principles by observing moral judgments in our own world’s possible futures.
There is some chance that this proposal is inadequate for some reason which won’t be apparent upon reflection, but then by definition this is a fact which we cannot possibly hope to learn by deliberating now. It therefore seems quite difficult to maintain objections to the proposal along these lines.
One aspect of the proposal does get “locked in,” however, after being considered by only one human rather than by a large civilization: the distribution of authority amongst different humans, and the nature of mechanisms for resolving differing value judgments.
Here we have two possible defenses. One is that the mechanism for resolving such disagreements can be reflected on at length by the individual simulated in H. This individual can spend generations of subjective time, and greatly expand her own cognitive capacities, while attempting to determine the appropriate way to resolve such disagreements. However, this defense is not completely satisfactory: we may be able to rely on this individual to produce a very technically sound and generally efficient proposal, but the proposal itself is quite value laden and relying on one individual to make such a judgment is in some sense begging the question.
A second, more compelling, defense, is that the structure of our world has already provided a mechanism for resolving value disagreements. By assigning decision-making weight in a way that depends on current influence (for example, as determined by the simulated ability of various coalitions to achieve various goals), we can generate a class of proposals which are at a minimum no worse than the status quo. Of course, these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by U–for example, if a nation were to create a U-maximizer, they might first adopt an internal policy for assigning influence on U. By performing this decision making in an idealized environment, we can also reduce the likelihood of destructive conflict and increase the opportunities for mutually beneficial bargaining. We may have moral objections to codifying this sort of “might makes right” policy, favoring a more democratic proposal or something else entirely, but as a matter of empirical fact a more ‘cosmopolitan’ proposal will be adopted only if it is supported by those with the appropriate forms of influence, a situation which is unchanged by precisely codifying existing power structure.
Finally, the values of the simulations in this process may diverge from the values of the original human models, for one reaosn or another. For example, the simulated humans may predictably disagree with the original models about ethical questions by virtue of (probably) having no physical instantiation. That is, the output of this process is defined in terms of what a particular human would do, in a situation which that human knows will never come to pass. If I ask “What would I do, if I were to wake up in a featureless room and told that the future of humanity depended on my actions?” the answer might begin with “become distressed that I am clearly inhabiting a hypothetical situation, and adjust my ethical views to take into account the fact that people in hypothetical situations apparently have relevant first-person experience.” Setting aside the question of whether such adjustments are justified, they at least raise the possibility that our values may diverge from those of the simulations in this process.
These changes might be minimized, by understanding their nature in advance and treating them on a case-by-case basis (if we can become convinced that our understanding is exhaustive). For example, we could try and use humans who robustly employ updateless decision theories which never undergo such predictable changes, or we could attempt to engineer a situation in which all of the humans being emulated do have physical instantiations, and naive self-interest for those emulations aligns roughly with the desired behavior (for example, by allowing the early emulations to “write themselves into” our world).
We can imagine many ways in which this process can fail to work as intended–the original brain emulations may accurately model human behavior, the original subject may deviate from the intended plans, or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic.
We can argue that the proposal is likely to succeed, and can bolster the argument in various ways (by reducing the number of assumptions necessary for succees, building in fault-tolerance, justifying each assumption more rigorously, and so on). However, we are unlikely to eliminate the possibility of error. Therefore we need to argue that if the process fails with some small probability, the resulting values will only be slightly disturbed.
This is the reason for requiring U to lie in the interval [0, 1]–we will see that this restriction bounds the damage which may be done by an unlikely failure.
If the process fails with some small probability ε, then we can represent the resulting utility function as U = (1 — ε) U1 + ε U2, where U1 is the intended utility function and U2 is a utility function produced by some arbitrary error process. Now consider two possible states of affairs A and B such that U1(A) > U1(B) + ε /(1 — ε) ≈ U1(B) + ε. Then since 0 ≤ U2 ≤ 1, we have:
U(A) = (1 — ε) U1(A) + ε U2(A) > (1 — ε) U1(B) + ε ≥ (1 — ε) U1(B) + ε U2(B) = U(B)
Thus if A is substantially better than B according to U1, then A is better than B according to U. This shows that a small probability of error, whether coming from the stochasticity of our process or an agent’s uncertainty about the process’ output, has only a small effect on the resulting values.
Moreover, the process contains a humans who have access to a simulation of our world. This implies, in particular, that they have access to a simulation of whatever U-maximizing agents exist in the world, and they have knowledge of those agents’ beliefs about U. This allows them to choose U with perfect knowledge of the effects of error in these agents’ judgments.
In some cases this will allow them to completely negate the effect of error terms. For example, if the randomness in our process causes a perfectly cooperate community of simulated humans to “control” U with probability 2⁄3, and causes an arbitrary adversary to control it with probability 1⁄3, then the simulated humans can spend half of their mass outputting a utility function which exactly counters the effect of the adversary.
In general, the situation is not quite so simple: the fraction of mass controlled by any particular coalition will vary as the system’s uncertainty about U varies, and so it will be impossible to counteract the effect of an error term in a way which is time-independent. Instead, we will argue later that an appropriate choice of a bounded and noisy U can be used to achieve a very wide variety of effective behaviors of U-maximizers, overcoming the limitations both of bounded utility maximization and of noisy specification of utility functions.
Many possible problems with this scheme were described or implicitly addressed above. But that discussion was not exhaustive, and there are some classes of errors that fall through the cracks.
One interesting class of failures concerns changes in the values of the hypothetical human H. This human is in a very strange situation, and it seems quite possible that the physical universe we know contains extremely few instances of that situation (especially as the process unfolds and becomes more exotic). So H’s first-person experience of this situation may lead to significant changes in H’s views.
For example, our intuition that our own universe is valuable seems to be derived substantially from our judgment that our own first-person experiences are valuable. If hypothetically we found ourselves in a very alien universe, it seems quite plausible that we would judge the experiences within that universe to be morally valuable as well (depending perhaps on our initial philosophical inclinations).
Another example concerns our self-interest: much of individual humans’ values seem to depend on their own anticipations about what will happen to them, especially when faced with the prospect of very negative outcomes. If hypothetically we woke up in a completely non-physical situation, it is not exactly clear what we would anticipate, and this may distort our behavior. Would we anticipate the planned thought experiment occurring as planned? Would we focus our attention on those locations in the universe where a simulation of the thought experiment might be occurring? This possibility is particularly troubling in light of the incentives our scheme creates — anyone who can manipulate H’s behavior can have a significant effect on the future of our world, and so many may be motivated to create simulations of H.
A realistic U-maximizer will not be able to carry out the process described in the definition of U–in fact, this process probably requires immensely more computing resources than are available in the universe. (It may even involve the reaction of a simulated human to watching a simulation of the universe!) To what extent can we make robust guarantees about the behavior of such an agent?
We have already touched on this difficulty when discussing the maxim “A state of affairs is valuable to the extent I would judge it valuable after a century of reflection.” We cannot generally predict our own judgments in a hundred years’ time, but we can have well-founded beliefs about those judgments and act on the basis of those beliefs. We can also have beliefs about the value of further deliberation, and can strike a balance between such deliberation and acting on our current best guess.
A U-maximizer faces a similar set of problems: it cannot understand the exact form of U, but it can still have well-founded beliefs about U, and about what sorts of actions are good according to U. For example, if we suppose that the U-maximizer can carry out any reasoning that we can carry out, then the U-maximizer knows to avoid anything which we suspect would be bad according to U (for example, torturing humans). Even if the U-maximizer cannot carry out this reasoning, as long as it can recognize that humans have powerful predictive models for other humans, it can simply appropriate those models (either by carrying out reasoning inspired by human models, or by simply asking).
Moreover, the community of humans being simulated in our process has access to a simulation of whatever U-maximizer is operating under this uncertainty, and has a detailed understanding of that uncertainty. This allows the community to shape their actions in a way with predictable (to the U-maximizer) consequences.
It is easily conceivable that our values cannot be captured by a bounded utility function. Easiest to imagine is the possibility that some states of the world are much better than others, in a way that requires unbounded utility functions. But it is also conceivable that the framework of utility maximization is fundamentally not an appropriate one for guiding such an agent’s action, or that the notion of utility maximization hides subtleties which we do not yet appreciate.
We will argue that it is possible to transform bounded utility maximization into an arbitrary alternative system of decision-making, by designing a utility function which rewards worlds in which the U-maximizer replaced itself with an alternative decision-maker.
It is straightforward to design a utility function which is maximized in worlds where any particular U-maximizer converted itself into a non-U-maximizer–even if no simple characterization can be found for the desired act, we can simply instantiate many communities of humans to look over a world history and decide whether or not they judge the U-maximizer to have acted appropriately.
The more complicated question is whether a realistic U-maximizer can be made to convert itself into a non-U-maximizer, given that it is logically uncertain about the nature of U. It is at least conceivable that it couldn’t: if the desirability of some other behavior is only revealed by philosophical considerations which are too complex to ever be discovered by physically limited agents, then we should not expect any physically limited U-maximizer to respond to those considerations. Of course, in this case we could also not expect normal human deliberation to correctly capture our values. The relevant question is whether a U-maximizer could switch to a different normative framework, if an ordinary investment of effort by human society revealed that a different normative framework was more appropriate.
If a U-maximizer does not spend any time investigating this possibility, than it may not be expected to act on it. But to the extent that we assign a significant probability to the simulated humans deciding that a different normative framework is more appropriate, and to the extent that the U-maximizer is able to either emulate or accept our reasoning, it will also assign a significant probability to this possibility (unless it is able to rule it out by more sophisticated reasoning). If we (and the U-maximizer) expect the simulations to output a U which rewards a switch to a different normative framework, and this possibility is considered seriously, then U-maximization entails exploring this possibility. If these explorations suggest that the simulated humans probably do recommend some particular alternative framework, and will output a U which assigns high value to worlds in which this framework is adopted and low value to worlds in which it isn’t, then a U-maximizer will change frameworks.
Such a “change of frameworks” may involve sweeping action in the world. For example, the U-maximizer may have created many other agents which are pursuing activities instrumentally useful to maximizing U. These agents may then need to be destroyed or altered; anticipating this possibility, the U-maximizer is likely to take actions to ensure that its current “best guess” about U does not get locked in.
This argument suggests that a U-maximizer could adopt an arbitrary alternative framework, if it were feasible to conclude that humans would endorse that framework upon reflection.
Our proposal appears to be something of a cop out, in that it declines to directly take a stance on any ethical issues. Indeed, not only do we fail to specify a utility function ourselves, but we expect the simulations to which we have delegated the problem to in turn delegate it at least a few more times. Clearly at some point this process must bottom out with actual value judgments, and we may be concerned that this sort of “passing the buck” is just obscuring deeper problems which will arise when the process does bottom out.
As observed above, whatever such concerns we might have can also be discovered by the simulations we create. If there is some fundamental difficulty which always arises when trying to assign values, then we certainly have not exacerbated this problem by delegation. Nevertheless, there are at least two coherent objections one might raise:
Both of these objections can be met with a single response. In the current world, we face a broad range of difficult and often urgent problems. By passing the buck the first time, we delegate resolution of ethical challenges to a civilization which does not have to deal with some of these difficulties–in particular, it faces no urgent existential threats. This allows us to divert as much energy as possible to dealing with practical problems today, while still capturing most of the benefits of nearly arbitrarily extensive ethical deliberation.
This process is defined in terms of the behavior of unthinkably many hypothetical brain emulations. It is conceivable that the moral status of these emulations may be significant.
We must make a distinction between two possible sources of moral value: it could be the case that a U-maximizer carries out simulations on physical hardware in order to better understand U, and these simulations have moral value, or it could be the case that the hypothetical emulations themselves have moral value.
In the first case, we can remark that the moral value of such simulations is itself incorporated into the definition of U. Therefore a U-maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about U–as long as it believes that we may might be concerned about the simulations’ welfare, upon reflection, it can rely as much as possible on approaches which do not involve running simulations, which deprive simulations of the first-person experience of discomfort, or which estimate outcomes by running simulations in more pleasant circumstances. If the U-maximizer is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile, then it will make those sacrifices. In general, in the same way that we can argue that estimates of U reflect our values over states of affairs, we can argue that estimates of U reflects our values over processes for learning about U.
In the second case, a U-maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of U. However, the possible disvalue of these simulations’ experiences are probably seriously diminished.
In general the moral value of such hypothetical simulations’ experiences is somewhat dubious. If we simply write down the definition of U, these simulations seem to have no more reality than story-book characters whose activities we describe.
The best arguments for their moral relevance comes from the great causal significance of their decisions: if the actions of a powerful U-maximizer depend on its beliefs about what a particular simulation would do in a particular situation, including for example that simulation’s awareness of discomfort or fear, or confusion at the absurdity of the hypothetical situation in which they find themselves, then it may be the case that those emotional responses are granted moral significance. However, although we may define astronomical numbers of hypothetical simulations, the detailed emotional responses of very view of these simulations will play an important role in the definition of U.
Moreover, for the most part the existences of the hypothetical simulations we define are extremely well-controlled by those simulations themselves, and may be expected to be counted as unusually happy by the lights of the simulations themselves. The early simulations (who have less such control) are created from an individual who has provided consent and is selected to find such situations particularly non-distressing.
Finally, we observe that U can exert control over the experiences of even hypothetical simulations. If the early simulations would experience morally relevant suffering because of their causal significance, but the later simulations they generate robustly disvalue this suffering, the later simulations can simulate each other and ensure that they all take the same actions, eliminating the causal significance of the earlier simulations.
Originally published at ordinaryideas.wordpress.com on April 21, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
OpenAI
Aligning AI systems with human interests.
",post outlines formalization nick bostrom calls indirect normativity dont think adequate solution ai control problem knowledge first precise specification goal meets terrible bar ie lead terrible consequences pursued without caveats restrictions proposal outlined sketched early 2012 visiting fhi first serious foray ai control faced challenge writing precise moral principles adhering standards demanded mathematics moral philosophers encounter two serious difficulties light difficulties moral philosopher might simply declare place aspire mathematical standards precision ethics project inherently requires shared language understanding experience becomes impossible meaningless without may defensible philosophical position unfortunately issue entirely philosophical interest building institutions machines reliably pursue value may one day forced describe precisely value way depend charitable common sense interpretation way today must describe want done precisely computers often considerable effort aspects values cannot described formally may difficult use institutions machines reliably satisfy say describing values formally necessary satisfying merely might make easier since focusing finding precise satisfactory moral theory rather resolving disputes moral philosophy adopt consequentialist approach without justification focus axiology moreover begin standpoint expected utility maximization leave aside questions space maximization performed aim mathematically define utility function u would willing build hypothetical machine exceptionlessly maximized u possibly catastrophic expense values assume machine ability reason least rivals humans willing tolerate arbitrarily complex definitions u within ability reason adopt indirect approach rather specifying exactly want specify process determining want process extremely complex computationally limited agent always uncertain process output however reasoning process possible make judgments action highest expected utility light uncertainty example might adopt principle state affairs valuable extent would judge valuable century reflection general uncertain would say century act basis best guesses century probably prefer worlds happiness today prefer worlds happiness century small probability valuing trees feelings today go way avoid hurting either instrumentally useful extremely easy spend time thinking beliefs would say century may change start pursue different states affairs even though formal definition values static similarly might desire think value trees feelings expect opinions unstable spend month thinking trees current views much better predictor views hundred years know better whether trees feelings valuable make better decisions example quite informal communicates main idea approach stress value contribution possibility precise formulation proposal relatively informal instead description would arrive precise formulation use indirection seems necessary achieve desired level precision proposal contains two explicit steps steps requires substantial elaboration must also specify expect human tools proposal best understood context fantasticseeming proposals utility whatever would write reflected thousand years without interruption biological decay counterfactual events take place within definition far beyond realm intuition recognizes realistic place except thought experiments extent reason counterfactuals change behavior basis reasoning motivated already see fantastic situations could affect prosaic reality remainder document consists brief elaboration steps arguments desirable process first step proposal highfidelity mathematical model human cognition set aside philosophical troubles assume human brain purely physical system may characterized mathematically even granting clear realistically obtain characterization obvious approach characterizing brain combine measurements behavior architecture understanding biology chemistry physics project represents massive engineering effort currently beginning pessimistically proposal could postponed projects completion could still long mathematical characterization brain becomes useful running experiments automating human activities interested definition care computational resources necessary simulate brain impractical mathematical definition however may much easier obtain define model brain terms exhaustive searches could never practically carried example given observations neuron formally define brute force search model neuron similarly given models individual neurons may able specify brute force search ways connecting neurons account observations brain say data acquired functional neuroimaging may possible carry definition without exploiting structural knowledge brain beyond necessary measure effectively collecting imaging data human exposed wide variety stimuli recover large corpus data must explained model human brain moreover using explicit knowledge human cognition algorithmically generate extensive range tests identify successful simulation probing responses questions performance games puzzles fact project may possible using existing resources complexity human brain unapproachable may first appear though may contain 1014synapses described many parameters specified much compactly newborns brain specified 109bits genetic information together recipe physical simulation development human brain appears form new longterm memories rate 12 bits per second suggesting may possible specify adult brain using 109additional bits experiential information suggests may require 1010bits information specify human brain limits reasonably collected existing technology functional neuroimaging discussion glossed least one question mean brain emulation human cognition reside physical system sharp boundaries clear would define use simulation inputoutput behavior object focus system precisely defined inputoutput behavior captures important aspects human cognition consider system containing human keyboard monitor auxiliary instruments wellinsulated environment except wires carrying inputs monitor outputs keyboard auxiliary instruments wires carrying power inputs system simply screens displayed monitor say delivered sequence displayed one another 30 frames per second outputs information conveyed keyboard measuring apparatuses also delivered sequence data dumps recording activity last 30th second human box system easily formally defined precise description human brain coarse descriptions human body environment available alternatively inputoutput behavior human box directly observed computational model constructed entire system let h mathematical definition resulting randomized function input sequences in1 in2 ink next output outk h design good approximation human would output presented particular input sequence using h mathematically define would happen human interacted wide variety systems example deliver outk input abstract computer running arbitrary software define ink1 screen would next display mathematically define distribution transcripts would arisen human interacted abstract computer computer could running interactive shell video game messaging client note h reflects behavior particular human particular mental state state determined process used design h data used learn general control h choosing appropriate human providing appropriate instructions training emulations could produced similar measures necessary using single human may seem problematic rely lone individual make relevant ethical judgments instead try select human motivational stability carry subsequent steps faithfully define u using judgment community consisting many humans discussion brief necessarily glossed several important difficulties one difficulty danger using computationally unbounded brute force search given possibility short programs exhibit goaloriented behavior another difficulty unless emulation project extremely conservative models produces likely fullyfunctional humans thoughts may blurred various ways may missing many memories skills may lack important functionalities longterm memory formation emotional expression scope issues depends availability data learn relevant aspects human cognition realistic proposals along lines need accommodate shortcomings relying distorted emulations tool construct increasingly accurate models idealized software distinguished instruction return use h mathematically define distribution return values would result human interact software informally define particular program provides rich environment remainder proposal implemented technical perspective last step proposal remaining steps reflected intentions behavior human simulated h fix convenient adequately expressive language say dialect python designed run abstract machine implements standard interface interactive shell language user look past instructions executed return values rendered strings execute new instruction also provide symbols representing h functions sequences k inputs value kth output also provide useful information snapshot internet information process used create h encode bit string store single environment variable data assume language choice return instruction return whenever user executes instruction care needs taken define behavior enters infinite loopwe want minimize probability human accidentally hangs terminal catastrophic consequences cannot provide complete safetynet without running unresolvable issues selfreference define u value returned h interacting h represented unfortunate mental state interaction could short unproductive simulated human could decide type return 0 done however choosing appropriate human simulate inculcating appropriate mental state direct process intend h use resources initiate larger deliberative process example first step process may instantiate many copies h interacting variants messaging clients contact return value original process could defined value returned designated leader community majority vote amongst copies h another step might create appropriate realistic virtual environments simulated brains rather confining boxes motivational stability may helpful design various coordination mechanisms involving frameworks interaction cached mental states frequently reinstantiated sanity checks whereby one copy h monitors behavior another resulting communities simulated brains engage protracted planning process ensuring subsequent steps carried safely developing alternative approaches main priority community reduce probability errors far possible exactly constitutes error discussed length later end process obtain formal definition new protocol h submits inputs consideration large community produces outputs using deliberation mechanism democratic vote one leader using rest community advisors etc next step requires community simulated brains construct detailed simulation earth observe manipulate simulation access data would available earth particular explore many possible futures construct simulations living human order locate earth leverage exhaustive search first h decides informal desiderata earth simulation likely follows h decided desiderata uses brute force search find simulation satisfying possible program instantiates new copy h tasked evaluating whether program acceptable simulation define e uniform distribution programs pass evaluation might doubts whether process produces real earthperhaps even verified identical according laundry list measures may still different important ways two reasons might care differences first simulated earth substantially different set people real earth different set people involved subsequent decision making care particularly opinions people actually exist reader might well amongst people may unsatisfactory second events transpire significantly differently simulated earth real earth value judgments designed guide behavior appropriately simulated earth may lead less appropriate behaviors real earth problem ultimate definition u consists universalizable ethical principles see u might take forms concerns addressed broad arguments first checking detailed arbitrary laundry list actually provides strong guarantee example laundry list includes verifying snapshot internet every event person documented internet must exist unchanged every keystroke every person composing document internet must disturbed world well interconnected may difficult modify parts world without substantial effects elsewhere long enough arbitrary list properties fixed expect nearly world well second essential character world fixed detailed varied expect sort moral judgments reached consensus relatively constant finally system whose behavior depends moral judgments identical real simulated worlds outputting u causes system behave certain way simulated world also cause system behave way real world h defined simulation world permits inspection intervention careful trial error h inspect variety possible futures particular find interventions cause simulated human society conduct real brain emulation project produce highfidelity brain scans living humans scans obtained h use define u output new community h draws expertise living humans operating ideal conditions two important degrees flexibility arrange community efficient communication deliberation delegate authority define u terms organization distinction different approaches probably important example would probably perfectly satisfactory start community humans interacting something like existing internet abstract secure infrastructure important safety measures would place mechanism resolving differences value different simulated humans basic approach resolving disputes allow human independently create utility function u bounded interval 0 1 return average average either unweighted weighted measure individuals influence real world accordance gametheoretic notion like shapley value applied abstract games simulations original world sophisticated mechanisms also possible may desirable course questions addressed part h deliberation previous step h access unlimited length time deliberate infinitely powerful computational aids role reasoning stage simply suggest reasonably expect h discover effective solutions discussing discovering brain simulation brute force skipped critical issues section general brute force searches particularly programs would like run quite dangerous searches discover many programs destructive goaloriented behaviors deal issues cases must rely patience powerful safety measures formal description community interacting humans given much time necessary deliberate equipped infinitely powerful computational aids becomes increasingly difficult make coherent predictions behavior critically though also become increasingly confident outcome behavior reflect intentions sketch possibilities illustrate degree flexibility available perhaps natural possibility community solve outstanding philosophical problems produce utility function directly captures preferences however even quickly discovered formulation appeared attractive would still wise spend great length time leverage techniques ensure proposed solution really satisfactory another natural possibility eschew comprehensive theory ethics define value terms communitys judgment define utility function terms hypothetical judgments astronomical numbers simulated humans collaboratively evaluating goodness state affairs examining history atomic level understanding relevant higherorder structure applying human intuitions seems quite likely community gradually engage selfmodifications enlarging cognitive capacity along various dimensions come understand relevant aspects cognition judge modifications preserve essential character either independently outgrowth process may gradually abruptly pass control machine intelligences suitably confident expresses values process could used acquire power necessary define utility function one frameworks understanding valuepreserving selfmodification machine intelligence may prove important ingredient formalizing value operations would performed considerable analysis original simulated humans extremely confident desirability results whatever path take whatever coordination mechanisms use eventually output utility function u define u 0 u 0 u 1 u 1 u u otherwise point offered proposal formally defining function u made general observations definition entails may wonder extent u reflects values relevantly extent values served creation umaximizers concerns may divided natural categories respond objections turn process works intended reach stage large community humans reflects values undergoes process discovery potentially selfmodification outputs result may concerned dynamic adequately capture value example may believe extrapolation dynamic captures values morally desirable act basis current beliefs without reflection presence realistic disruptions threat catastrophe important role shaping moral deliberation important observation defense proposal whatever objections could think today could think within simulation upon reflection decide much reflection undesirable simply change plans appropriately decide realistic interference important moral deliberation construct simulation interference occurs determine moral principles observing moral judgments worlds possible futures chance proposal inadequate reason wont apparent upon reflection definition fact cannot possibly hope learn deliberating therefore seems quite difficult maintain objections proposal along lines one aspect proposal get locked however considered one human rather large civilization distribution authority amongst different humans nature mechanisms resolving differing value judgments two possible defenses one mechanism resolving disagreements reflected length individual simulated h individual spend generations subjective time greatly expand cognitive capacities attempting determine appropriate way resolve disagreements however defense completely satisfactory may able rely individual produce technically sound generally efficient proposal proposal quite value laden relying one individual make judgment sense begging question second compelling defense structure world already provided mechanism resolving value disagreements assigning decisionmaking weight way depends current influence example determined simulated ability various coalitions achieve various goals generate class proposals minimum worse status quo course considerations also shaped conditions surrounding creation maintenance systems guided ufor example nation create umaximizer might first adopt internal policy assigning influence u performing decision making idealized environment also reduce likelihood destructive conflict increase opportunities mutually beneficial bargaining may moral objections codifying sort might makes right policy favoring democratic proposal something else entirely matter empirical fact cosmopolitan proposal adopted supported appropriate forms influence situation unchanged precisely codifying existing power structure finally values simulations process may diverge values original human models one reaosn another example simulated humans may predictably disagree original models ethical questions virtue probably physical instantiation output process defined terms particular human would situation human knows never come pass ask would wake featureless room told future humanity depended actions answer might begin become distressed clearly inhabiting hypothetical situation adjust ethical views take account fact people hypothetical situations apparently relevant firstperson experience setting aside question whether adjustments justified least raise possibility values may diverge simulations process changes might minimized understanding nature advance treating casebycase basis become convinced understanding exhaustive example could try use humans robustly employ updateless decision theories never undergo predictable changes could attempt engineer situation humans emulated physical instantiations naive selfinterest emulations aligns roughly desired behavior example allowing early emulations write world imagine many ways process fail work intendedthe original brain emulations may accurately model human behavior original subject may deviate intended plans simulated humans make error interacting virtual environment causes process get hijacked unintended dynamic argue proposal likely succeed bolster argument various ways reducing number assumptions necessary succees building faulttolerance justifying assumption rigorously however unlikely eliminate possibility error therefore need argue process fails small probability resulting values slightly disturbed reason requiring u lie interval 0 1we see restriction bounds damage may done unlikely failure process fails small probability represent resulting utility function u 1 u1 u2 u1 intended utility function u2 utility function produced arbitrary error process consider two possible states affairs b u1a u1b 1 u1b since 0 u2 1 ua 1 u1a u2a 1 u1b 1 u1b u2b ub thus substantially better b according u1 better b according u shows small probability error whether coming stochasticity process agents uncertainty process output small effect resulting values moreover process contains humans access simulation world implies particular access simulation whatever umaximizing agents exist world knowledge agents beliefs u allows choose u perfect knowledge effects error agents judgments cases allow completely negate effect error terms example randomness process causes perfectly cooperate community simulated humans control u probability 23 causes arbitrary adversary control probability 13 simulated humans spend half mass outputting utility function exactly counters effect adversary general situation quite simple fraction mass controlled particular coalition vary systems uncertainty u varies impossible counteract effect error term way timeindependent instead argue later appropriate choice bounded noisy u used achieve wide variety effective behaviors umaximizers overcoming limitations bounded utility maximization noisy specification utility functions many possible problems scheme described implicitly addressed discussion exhaustive classes errors fall cracks one interesting class failures concerns changes values hypothetical human h human strange situation seems quite possible physical universe know contains extremely instances situation especially process unfolds becomes exotic hs firstperson experience situation may lead significant changes hs views example intuition universe valuable seems derived substantially judgment firstperson experiences valuable hypothetically found alien universe seems quite plausible would judge experiences within universe morally valuable well depending perhaps initial philosophical inclinations another example concerns selfinterest much individual humans values seem depend anticipations happen especially faced prospect negative outcomes hypothetically woke completely nonphysical situation exactly clear would anticipate may distort behavior would anticipate planned thought experiment occurring planned would focus attention locations universe simulation thought experiment might occurring possibility particularly troubling light incentives scheme creates anyone manipulate hs behavior significant effect future world many may motivated create simulations h realistic umaximizer able carry process described definition uin fact process probably requires immensely computing resources available universe may even involve reaction simulated human watching simulation universe extent make robust guarantees behavior agent already touched difficulty discussing maxim state affairs valuable extent would judge valuable century reflection cannot generally predict judgments hundred years time wellfounded beliefs judgments act basis beliefs also beliefs value deliberation strike balance deliberation acting current best guess umaximizer faces similar set problems cannot understand exact form u still wellfounded beliefs u sorts actions good according u example suppose umaximizer carry reasoning carry umaximizer knows avoid anything suspect would bad according u example torturing humans even umaximizer cannot carry reasoning long recognize humans powerful predictive models humans simply appropriate models either carrying reasoning inspired human models simply asking moreover community humans simulated process access simulation whatever umaximizer operating uncertainty detailed understanding uncertainty allows community shape actions way predictable umaximizer consequences easily conceivable values cannot captured bounded utility function easiest imagine possibility states world much better others way requires unbounded utility functions also conceivable framework utility maximization fundamentally appropriate one guiding agents action notion utility maximization hides subtleties yet appreciate argue possible transform bounded utility maximization arbitrary alternative system decisionmaking designing utility function rewards worlds umaximizer replaced alternative decisionmaker straightforward design utility function maximized worlds particular umaximizer converted nonumaximizereven simple characterization found desired act simply instantiate many communities humans look world history decide whether judge umaximizer acted appropriately complicated question whether realistic umaximizer made convert nonumaximizer given logically uncertain nature u least conceivable couldnt desirability behavior revealed philosophical considerations complex ever discovered physically limited agents expect physically limited umaximizer respond considerations course case could also expect normal human deliberation correctly capture values relevant question whether umaximizer could switch different normative framework ordinary investment effort human society revealed different normative framework appropriate umaximizer spend time investigating possibility may expected act extent assign significant probability simulated humans deciding different normative framework appropriate extent umaximizer able either emulate accept reasoning also assign significant probability possibility unless able rule sophisticated reasoning umaximizer expect simulations output u rewards switch different normative framework possibility considered seriously umaximization entails exploring possibility explorations suggest simulated humans probably recommend particular alternative framework output u assigns high value worlds framework adopted low value worlds isnt umaximizer change frameworks change frameworks may involve sweeping action world example umaximizer may created many agents pursuing activities instrumentally useful maximizing u agents may need destroyed altered anticipating possibility umaximizer likely take actions ensure current best guess u get locked argument suggests umaximizer could adopt arbitrary alternative framework feasible conclude humans would endorse framework upon reflection proposal appears something cop declines directly take stance ethical issues indeed fail specify utility function expect simulations delegated problem turn delegate least times clearly point process must bottom actual value judgments may concerned sort passing buck obscuring deeper problems arise process bottom observed whatever concerns might also discovered simulations create fundamental difficulty always arises trying assign values certainly exacerbated problem delegation nevertheless least two coherent objections one might raise objections met single response current world face broad range difficult often urgent problems passing buck first time delegate resolution ethical challenges civilization deal difficultiesin particular faces urgent existential threats allows us divert much energy possible dealing practical problems today still capturing benefits nearly arbitrarily extensive ethical deliberation process defined terms behavior unthinkably many hypothetical brain emulations conceivable moral status emulations may significant must make distinction two possible sources moral value could case umaximizer carries simulations physical hardware order better understand u simulations moral value could case hypothetical emulations moral value first case remark moral value simulations incorporated definition u therefore umaximizer sensitive possible suffering simulations runs trying learn uas long believes may might concerned simulations welfare upon reflection rely much possible approaches involve running simulations deprive simulations firstperson experience discomfort estimate outcomes running simulations pleasant circumstances umaximizer able foresee consider certain sacrifices simulation welfare worthwhile make sacrifices general way argue estimates u reflect values states affairs argue estimates u reflects values processes learning u second case umaximizer world may little ability influence welfare hypothetical simulations invoked definition u however possible disvalue simulations experiences probably seriously diminished general moral value hypothetical simulations experiences somewhat dubious simply write definition u simulations seem reality storybook characters whose activities describe best arguments moral relevance comes great causal significance decisions actions powerful umaximizer depend beliefs particular simulation would particular situation including example simulations awareness discomfort fear confusion absurdity hypothetical situation find may case emotional responses granted moral significance however although may define astronomical numbers hypothetical simulations detailed emotional responses view simulations play important role definition u moreover part existences hypothetical simulations define extremely wellcontrolled simulations may expected counted unusually happy lights simulations early simulations less control created individual provided consent selected find situations particularly nondistressing finally observe u exert control experiences even hypothetical simulations early simulations would experience morally relevant suffering causal significance later simulations generate robustly disvalue suffering later simulations simulate ensure take actions eliminating causal significance earlier simulations originally published ordinaryideaswordpresscom april 21 2012 quick cheer standing ovation clap show much enjoyed story openai aligning ai systems human interests,en,"['AI', 'FHI', 'U', 'Python', 'U.', 'Shapley', 'U’', 'U1', 'U2', 'U. Therefore a', 'ordinaryideas.wordpress.com']"
24,Robbie Tilton,3,Emotional Computing – Robbie Tilton – Medium,"Investigating the human to computer relationship through reverse engineering the Turing test
Humans are getting closer to creating a computer with the ability to feel and think. Although the processes of the human brain are at large unknown, computer scientists have been working to simulate the human capacity to feel and understand emotions. This paper explores what it means to live in an age where computers can have emotional depth and what this means for the future of human to computer interactions. In an experiment between a human and a human disguised as a computer, the Turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind. Implications for this study are discussed and the direction for future research suggested.
The computer is a gateway technology that has opened up new ways of creation, communication, and expression. Computers in first world countries are a standard household item (approximately 70% of Americans owning one as of 2009 (US Census Bereau)) and are utilized as a tool to achieve a diverse range of goals. As this product continues to become more globalized, transistors are becoming smaller, processors are becoming faster, hard drives are holding information in new networked patterns, and humans are adapting to the methods of interaction expected of machines. At the same time, with more powerful computers and quicker means of communication — many researchers are exploring how a computer can serve as a tool to simulate the brains cognition. If a computer is able to achieve the same intellectual and emotional properties as the human brain — we could potentially understand how we ourselves think and feel.
Coined by MIT, the term Affective Computing relates to computation of emotion or the affective phenomena and is a study that breaks down complex processes of the brain relating them to machine-like activities. Marvin Minsky, Rosalind Picard, Clifford Nass, and Scott Brave — along with many others — have contributed to this field and what it would mean to have a computer that could fully understand its users. In their research it is very clear that humans have the capacity to associate human emotions and personality traits with a machine (Nass and Brave, 2005), but can a human ever truly treat machine as a person? In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions.
The human to computer relationship is continuously evolving and is dependent on the software interface users interact with. With regards to current wide scale interfaces — OSX, Windows, Linux, iOS, and Android — the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial purposes. This relationship to software is driven by utilitarian needs and humans do not expect emotional comprehension or intellectually equivalent thoughts in their household devices.
As face tracking, eye tracking, speech recognition, and kinetic recognition are advancing in their experimental laboratories, it is anticipated that these technologies will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its users and how a user can interact with a computer.
This paper is not about if a computer will have the ability to feel and love its user, but asks the question — to what capacity will humans be able to reciprocate feelings to a machine.
How does Intelligence Quotient (IQ) differ from Emotional Quotient (EQ). An IQ is a representational relationship of intelligence that measures cognitive abilities like learning, understanding, and dealing with new situations. An EQ is a method of measuring emotional intelligence and the ability to both use emotions and cognitive skills (Cherry).
Advances in computer IQ have been astonishing and have proved that machines are capable of answering difficult questions accurately, are able to hold a conversation with human-like understanding, and allow for emotional connections between a human and machine. The Turing test in particular has shown the machines ability to think and even fool a person into believing that it is a human (Turing test explained in detail in section 4). Machines like, Deep Blue, Watson, Eliza, Svetlana, CleverBot, and many more — have all expanded the perceptions of what a computer is and can be.
If an increased computational IQ can allow a human to computer relationship to feel more like a human to human interaction, what would the advancement of computational EQ bring us? Peter Robinson, a professor at the University of Cambridge, states that if a computer understands its users’ feelings that it can then respond with an interaction that is more intuitive for its users’
(Robinson). In essence, EQ advocates feel that it can facilitate a more natural interaction process where collaboration can occur with a computer.
In Alan Turing’s, Computing Machinery and Intelligence (Turing, 1950), a variant on the classic British parlor “imitation game” is proposed. The original game revolves around three players: a man (A), a woman (B), and an interrogator ©. The interrogator stays in a room apart from A and B and only can communicate to the participants through text-based communication (a typewriter or instant messenger style interface). When the game begins one contestant (A or B) is asked to pretend to be the opposite gender and to try and convince the interrogator © of this. At the same time the opposing participant is given full knowledge that the other contestant is trying to fool the interrogator. With Alan Turing’s computational background, he took this imitation game one step further by replacing one of the participants (A or B) with a machine — thus making the investigator try and depict if he/she was speaking to a human or machine. In 1950, Turing proposed that by 2000 the average interrogator would not have more than a 70 percent chance of making the right identification after five minutes of questioning. The Turing test was first passed in 1966, with Eliza by Joseph Weizenbaum, a chat robot programmed to act like a Rogerian psychotherapist (Weizenbaum, 1966). In 1972, Kenneth Colby created a similar bot called PARRY that incorporated more personality than Eliza and was programmed to act like a paranoid schizophrenic (Bowden, 2006). Since these initial victories for the test, the 21st century has proven to continue to provide machines with more human-like qualities and traits that have made people fall in love with them, convinced them of being human, and have human-like reasoning.
Brian Christian, the author of The Most Human Human, argues that the problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking, that they have no “self”. They are mere accumulations of identities and thoughts that are foreign to the machine and have no central identity of their own. He also argues that people are beginning to idealize the machine and admire machines capabilities more than their fellow humans — in essence — he argues humans are evolving to become more like machines with less of a notion of self (Christian 2011).
Turing states, “we like to believe that Man is in some subtle way superior to the rest of creation” and “it is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.” If this is true, will humans idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation? Reversing the Turing test allows us to understand how humans will treat machines when machines provide an equivalent emotional and intellectual capacity. This also hits directly on Jefferson Lister’s quote, “Not until a machine can write a sonnet or compose a
concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it.”
Participants were given a chat-room simulation between two participants (A) a human interrogator and (B) a human disguised as a computer. In this simulation A and B were both placed in different rooms to avoid influence and communicated through a text-based interface. (A) was informed that (B) was an advanced computer chat-bot with the capacity to feel, understand, learn, and speak like a human. (B) was informed to be his or herself. Text-based communication was chosen to follow Turing’s argument that a computers voice should not help an interrogator determine if it’s a human or computer. Pairings of participants were chosen to participate in the interaction one at a time to avoid influence from other participants. Each experiment was five minutes in length to replicate Turing’s time restraints.
Twenty-eight graduate students were recruited from the NYU Interactive Telecommunications Program to participate in the study — 50% male and 50% female. The experiment was evenly distributed across men and women. After being recruited in-person, participants were directed to a website that gave instructions and ran the experiment. Upon entering the website, (A) participants were told that we were in the process of evaluating an advanced cloud based computing system that had the capacity to feel emotion, understand, learn, and converse like a human. (B) participants were instructed that they would be communicating with another person through text and to be themselves. They were also told that participant (A) thinks they are a computer, but that they shouldn’t act like a computer or pretend to be one in any way. This allowed (A) to explicitly understand that they were talking to a computer while (B) knew (A) perspective and explicitly were not going to play the role of a computer. Participants were then directed to communicate with the bot or human freely without restrictions. After five minutes of conversation the participants were asked to stop and then filled out a questionnaire.
Participants were asked to rate IQ and EQ of the person they were conversing with. (A) participants perceived the following of (B): IQ: 0% — Not Good / 0% — Barely Acceptable / 21.4% — Okay / 50% — Great / 28.6% Excellent IQ Average Rating: 81.4% EQ: 0% — Not Good / 7.1% — Barely Acceptable / 50% — Okay / 14.3% — Great / 28.6% — Excellent EQ Average Rating: 72.8% Ability to hold a conversation: 0% — Not Good / 0% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 35.7% — Excellent Ability to hold a conversation Average: 81.4%
(B) participants perceived the following of (A): IQ: 0% — Not Good / 21.4% — Barely Acceptable / 35.7% — Okay / 28.6% — Great / 14.3% Excellent IQ Average Rating: 67% EQ: 7.1% — Not Good / 14.3% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 14.3% — Excellent EQ Average Rating: 67% Ability to hold a conversation: 7.1% — Not Good / 28.6% — Barely Acceptable / 35.7% — Okay / 0% — Great / 28.6% — Excellent Ability to hold a conversation Average: 62.8%
Overall, (A) participants gave the perceived Chabot higher ratings than (B) participants gave (A). In particular, the highest rating was in regards to the chat- bot’s IQ. This data states that people viewed the chat-bot to be more intellectually competent. It also implies that people talking with bots decrease their IQ, EQ, and conversation ability when communicating with computers.
(A) participants were allowed to decide their username within the chat system to best reflect how they wanted to portray themselves to the machine. (B) participants were designated the gender neutral name “Bot” in an attempt to ganger gender perceptions for the machine. The male to female ratio was divided evenly with all participants: 50% being male and 50% being female.
(A) participants 50% of the time thought (B) was a male, 7.1% a female, and 42.9% gender neutral. On the other hand, (B) participants 28.6% of the time thought (A) was a male, 57.1% a female, and 14.3% gender neutral.
The usernames (A) chose are as follows: Hihi, Inessah Somade3 Willzing Jihyun, G, Ann, Divagrrl93, Thisdoug, Jono, Minion10, P, 123, itslynnburke
From these results, it is clear that people associate the male gender and gender neutrality with machines. It also demonstrates that people modify their identities when speaking with machines.
(B) participants were asked if they would like to pursue a friendship with the person they chatted with. 50% of participants responded affirmatively that they would indeed like to pursue a friendship while 50% said maybe or no. One response stated, “I would like to continue the conversation, but I don’t think I would be enticed to pursue a friendship.” Another responded, “Maybe? I like people who are intellectually curious, but I worry that the person might be a bit of a smart-ass.” Overall the participant disguised as a machine may or may not pursue a friendship after five minutes of text-based conversation.
(B) participants were also asked if they felt (A) cared about their feelings. 21.4% stated that (A) indeed did care about their feelings, 21.4% stated that they weren’t sure if (A) cared about their feelings, and 57.2% stated that (A) did not care about their feelings. These results indicate a user’s lack of attention to (B)’s emotional state.
(A) participants were asked what they felt could be improved about the (B) participants. The following improvements were noted, “Should be funny” “Give it a better sense of humor” “It can be better if he knows about my friends or preference” “The response was inconsistent and too slow”“It should share more about itself. Your algorithm is prime prude, just like that LETDOWN Siri. Well, I guess I liked it better, but it should be more engaged and human consistency, not after the first cold prompt.” “It pushed me on too many questions” “I felt that it gave up on answering and the response time was a bit slow. Outsource the chatbot to fluent English speakers elsewhere and pretend they are bots — if the responses are this slow to this many inquiries, then it should be about the same experience.” “I was very impressed with its parsing ability so far. Not as much with its reasoning. I think some parameters for the conversation would help, like ‘Ask a question’” “Maybe make the response faster”“I was confused at first, because I asked a question, waited a bit, then asked another question, waited and then got a response from the bot...”
The responses from this indicate that even if a computer is a human that its user may not necessarily be fully satisfied with its performance. The response implies that each user would like the machine to accommodate his or her needs in order to cause less personality and cognitive friction. With several participant comments incorporating response time, it also indicates people expect machines to have consistent response times. Humans clearly vary in speed when listening, thinking, and responding, but it is expected of machines to act in a rhythmic fashion. It also suggests that there is an expectation that a machine will answer all questions asked and will not ask its users more questions than perceived necessary.
(A) participants were asked if they felt (B)’s Artificial Intelligence could improve their relationship to computers if integrated in their daily products. 57.1% of participants responded affirmatively that they felt this could improve their relationship:“Well- I think I prefer talking to a person better. But yes for ipod, smart phones, etc. would be very handy for everyday use products”“Yes. Especially iphone is always with me. So it can track my daily behaviors. That makes the algorithm smarter”“Possibly, I should have queries it for information that would have been more relevant to me”“Absolutely!”“Yes”
The 42.9% which responded negatively had doubts that it would be necessary or desirable:“Not sure, it might creep me out if it were.”“I like Siri as much as the next gal, but honestly we’re approaching the uncanny valley now.”“Its not clear to me why this type of relationship needs to improve, i think human relationships still need a lot of work.”“Nope, I still prefer flesh sacks.“No”
The findings of the paper are relevant to the future of Affective Computation: whether a super computer with a human-like IQ and EQ can improve the human-to-computer interaction. The uncertainty of computational equivalency that Turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers.
The responses from the experiment affirm gender perceptions of machines and show how we display ourselves to machines. It seems that we limit our intelligence, limit our emotions, and obscure our identities when communicating to a machine. This leads us to question if we would want to give our true self to a computer if it doesn’t have a self of its own. It also could indicate that people censor themselves for machines because they lack a similarity that bonds humans to humans or that there’s a stigma associated with placing information in a digital device. The inverse relationship is also shown through the data that people perceive a bots IQ, EQ, and discussion ability to be high. Even though the chat-bot was indeed a human this data can imply humans perceive bots to not have restrictions and to be competent at certain procedures.
The results also imply that humans aren’t really sure what they want out of Artificial Intelligence in the future and that we are not certain that an Affective computer would even enjoy a users company and/or conversation. The results also state that we currently think of computers as a very personal device that should be passive (not active), but reactive when interacted with. It suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it takes from us.
A major limitation of this experiment is the sample size and sample diversity. The sample size of twenty-eight students is too small to fully understand and gather a stable result set. It was also only conducted with NYU: Interactive Telecommunications Students who all have extensive experience with computers and technology. To get a more accurate assessment of emotions a more diverse sample range needs to be taken.
Five minutes is a short amount of time to create an emotional connection or friendship. To stay true to the Turing tests limitations this was enforced, but further relational understanding could be understood if more time was granted.
Beside the visual interface of the chat window it would be important to show the emotions of participant (B) through a virtual avatar. Not having this visual feedback could have limited emotional resonance with participants (A).
Time is also a limitation. People aren’t used to speaking to inquisitive machines yet and even through a familiar interface (a chat-room) many participants haven’t held conversations with machines previously. Perhaps if chat-bots become more active conversational participants’ in commercial applications users will feel less censored to give themselves to the conversation.
In addition to the refinements noted in the limitations described above, there are several other experiments for possible future studies. For example, investigating a long-term human-to-bot relationship. This would provide a better understanding toward the emotions a human can share with a machine and how a machine can reciprocate these emotions. It would also better allow computer scientists to understand what really creates a significant relationship when physical limitations are present.
Future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with higher intellectual and emotional understanding. It should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising either parties capacities.
This paper demonstrates the diverse range of emotions that people can feel for affective computation and indicates that we are not in a time where computational equivalency is fully desired or accepted. Positive reactions indicate that there is optimism for more adept artificial intelligence and that there is interest in the field for commercial use. It also provides insight that humans limit themselves when communicating with machines and that inversely machines don’t limit themselves when communicating with humans.
Books & ArticlesBowden M., 2006, Minds as Machine: A History of Cognitive Science, Oxford University Press
Christian B., 2011, The Most Human Human
Marvin M., 2006. The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind, Simon & Schuster Paperbacks
Nass C., Brave S., 2005. Wired For Speech: How Voice Activates and Advances the Human-Computer Relationship, MIT Press
Nass C., Brave S., 2005, Hutchinson K., Computers that care: Investigating the effects of orientation of emotion exhibited by an embodied computer agent, Human-Computer Studies, 161- 178, Elsevier
Picard, R., 1997. Affective Computing, MIT Press
Searle J., 1980, Minds, Brains, and Programs, Cambridge University Press, 417–457
Turing, A., 1950, Computing Machinery and Intelligence, Mind, Stor, 59, 433–460
Wilson R., Keil F., 2001, The MIT Encyclopedia of the Cognitive Sciences, MIT Press
Weizenbaum J., 1966, ELIZA — A Computer Program For the Study of Natural Language Communication Between Man and Machine, Communications of the ACM, 36–45
Websites Cherry K., What is Emotional Intelligence?, http://psychology.about.com/od/personalitydevelopment/a/emotionalintell.htm
Epstein R., 2006, Clever Bots, Radio Lab, http://www.radiolab.org/2011/may/31/clever-bots/ IBM, 1977, Deep Blue, IBM, http://www.research.ibm.com/deepblue/ IBM, 2011, Watson, IBM, http://www-03.ibm.com/innovation/us/watson/index.html
Leavitt D., 2011, I Took the Turing Test, New York Times, http://www.nytimes.com/2011/03/20/books/review/book-review-the-most-human-human-by-brian- christian.html
Personal Robotics Group, 2008, Nexi, MIT. http://robotic.media.mit.edu/ Robinson P., The Emotional Computer, Camrbidge Ideas,
http://www.cam.ac.uk/research/news/the-emotional-computer/
US Census Bereau, 2009, Households with a Computer and Internet Use: 1984 to 2009. http://www.census.gov/hhes/computer/
1960’s, Eliza, MIT, http://www.manifestation.com/neurotoys/eliza.php3
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",investigating human computer relationship reverse engineering turing test humans getting closer creating computer ability feel think although processes human brain large unknown computer scientists working simulate human capacity feel understand emotions paper explores means live age computers emotional depth means future human computer interactions experiment human human disguised computer turing test reverse engineered order understand role computers play become adept processes human mind implications study discussed direction future research suggested computer gateway technology opened new ways creation communication expression computers first world countries standard household item approximately 70 americans owning one 2009 us census bereau utilized tool achieve diverse range goals product continues become globalized transistors becoming smaller processors becoming faster hard drives holding information new networked patterns humans adapting methods interaction expected machines time powerful computers quicker means communication many researchers exploring computer serve tool simulate brains cognition computer able achieve intellectual emotional properties human brain could potentially understand think feel coined mit term affective computing relates computation emotion affective phenomena study breaks complex processes brain relating machinelike activities marvin minsky rosalind picard clifford nass scott brave along many others contributed field would mean computer could fully understand users research clear humans capacity associate human emotions personality traits machine nass brave 2005 human ever truly treat machine person paper uncover means humans interact machines greater intelligence attempt predict future human computer interactions human computer relationship continuously evolving dependent software interface users interact regards current wide scale interfaces osx windows linux ios android tools abilities computer provide remains central focus computational advancements commercial purposes relationship software driven utilitarian needs humans expect emotional comprehension intellectually equivalent thoughts household devices face tracking eye tracking speech recognition kinetic recognition advancing experimental laboratories anticipated technologies eventually make way mainstream market provide new relationship computer understand users user interact computer paper computer ability feel love user asks question capacity humans able reciprocate feelings machine intelligence quotient iq differ emotional quotient eq iq representational relationship intelligence measures cognitive abilities like learning understanding dealing new situations eq method measuring emotional intelligence ability use emotions cognitive skills cherry advances computer iq astonishing proved machines capable answering difficult questions accurately able hold conversation humanlike understanding allow emotional connections human machine turing test particular shown machines ability think even fool person believing human turing test explained detail section 4 machines like deep blue watson eliza svetlana cleverbot many expanded perceptions computer increased computational iq allow human computer relationship feel like human human interaction would advancement computational eq bring us peter robinson professor university cambridge states computer understands users feelings respond interaction intuitive users robinson essence eq advocates feel facilitate natural interaction process collaboration occur computer alan turings computing machinery intelligence turing 1950 variant classic british parlor imitation game proposed original game revolves around three players man woman b interrogator interrogator stays room apart b communicate participants textbased communication typewriter instant messenger style interface game begins one contestant b asked pretend opposite gender try convince interrogator time opposing participant given full knowledge contestant trying fool interrogator alan turings computational background took imitation game one step replacing one participants b machine thus making investigator try depict heshe speaking human machine 1950 turing proposed 2000 average interrogator would 70 percent chance making right identification five minutes questioning turing test first passed 1966 eliza joseph weizenbaum chat robot programmed act like rogerian psychotherapist weizenbaum 1966 1972 kenneth colby created similar bot called parry incorporated personality eliza programmed act like paranoid schizophrenic bowden 2006 since initial victories test 21st century proven continue provide machines humanlike qualities traits made people fall love convinced human humanlike reasoning brian christian author human human argues problem designing artificial intelligence greater ability even though machines capable learning speaking self mere accumulations identities thoughts foreign machine central identity also argues people beginning idealize machine admire machines capabilities fellow humans essence argues humans evolving become like machines less notion self christian 2011 turing states like believe man subtle way superior rest creation likely quite strong intellectual people since value power thinking highly others inclined base belief superiority man power true humans idealize future machine intelligence remain inferior object creation reversing turing test allows us understand humans treat machines machines provide equivalent emotional intellectual capacity also hits directly jefferson listers quote machine write sonnet compose concerto thoughts emotions felt chance fall symbols could agree machine equals brainthat write know written participants given chatroom simulation two participants human interrogator b human disguised computer simulation b placed different rooms avoid influence communicated textbased interface informed b advanced computer chatbot capacity feel understand learn speak like human b informed textbased communication chosen follow turings argument computers voice help interrogator determine human computer pairings participants chosen participate interaction one time avoid influence participants experiment five minutes length replicate turings time restraints twentyeight graduate students recruited nyu interactive telecommunications program participate study 50 male 50 female experiment evenly distributed across men women recruited inperson participants directed website gave instructions ran experiment upon entering website participants told process evaluating advanced cloud based computing system capacity feel emotion understand learn converse like human b participants instructed would communicating another person text also told participant thinks computer shouldnt act like computer pretend one way allowed explicitly understand talking computer b knew perspective explicitly going play role computer participants directed communicate bot human freely without restrictions five minutes conversation participants asked stop filled questionnaire participants asked rate iq eq person conversing participants perceived following b iq 0 good 0 barely acceptable 214 okay 50 great 286 excellent iq average rating 814 eq 0 good 71 barely acceptable 50 okay 143 great 286 excellent eq average rating 728 ability hold conversation 0 good 0 barely acceptable 286 okay 357 great 357 excellent ability hold conversation average 814 b participants perceived following iq 0 good 214 barely acceptable 357 okay 286 great 143 excellent iq average rating 67 eq 71 good 143 barely acceptable 286 okay 357 great 143 excellent eq average rating 67 ability hold conversation 71 good 286 barely acceptable 357 okay 0 great 286 excellent ability hold conversation average 628 overall participants gave perceived chabot higher ratings b participants gave particular highest rating regards chat bots iq data states people viewed chatbot intellectually competent also implies people talking bots decrease iq eq conversation ability communicating computers participants allowed decide username within chat system best reflect wanted portray machine b participants designated gender neutral name bot attempt ganger gender perceptions machine male female ratio divided evenly participants 50 male 50 female participants 50 time thought b male 71 female 429 gender neutral hand b participants 286 time thought male 571 female 143 gender neutral usernames chose follows hihi inessah somade3 willzing jihyun g ann divagrrl93 thisdoug jono minion10 p 123 itslynnburke results clear people associate male gender gender neutrality machines also demonstrates people modify identities speaking machines b participants asked would like pursue friendship person chatted 50 participants responded affirmatively would indeed like pursue friendship 50 said maybe one response stated would like continue conversation dont think would enticed pursue friendship another responded maybe like people intellectually curious worry person might bit smartass overall participant disguised machine may may pursue friendship five minutes textbased conversation b participants also asked felt cared feelings 214 stated indeed care feelings 214 stated werent sure cared feelings 572 stated care feelings results indicate users lack attention bs emotional state participants asked felt could improved b participants following improvements noted funny give better sense humor better knows friends preference response inconsistent slowit share algorithm prime prude like letdown siri well guess liked better engaged human consistency first cold prompt pushed many questions felt gave answering response time bit slow outsource chatbot fluent english speakers elsewhere pretend bots responses slow many inquiries experience impressed parsing ability far much reasoning think parameters conversation would help like ask question maybe make response fasteri confused first asked question waited bit asked another question waited got response bot responses indicate even computer human user may necessarily fully satisfied performance response implies user would like machine accommodate needs order cause less personality cognitive friction several participant comments incorporating response time also indicates people expect machines consistent response times humans clearly vary speed listening thinking responding expected machines act rhythmic fashion also suggests expectation machine answer questions asked ask users questions perceived necessary participants asked felt bs artificial intelligence could improve relationship computers integrated daily products 571 participants responded affirmatively felt could improve relationshipwell think prefer talking person better yes ipod smart phones etc would handy everyday use productsyes especially iphone always track daily behaviors makes algorithm smarterpossibly queries information would relevant meabsolutelyyes 429 responded negatively doubts would necessary desirablenot sure might creep werei like siri much next gal honestly approaching uncanny valley nowits clear type relationship needs improve think human relationships still need lot worknope still prefer flesh sacksno findings paper relevant future affective computation whether super computer humanlike iq eq improve humantocomputer interaction uncertainty computational equivalency turing brought forth indeed interesting starting point understand want future computers responses experiment affirm gender perceptions machines show display machines seems limit intelligence limit emotions obscure identities communicating machine leads us question would want give true self computer doesnt self also could indicate people censor machines lack similarity bonds humans humans theres stigma associated placing information digital device inverse relationship also shown data people perceive bots iq eq discussion ability high even though chatbot indeed human data imply humans perceive bots restrictions competent certain procedures results also imply humans arent really sure want artificial intelligence future certain affective computer would even enjoy users company andor conversation results also state currently think computers personal device passive active reactive interacted suggests consistent reliability expect upon machines expect take information machine takes us major limitation experiment sample size sample diversity sample size twentyeight students small fully understand gather stable result set also conducted nyu interactive telecommunications students extensive experience computers technology get accurate assessment emotions diverse sample range needs taken five minutes short amount time create emotional connection friendship stay true turing tests limitations enforced relational understanding could understood time granted beside visual interface chat window would important show emotions participant b virtual avatar visual feedback could limited emotional resonance participants time also limitation people arent used speaking inquisitive machines yet even familiar interface chatroom many participants havent held conversations machines previously perhaps chatbots become active conversational participants commercial applications users feel less censored give conversation addition refinements noted limitations described several experiments possible future studies example investigating longterm humantobot relationship would provide better understanding toward emotions human share machine machine reciprocate emotions would also better allow computer scientists understand really creates significant relationship physical limitations present future studies attempt push results understanding larger sample reacts computer algorithm higher intellectual emotional understanding also attempt understand boundaries emotional computing ideal user ideal machine without compromising either parties capacities paper demonstrates diverse range emotions people feel affective computation indicates time computational equivalency fully desired accepted positive reactions indicate optimism adept artificial intelligence interest field commercial use also provides insight humans limit communicating machines inversely machines dont limit communicating humans books articlesbowden 2006 minds machine history cognitive science oxford university press christian b 2011 human human marvin 2006 emotion machine commonsense thinking artificial intelligence future human mind simon schuster paperbacks nass c brave 2005 wired speech voice activates advances humancomputer relationship mit press nass c brave 2005 hutchinson k computers care investigating effects orientation emotion exhibited embodied computer agent humancomputer studies 161 178 elsevier picard r 1997 affective computing mit press searle j 1980 minds brains programs cambridge university press 417457 turing 1950 computing machinery intelligence mind stor 59 433460 wilson r keil f 2001 mit encyclopedia cognitive sciences mit press weizenbaum j 1966 eliza computer program study natural language communication man machine communications acm 3645 websites cherry k emotional intelligence httppsychologyaboutcomodpersonalitydevelopmentaemotionalintellhtm epstein r 2006 clever bots radio lab httpwwwradiolaborg2011may31cleverbots ibm 1977 deep blue ibm httpwwwresearchibmcomdeepblue ibm 2011 watson ibm httpwww03ibmcominnovationuswatsonindexhtml leavitt 2011 took turing test new york times httpwwwnytimescom20110320booksreviewbookreviewthemosthumanhumanbybrian christianhtml personal robotics group 2008 nexi mit httproboticmediamitedu robinson p emotional computer camrbidge ideas httpwwwcamacukresearchnewstheemotionalcomputer us census bereau 2009 households computer internet use 1984 2009 httpwwwcensusgovhhescomputer 1960s eliza mit httpwwwmanifestationcomneurotoyselizaphp3 quick cheer standing ovation clap show much enjoyed story,en,"['MIT', 'Android', 'Emotional Quotient (EQ', 'EQ', 'Deep Blue', 'Watson, Eliza', 'Svetlana', 'CleverBot', 'the University of Cambridge', 'Computing Machinery and Intelligence', 'Weizenbaum', 'The Most Human Human', 'the NYU Interactive Telecommunications Program', 'Not Good /', 'Okay /', 'Great /', 'Not Good / 7.1%', 'Acceptable', 'Not Good / 0%', 'Thisdoug, Jono, Minion10, P', 'algorithm', 'Artificial Intelligence', 'the algorithm smarter”“Possibly', 'Affective', 'NYU', 'Interactive Telecommunications Students', 'Time', 'Books & ArticlesBowden M., 2006', 'Oxford University Press', 'MIT Press', 'Human-Computer Studies', 'Minds, Brains, and Programs', 'Cambridge University Press', 'The MIT Encyclopedia', 'the Cognitive Sciences', 'ELIZA', 'Communications of the ACM', 'IBM', 'Watson', 'http://www.nytimes.com/2011/03/20/books/review/book-review-the-most-human-human-by-brian-', 'Personal Robotics Group', 'Nexi', 'The Emotional Computer', 'Households', 'Eliza']"
25,Netflix Technology Blog,330,System Architectures for Personalization and Recommendation,"by Xavier Amatriain and Justin Basilico
In our previous posts about Netflix personalization, we highlighted the importance of using both data and algorithms to create the best possible experience for Netflix members. We also talked about the importance of enriching the interaction and engaging the user with the recommendation system. Today we’re exploring another important piece of the puzzle: how to create a software architecture that can deliver this experience and support rapid innovation. Coming up with a software architecture that handles large volumes of existing data, is responsive to user interactions, and makes it easy to experiment with new recommendation approaches is not a trivial task. In this post we will describe how we address some of these challenges at Netflix.
To start with, we present an overall system diagram for recommendation systems in the following figure. The main components of the architecture contain one or more machine learning algorithms.
The simplest thing we can do with data is to store it for later offline processing, which leads to part of the architecture for managing Offline jobs. However, computation can be done offline, nearline, or online. Online computation can respond better to recent events and user interaction, but has to respond to requests in real-time. This can limit the computational complexity of the algorithms employed as well as the amount of data that can be processed. Offline computation has less limitations on the amount of data and the computational complexity of the algorithms since it runs in a batch manner with relaxed timing requirements. However, it can easily grow stale between updates because the most recent data is not incorporated. One of the key issues in a personalization architecture is how to combine and manage online and offline computation in a seamless manner. Nearline computation is an intermediate compromise between these two modes in which we can perform online-like computations, but do not require them to be served in real-time. Model training is another form of computation that uses existing data to generate a model that will later be used during the actual computation of results. Another part of the architecture describes how the different kinds of events and data need to be handled by the Event and Data Distribution system. A related issue is how to combine the different Signals and Models that are needed across the offline, nearline, and online regimes. Finally, we also need to figure out how to combine intermediate Recommendation Results in a way that makes sense for the user. The rest of this post will detail these components of this architecture as well as their interactions. In order to do so, we will break the general diagram into different sub-systems and we will go into the details of each of them. As you read on, it is worth keeping in mind that our whole infrastructure runs across the public Amazon Web Services cloud.
As mentioned above, our algorithmic results can be computed either online in real-time, offline in batch, or nearline in between. Each approach has its advantages and disadvantages, which need to be taken into account for each use case.
Online computation can respond quickly to events and use the most recent data. An example is to assemble a gallery of action movies sorted for the member using the current context. Online components are subject to an availability and response time Service Level Agreements (SLA) that specifies the maximum latency of the process in responding to requests from client applications while our member is waiting for recommendations to appear. This can make it harder to fit complex and computationally costly algorithms in this approach. Also, a purely online computation may fail to meet its SLA in some circumstances, so it is always important to think of a fast fallback mechanism such as reverting to a precomputed result. Computing online also means that the various data sources involved also need to be available online, which can require additional infrastructure.
On the other end of the spectrum, offline computation allows for more choices in algorithmic approach such as complex algorithms and less limitations on the amount of data that is used. A trivial example might be to periodically aggregate statistics from millions of movie play events to compile baseline popularity metrics for recommendations. Offline systems also have simpler engineering requirements. For example, relaxed response time SLAs imposed by clients can be easily met. New algorithms can be deployed in production without the need to put too much effort into performance tuning. This flexibility supports agile innovation. At Netflix we take advantage of this to support rapid experimentation: if a new experimental algorithm is slower to execute, we can choose to simply deploy more Amazon EC2 instances to achieve the throughput required to run the experiment, instead of spending valuable engineering time optimizing performance for an algorithm that may prove to be of little business value. However, because offline processing does not have strong latency requirements, it will not react quickly to changes in context or new data. Ultimately, this can lead to staleness that may degrade the member experience. Offline computation also requires having infrastructure for storing, computing, and accessing large sets of precomputed results.
Nearline computation can be seen as a compromise between the two previous modes. In this case, computation is performed exactly like in the online case. However, we remove the requirement to serve results as soon as they are computed and can instead store them, allowing it to be asynchronous. The nearline computation is done in response to user events so that the system can be more responsive between requests. This opens the door for potentially more complex processing to be done per event. An example is to update recommendations to reflect that a movie has been watched immediately after a member begins to watch it. Results can be stored in an intermediate caching or storage back-end. Nearline computation is also a natural setting for applying incremental learning algorithms.
In any case, the choice of online/nearline/offline processing is not an either/or question. All approaches can and should be combined. There are many ways to combine them. We already mentioned the idea of using offline computation as a fallback. Another option is to precompute part of a result with an offline process and leave the less costly or more context-sensitive parts of the algorithms for online computation.
Even the modeling part can be done in a hybrid offline/online manner. This is not a natural fit for traditional supervised classification applications where the classifier has to be trained in batch from labeled data and will only be applied online to classify new inputs. However, approaches such as Matrix Factorization are a more natural fit for hybrid online/offline modeling: some factors can be precomputed offline while others can be updated in real-time to create a more fresh result. Other unsupervised approaches such as clustering also allow for offline computation of the cluster centers and online assignment of clusters. These examples point to the possibility of separating our model training into a large-scale and potentially complex global model training on the one hand and a lighter user-specific model training or updating phase that can be performed online.
Much of the computation we need to do when running personalization machine learning algorithms can be done offline. This means that the jobs can be scheduled to be executed periodically and their execution does not need to be synchronous with the request or presentation of the results. There are two main kinds of tasks that fall in this category: model training and batch computation of intermediate or final results. In the model training jobs, we collect relevant existing data and apply a machine learning algorithm produces a set of model parameters (which we will henceforth refer to as the model). This model will usually be encoded and stored in a file for later consumption. Although most of the models are trained offline in batch mode, we also have some online learning techniques where incremental training is indeed performed online. Batch computation of results is the offline computation process defined above in which we use existing models and corresponding input data to compute results that will be used at a later time either for subsequent online processing or direct presentation to the user.
Both of these tasks need refined data to process, which usually is generated by running a database query. Since these queries run over large amounts of data, it can be beneficial to run them in a distributed fashion, which makes them very good candidates for running on Hadoop via either Hive or Pig jobs. Once the queries have completed, we need a mechanism for publishing the resulting data. We have several requirements for that mechanism: First, it should notify subscribers when the result of a query is ready. Second, it should support different repositories (not only HDFS, but also S3 or Cassandra, for instance). Finally, it should transparently handle errors, allow for monitoring, and alerting. At Netflix we use an internal tool named Hermes that provides all of these capabilities and integrates them into a coherent publish-subscribe framework. It allows data to be delivered to subscribers in near real-time. In some sense, it covers some of the same use cases as Apache Kafka, but it is not a message/event queue system.
Regardless of whether we are doing an online or offline computation, we need to think about how an algorithm will handle three kinds of inputs: models, data, and signals. Models are usually small files of parameters that have been previously trained offline. Data is previously processed information that has been stored in some sort of database, such as movie metadata or popularity. We use the term “signals” to refer to fresh information we input to algorithms. This data is obtained from live services and can be made of user-related information, such as what the member has watched recently, or context data such as session, device, date, or time.
Our goal is to turn member interaction data into insights that can be used to improve the member’s experience. For that reason, we would like the various Netflix user interface applications (Smart TVs, tablets, game consoles, etc.) to not only deliver a delightful user experience but also collect as many user events as possible. These actions can be related to clicks, browsing, viewing, or even the content of the viewport at any time. Events can then be aggregated to provide base data for our algorithms. Here we try to make a distinction between data and events, although the boundary is certainly blurry. We think of events as small units of time-sensitive information that need to be processed with the least amount of latency possible. These events are routed to trigger a subsequent action or process, such as updating a nearline result set. On the other hand, we think of data as more dense information units that might need to be processed and stored for later use. Here the latency is not as important as the information quality and quantity. Of course, there are user events that can be treated as both events and data and therefore sent to both flows.
At Netflix, our near-real-time event flow is managed through an internal framework called Manhattan. Manhattan is a distributed computation system that is central to our algorithmic architecture for recommendation. It is somewhat similar to Twitter’s Storm, but it addresses different concerns and responds to a different set of internal requirements. The data flow is managed mostly through logging through Chukwa to Hadoop for the initial steps of the process. Later we use Hermes as our publish-subscribe mechanism.
The goal of our machine learning approach is to come up with personalized recommendations. These recommendation results can be serviced directly from lists that we have previously computed or they can be generated on the fly by online algorithms. Of course, we can think of using a combination of both where the bulk of the recommendations are computed offline and we add some freshness by post-processing the lists with online algorithms that use real-time signals.
At Netflix, we store offline and intermediate results in various repositories to be later consumed at request time: the primary data stores we use are Cassandra, EVCache, and MySQL. Each solution has advantages and disadvantages over the others. MySQL allows for storage of structured relational data that might be required for some future process through general-purpose querying. However, the generality comes at the cost of scalability issues in distributed environments. Cassandra and EVCache both offer the advantages of key-value stores. Cassandra is a well-known and standard solution when in need of a distributed and scalable no-SQL store. Cassandra works well in some situations, however in cases where we need intensive and constant write operations we find EVCache to be a better fit. The key issue, however, is not so much where to store them as to how to handle the requirements in a way that conflicting goals such as query complexity, read/write latency, and transactional consistency meet at an optimal point for each use case.
In previous posts, we have highlighted the importance of data, models, and user interfaces for creating a world-class recommendation system. When building such a system it is critical to also think of the software architecture in which it will be deployed. We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data. We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged-in easily. Plus, we want our recommendation results to be fresh and respond quickly to new data and user actions. Finding the sweet spot between these desires is not trivial: it requires a thoughtful analysis of requirements, careful selection of technologies, and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members. We are always looking for great engineers to join our team. If you think you can help us, be sure to look at our jobs page.
Originally published at techblog.netflix.com on March 27, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
",xavier amatriain justin basilico previous posts netflix personalization highlighted importance using data algorithms create best possible experience netflix members also talked importance enriching interaction engaging user recommendation system today exploring another important piece puzzle create software architecture deliver experience support rapid innovation coming software architecture handles large volumes existing data responsive user interactions makes easy experiment new recommendation approaches trivial task post describe address challenges netflix start present overall system diagram recommendation systems following figure main components architecture contain one machine learning algorithms simplest thing data store later offline processing leads part architecture managing offline jobs however computation done offline nearline online online computation respond better recent events user interaction respond requests realtime limit computational complexity algorithms employed well amount data processed offline computation less limitations amount data computational complexity algorithms since runs batch manner relaxed timing requirements however easily grow stale updates recent data incorporated one key issues personalization architecture combine manage online offline computation seamless manner nearline computation intermediate compromise two modes perform onlinelike computations require served realtime model training another form computation uses existing data generate model later used actual computation results another part architecture describes different kinds events data need handled event data distribution system related issue combine different signals models needed across offline nearline online regimes finally also need figure combine intermediate recommendation results way makes sense user rest post detail components architecture well interactions order break general diagram different subsystems go details read worth keeping mind whole infrastructure runs across public amazon web services cloud mentioned algorithmic results computed either online realtime offline batch nearline approach advantages disadvantages need taken account use case online computation respond quickly events use recent data example assemble gallery action movies sorted member using current context online components subject availability response time service level agreements sla specifies maximum latency process responding requests client applications member waiting recommendations appear make harder fit complex computationally costly algorithms approach also purely online computation may fail meet sla circumstances always important think fast fallback mechanism reverting precomputed result computing online also means various data sources involved also need available online require additional infrastructure end spectrum offline computation allows choices algorithmic approach complex algorithms less limitations amount data used trivial example might periodically aggregate statistics millions movie play events compile baseline popularity metrics recommendations offline systems also simpler engineering requirements example relaxed response time slas imposed clients easily met new algorithms deployed production without need put much effort performance tuning flexibility supports agile innovation netflix take advantage support rapid experimentation new experimental algorithm slower execute choose simply deploy amazon ec2 instances achieve throughput required run experiment instead spending valuable engineering time optimizing performance algorithm may prove little business value however offline processing strong latency requirements react quickly changes context new data ultimately lead staleness may degrade member experience offline computation also requires infrastructure storing computing accessing large sets precomputed results nearline computation seen compromise two previous modes case computation performed exactly like online case however remove requirement serve results soon computed instead store allowing asynchronous nearline computation done response user events system responsive requests opens door potentially complex processing done per event example update recommendations reflect movie watched immediately member begins watch results stored intermediate caching storage backend nearline computation also natural setting applying incremental learning algorithms case choice onlinenearlineoffline processing eitheror question approaches combined many ways combine already mentioned idea using offline computation fallback another option precompute part result offline process leave less costly contextsensitive parts algorithms online computation even modeling part done hybrid offlineonline manner natural fit traditional supervised classification applications classifier trained batch labeled data applied online classify new inputs however approaches matrix factorization natural fit hybrid onlineoffline modeling factors precomputed offline others updated realtime create fresh result unsupervised approaches clustering also allow offline computation cluster centers online assignment clusters examples point possibility separating model training largescale potentially complex global model training one hand lighter userspecific model training updating phase performed online much computation need running personalization machine learning algorithms done offline means jobs scheduled executed periodically execution need synchronous request presentation results two main kinds tasks fall category model training batch computation intermediate final results model training jobs collect relevant existing data apply machine learning algorithm produces set model parameters henceforth refer model model usually encoded stored file later consumption although models trained offline batch mode also online learning techniques incremental training indeed performed online batch computation results offline computation process defined use existing models corresponding input data compute results used later time either subsequent online processing direct presentation user tasks need refined data process usually generated running database query since queries run large amounts data beneficial run distributed fashion makes good candidates running hadoop via either hive pig jobs queries completed need mechanism publishing resulting data several requirements mechanism first notify subscribers result query ready second support different repositories hdfs also s3 cassandra instance finally transparently handle errors allow monitoring alerting netflix use internal tool named hermes provides capabilities integrates coherent publishsubscribe framework allows data delivered subscribers near realtime sense covers use cases apache kafka messageevent queue system regardless whether online offline computation need think algorithm handle three kinds inputs models data signals models usually small files parameters previously trained offline data previously processed information stored sort database movie metadata popularity use term signals refer fresh information input algorithms data obtained live services made userrelated information member watched recently context data session device date time goal turn member interaction data insights used improve members experience reason would like various netflix user interface applications smart tvs tablets game consoles etc deliver delightful user experience also collect many user events possible actions related clicks browsing viewing even content viewport time events aggregated provide base data algorithms try make distinction data events although boundary certainly blurry think events small units timesensitive information need processed least amount latency possible events routed trigger subsequent action process updating nearline result set hand think data dense information units might need processed stored later use latency important information quality quantity course user events treated events data therefore sent flows netflix nearrealtime event flow managed internal framework called manhattan manhattan distributed computation system central algorithmic architecture recommendation somewhat similar twitters storm addresses different concerns responds different set internal requirements data flow managed mostly logging chukwa hadoop initial steps process later use hermes publishsubscribe mechanism goal machine learning approach come personalized recommendations recommendation results serviced directly lists previously computed generated fly online algorithms course think using combination bulk recommendations computed offline add freshness postprocessing lists online algorithms use realtime signals netflix store offline intermediate results various repositories later consumed request time primary data stores use cassandra evcache mysql solution advantages disadvantages others mysql allows storage structured relational data might required future process generalpurpose querying however generality comes cost scalability issues distributed environments cassandra evcache offer advantages keyvalue stores cassandra wellknown standard solution need distributed scalable nosql store cassandra works well situations however cases need intensive constant write operations find evcache better fit key issue however much store handle requirements way conflicting goals query complexity readwrite latency transactional consistency meet optimal point use case previous posts highlighted importance data models user interfaces creating worldclass recommendation system building system critical also think software architecture deployed want ability use sophisticated machine learning algorithms grow arbitrary complexity deal huge amounts data also want architecture allows flexible agile innovation new approaches developed pluggedin easily plus want recommendation results fresh respond quickly new data user actions finding sweet spot desires trivial requires thoughtful analysis requirements careful selection technologies strategic decomposition recommendation algorithms achieve best outcomes members always looking great engineers join team think help us sure look jobs page originally published techblognetflixcom march 27 2013 quick cheer standing ovation clap show much enjoyed story learn netflix designs builds operates systems engineering organizations learn netflixs world class engineering efforts company culture product developments,en,"['Netflix', 'Amazon Web Services', 'Service Level Agreements', 'SLA', 'Amazon EC2', 'Matrix Factorization', 'algorithm', 'Smart', 'EVCache', 'Cassandra', 'SQL', 'techblog.netflix.com']"
26,James Faghmous ,187,New to Machine Learning? Avoid these three mistakes,"Machine learning (ML) is one of the hottest fields in data science. As soon as ML entered the mainstream through Amazon, Netflix, and Facebook people have been giddy about what they can learn from their data. However, modern machine learning (i.e. not the theoretical statistical learning that emerged in the 70s) is very much an evolving field and despite its many successes we are still learning what exactly can ML do for data practitioners. I gave a talk on this topic earlier this fall at Northwestern University and I wanted to share these cautionary tales with a wider audience.
Machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed.To do so, algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future (unseen) data. Take the left panel in the figure in the header, the crosses denote the observed data projected in a two-dimensional space — in this case house prices and their corresponding size in square meters. The blue line is the algorithm’s best hypothesis to explain the observed data. It states “there is a linear relationship between the price and size of a house. As the house’s size increases, so does its price in linear increments.” Now using this hypothesis, I can predict the price of an unseen datapoint based on its size. As the dimensions of the data increase, the hypotheses that explain the data become more complex.However, given that we are using a finite sample of observations to learn our hypothesis, finding an adequate hypothesis that generalizes to unseen data is nontrivial. There are three major pitfalls one can fall into that will prevent you from having a generalizable model and hence the conclusions of your hypothesis will be in doubt.
Occam’s razor is a principle attributed to William of Occam a 14th century philosopher. Occam’s razor advocates for choosing the simplest hypothesis that explains your data, yet no simpler. While this notion is simple and elegant, it is often misunderstood to mean that we must select the simplest hypothesis possible regardless of performance.
In their 2008 paper in Nature, Johan Nyberg and colleagues used a 4-level artificial neural network to predict seasonal hurricane counts using two or three environmental variables. The authors reported stellar accuracy in predicting seasonal North Atlantic hurricane counts, however their model violates Occam’s razor and most certainly doesn’t generalize to unseen data. The razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four-layer neural network. A four-layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen data. The rightmost panel in the top figure shows such incident. The hypothesis selected by the algorithm (the blue curve) to explain the data is so complex that it fits through every single data point. That is: for any given house size in the training data, I can give you with pinpoint accuracy the price it would sell for. It doesn’t take much to observe that even a human couldn’t be that accurate. We could give you a very close estimate of the price, but to predict the selling price of a house, within a few dollars , every single time is impossible.
The pitfall of selecting too complex a hypothesis is known as overfitting. Think of overfitting as memorizing as opposed to learning. If you are a child and you are memorizing how to add numbers you may memorize the sums of any pair of integers between 0 and 10. However, when asked to calculate 11 + 12 you will be unable to because you have never seen 11 or 12, and therefore couldn’t memorize their sum. That’s what happens to an overfitted model, it gets too lazy to learn the general principle that explains the data and instead memorizes the data.
Data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict. The most basic form of data leakage would be to use the same data that we want to predict as input to our model (e.g. use the price of a house to predict the price of the same house). However, most often data leakage occurs subtly and inadvertently. For example, one may wish to learn for anomalies as opposed to raw data, that is a deviations from a long-term mean. However, many fail to remove the test data before computing the anomalies and hence the anomalies carry some information about the data you want to predict since they influenced the mean and standard deviation before being removed.
The are several ways to avoid data leakage as outlined by Claudia Perlich in her great paper on the subject. However, there is no silver bullet — sometimes you may inherit a corrupt dataset without even realizing it. One way to spot data leakage is if you are doing very poorly on unseen independent data. For example, say you got a dataset from someone that spanned 2000-2010, but you started collecting you own data from 2011 onward. If your model’s performance is poor on the newly collected data it may be a sign of data leakage. You must resist the urge to retrain the model with both the potentially corrupt and new data. Instated, either try to identify the causes of poor performance on the new data or, better yet, independently reconstruct the entire dataset. As a rule of thumb, your best defense is to always be mindful of the possibility of data leakage in any dataset.
Sampling bias is the case when you shortchange your model by training it on a biased or non-random dataset, which results in a poorly generalizable hypothesis. In the case of housing prices, sampling bias occurs if, for some reason, all the house prices/sizes you collected were of huge mansions. However, when it was time to test your model and the first price you needed to predict was that of a 2-bedroom apartment you couldn’t predict it. Sampling bias happens very frequently mainly because, as humans, we are notorious for being biased (nonrandom) samplers. One of the most common examples of this bias happens in startups and investing. If you attend any business school course, they will use all these “case studies” of how to build a successful company. Such case studies actually depict the anomalies and not the norm as most companies fail — For every Apple that became a success there were 1000 other startups that died trying. So to build an automated data-driven investment strategy you would need samples from both successful and unsuccessful companies.
The figure above (Figure 13) is a concrete example of sampling bias. Say you want to predict whether a tornado is going to originate at certain location based on two environmental conditions: wind shear and convective available potential energy (CAPE). We don’t have to worry about what these variables actually mean, but Figure 13 shows the wind shear and CAPE associated with 242 tornado cases. We can fit a model to these data but it will certainly not generalize because we failed to include shear and CAPE values when tornados did not occur. In order for our model to separate between positive (tornados) and negative (no tornados) events we must train it using both populations.
There you have it. Being mindful of these limitations does not guarantee that your ML algorithm will solve all your problems, but it certainly reduces the risk of being disappointed when your model doesn’t generalize to unseen data. Now go on young Jedi: train your model, you must!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@nomadic_mind. Sometimes the difference between success and failure is the same as between = and ==. Living is in the details. 
",machine learning ml one hottest fields data science soon ml entered mainstream amazon netflix facebook people giddy learn data however modern machine learning ie theoretical statistical learning emerged 70s much evolving field despite many successes still learning exactly ml data practitioners gave talk topic earlier fall northwestern university wanted share cautionary tales wider audience machine learning field computer science algorithms improve performance certain task data observedto algorithms select hypothesis best explains data hand hope hypothesis would generalize future unseen data take left panel figure header crosses denote observed data projected twodimensional space case house prices corresponding size square meters blue line algorithms best hypothesis explain observed data states linear relationship price size house houses size increases price linear increments using hypothesis predict price unseen datapoint based size dimensions data increase hypotheses explain data become complexhowever given using finite sample observations learn hypothesis finding adequate hypothesis generalizes unseen data nontrivial three major pitfalls one fall prevent generalizable model hence conclusions hypothesis doubt occams razor principle attributed william occam 14th century philosopher occams razor advocates choosing simplest hypothesis explains data yet simpler notion simple elegant often misunderstood mean must select simplest hypothesis possible regardless performance 2008 paper nature johan nyberg colleagues used 4level artificial neural network predict seasonal hurricane counts using two three environmental variables authors reported stellar accuracy predicting seasonal north atlantic hurricane counts however model violates occams razor certainly doesnt generalize unseen data razor violated hypothesis model selected describe relationship environmental data seasonal hurricane counts generated using fourlayer neural network fourlayer neural network model virtually function matter complex could fit small dataset well fail generalize unseen data rightmost panel top figure shows incident hypothesis selected algorithm blue curve explain data complex fits every single data point given house size training data give pinpoint accuracy price would sell doesnt take much observe even human couldnt accurate could give close estimate price predict selling price house within dollars every single time impossible pitfall selecting complex hypothesis known overfitting think overfitting memorizing opposed learning child memorizing add numbers may memorize sums pair integers 0 10 however asked calculate 11 12 unable never seen 11 12 therefore couldnt memorize sum thats happens overfitted model gets lazy learn general principle explains data instead memorizes data data leakage occurs data using learn hypothesis happens information trying predict basic form data leakage would use data want predict input model eg use price house predict price house however often data leakage occurs subtly inadvertently example one may wish learn anomalies opposed raw data deviations longterm mean however many fail remove test data computing anomalies hence anomalies carry information data want predict since influenced mean standard deviation removed several ways avoid data leakage outlined claudia perlich great paper subject however silver bullet sometimes may inherit corrupt dataset without even realizing one way spot data leakage poorly unseen independent data example say got dataset someone spanned 20002010 started collecting data 2011 onward models performance poor newly collected data may sign data leakage must resist urge retrain model potentially corrupt new data instated either try identify causes poor performance new data better yet independently reconstruct entire dataset rule thumb best defense always mindful possibility data leakage dataset sampling bias case shortchange model training biased nonrandom dataset results poorly generalizable hypothesis case housing prices sampling bias occurs reason house pricessizes collected huge mansions however time test model first price needed predict 2bedroom apartment couldnt predict sampling bias happens frequently mainly humans notorious biased nonrandom samplers one common examples bias happens startups investing attend business school course use case studies build successful company case studies actually depict anomalies norm companies fail every apple became success 1000 startups died trying build automated datadriven investment strategy would need samples successful unsuccessful companies figure figure 13 concrete example sampling bias say want predict whether tornado going originate certain location based two environmental conditions wind shear convective available potential energy cape dont worry variables actually mean figure 13 shows wind shear cape associated 242 tornado cases fit model data certainly generalize failed include shear cape values tornados occur order model separate positive tornados negative tornados events must train using populations mindful limitations guarantee ml algorithm solve problems certainly reduces risk disappointed model doesnt generalize unseen data go young jedi train model must quick cheer standing ovation clap show much enjoyed story nomadic_mind sometimes difference success failure living details,en,"['Amazon', 'Facebook', 'Northwestern University', 'algorithm', 'house', 'linear', 'unseen datapoint', 'unseen data', 'William of Occam', 'Apple']"
27,Datafiniti,3,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,"At Datafiniti, we have a strong need for converting unstructured web content into structured data. For example, we’d like to find a page like:
and do the following:
Both of these are hard things for a computer to do in an automated manner. While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:
Or
Both of these pages share many similarities to the actual product page, but also have many key differences. The real challenge, though, is that if we look at the entire set of possible web pages, those similarities and differences become somewhat blurred, which means hard and fast rules for classifications will fail often. In fact, we can’t even rely on just looking at the underlying HTML, since there are huge variations in how product pages are laid out in HTML.
While we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page, doing so would be extremely time consuming, and frankly, incredibly boring work. Instead, we can try using a classical technique out of the artificial intelligence handbook: neural networks.
Here’s a quick primer on neural networks. Let’s say we want to know whether any particular mushroom is poisonous or not. We’re not entirely sure what determines this, but we do have a record of mushrooms with their diameters and heights, along with which of these mushrooms were poisonous to eat, for sure. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:
A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible.
Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. By constantly adjusting A and B this way, we can quickly get to the best possible values for them.
In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:
For a more detailed explanation of neural networks, you can check out the following links:
In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. Our input layer modeled several features, including:
Our output layer had the following:
Our algorithm for the neural network took the following steps:
The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. This works like so:
So how did we do? In order to determine how successful we were in our predictions, we need to determine how to measure success. In general, we want to measure how many true positive (TP) results as compared to false positives (FP) and false negatives (FN). Conventional measurements for these are:
Our implementation had the following results:
These scores are just over our training set, of course. The actual scores on real-life data may be a bit lower, but not by much. This is pretty good! We should have an algorithm on our hands that can accurately classify product pages about 90% of the time.
Of course, identifying product pages isn’t enough. We also want to pull out the actual structured data! In particular, we’re interested in product name, price, and any unique identifiers (e.g., UPC, EAN, & ISBN). This information would help us fill out our product search.
We don’t actually use neural networks for doing this. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. Instead, we use a variety of heuristics specific to each attribute we’re trying to extract. For example, for product name, we look at the <h1> and <h2> tags, and use a few metrics to determine the best choice. We’ve been able to achieve around a 80% accuracy here. We may go into the actual metrics and methodology for developing them in a separate post!
We feel pretty good about our ability to classify and extract product data. The extraction part could be better, but it’s steadily being improved. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. This means that we can scan the entire Internet and pull out any available data that exists out there. Exciting stuff!
You can connect with us and learn more about our business, people, product, and property APIs and datasets by selecting one of the options below.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Instant Access to Web Data
Building the world’s largest database of web data — follow our journey.
",datafiniti strong need converting unstructured web content structured data example wed like find page like following hard things computer automated manner easy realize web page selling jeans computer would hard time making distinction page either following web pages pages share many similarities actual product page also many key differences real challenge though look entire set possible web pages similarities differences become somewhat blurred means hard fast rules classifications fail often fact cant even rely looking underlying html since huge variations product pages laid html could try develop complicated set rules account conditions perfectly identify product page would extremely time consuming frankly incredibly boring work instead try using classical technique artificial intelligence handbook neural networks heres quick primer neural networks lets say want know whether particular mushroom poisonous entirely sure determines record mushrooms diameters heights along mushrooms poisonous eat sure order see could use diameter heights determine poisonousness could set following equation diameter b height 0 1 notpoisonous poisonous would try various combinations b possible diameters heights found combination correctly determined poisonousness many mushrooms possible neural networks provide structure using output one set input data adjust b likely best values next set input data constantly adjusting b way quickly get best possible values order introduce complex relationships data introduce hidden layers model would end looking something like detailed explanation neural networks check following links product page classifier algorithm setup neural network 1 input layer 27 nodes 1 hidden layer 25 nodes 1 output layer 3 output nodes input layer modeled several features including output layer following algorithm neural network took following steps ultimate output two sets input layers t1 t2 use matrix equation predict page type given web page works like order determine successful predictions need determine measure success general want measure many true positive tp results compared false positives fp false negatives fn conventional measurements implementation following results scores training set course actual scores reallife data may bit lower much pretty good algorithm hands accurately classify product pages 90 time course identifying product pages isnt enough also want pull actual structured data particular interested product name price unique identifiers eg upc ean isbn information would help us fill product search dont actually use neural networks neural networks bettersuited toward classification problems extracting data web page different type problem instead use variety heuristics specific attribute trying extract example product name look h1 h2 tags use metrics determine best choice weve able achieve around 80 accuracy may go actual metrics methodology developing separate post feel pretty good ability classify extract product data extraction part could better steadily improved meantime also working classifying types pages business data company team pages event data moreas rollout classifiers data extractors including one crawl entire internet means scan entire internet pull available data exists exciting stuff connect us learn business people product property apis datasets selecting one options quick cheer standing ovation clap show much enjoyed story instant access web data building worlds largest database web data follow journey,en,"['T1', 'T2', 'FP', 'UPC', 'EAN', '& ISBN', 'Web Data']"
28,Arjan Haring 🔮🔨,3,Reinventing Social Sciences in the Era of Big Data – I love experiments – Medium,"Sune Lehmann is an Associate Professor at DTU Informatics, Technical University of Denmark. In the past, he has worked as a Postdoctoral Fellow at Institute for Quantitative Social Science at Harvard University and the College of Computer and Information Science at Northeasthern University; before that, he was at Laszlo Barabási’s Center for Complex Network Research at Northeastern University and the Center for Cancer Systems Biology at the Dana Farber Cancer Institute.
I wouldn’t call him stupid. He is okay. Well he is actually pretty great. Forget that, he is freaking fantastic! We should get him over for one of our events! And so we did. Sune spoke at the 2nd #projectwaalhalla.
This time, let’s begin at the beginning, before we dive in deeper. Your main research project has to do with measuring real social networks with high resolution. I know for a fact you don’t mean 3D printed social networks.
But what are you aiming for, and how are you going to get there?
My (humble) research goal is to reinvent social sciences in the age of big data. My background is in mathematical analysis of large networks. But over the past 10 years, I’ve slowly grown more and more interested in understanding social systems.
As a scientist I was blown away by the promise of all of the digital traces of human behavior collected as a consequence of cheap hard drives and databases everywhere. But in spite of the promise of big data, the results so far have been less exciting than I had hoped. For all the hype, deep new scientific insights from big data are far and few between.
A central hypothesis in my work is that in order to advance our quantitative understanding of social interaction, we cannot get by with noisy, incomplete big data: We need good data. Let me explain why and use my own field as an example. Let’s say you have a massive cell phone data set from a telco that provides service to 30% or the population of a large country of 66 million people. That’s something like 20 million people and easily terabytes of monthly data, so a massive dataset.
But when you start thinking about the network, you run into problems. The standard approach is to simply look at the network between the individuals in your sample. Assuming that people are randomly sampled, and links are randomly distributed, you realize that 30% of the population corresponds to only 9% of the links. Is 9% of cell phone calls enough to understand how the network works? With only one in ten links remaining in the dataset, the social structure almost completely erased.
And it gets worse. Telecommunication is only one (small & biased) aspect of human communication. Human interactions may also unfold face-to-face, via text message, email, Facebook, Skype, etc. And these streams are collected in silos, where we cannot generally identify individuals/entities across datasets. So if we think about all these ways we can communicate. Access to only one in ten of my cell phone contacts is very likely insufficient for making valid inferences.
And the worst part is that we can’t know. Without access to the full data set, we can’t even tell what we can and can’t tell from a sample. So when I started out as an assistant professor, I decided to change the course of my career and move from sitting comfortably in front of my computer as a computational/theoretical scientist to becoming an experimenter, to try and attack this problem head on.Now, a few of years later, we have put together a dataset of human social interactions that is unparalleled in terms of quality and size. We recording social interactions within more than 1000 students at my university, using top-of-the-line cell phones as censors. We can capture detailed interaction patterns, such as face-to-face (via bluetooth), social network data (e.g. Facebook and Twitter) via apps, telecommunication data from call logs, and geolocation via GPS & Wifi.
We like to call this type of data ‘Deep Data’: A densely connected group of participants (all the links), observations across many communication channels, high frequency observations (minute-by-minute scale), but with long observation windows (years of collection), and with behavioral data supplemented by classic questionnaires, as well as the possibility of running intervention experiments.
But my expertise (and ultimate interest) is not in building a Deep Data collection platform (although that has been a lot of fun). I want to get back to the questions that motivated the enthusiasm for computational social science in the first place. Reinventing social sciences is what it’s all about.
What can we learn from just one channel? Now that we know about all the communication channels, we can begin to understand what kind of things one may learn from a single channel. Let’s get quantitative about the usefulness of e.g. large cell phone data sets or Facebook, when that’s the only data available.
My heart is still with the network science. In some ways, this whole project is designed to build a system that will really take us places in terms of modeling human social networks. Lots of network science is still about unweighted, undirected static networks; we are already using this dataset to create better models for dynamic, multiplex networks.
Understanding spreading processes (influence, behavior, disease, etc) is a central goal if we look a bit forward in time. We have an system, where N is big enough to perform intervention experiments with randomized controls, etc. We’re still far from implementing this goal, but we’re working on finding the right questions — and working closely with social scientists to get our protocols for these questions just right.
What a coincidence...
We are all about modeling behavior and learning across channels. And with ContagionAPI prominently on our product roadmap we want to start dabbling with spreading processes as well in the near future.
What would you say were major challenges the last years in modeling behavior, and what do see as biggest challenges & opportunities for the future?
There are many challenges. Although we’ve made amazing progress in network science, for example, it’s still a fact that our fundamental understanding of dynamic/multi-channel networks is still in its infancy, there aren’t a lot of easily interpretable models that really explain the underlying networks.
So that’s an area with lots of challenges and corresponding opportunities. And when we want to figure out questions about things taking place on networks, we run into all kinds of problems about how to do statistics right. Brilliant statisticians have shown that homophily and contagion are generically confounded in observational social network studies. On that front, guys like Sinan Aral are doing really exciting work using interventions to get at some of the issues, but there is still lots to do in that area.
Finally, privacy is a big issue. We’re working closely with collaborators at the MIT MediaLab to develop new, responsible solutions — and we’ve already gotten far on that topic. But in terms of data sharing that respects the privacy of study participants, there is still a long way to go. But since studies of digital traces of human behavior will not be going away anytime soon, we have to make progress in this area.
And oh yeah, why does this all matter? And should we be concerned by these things?
I think there are many reasons to be concerned and excited. The more we learn about how systems work, the more we are able to influence them, to control them. That is also true for systems of humans. If we think about spreading of disease, it’d be great to know how to slow down or stop the spread of SARS or similar contagious viruses.
Or, as a society we may be able to increase spread of things we support, such as tolerance, good exercise habits, etc ... and similarly, we can use an understanding influence in social systems to inhibit negative behavior such as intolerance, smoking, etc.
And all this ties into another good reason to be concerned. Companies like Google, Facebook, Apple (or governmental agencies like NSA) are committing serious resources to research in this area. It’s not a coincidence that both Google and Facebook are developing their own cell-phones.
But none of these walled-off players are sharing their results. They’re simply applying them to the public. In my opinion that’s one of the key problems of the current state of affairs, the imbalance of information. We hand over our personal data to powerful corporations, but have nearly zero insight into a) what they know about us and b) what they’re doing with all the stuff they know about us. By doing research that is open, collaborative, explicit about privacy, and public, I hope we can act as a counter-point and work to diminish the information-gap.
Okay, great. But should companies be interested in the stuff you are doing? And if so, why?
I think so! One of the exciting things about this area is that basic research is very close to applied research. Insight into the mechanisms that drive human nature is indeed valuable for companies (I presume that’s why Science Rockstars exists, for example) [note from the editor: not stupid at all].
We already know that human behavior can be influenced significantly with “nudging”, that certain kinds of collective behaviors influence our opinions (and purchasing behaviors). The more we uncover about the details of these mechanism, the more precise and effective we can be about influencing others (let’s discuss the ethics of this another time).
But it’s not just marketing. If used for good, this is the science of what makes people happy. So inside organizations, work like this could be used to re-think organizational structures, incentives, etc; to make employees happier & more fulfilled. Or if we think about organizations as organisms, having access to realtime information about employees can be thought of as a “nervous system” for the company, allowing for faster reaction times when crises arise, identification of pain points, etc.
Finally, for the medical field, we know that genes only explain part of what makes us sick. Being able to quantify and analyze behavior means knowing more about the environment, the nurture part of nurture vs nature. In that sense, detailed data on how we behave could also help us understand how to be healthier.
Originally published at www.sciencerockstars.com on November 2, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Let’s Fix the Future: Scientific Advisor @jadatascience
A blog series about the discipline of business experimentation. How to run and learn from experiments in different contexts is a complex matter, but lays at the heart of innovation.
",sune lehmann associate professor dtu informatics technical university denmark past worked postdoctoral fellow institute quantitative social science harvard university college computer information science northeasthern university laszlo barabasis center complex network research northeastern university center cancer systems biology dana farber cancer institute wouldnt call stupid okay well actually pretty great forget freaking fantastic get one events sune spoke 2nd projectwaalhalla time lets begin beginning dive deeper main research project measuring real social networks high resolution know fact dont mean 3d printed social networks aiming going get humble research goal reinvent social sciences age big data background mathematical analysis large networks past 10 years ive slowly grown interested understanding social systems scientist blown away promise digital traces human behavior collected consequence cheap hard drives databases everywhere spite promise big data results far less exciting hoped hype deep new scientific insights big data far central hypothesis work order advance quantitative understanding social interaction cannot get noisy incomplete big data need good data let explain use field example lets say massive cell phone data set telco provides service 30 population large country 66 million people thats something like 20 million people easily terabytes monthly data massive dataset start thinking network run problems standard approach simply look network individuals sample assuming people randomly sampled links randomly distributed realize 30 population corresponds 9 links 9 cell phone calls enough understand network works one ten links remaining dataset social structure almost completely erased gets worse telecommunication one small biased aspect human communication human interactions may also unfold facetoface via text message email facebook skype etc streams collected silos cannot generally identify individualsentities across datasets think ways communicate access one ten cell phone contacts likely insufficient making valid inferences worst part cant know without access full data set cant even tell cant tell sample started assistant professor decided change course career move sitting comfortably front computer computationaltheoretical scientist becoming experimenter try attack problem head onnow years later put together dataset human social interactions unparalleled terms quality size recording social interactions within 1000 students university using topoftheline cell phones censors capture detailed interaction patterns facetoface via bluetooth social network data eg facebook twitter via apps telecommunication data call logs geolocation via gps wifi like call type data deep data densely connected group participants links observations across many communication channels high frequency observations minutebyminute scale long observation windows years collection behavioral data supplemented classic questionnaires well possibility running intervention experiments expertise ultimate interest building deep data collection platform although lot fun want get back questions motivated enthusiasm computational social science first place reinventing social sciences learn one channel know communication channels begin understand kind things one may learn single channel lets get quantitative usefulness eg large cell phone data sets facebook thats data available heart still network science ways whole project designed build system really take us places terms modeling human social networks lots network science still unweighted undirected static networks already using dataset create better models dynamic multiplex networks understanding spreading processes influence behavior disease etc central goal look bit forward time system n big enough perform intervention experiments randomized controls etc still far implementing goal working finding right questions working closely social scientists get protocols questions right coincidence modeling behavior learning across channels contagionapi prominently product roadmap want start dabbling spreading processes well near future would say major challenges last years modeling behavior see biggest challenges opportunities future many challenges although weve made amazing progress network science example still fact fundamental understanding dynamicmultichannel networks still infancy arent lot easily interpretable models really explain underlying networks thats area lots challenges corresponding opportunities want figure questions things taking place networks run kinds problems statistics right brilliant statisticians shown homophily contagion generically confounded observational social network studies front guys like sinan aral really exciting work using interventions get issues still lots area finally privacy big issue working closely collaborators mit medialab develop new responsible solutions weve already gotten far topic terms data sharing respects privacy study participants still long way go since studies digital traces human behavior going away anytime soon make progress area oh yeah matter concerned things think many reasons concerned excited learn systems work able influence control also true systems humans think spreading disease itd great know slow stop spread sars similar contagious viruses society may able increase spread things support tolerance good exercise habits etc similarly use understanding influence social systems inhibit negative behavior intolerance smoking etc ties another good reason concerned companies like google facebook apple governmental agencies like nsa committing serious resources research area coincidence google facebook developing cellphones none walledoff players sharing results theyre simply applying public opinion thats one key problems current state affairs imbalance information hand personal data powerful corporations nearly zero insight know us b theyre stuff know us research open collaborative explicit privacy public hope act counterpoint work diminish informationgap okay great companies interested stuff think one exciting things area basic research close applied research insight mechanisms drive human nature indeed valuable companies presume thats science rockstars exists example note editor stupid already know human behavior influenced significantly nudging certain kinds collective behaviors influence opinions purchasing behaviors uncover details mechanism precise effective influencing others lets discuss ethics another time marketing used good science makes people happy inside organizations work like could used rethink organizational structures incentives etc make employees happier fulfilled think organizations organisms access realtime information employees thought nervous system company allowing faster reaction times crises arise identification pain points etc finally medical field know genes explain part makes us sick able quantify analyze behavior means knowing environment nurture part nurture vs nature sense detailed data behave could also help us understand healthier originally published wwwsciencerockstarscom november 2 2013 quick cheer standing ovation clap show much enjoyed story lets fix future scientific advisor jadatascience blog series discipline business experimentation run learn experiments different contexts complex matter lays heart innovation,en,"['DTU Informatics', 'Technical University of Denmark', 'Postdoctoral Fellow', 'Institute for Quantitative Social Science', 'Harvard University', 'the College of Computer and Information Science at Northeasthern University', 'Center for Complex Network Research', 'Northeastern University', 'the Center for Cancer Systems Biology', 'the Dana Farber Cancer Institute', 'social network', 'GPS & Wifi', 'Deep Data', 'Facebook', 'N', 'the MIT MediaLab', 'Google', 'Apple', 'NSA']"
29,Eventbrite,10,Multi-Index Locality Sensitive Hashing for Fun and Profit,"One way that we deal with this volume of data, is to cluster up all the similar messages together to find patterns in behavior of senders. For example, if someone is contacting thousands of different organizers with similar messages, that behavior is suspect and will be examined.
The big question is, how can we compare every single message we see with every other message efficiently and accurately? In this article, we’ll be exploring a technique known as Multi-Index Locality Sensitive Hashing.
To perform the the comparison efficiently, we pre-process the data with a series of steps:
Let’s first define what similar messages are. Here we have and example of two similar messages A and B:
To our human eyes of course they’re similar, but we want determine this similarity quantitatively. The solution is to break up the message into tokens, and then treat each message as a bag of tokens. The simplest, naive way to do tokenization is to split up a message on spaces/punctuation and convert each character to lowercase. So our result from our tokenization of the above messages would be:
I’ll leave as an exercise to the reader to come up with more interesting ways to do tokenization for handling contractions, plurals, foreign languages, etc.
To calculate the similarity between these two bags of tokens, we’ll use an estimation known as the Jaccard Similarity Coefficient. This is defined as “the ratio of sizes of the intersection and union of A and B”. Therefore, in our example:
We’ll then set a threshold, above which, we will consider two messages to be similar. So then, when given a set of M messages, we simply compute the similarity of a message to every other message. This works in theory, but in practice there are cases where this metric is unreliable (eg. if one message is significantly longer than the other); not to mention horribly inefficient (O(N2 M2), where N is the number of tokens per message). We need do things smarter!
One problem with doing a simple Jaccard similarity is that the scale of the value changes with the size (number of tokens) of the message. To address this, we can transform our tokens with a method known as minHash. Here’s a psuedo-code snippet:
The interesting property of the minHash transformation is that it leaves us with a constant N number of hashes, and that “chosen” hashes will be in the same positions in the vector. After the minHash transformation, the Jaccard similarity can be approximated by an element-wise comparison of two hash vectors (implemented as pseudo-code above).
So, we can stop here, but we’re having so much fun... and we can do so much better. Notice when we do comparison, we have to to O(N) integer comparisons, and if we have M messages then comparing every message to each other is O(N M2) integer comparisons. This is still not acceptable.
To reduce the time complexity of comparing minHashes to each other, we can do better with a technique known as bit sampling. The main idea is that we don’t need to know the exact value of each hash, but only that the hashes are equal at their respective positions in each hash vector. With this insight, let’s only look at the least significant bit (LSB) of each hash value.
More pseudo-code:
When comparing two messages, if the hashes are equal in the same position in the minHash vector, then the bits in the equivalent position after bit sampling should be also equal. So, we can emulate the Jaccard similarity of two minHashes by counting the equal bits in the two bit vectors (aka. the Hamming Distance) and dividing by the number of bits. Of course, two different hashes will have the same LSB 50% of the time; to increase our efficacy, we would pick a large N initially. Here is some naive and inefficient pseudo-code:
In practice, more efficient implementations of the bitSimilarity function can calculate in near O(1) time for reasonable sizes of N (Bit Twiddling Hacks). This means that when comparing M messages to each other, we’ve reduced the time complexity to O(M2). But wait, there’s more!
Remember how I said we have a lot of data? O(M2) is still unreasonable when M is a very large number of messages. So we need to try to reduce the number of comparisons to make using a “divide and conquer” strategy.
Lets start with an example where we set N=32, and we want to have a bitSimilarity of .9: In the worst case, to do this, we need 28 of the 32 bits to be equal, or 4 bits unequal. We will refer to the number of unequal bits as the radius of the bit vectors; ie. if two bit vectors are within a certain radius of bits, then they are similar. The unequal bits can be found by taking the bit-wise XOR of the two bit vectors. For example:
If we split up XOR_mask into 4 chunks of 8 bits, then at least one chunk will have exactly zero or exactly one of the bit differences (pigeonhole principal). More generally, if we split XOR_mask of size N into K chunks, with an expected radius R, then at least one chunk is guaranteed to have floor(R / K) or less bits unequal. For the purpose of explanation, we will assume that we have chosen all the parameters such that floor(R / K) = 1.
Now you’re wondering how this piece of logic help us? We can now design a data structure LshTable to index the bit vectors to reduce the number of bitSimilarity comparisons drastically (but increase memory consumption in O(M)) [Fast Search in Hamming Space with Multi-Index Hashing].
We will define LshTable with some pseudo-code:
Basically, in LshTable initialization, we create K hash tables for each K chunks. During add() of a bit vector, we split the bit vector into K chunks. For each of these chunks, we add the original bit vector into the associated hash table under the index chunk.
Upon the lookup() of a bit vector, we once again split it into chunks and for each chunk look up the associated hash table for a chunk that’s close (zero or one bits off). The returned list is a set of candidate bit vectors to check bitSimilarity. Because of the property explained in the previous section, at least one hash table will contain a set of candidates that contains a similar bit vector.
To compare every M message to every other message we first insert its bit vector into an LshTable (an O(K) operation, K is constant). Then to find similar messages, we simply do a lookup from the LshTable (another O(K) operation), and then check bitSimilarity for each of the candidates returned. The number of candidates to check is usually on the order of M / 2^(N/K), if at all. Therefore, the time complexity to compare all M messages to each other is O(M * M / 2^(N/K)). In practice, N and K are empirically chosen such that 2^(N/K) >> M, so the final time complexity is O(M) — remember we started with O(N M2)!
Phew, what a ride. So, we’ve detailed how to find similar messages in a very large set of messages efficiently. By using Multi-Index Locality Sensitivity Hashing, we can reduce the time complexity of from quadratic (with a very high constant) to near linear (with a more manageable constant).
I should also mention that many of the ancillary pseudo-code excerpts used here describe the most naive implementation of each method, and are for instructive purposes only.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
We help bring the world together through live experiences.
",one way deal volume data cluster similar messages together find patterns behavior senders example someone contacting thousands different organizers similar messages behavior suspect examined big question compare every single message see every message efficiently accurately article well exploring technique known multiindex locality sensitive hashing perform comparison efficiently preprocess data series steps lets first define similar messages example two similar messages b human eyes course theyre similar want determine similarity quantitatively solution break message tokens treat message bag tokens simplest naive way tokenization split message spacespunctuation convert character lowercase result tokenization messages would ill leave exercise reader come interesting ways tokenization handling contractions plurals foreign languages etc calculate similarity two bags tokens well use estimation known jaccard similarity coefficient defined ratio sizes intersection union b therefore example well set threshold consider two messages similar given set messages simply compute similarity message every message works theory practice cases metric unreliable eg one message significantly longer mention horribly inefficient on2 m2 n number tokens per message need things smarter one problem simple jaccard similarity scale value changes size number tokens message address transform tokens method known minhash heres psuedocode snippet interesting property minhash transformation leaves us constant n number hashes chosen hashes positions vector minhash transformation jaccard similarity approximated elementwise comparison two hash vectors implemented pseudocode stop much fun much better notice comparison integer comparisons messages comparing every message m2 integer comparisons still acceptable reduce time complexity comparing minhashes better technique known bit sampling main idea dont need know exact value hash hashes equal respective positions hash vector insight lets look least significant bit lsb hash value pseudocode comparing two messages hashes equal position minhash vector bits equivalent position bit sampling also equal emulate jaccard similarity two minhashes counting equal bits two bit vectors aka hamming distance dividing number bits course two different hashes lsb 50 time increase efficacy would pick large n initially naive inefficient pseudocode practice efficient implementations bitsimilarity function calculate near o1 time reasonable sizes n bit twiddling hacks means comparing messages weve reduced time complexity om2 wait theres remember said lot data om2 still unreasonable large number messages need try reduce number comparisons make using divide conquer strategy lets start example set n32 want bitsimilarity 9 worst case need 28 32 bits equal 4 bits unequal refer number unequal bits radius bit vectors ie two bit vectors within certain radius bits similar unequal bits found taking bitwise xor two bit vectors example split xor_mask 4 chunks 8 bits least one chunk exactly zero exactly one bit differences pigeonhole principal generally split xor_mask size n k chunks expected radius r least one chunk guaranteed floorr k less bits unequal purpose explanation assume chosen parameters floorr k 1 youre wondering piece logic help us design data structure lshtable index bit vectors reduce number bitsimilarity comparisons drastically increase memory consumption om fast search hamming space multiindex hashing define lshtable pseudocode basically lshtable initialization create k hash tables k chunks add bit vector split bit vector k chunks chunks add original bit vector associated hash table index chunk upon lookup bit vector split chunks chunk look associated hash table chunk thats close zero one bits returned list set candidate bit vectors check bitsimilarity property explained previous section least one hash table contain set candidates contains similar bit vector compare every message every message first insert bit vector lshtable ok operation k constant find similar messages simply lookup lshtable another ok operation check bitsimilarity candidates returned number candidates check usually order 2nk therefore time complexity compare messages om 2nk practice n k empirically chosen 2nk final time complexity om remember started m2 phew ride weve detailed find similar messages large set messages efficiently using multiindex locality sensitivity hashing reduce time complexity quadratic high constant near linear manageable constant also mention many ancillary pseudocode excerpts used describe naive implementation method instructive purposes quick cheer standing ovation clap show much enjoyed story help bring world together live experiences,en,"['the Jaccard Similarity Coefficient', 'N', 'the Hamming Distance', 'bitSimilarity', 'N (Bit Twiddling Hacks', 'LshTable', 'K', '2^(N/K']"
30,Akash Shende,1,Color Based Object Segmentation – Akash Shende – Medium,"In this picture, Pranav Mistry is using color marker on his fingers to track the gesture and his wearable computer perform action based on gestures. That sounds easy! But No, it’s not. Computer need to understand those color marker first, for that it needs to separate marker from any surroundings.
Segmentation can be helpful to achieve this. Various methods are available for segmentation, however, this article talks about robust Color based object segmentation.
Create binary mask that separates blue T-shirt from rest.
To find blue t-shirt in given image, I used OpenCV’s inRange method: Which takes color (or greyscale) image, lower & higher range value as its parameter and returns binary image, where pixel value set to 0 when input pixel doesn’t fall in specified range, otherwise pixel value set to 1. With the help of this function and after determining range values, I ended up with this mask.
But you can see there are problems! It’s not able to create mask for complete t-shirt, also it mask eyes which aren’t blue. This is happening because light from one side of body whitens the right side at the same time creates shadow in left region. Thus, it creates different shades of blue and results into partial segmentation.
Normalization of color plane reduces variation in light by averaging pixel values, thus it removes highlighted and shadowed region and make image flatten. Following image is free from highlights & shadows and it is divided into one large green background, blue t-shirt and skin. Now the inRange method able to mask only t-shirt.
Following function converts a pixel at X, Y location into its corresponding normalized rgb pixel.
Let R,G,B are pixel values, then normalized pixel g(x,y) is calculated as,divide the individual color component by sum of all color components and multiply by 255. Division results into floating point number in range of 0.0 to 1.0 and as this is 8 bit image result is scaled up by 255.
This function accepts 8 bit RGB image matrix of size 800x600 and returns normalized RGB image.
Originally published at akash0x53.github.io on April 29, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Python आणि बरच काही .
",picture pranav mistry using color marker fingers track gesture wearable computer perform action based gestures sounds easy computer need understand color marker first needs separate marker surroundings segmentation helpful achieve various methods available segmentation however article talks robust color based object segmentation create binary mask separates blue tshirt rest find blue tshirt given image used opencvs inrange method takes color greyscale image lower higher range value parameter returns binary image pixel value set 0 input pixel doesnt fall specified range otherwise pixel value set 1 help function determining range values ended mask see problems able create mask complete tshirt also mask eyes arent blue happening light one side body whitens right side time creates shadow left region thus creates different shades blue results partial segmentation normalization color plane reduces variation light averaging pixel values thus removes highlighted shadowed region make image flatten following image free highlights shadows divided one large green background blue tshirt skin inrange method able mask tshirt following function converts pixel x location corresponding normalized rgb pixel let rgb pixel values normalized pixel gxy calculated asdivide individual color component sum color components multiply 255 division results floating point number range 00 10 8 bit image result scaled 255 function accepts 8 bit rgb image matrix size 800x600 returns normalized rgb image originally published akash0x53githubio april 29 2013 quick cheer standing ovation clap show much enjoyed story python,en,"['Pranav Mistry', 'Color', 'pixel', 'highlights & shadows', 'inRange', 'Division', 'RGB', 'Python आणि बरच']"
31,Hrishikesh Huilgolkar,1,Traveling santa Problem — An incompetent algorist’s attempt,"Kaggle announced the Traveling santa problem in the christmas season. I joined in excitedly.. but soon realized this is not an easy problem. Solving this problem would require expertise on data structures and some good familiarity with TSP problems and its many heuristic algorithms. I had neither.. I had to find a way to deal with this problem. I compenseted my lack of algorithmic expertise with common sense, logic and intuition. I finished 65th out of 356 total competitors.
I did some research on packaged TSP solvers and top TSP algorithms. I found concorde but I could not get it to work on my ubuntu machine. So I settled with LKH which uses Lin-Kernighan heuristic for solving TSP and related problems. I wrote scripts for file conversions and for running LKH.
LKH easily solved my tsp problem in around 30 hours. But it was just one path. I still had to figure out how to make it find the second path.A simple idea to get 2 disjoint paths is to generate first path and then make weight of those edges infinite and run LKH on the problem again. But this required the problem to be in Distance Matrix format.Then I found a major problem.
Problem: Ram too lowCreating distance matrix for 150,000 points was unimaginable.It would requirememory for one digit * 150,000 * 150,000assuming memory for one digit = 8 bytes
memory required = 8*150,0002which is 167 GB!
(Correct me if I am wrong)
Solution:A simple solution was to divide the map in manageable chunks.I used scipy’s distance matrix creation function scipy.spatial.distance.pdist() It creates distance matrix from coordinates.The matrix created by pdist is in compressed form (a flattened matrix of upper diagonal elements. scipy.spatial.distance.squareform() can create a square distance matrix from compressed matrix but that would waste a lot of ram.So I created a custom function which divided compressed matrix by rows so LKH can read it.
Input:(coordinates)1 12 34 1
output of pdist:(compressed upper column)1 2 4
Output of squreform():(Uncompressed square matrix)0 1 21 0 42 4 0
Output of my function which processed compressed matrix:(Upper diagonal elements)[[1,2],[4]]
Lots of ram saved!
I tried using Manhattan distance instead of euclidean distance. But after dividing the problem in grids, time taken by distance calculation was manageable so I stuck with euclidean distance.
Through trial and error, I found that on my laptop with 4 GB ram, a 6 by 6 grid in the above format was manageable for both creating distance matrix and for LKH.
I ran LKH on resulting distance matrices and joined the individual solutions.
I joined the resulting solutions in different combination for both paths so as to avoid common paths.
I got 7,415,334 with this method.
I tried time limit on LKH algorithm. From 40,000 seconds I reduced it to 300, 20, 5 ,1 seconds but It made the results slightly worse.
Mingle
The solution above was good but It could have been better. The problem was that the first path was so good that the second path struggled to find good path. The difference between the two paths was big.
Path1 ~= 6.2MPath2 ~= 7.4MFor a long time I thought this would require either solving both paths simultaneously or using genetic algorithm or similar algorithm to combine both paths. Both were pretty difficult to implement.Then I got a simple idea. My map was divided in 36 squares. If I combine 18 squares of first path and 18 squares of second path, I will have a path whose distance will be approximately average of the two paths.I tried this trick and used different combinations of the two paths squares and got the best score of 6,807,498
For new path1, select blue squres from old path1 and grey square from old path2
Use remaining squares for new path2.Remove cross lines
My squares were joined in a zigzag manner. I removed the zig-zag lines for a further improvement.
I scored 6,744,291 which was my best score.Another idea was to make end point of one square and the beginning point of next square as near as possible but I couldn’t implement the idea before deadline.My score was around 200,000 points away from the first place which was 6,526,972. Not bad!
Public repo: https://bitbucket.org/hrishikeshio/traveling-santa (More documentation for source code coming soon)
Originally published at www.blogicious.com on January 19, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Blockchain, cryptocurrencies and the decentralised future
",kaggle announced traveling santa problem christmas season joined excitedly soon realized easy problem solving problem would require expertise data structures good familiarity tsp problems many heuristic algorithms neither find way deal problem compenseted lack algorithmic expertise common sense logic intuition finished 65th 356 total competitors research packaged tsp solvers top tsp algorithms found concorde could get work ubuntu machine settled lkh uses linkernighan heuristic solving tsp related problems wrote scripts file conversions running lkh lkh easily solved tsp problem around 30 hours one path still figure make find second patha simple idea get 2 disjoint paths generate first path make weight edges infinite run lkh problem required problem distance matrix formatthen found major problem problem ram lowcreating distance matrix 150000 points unimaginableit would requirememory one digit 150000 150000assuming memory one digit 8 bytes memory required 81500002which 167 gb correct wrong solutiona simple solution divide map manageable chunksi used scipys distance matrix creation function scipyspatialdistancepdist creates distance matrix coordinatesthe matrix created pdist compressed form flattened matrix upper diagonal elements scipyspatialdistancesquareform create square distance matrix compressed matrix would waste lot ramso created custom function divided compressed matrix rows lkh read inputcoordinates1 12 34 1 output pdistcompressed upper column1 2 4 output squreformuncompressed square matrix0 1 21 0 42 4 0 output function processed compressed matrixupper diagonal elements124 lots ram saved tried using manhattan distance instead euclidean distance dividing problem grids time taken distance calculation manageable stuck euclidean distance trial error found laptop 4 gb ram 6 6 grid format manageable creating distance matrix lkh ran lkh resulting distance matrices joined individual solutions joined resulting solutions different combination paths avoid common paths got 7415334 method tried time limit lkh algorithm 40000 seconds reduced 300 20 5 1 seconds made results slightly worse mingle solution good could better problem first path good second path struggled find good path difference two paths big path1 62mpath2 74mfor long time thought would require either solving paths simultaneously using genetic algorithm similar algorithm combine paths pretty difficult implementthen got simple idea map divided 36 squares combine 18 squares first path 18 squares second path path whose distance approximately average two pathsi tried trick used different combinations two paths squares got best score 6807498 new path1 select blue squres old path1 grey square old path2 use remaining squares new path2remove cross lines squares joined zigzag manner removed zigzag lines improvement scored 6744291 best scoreanother idea make end point one square beginning point next square near possible couldnt implement idea deadlinemy score around 200000 points away first place 6526972 bad public repo httpsbitbucketorghrishikeshiotravelingsanta documentation source code coming soon originally published wwwblogiciouscom january 19 2013 quick cheer standing ovation clap show much enjoyed story blockchain cryptocurrencies decentralised future,en,"['Kaggle', 'Traveling', 'LKH', 'TSP', 'Mingle', '~= 6.2MPath2 ~= 7.4MFor']"
32,Shivon Zilis,1200,The Current State of Machine Intelligence – Shivon Zilis – Medium,"(The 2016 Machine Intelligence landscape and post can be found here)
I spent the last three months learning about every artificial intelligence, machine learning, or data related startup I could find — my current list has 2,529 of them to be exact. Yes, I should find better things to do with my evenings and weekends but until then...
Why do this?
A few years ago, investors and startups were chasing “big data” (I helped put together a landscape on that industry). Now we’re seeing a similar explosion of companies calling themselves artificial intelligence, machine learning, or somesuch — collectively I call these “machine intelligence” (I’ll get into the definitions in a second). Our fund, Bloomberg Beta, which is focused on the future of work, has been investing in these approaches. I created this landscape to start to put startups into context. I’m a thesis-oriented investor and it’s much easier to identify crowded areas and see white space once the landscape has some sort of taxonomy.
What is “machine intelligence,” anyway?
I mean “machine intelligence” as a unifying term for what others call machine learning and artificial intelligence. (Some others have used the term before, without quite describing it or understanding how laden this field has been with debates over descriptions.) I would have preferred to avoid a different label but when I tried either “artificial intelligence” or “machine learning” both proved to too narrow: when I called it “artificial intelligence” too many people were distracted by whether certain companies were “true AI,” and when I called it “machine learning,” many thought I wasn’t doing justice to the more “AI-esque” like the various flavors of deep learning. People have immediately grasped “machine intelligence” so here we are. ☺
Computers are learning to think, read, and write. They’re also picking up human sensory function, with the ability to see and hear (arguably to touch, taste, and smell, though those have been of a lesser focus). Machine intelligence technologies cut across a vast array of problem types (from classification and clustering to natural language processing and computer vision) and methods (from support vector machines to deep belief networks). All of these technologies are reflected on this landscape.
What this landscape doesn’t include, however important, is “big data” technologies. Some have used this term interchangeably with machine learning and artificial intelligence, but I want to focus on the intelligence methods rather than data, storage, and computation pieces of the puzzle for this landscape (though of course data technologies enable machine intelligence).
Which companies are on the landscape?
I considered thousands of companies, so while the chart is crowded it’s still a small subset of the overall ecosystem. “Admissions rates” to the chart were fairly in line with those of Yale or Harvard, and perhaps equally arbitrary. ☺
I tried to pick companies that used machine intelligence methods as a defining part of their technology. Many of these companies clearly belong in multiple areas but for the sake of simplicity I tried to keep companies in their primary area and categorized them by the language they use to describe themselves (instead of quibbling over whether a company used “NLP” accurately in its self-description).
If you want to get a sense for innovations at the heart of machine intelligence, focus on the core technologies layer. Some of these companies have APIs that power other applications, some sell their platforms directly into enterprise, some are at the stage of cryptic demos, and some are so stealthy that all we have is a few sentences to describe them.
The most exciting part for me was seeing how much is happening in the application space. These companies separated nicely into those that reinvent the enterprise, industries, and ourselves.
If I were looking to build a company right now, I’d use this landscape to help figure out what core and supporting technologies I could package into a novel industry application. Everyone likes solving the sexy problems but there are an incredible amount of ‘unsexy’ industry use cases that have massive market opportunities and powerful enabling technologies that are begging to be used for creative applications (e.g., Watson Developer Cloud, AlchemyAPI).
Reflections on the landscape:
We’ve seen a few great articles recently outlining why machine intelligence is experiencing a resurgence, documenting the enabling factors of this resurgence. (Kevin Kelly, for example chalks it up to cheap parallel computing, large datasets, and better algorithms.) I focused on understanding the ecosystem on a company-by-company level and drawing implications from that.
Yes, it’s true, machine intelligence is transforming the enterprise, industries and humans alike.
On a high level it’s easy to understand why machine intelligence is important, but it wasn’t until I laid out what many of these companies are actually doing that I started to grok how much it is already transforming everything around us. As Kevin Kelly more provocatively put it, “the business plans of the next 10,000 startups are easy to forecast: Take X and add AI”. In many cases you don’t even need the X — machine intelligence will certainly transform existing industries, but will also likely create entirely new ones.
Machine intelligence is enabling applications we already expect like automated assistants (Siri), adorable robots (Jibo), and identifying people in images (like the highly effective but unfortunately named DeepFace). However, it’s also doing the unexpected: protecting children from sex trafficking, reducing the chemical content in the lettuce we eat, helping us buy shoes online that fit our feet precisely, and destroying 80's classic video games.
Many companies will be acquired.
I was surprised to find that over 10% of the eligible (non-public) companies on the slide have been acquired. It was in stark contrast to big data landscape we created, which had very few acquisitions at the time.
No jaw will drop when I reveal that Google is the number one acquirer, though there were more than 15 different acquirers just for the companies on this chart. My guess is that by the end of 2015 almost another 10% will be acquired. For thoughts on which specific ones will get snapped up in the next year you’ll have to twist my arm...
Big companies have a disproportionate advantage, especially those that build consumer products.
The giants in search (Google, Baidu), social networks (Facebook, LinkedIn, Pinterest), content (Netflix, Yahoo!), mobile (Apple) and e-commerce (Amazon) are in an incredible position. They have massive datasets and constant consumer interactions that enable tight feedback loops for their algorithms (and these factors combine to create powerful network effects) — and they have the most to gain from the low hanging fruit that machine intelligence bears.
Best-in-class personalization and recommendation algorithms have enabled these companies’ success (it’s both impressive and disconcerting that Facebook recommends you add the person you had a crush on in college and Netflix tees up that perfect guilty pleasure sitcom). Now they are all competing in a new battlefield: the move to mobile. Winning mobile will require lots of machine intelligence: state of the art natural language interfaces (like Apple’s Siri), visual search (like Amazon’s “FireFly”), and dynamic question answering technology that tells you the answer instead of providing a menu of links (all of the search companies are wrestling with this).Large enterprise companies (IBM and Microsoft) have also made incredible strides in the field, though they don’t have the same human-facing requirements so are focusing their attention more on knowledge representation tasks on large industry datasets, like IBM Watson’s application to assist doctors with diagnoses.
The talent’s in the New (AI)vy League.
In the last 20 years, most of the best minds in machine intelligence (especially the ‘hardcore AI’ types) worked in academia. They developed new machine intelligence methods, but there were few real world applications that could drive business value.
Now that real world applications of more complex machine intelligence methods like deep belief nets and hierarchical neural networks are starting to solve real world problems, we’re seeing academic talent move to corporate settings. Facebook recruited NYU professors Yann LeCun and Rob Fergus to their AI Lab, Google hired University of Toronto’s Geoffrey Hinton, Baidu wooed Andrew Ng. It’s important to note that they all still give back significantly to the academic community (one of LeCun’s lab mandates is to work on core research to give back to the community, Hinton spends half of his time teaching, Ng has made machine intelligence more accessible through Coursera) but it is clear that a lot of the intellectual horsepower is moving away from academia.
For aspiring minds in the space, these corporate labs not only offer lucrative salaries and access to the “godfathers” of the industry, but, the most important ingredient: data. These labs offer talent access to datasets they could never get otherwise (the ImageNet dataset is fantastic, but can’t compare to what Facebook, Google, and Baidu have in house). As a result, we’ll likely see corporations become the home of many of the most important innovations in machine intelligence and recruit many of the graduate students and postdocs that would have otherwise stayed in academia.
There will be a peace dividend.
Big companies have an inherent advantage and it’s likely that the ones who will win the machine intelligence race will be even more powerful than they are today. However, the good news for the rest of the world is that the core technology they develop will rapidly spill into other areas, both via departing talent and published research.
Similar to the big data revolution, which was sparked by the release of Google’s BigTable and BigQuery papers, we will see corporations release equally groundbreaking new technologies into the community. Those innovations will be adapted to new industries and use cases that the Googles of the world don’t have the DNA or desire to tackle.
Opportunities for entrepreneurs:
“My company does deep learning for X”
Few words will make you more popular in 2015. That is, if you can credibly say them.
Deep learning is a particularly popular method in the machine intelligence field that has been getting a lot of attention. Google, Facebook, and Baidu have achieved excellent results with the method for vision and language based tasks and startups like Enlitic have shown promising results as well.
Yes, it will be an overused buzzword with excitement ahead of results and business models, but unlike the hundreds of companies that say they do “big data”, it’s much easier to cut to the chase in terms of verifying credibility here if you’re paying attention.
The most exciting part about the deep learning method is that when applied with the appropriate levels of care and feeding, it can replace some of the intuition that comes from domain expertise with automatically-learned features. The hope is that, in many cases, it will allow us to fundamentally rethink what a best-in-class solution is.
As an investor who is curious about the quirkier applications of data and machine intelligence, I can’t wait to see what creative problems deep learning practitioners try to solve. I completely agree with Jeff Hawkins when he says a lot of the killer applications of these types of technologies will sneak up on us. I fully intend to keep an open mind.
“Acquihire as a business model”
People say that data scientists are unicorns in short supply. The talent crunch in machine intelligence will make it look like we had a glut of data scientists. In the data field, many people had industry experience over the past decade. Most hardcore machine intelligence work has only been in academia. We won’t be able to grow this talent overnight.
This shortage of talent is a boon for founders who actually understand machine intelligence. A lot of companies in the space will get seed funding because there are early signs that the acquihire price for a machine intelligence expert is north of 5x that of a normal technical acquihire (take, for example Deep Mind, where price per technical head was somewhere between $5–10M, if we choose to consider it in the acquihire category). I’ve had multiple friends ask me, only semi-jokingly, “Shivon, should I just round up all of my smartest friends in the AI world and call it a company?” To be honest, I’m not sure what to tell them. (At Bloomberg Beta, we’d rather back companies building for the long term, but that doesn’t mean this won’t be a lucrative strategy for many enterprising founders.)
A good demo is disproportionately valuable in machine intelligence
I remember watching Watson play Jeopardy. When it struggled at the beginning I felt really sad for it. When it started trouncing its competitors I remember cheering it on as if it were the Toronto Maple Leafs in the Stanley Cup finals (disclaimers: (1) I was an IBMer at the time so was biased towards my team (2) the Maple Leafs have not made the finals during my lifetime — yet — so that was purely a hypothetical).
Why do these awe-inspiring demos matter? The last wave of technology companies to IPO didn’t have demos that most of us would watch, so why should machine intelligence companies? The last wave of companies were very computer-like: database companies, enterprise applications, and the like. Sure, I’d like to see a 10x more performant database, but most people wouldn’t care. Machine intelligence wins and loses on demos because 1) the technology is very human, enough to inspire shock and awe, 2) business models tend to take a while to form, so they need more funding for longer period of time to get them there, 3) they are fantastic acquisition bait.
Watson beat the world’s best humans at trivia, even if it thought Toronto was a US city. DeepMind blew people away by beating video games. Vicarious took on CAPTCHA. There are a few companies still in stealth that promise to impress beyond that, and I can’t wait to see if they get there.
Demo or not, I’d love to talk to anyone using machine intelligence to change the world. There’s no industry too unsexy, no problem too geeky. I’d love to be there to help so don’t be shy.
I hope this landscape chart sparks a conversation. The goal to is make this a living document and I want to know if there are companies or categories missing. I welcome feedback and would like to put together a dynamic visualization where I can add more companies and dimensions to the data (methods used, data types, end users, investment to date, location, etc.) so that folks can interact with it to better explore the space.
Questions and comments: Please email me. Thank you to Andrew Paprocki, Aria Haghighi, Beau Cronin, Ben Lorica, Doug Fulop, David Andrzejewski, Eric Berlow, Eric Jonas, Gary Kazantsev, Gideon Mann, Greg Smithies, Heidi Skinner, Jack Clark, Jon Lehr, Kurt Keutzer, Lauren Barless, Pete Skomoroch, Pete Warden, Roger Magoulas, Sean Gourley, Stephen Purpura, Wes McKinney, Zach Bogue, the Quid team, and the Bloomberg Beta team for your ever-helpful perspectives!
Disclaimer: Bloomberg Beta is an investor in Adatao, Alation, Aviso, BrightFunnel, Context Relevant, Mavrx, Newsle, Orbital Insights, Pop Up Archive, and two others on the chart that are still undisclosed. We’re also investors in a few other machine intelligence companies that aren’t focusing on areas that were a fit for this landscape, so we left them off.
For the full resolution version of the landscape please click here.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Partner at Bloomberg Beta. All about machine intelligence for good. Equal parts nerd and athlete. Straight up Canadian stereotype and proud of it.
",2016 machine intelligence landscape post found spent last three months learning every artificial intelligence machine learning data related startup could find current list 2529 exact yes find better things evenings weekends years ago investors startups chasing big data helped put together landscape industry seeing similar explosion companies calling artificial intelligence machine learning somesuch collectively call machine intelligence ill get definitions second fund bloomberg beta focused future work investing approaches created landscape start put startups context im thesisoriented investor much easier identify crowded areas see white space landscape sort taxonomy machine intelligence anyway mean machine intelligence unifying term others call machine learning artificial intelligence others used term without quite describing understanding laden field debates descriptions would preferred avoid different label tried either artificial intelligence machine learning proved narrow called artificial intelligence many people distracted whether certain companies true ai called machine learning many thought wasnt justice aiesque like various flavors deep learning people immediately grasped machine intelligence computers learning think read write theyre also picking human sensory function ability see hear arguably touch taste smell though lesser focus machine intelligence technologies cut across vast array problem types classification clustering natural language processing computer vision methods support vector machines deep belief networks technologies reflected landscape landscape doesnt include however important big data technologies used term interchangeably machine learning artificial intelligence want focus intelligence methods rather data storage computation pieces puzzle landscape though course data technologies enable machine intelligence companies landscape considered thousands companies chart crowded still small subset overall ecosystem admissions rates chart fairly line yale harvard perhaps equally arbitrary tried pick companies used machine intelligence methods defining part technology many companies clearly belong multiple areas sake simplicity tried keep companies primary area categorized language use describe instead quibbling whether company used nlp accurately selfdescription want get sense innovations heart machine intelligence focus core technologies layer companies apis power applications sell platforms directly enterprise stage cryptic demos stealthy sentences describe exciting part seeing much happening application space companies separated nicely reinvent enterprise industries looking build company right id use landscape help figure core supporting technologies could package novel industry application everyone likes solving sexy problems incredible amount unsexy industry use cases massive market opportunities powerful enabling technologies begging used creative applications eg watson developer cloud alchemyapi reflections landscape weve seen great articles recently outlining machine intelligence experiencing resurgence documenting enabling factors resurgence kevin kelly example chalks cheap parallel computing large datasets better algorithms focused understanding ecosystem companybycompany level drawing implications yes true machine intelligence transforming enterprise industries humans alike high level easy understand machine intelligence important wasnt laid many companies actually started grok much already transforming everything around us kevin kelly provocatively put business plans next 10000 startups easy forecast take x add ai many cases dont even need x machine intelligence certainly transform existing industries also likely create entirely new ones machine intelligence enabling applications already expect like automated assistants siri adorable robots jibo identifying people images like highly effective unfortunately named deepface however also unexpected protecting children sex trafficking reducing chemical content lettuce eat helping us buy shoes online fit feet precisely destroying 80s classic video games many companies acquired surprised find 10 eligible nonpublic companies slide acquired stark contrast big data landscape created acquisitions time jaw drop reveal google number one acquirer though 15 different acquirers companies chart guess end 2015 almost another 10 acquired thoughts specific ones get snapped next year youll twist arm big companies disproportionate advantage especially build consumer products giants search google baidu social networks facebook linkedin pinterest content netflix yahoo mobile apple ecommerce amazon incredible position massive datasets constant consumer interactions enable tight feedback loops algorithms factors combine create powerful network effects gain low hanging fruit machine intelligence bears bestinclass personalization recommendation algorithms enabled companies success impressive disconcerting facebook recommends add person crush college netflix tees perfect guilty pleasure sitcom competing new battlefield move mobile winning mobile require lots machine intelligence state art natural language interfaces like apples siri visual search like amazons firefly dynamic question answering technology tells answer instead providing menu links search companies wrestling thislarge enterprise companies ibm microsoft also made incredible strides field though dont humanfacing requirements focusing attention knowledge representation tasks large industry datasets like ibm watsons application assist doctors diagnoses talents new aivy league last 20 years best minds machine intelligence especially hardcore ai types worked academia developed new machine intelligence methods real world applications could drive business value real world applications complex machine intelligence methods like deep belief nets hierarchical neural networks starting solve real world problems seeing academic talent move corporate settings facebook recruited nyu professors yann lecun rob fergus ai lab google hired university torontos geoffrey hinton baidu wooed andrew ng important note still give back significantly academic community one lecuns lab mandates work core research give back community hinton spends half time teaching ng made machine intelligence accessible coursera clear lot intellectual horsepower moving away academia aspiring minds space corporate labs offer lucrative salaries access godfathers industry important ingredient data labs offer talent access datasets could never get otherwise imagenet dataset fantastic cant compare facebook google baidu house result well likely see corporations become home many important innovations machine intelligence recruit many graduate students postdocs would otherwise stayed academia peace dividend big companies inherent advantage likely ones win machine intelligence race even powerful today however good news rest world core technology develop rapidly spill areas via departing talent published research similar big data revolution sparked release googles bigtable bigquery papers see corporations release equally groundbreaking new technologies community innovations adapted new industries use cases googles world dont dna desire tackle opportunities entrepreneurs company deep learning x words make popular 2015 credibly say deep learning particularly popular method machine intelligence field getting lot attention google facebook baidu achieved excellent results method vision language based tasks startups like enlitic shown promising results well yes overused buzzword excitement ahead results business models unlike hundreds companies say big data much easier cut chase terms verifying credibility youre paying attention exciting part deep learning method applied appropriate levels care feeding replace intuition comes domain expertise automaticallylearned features hope many cases allow us fundamentally rethink bestinclass solution investor curious quirkier applications data machine intelligence cant wait see creative problems deep learning practitioners try solve completely agree jeff hawkins says lot killer applications types technologies sneak us fully intend keep open mind acquihire business model people say data scientists unicorns short supply talent crunch machine intelligence make look like glut data scientists data field many people industry experience past decade hardcore machine intelligence work academia wont able grow talent overnight shortage talent boon founders actually understand machine intelligence lot companies space get seed funding early signs acquihire price machine intelligence expert north 5x normal technical acquihire take example deep mind price per technical head somewhere 510m choose consider acquihire category ive multiple friends ask semijokingly shivon round smartest friends ai world call company honest im sure tell bloomberg beta wed rather back companies building long term doesnt mean wont lucrative strategy many enterprising founders good demo disproportionately valuable machine intelligence remember watching watson play jeopardy struggled beginning felt really sad started trouncing competitors remember cheering toronto maple leafs stanley cup finals disclaimers 1 ibmer time biased towards team 2 maple leafs made finals lifetime yet purely hypothetical aweinspiring demos matter last wave technology companies ipo didnt demos us would watch machine intelligence companies last wave companies computerlike database companies enterprise applications like sure id like see 10x performant database people wouldnt care machine intelligence wins loses demos 1 technology human enough inspire shock awe 2 business models tend take form need funding longer period time get 3 fantastic acquisition bait watson beat worlds best humans trivia even thought toronto us city deepmind blew people away beating video games vicarious took captcha companies still stealth promise impress beyond cant wait see get demo id love talk anyone using machine intelligence change world theres industry unsexy problem geeky id love help dont shy hope landscape chart sparks conversation goal make living document want know companies categories missing welcome feedback would like put together dynamic visualization add companies dimensions data methods used data types end users investment date location etc folks interact better explore space questions comments please email thank andrew paprocki aria haghighi beau cronin ben lorica doug fulop david andrzejewski eric berlow eric jonas gary kazantsev gideon mann greg smithies heidi skinner jack clark jon lehr kurt keutzer lauren barless pete skomoroch pete warden roger magoulas sean gourley stephen purpura wes mckinney zach bogue quid team bloomberg beta team everhelpful perspectives disclaimer bloomberg beta investor adatao alation aviso brightfunnel context relevant mavrx newsle orbital insights pop archive two others chart still undisclosed also investors machine intelligence companies arent focusing areas fit landscape left full resolution version landscape please click quick cheer standing ovation clap show much enjoyed story partner bloomberg beta machine intelligence good equal parts nerd athlete straight canadian stereotype proud,en,"['☺', 'Yale', 'Harvard', 'NLP', 'Google', 'Baidu', 'LinkedIn', 'Netflix', 'Yahoo', 'Apple', 'Amazon', 'IBM', 'Microsoft', 'NYU', 'University of Toronto', 'LeCun', 'Hinton', 'Coursera', 'Facebook, Google', 'BigTable', 'Facebook', 'Bloomberg Beta', 'the Toronto Maple Leafs', 'IBMer', 'the Maple Leafs', 'demos', 'CAPTCHA', 'Demo', 'Adatao, Alation, Aviso, BrightFunnel', 'Context Relevant', 'Mavrx', 'Newsle']"
33,AirbnbEng,369,Architecting a Machine Learning System for Risk – Airbnb Engineering & Data Science – Medium,"By Naseem Hakim & Aaron Keys
At Airbnb, we want to build the world’s most trusted community. Guests trust Airbnb to connect them with world-class hosts for unique and memorable travel experiences. Airbnb hosts trust that guests will treat their home with the same care and respect that they would their own. The Airbnb review system helps users find community members who earn this trust through positive interactions with others, and the ecosystem as a whole prospers.
The overwhelming majority of web users act in good faith, but unfortunately, there exists a small number of bad actors who attempt to profit by defrauding websites and their communities. The trust and safety team at Airbnb works across many disciplines to help protect our users from these bad actors, ideally before they have the opportunity to impart negativity on the community.
There are many different kinds of risk that online businesses may have to protect against, with varying exposure depending on the particular business. For example, email providers devote significant resources to protecting users from spam, whereas payments companies deal more with credit card chargebacks.
We can mitigate the potential for bad actors to carry out different types of attacks in different ways.
Many risks can be mitigated through user-facing changes to the product that require additional verification from the user. For example, requiring email confirmation, or implementing 2FA to combat account takeovers, as many banks have done.
Scripted attacks are often associated with a noticeable increase in some measurable metric over a short period of time. For example, a sudden 1000% increase in reservations in a particular city could be a result of excellent marketing, or fraud.
Fraudulent actors often exhibit repetitive patterns. As we recognize these patterns, we can apply heuristics to predict when they are about to occur again, and help stop them. For complex, evolving fraud vectors, heuristics eventually become too complicated and therefore unwieldy. In such cases, we turn to machine learning, which will be the focus of this blog post.
For a more detailed look at other aspects of online risk management, check out Ohad Samet’s great ebook.
Different risk vectors can require different architectures. For example, some risk vectors are not time critical, but require computationally intensive techniques to detect. An offline architecture is best suited for this kind of detection. For the purposes of this post, we are focusing on risks requiring realtime or near-realtime action. From a broad perspective, a machine-learning pipeline for these kinds of risk must balance two important goals:
These may seem like competing goals, since optimizing for realtime calculations during a web transaction creates a focus on speed and reliability, whereas optimizing for model building and iteration creates more of a focus on flexibility. At Airbnb, engineering and data teams have worked closely together to develop a framework that accommodates both goals: a fast, robust scoring framework with an agile model-building pipeline.
In keeping with our service-oriented architecture, we built a separate fraud prediction service to handle deriving all the features for a particular model. When a critical event occurs in our system, e.g., a reservation is created, we query the fraud prediction service for this event. This service can then calculate all the features for the “reservation creation” model, and send these features to our Openscoring service, which is described in more detail below. The Openscoring service returns a score and a decision based on a threshold we’ve set, and the fraud prediction service can then use this information to take action (i.e., put the reservation on hold).
The fraud prediction service has to be fast, to ensure that we are taking action on suspicious events in near realtime. Like many of our backend services for which performance is critical, it is built in java, and we parallelize the database queries necessary for feature generation. However, we also want the freedom to occasionally do some heavy computation in deriving features, so we run it asynchronously so that we are never blocking for reservations, etc. This asynchronous model works for many situations where a few seconds of delay in fraud detection has no negative effect. It’s worth noting, however, that there are cases where you may want to react in realtime to block transactions, in which case a synchronous query and precomputed features may be necessary. This service is built in a very modular way, and exposes an internal restful API, making adding new events and models easy.
Openscoring is a Java service that provides a JSON REST interface to the Java Predictive Model Markup Language (PMML) evaluator JPMML. Both JPMML and Openscoring are open source projects released under the Apache 2.0 license and authored by Villu Ruusmann (edit — the most recent version is licensed the under AGPL 3.0) . The JPMML backend of Openscoring consumes PMML, an xml markup language that encodes several common types of machine learning models, including tree models, logit models, SVMs and neural networks. We have streamlined Openscoring for a production environment by adding several features, including kafka logging and statsd monitoring. Andy Kramolisch has modified Openscoring to permit using several models simultaneously.
As described below, there are several considerations that we weighed carefully before moving forward with Openscoring:
After considering all of these factors, we decided that Openscoring best satisfied our two-pronged goal of having a fast and robust, yet flexible machine learning framework.
A schematic of our model-building pipeline using PMML is illustrated above. The first step involves deriving features from the data stored on the site. Since the combination of features that gives the optimal signal is constantly changing, we store the features in a json format, which allows us to generalize the process of loading and transforming features, based on their names and types. We then transform the raw features through bucketing or binning values, and replacing missing values with reasonable estimates to improve signal. We also remove features that are shown to be statistically unimportant from our dataset. While we omit most of the details regarding how we perform these transformations for brevity here, it is important to recognize that these steps take a significant amount of time and care. We then use our transformed features to train and cross-validate the model using our favorite PMML-compatible machine learning library, and upload the PMML model to Openscoring. The final model is tested and then used for decision-making if it becomes the best performer.
The model-training step can be performed in any language with a library that outputs PMML. One commonly used and well-supported library is the R PMML package. As illustrated below, generating a PMML with R requires very little code.
This R script has the advantage of simplicity, and a script similar to this is a great way to start building PMMLs and to get a first model into production. In the long run, however, a setup like this has some disadvantages. First, our script requires that we perform feature transformation as a pre-processing step, and therefore we have add these transformation instructions to the PMML by editing it afterwards. The R PMML package supports many PMML transformations and data manipulations, but it is far from universal. We deploy the model as a separate step — post model-training — and so we have to manually test it for validity, which can be a time-consuming process. Yet another disadvantage of R is that the implementation of the PMML exporter is somewhat slow for a random forest model with many features and many trees. However, we’ve found that simply re-writing the export function in C++ decreases run time by a factor of 10,000, from a few days to a few seconds. We can get around the drawbacks of R while maintaining its advantages by building a pipeline based on Python and scikit-learn. Scikit-learn is a Python package that supports many standard machine learning models, and includes helpful utilities for validating models and performing feature transformations. We find that Python is a more natural language than R for ad-hoc data manipulation and feature extraction. We automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json; thus, new features can be incorporated into the model pipeline with no changes to the existing code. Deployment and testing can also be performed automatically in Python by using its standard network libraries to interface with Openscoring. Standard model performance tests (precision recall, ROC curves, etc.) are carried out using sklearn’s built-in capabilities. Sklearn does not support PMML export out of the box, so have written an in-house exporter for particular sklearn classifiers. When the PMML file is uploaded to Openscoring, it is automatically tested for correspondence with the scikit-learn model it represents. Because feature-transformation, model building, model validation, deployment and testing are all carried out in a single script, a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data, and then rapidly deploy the new model into production.
Although this blog post has focused mostly on our architecture and model building pipeline, the truth is that much of our time has been spent elsewhere. Our process was very successful for some models, but for others we encountered poor precision-recall. Initially we considered whether we were experiencing a bias or a variance problem, and tried using more data and more features. However, after finding no improvement, we started digging deeper into the data, and found that the problem was that our ground truth was not accurate.
Consider chargebacks as an example. A chargeback can be “Not As Described (NAD)” or “Fraud” (this is a simplification), and grouping both types of chargebacks together for a single model would be a bad idea because legitimate users can file NAD chargebacks. This is an easy problem to resolve, and not one we actually had (agents categorize chargebacks as part of our workflow); however, there are other types of attacks where distinguishing legitimate activity from illegitimate is more subtle, and necessitated the creation of new data stores and logging pipelines.
Most people who’ve worked in machine learning will find this obvious, but it’s worth re-stressing:
Towards this end, sometimes you don’t know what data you’re going to need until you’ve seen a new attack, especially if you haven’t worked in the risk space before, or have worked in the risk space but only in a different sector. So the best advice we can offer in this case is to log everything. Throw it all in HDFS, whether you need it now or not. In the future, you can always use this data to backfill new data stores if you find it useful. This can be invaluable in responding to a new attack vector.
Although our current ML pipeline uses scikit-learn and Openscoring, our system is constantly evolving. Our current setup is a function of the stage of the company and the amount of resources, both in terms of personnel and data, that are currently available. Smaller companies may only have a few ML models in production and a small number of analysts, and can take time to manually curate data and train the model in many non-standardized steps. Larger companies might have many, many models and require a high degree of automation, and get a sizable boost from online training. A unique challenge of working at a hyper-growth company is that landscape fundamentally changes year-over-year, and pipelines need to adjust to account for this.
As our data and logging pipelines improve, investing in improved learning algorithms will become more worthwhile, and we will likely shift to testing new algorithms, incorporating online learning, and expanding on our model building framework to support larger data sets. Additionally, some of the most important opportunities to improve our models are based on insights into our unique data, feature selection, and other aspects our risk systems that we are not able to share publicly. We would like to acknowledge the other engineers and analysts who have contributed to these critical aspects of this project. We work in a dynamic, highly-collaborative environment, and this project is an example of how engineers and data scientists at Airbnb work together to arrive at a solution that meets a diverse set of needs. If you’re interested in learning more, contact us about our data science and engineering teams!
Originally published at nerds.airbnb.com on June 16, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
",naseem hakim aaron keys airbnb want build worlds trusted community guests trust airbnb connect worldclass hosts unique memorable travel experiences airbnb hosts trust guests treat home care respect would airbnb review system helps users find community members earn trust positive interactions others ecosystem whole prospers overwhelming majority web users act good faith unfortunately exists small number bad actors attempt profit defrauding websites communities trust safety team airbnb works across many disciplines help protect users bad actors ideally opportunity impart negativity community many different kinds risk online businesses may protect varying exposure depending particular business example email providers devote significant resources protecting users spam whereas payments companies deal credit card chargebacks mitigate potential bad actors carry different types attacks different ways many risks mitigated userfacing changes product require additional verification user example requiring email confirmation implementing 2fa combat account takeovers many banks done scripted attacks often associated noticeable increase measurable metric short period time example sudden 1000 increase reservations particular city could result excellent marketing fraud fraudulent actors often exhibit repetitive patterns recognize patterns apply heuristics predict occur help stop complex evolving fraud vectors heuristics eventually become complicated therefore unwieldy cases turn machine learning focus blog post detailed look aspects online risk management check ohad samets great ebook different risk vectors require different architectures example risk vectors time critical require computationally intensive techniques detect offline architecture best suited kind detection purposes post focusing risks requiring realtime nearrealtime action broad perspective machinelearning pipeline kinds risk must balance two important goals may seem like competing goals since optimizing realtime calculations web transaction creates focus speed reliability whereas optimizing model building iteration creates focus flexibility airbnb engineering data teams worked closely together develop framework accommodates goals fast robust scoring framework agile modelbuilding pipeline keeping serviceoriented architecture built separate fraud prediction service handle deriving features particular model critical event occurs system eg reservation created query fraud prediction service event service calculate features reservation creation model send features openscoring service described detail openscoring service returns score decision based threshold weve set fraud prediction service use information take action ie put reservation hold fraud prediction service fast ensure taking action suspicious events near realtime like many backend services performance critical built java parallelize database queries necessary feature generation however also want freedom occasionally heavy computation deriving features run asynchronously never blocking reservations etc asynchronous model works many situations seconds delay fraud detection negative effect worth noting however cases may want react realtime block transactions case synchronous query precomputed features may necessary service built modular way exposes internal restful api making adding new events models easy openscoring java service provides json rest interface java predictive model markup language pmml evaluator jpmml jpmml openscoring open source projects released apache 20 license authored villu ruusmann edit recent version licensed agpl 30 jpmml backend openscoring consumes pmml xml markup language encodes several common types machine learning models including tree models logit models svms neural networks streamlined openscoring production environment adding several features including kafka logging statsd monitoring andy kramolisch modified openscoring permit using several models simultaneously described several considerations weighed carefully moving forward openscoring considering factors decided openscoring best satisfied twopronged goal fast robust yet flexible machine learning framework schematic modelbuilding pipeline using pmml illustrated first step involves deriving features data stored site since combination features gives optimal signal constantly changing store features json format allows us generalize process loading transforming features based names types transform raw features bucketing binning values replacing missing values reasonable estimates improve signal also remove features shown statistically unimportant dataset omit details regarding perform transformations brevity important recognize steps take significant amount time care use transformed features train crossvalidate model using favorite pmmlcompatible machine learning library upload pmml model openscoring final model tested used decisionmaking becomes best performer modeltraining step performed language library outputs pmml one commonly used wellsupported library r pmml package illustrated generating pmml r requires little code r script advantage simplicity script similar great way start building pmmls get first model production long run however setup like disadvantages first script requires perform feature transformation preprocessing step therefore add transformation instructions pmml editing afterwards r pmml package supports many pmml transformations data manipulations far universal deploy model separate step post modeltraining manually test validity timeconsuming process yet another disadvantage r implementation pmml exporter somewhat slow random forest model many features many trees however weve found simply rewriting export function c decreases run time factor 10000 days seconds get around drawbacks r maintaining advantages building pipeline based python scikitlearn scikitlearn python package supports many standard machine learning models includes helpful utilities validating models performing feature transformations find python natural language r adhoc data manipulation feature extraction automate process feature extraction based set rules encoded names types variables features json thus new features incorporated model pipeline changes existing code deployment testing also performed automatically python using standard network libraries interface openscoring standard model performance tests precision recall roc curves etc carried using sklearns builtin capabilities sklearn support pmml export box written inhouse exporter particular sklearn classifiers pmml file uploaded openscoring automatically tested correspondence scikitlearn model represents featuretransformation model building model validation deployment testing carried single script data scientist engineer able quickly iterate model based new features recent data rapidly deploy new model production although blog post focused mostly architecture model building pipeline truth much time spent elsewhere process successful models others encountered poor precisionrecall initially considered whether experiencing bias variance problem tried using data features however finding improvement started digging deeper data found problem ground truth accurate consider chargebacks example chargeback described nad fraud simplification grouping types chargebacks together single model would bad idea legitimate users file nad chargebacks easy problem resolve one actually agents categorize chargebacks part workflow however types attacks distinguishing legitimate activity illegitimate subtle necessitated creation new data stores logging pipelines people whove worked machine learning find obvious worth restressing towards end sometimes dont know data youre going need youve seen new attack especially havent worked risk space worked risk space different sector best advice offer case log everything throw hdfs whether need future always use data backfill new data stores find useful invaluable responding new attack vector although current ml pipeline uses scikitlearn openscoring system constantly evolving current setup function stage company amount resources terms personnel data currently available smaller companies may ml models production small number analysts take time manually curate data train model many nonstandardized steps larger companies might many many models require high degree automation get sizable boost online training unique challenge working hypergrowth company landscape fundamentally changes yearoveryear pipelines need adjust account data logging pipelines improve investing improved learning algorithms become worthwhile likely shift testing new algorithms incorporating online learning expanding model building framework support larger data sets additionally important opportunities improve models based insights unique data feature selection aspects risk systems able share publicly would like acknowledge engineers analysts contributed critical aspects project work dynamic highlycollaborative environment project example engineers data scientists airbnb work together arrive solution meets diverse set needs youre interested learning contact us data science engineering teams originally published nerdsairbnbcom june 16 2014 quick cheer standing ovation clap show much enjoyed story creative engineers data scientists building world belong anywhere httpairbnbio creative engineers data scientists building world belong anywhere httpairbnbio,en,"['Naseem Hakim & Aaron Keys', 'API', 'Java', 'the Java Predictive Model Markup Language (PMML', 'AGPL', 'PMML', 'HDFS', 'nerds.airbnb.com']"
34,Yingjie Miao ,43,From word2vec to doc2vec: an approach driven by Chinese restaurant process,"Google’s word2vec project has created lots of interests in the text mining community. It’s a neural network language model that is “both supervised and unsupervised”. Unsupervised in the sense that you only have to provide a big corpus, say English wiki. Supervised in the sense that the model cleverly generates supervised learning tasks from the corpus. How? Two approaches, known as Continuous Bag of Words (CBOW) and Skip-Gram (See Figure 1 in this paper). CBOW forces the neural net to predict current word by surrounding words, and Skip-Gram forces the neural net to predict surrounding words of the current word. Training is essentially a classic back-propagation method with a few optimization and approximation tricks (e.g. hierarchical softmax).
Word vectors generated by the neural net have nice semantic and syntactic behaviors. Semantically, “iOS” is close to “Android”. Syntactically, “boys” minus “boy” is close to “girls” minus “girl”. One can checkout more examples here.
Although this provides high quality word vectors, there is still no clear way to combine them into a high quality document vector. In this article, we discuss one possible heuristic, inspired by a stochastic process called Chinese Restaurant Process (CRP). Basic idea is to use CRP to drive a clustering process and summing word vectors in the right cluster.
Imagine we have an document about chicken recipe. It contains words like “chicken”, “pepper”, “salt”, “cheese”. It also contains words like “use”, “buy”, “definitely”, “my”, “the”. The word2vec model gives us a vector for each word. One could naively sum up every word vector as the doc vector. This clearly introduces lots of noise. A better heuristic is to use a weighted sum, based on other information like idf or Part of Speech (POS) tag.
The question is: could we be more selective when adding terms? If this is a chicken recipe document, I shouldn’t even consider words like “definitely”, “use”, “my” in the summation. One can argue that idf based weights can significantly reduce noise of boring words like “the” and “is”. However, for words like “definitely”, “overwhelming”, the idfs are not necessarily small as you would hope.
It’s natural to think that if we can first group words into clusters, words like “chicken”, “pepper” may stay in one cluster, along with other clusters of “junk” words. If we can identify the “relevant” clusters, and only summing up word vectors from relevant clusters, we should have a good doc vector.
This boils down to clustering the words in the document. One can of course use off-the-shelf algorithms like K-means, but most these algorithms require a distance metric. Word2vec behaves nicely by cosine similarity, this doesn’t necessarily mean it behaves as well under Eucledian distance (even after projection to unit sphere, it’s perhaps best to use geodesic distance.)
It would be nice if we can directly work with cosine similarity. We have done a quick experiment on clustering words driven by CRP-like stochastic process. It worked surprisingly well — so far.
Now let’s explain CRP. Imagine you go to a (Chinese) restaurant. There are already n tables with different number of peoples. There is also an empty table. CRP has a hyperparamter r > 0, which can be regarded as the “imagined” number of people on the empty table. You go to one of the (n+1) tables with probability proportional to existing number of people on the table. (For the empty table, the number is r). If you go to one of the n existing tables, you are done. If you decide to sit down at the empty table, the Chinese restaurant will automatically create a new empty table. In that case, the next customer comes in will choose from (n+2) tables (including the new empty table).
Inspired by CRP, we tried the following variations of CRP to include the similarity factor. Common setup is the following: we are given M vectors to be clustered. We maintain two things: cluster sum (not centroid!), and vectors in clusters. We iterate through vectors. For current vector V, suppose we have n clusters already. Now we find the cluster C whose cluster sum is most similar to current vector. Call this score sim(V, C).
Variant 1: v creates a new cluster with probability 1/(1 + n). Otherwise v goes to cluster C.
Variant 2: If sim(V, C) > 1/(1 + n), goes to cluster C. Otherwise with probability 1/(1+n) it creates a new cluster and with probability n/(1+n) it goes to C.
In any of the two variants, if v goes to a cluster, we update cluster sum and cluster membership.
There is one distinct difference to traditional CRP: if we don’t go to empty table, we deterministically go to the “most similar” table.
In practice, we find these variants create similar results. One difference is that variant 1 tend to have more clusters and smaller clusters, variant 2 tend to have fewer but larger clusters. The examples below are from variant 2.
For example, for a chicken recipe document, the clusters look like this:
Apparently, the first cluster is most relevant. Now let’s take the cluster sum vector (which is the sum of all vectors from this cluster), and test if it really preserves semantic. Below is a snippet of python console. We trained word vector using the c implementation on a fraction of English Wiki, and read the model file using python library gensim.model.word2vec. c[0] below denotes the cluster 0.
Looks like the semantic is preserved well. It’s convincing that we can use this as the doc vector.
The recipe document seems easy. Now let’s try something more challenging, like a news article. News articles tend to tell stories, and thus has less concentrated “topic words”. We tried the clustering on this article, titled “Signals on Radar Puzzle Officials in Hunt for Malaysian Jet”. We got 4 clusters:
Again, looks decent. Note that this is a simple 1-pass clustering process and we don’t have to specify number of clusters! Could be very helpful for latency sensitive services.
There is still a missing step: how to find out the relevant cluster(s)? We haven’t yet done extensive experiments on this part. A few heuristics to consider:
There are other problems to think about: 1) how do we merge clusters? Based on similarity among cluster sum vectors? Or averaging similarity between cluster members? 2) what is the minimal set of words that can reconstruct cluster sum vector (in the sense of cosine similarity)? This could be used as a semantic keyword extraction method.
Conclusion: Google’s word2vec provides powerful word vectors. We are interested in using these vectors to generate high quality document vectors in an efficient way. We tried a strategy based on a variant of Chinese Restaurant Process and obtained interesting results. There are some open problems to explore, and we would like to hear what you think.
Appendix: python style pseudo-code for similarity driven CRP
We wrote this post while working on Kifi — Connecting people with knowledge. Learn more.
Originally published at eng.kifi.com on March 17, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The Kifi Engineering Blog
",googles word2vec project created lots interests text mining community neural network language model supervised unsupervised unsupervised sense provide big corpus say english wiki supervised sense model cleverly generates supervised learning tasks corpus two approaches known continuous bag words cbow skipgram see figure 1 paper cbow forces neural net predict current word surrounding words skipgram forces neural net predict surrounding words current word training essentially classic backpropagation method optimization approximation tricks eg hierarchical softmax word vectors generated neural net nice semantic syntactic behaviors semantically ios close android syntactically boys minus boy close girls minus girl one checkout examples although provides high quality word vectors still clear way combine high quality document vector article discuss one possible heuristic inspired stochastic process called chinese restaurant process crp basic idea use crp drive clustering process summing word vectors right cluster imagine document chicken recipe contains words like chicken pepper salt cheese also contains words like use buy definitely word2vec model gives us vector word one could naively sum every word vector doc vector clearly introduces lots noise better heuristic use weighted sum based information like idf part speech pos tag question could selective adding terms chicken recipe document shouldnt even consider words like definitely use summation one argue idf based weights significantly reduce noise boring words like however words like definitely overwhelming idfs necessarily small would hope natural think first group words clusters words like chicken pepper may stay one cluster along clusters junk words identify relevant clusters summing word vectors relevant clusters good doc vector boils clustering words document one course use offtheshelf algorithms like kmeans algorithms require distance metric word2vec behaves nicely cosine similarity doesnt necessarily mean behaves well eucledian distance even projection unit sphere perhaps best use geodesic distance would nice directly work cosine similarity done quick experiment clustering words driven crplike stochastic process worked surprisingly well far lets explain crp imagine go chinese restaurant already n tables different number peoples also empty table crp hyperparamter r 0 regarded imagined number people empty table go one n1 tables probability proportional existing number people table empty table number r go one n existing tables done decide sit empty table chinese restaurant automatically create new empty table case next customer comes choose n2 tables including new empty table inspired crp tried following variations crp include similarity factor common setup following given vectors clustered maintain two things cluster sum centroid vectors clusters iterate vectors current vector v suppose n clusters already find cluster c whose cluster sum similar current vector call score simv c variant 1 v creates new cluster probability 11 n otherwise v goes cluster c variant 2 simv c 11 n goes cluster c otherwise probability 11n creates new cluster probability n1n goes c two variants v goes cluster update cluster sum cluster membership one distinct difference traditional crp dont go empty table deterministically go similar table practice find variants create similar results one difference variant 1 tend clusters smaller clusters variant 2 tend fewer larger clusters examples variant 2 example chicken recipe document clusters look like apparently first cluster relevant lets take cluster sum vector sum vectors cluster test really preserves semantic snippet python console trained word vector using c implementation fraction english wiki read model file using python library gensimmodelword2vec c0 denotes cluster 0 looks like semantic preserved well convincing use doc vector recipe document seems easy lets try something challenging like news article news articles tend tell stories thus less concentrated topic words tried clustering article titled signals radar puzzle officials hunt malaysian jet got 4 clusters looks decent note simple 1pass clustering process dont specify number clusters could helpful latency sensitive services still missing step find relevant clusters havent yet done extensive experiments part heuristics consider problems think 1 merge clusters based similarity among cluster sum vectors averaging similarity cluster members 2 minimal set words reconstruct cluster sum vector sense cosine similarity could used semantic keyword extraction method conclusion googles word2vec provides powerful word vectors interested using vectors generate high quality document vectors efficient way tried strategy based variant chinese restaurant process obtained interesting results open problems explore would like hear think appendix python style pseudocode similarity driven crp wrote post working kifi connecting people knowledge learn originally published engkificom march 17 2014 quick cheer standing ovation clap show much enjoyed story kifi engineering blog,en,"['Google', 'CBOW', 'Skip-Gram', 'CRP', 'doc', '1/(1 + n)', 'python library gensim.model.word2vec', 'eng.kifi.com']"
35,Pinterest Engineering,113,Building a smarter home feed – Pinterest Engineering – Medium,"Chris Pinchak | Pinterest engineer, Discovery
The home feed should be a reflection of what each user cares about. Content is sourced from inputs such as people and boards the user follows, interests, and recommendations. To ensure we maintain fast, reliable and personalized home feeds, we built the smart feed with the following design values in mind:
1. Different sources of Pins should be mixed together at different rates.
2. Some Pins should be selectively dropped or deferred until a later time. Some sources may produce Pins of poor quality for a user, so instead of showing everything available immediately, we can be selective about what to show and what to hold back for a future session.
3. Pins should be arranged in the order of best-first rather than newest-first. For some sources, newer Pins are intuitively better, while for others, newness is less important.
We shifted away from our previously time-ordered home feed system and onto a more flexible one. The core feature of the smart feed architecture is its separation of available, but unseen, content and content that’s already been presented to the user. We leverage knowledge of what the user hasn’t yet seen to our advantage when deciding how the feed evolves over time.
Smart feed is a composition of three independent services, each of which has a specific role in the construction of a home feed.
The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.
Essentially, the worker manages Pins as they become newly available, such as those from the repins of the people the user follows. Pins have varying value to the receiving user, so the worker is tasked with deciding the magnitude of their subjective quality.
Incoming Pins are currently obtained from three separate sources: repins made by followed users, related Pins, and Pins from followed interests. Each is scored by the worker and then inserted into a pool for that particular type of pin. Each pool is a priority queue sorted on score and belongs to a single user. Newly added Pins mix with those added before, allowing the highest quality Pins to be accessible over time at the front of the queue.
Pools can be implemented in a variety of ways so long as the priority queue requirement is met. We choose to do this by exploiting the key-based sorting of HBase. Each key is a combination of user, score and Pin such that, for any user, we may scan a list of available Pins according to their score. Newly added triples will be inserted at their appropriate location to maintain the score order. This combination of user, score, and Pin into a key value can be used to create a priority queue in other storage systems aside from HBase, a property we may use in the future depending on evolving storage requirements.
Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.
The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.
The content generator is generally free to perform any rearrangements it likes, but is bound to the priority queue nature of the pools. When the generator asks for n pins from a pool, it’ll get the n highest scoring (i.e., best) Pins available. Therefore, the generator doesn’t need to concern itself with finding the best available content, but instead with how the best available content should be presented.
In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.
The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.
The smart feed service relies on the content generator to provide new Pins. If the generator experiences a degradation in performance, the service can gracefully handle the loss of its availability. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. In this instance, the feed will appear to the end user as unchanged from last time. Future feed views will produce chunks as large as, or larger than, the last so that eventually the user will see new Pins.
By moving to smart feed, we achieved the goals of a highly flexible architecture and better control over the composition of home feeds. The home feed is now powered by three separate services, each with a well-defined role in its production and distribution. The individual services can be altered or replaced with components that serve the same general purpose. The use of pools to buffer Pins according to their quality allows us a greater amount of control over the composition of home feeds.
Continuing with this project, we intend to better model users’ preferences with respect to Pins in their home feeds. Our accuracy of recommendation quality varies considerably over our user base, and we would benefit from using preference information gathered from recent interactions with the home feed. Knowledge of personal preference will also help us order home feeds so the Pins of most value can be discovered with the least amount of effort.
If you’re interested in tackling challenges and making improvements like this, join our team!
Chris Pinchak is a software engineer at Pinterest.
Acknowledgements: This technology was built in collaboration with Dan Feng, Dmitry Chechik, Raghavendra Prabhu, Jeremy Carroll, Xun Liu, Varun Sharma, Joe Lau, Yuchen Liu, Tian-Ying Chang, and Yun Park. This team, as well as people from across the company, helped make this project a reality with their technical insights and invaluable feedback.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering
",chris pinchak pinterest engineer discovery home feed reflection user cares content sourced inputs people boards user follows interests recommendations ensure maintain fast reliable personalized home feeds built smart feed following design values mind 1 different sources pins mixed together different rates 2 pins selectively dropped deferred later time sources may produce pins poor quality user instead showing everything available immediately selective show hold back future session 3 pins arranged order bestfirst rather newestfirst sources newer pins intuitively better others newness less important shifted away previously timeordered home feed system onto flexible one core feature smart feed architecture separation available unseen content content thats already presented user leverage knowledge user hasnt yet seen advantage deciding feed evolves time smart feed composition three independent services specific role construction home feed smart feed worker first process pins two primary responsibilities accept incoming pins assign score proportional quality value receiving user remember scored pins storage later consumption essentially worker manages pins become newly available repins people user follows pins varying value receiving user worker tasked deciding magnitude subjective quality incoming pins currently obtained three separate sources repins made followed users related pins pins followed interests scored worker inserted pool particular type pin pool priority queue sorted score belongs single user newly added pins mix added allowing highest quality pins accessible time front queue pools implemented variety ways long priority queue requirement met choose exploiting keybased sorting hbase key combination user score pin user may scan list available pins according score newly added triples inserted appropriate location maintain score order combination user score pin key value used create priority queue storage systems aside hbase property may use future depending evolving storage requirements distinct smart feed worker smart feed content generator concerned primarily defining new means context home feed user accesses home feed ask content generator new pins since last visit generator decides quantity composition arrangement new pins return response request content generator assembles available pins chunks consumption user part home feed generator free choose arrangement based variety input signals may elect use pins available pools pins selected inclusion chunk thereafter removed pools cannot returned part subsequent chunks content generator generally free perform rearrangements likes bound priority queue nature pools generator asks n pins pool itll get n highest scoring ie best pins available therefore generator doesnt need concern finding best available content instead best available content presented addition providing high availability home feed smart feed service responsible combining new pins returned content generator previously appeared home feed separate chunk returned content generator materialized feed managed smart feed service materialized feed represents frozen view feed last time user viewed materialized pins add pins content generator chunk service makes decisions order instead adds pins exactly order given chunk fairly low rate reading writing materialized feed likely suffer fewer availability events addition feeds trimmed restrict maximum size need less storage means easily increase availability reliability materialized feed replication use faster storage hardware smart feed service relies content generator provide new pins generator experiences degradation performance service gracefully handle loss availability event content generator encounters exception generating chunk simply takes long produce one smart feed service return content contained materialized feed instance feed appear end user unchanged last time future feed views produce chunks large larger last eventually user see new pins moving smart feed achieved goals highly flexible architecture better control composition home feeds home feed powered three separate services welldefined role production distribution individual services altered replaced components serve general purpose use pools buffer pins according quality allows us greater amount control composition home feeds continuing project intend better model users preferences respect pins home feeds accuracy recommendation quality varies considerably user base would benefit using preference information gathered recent interactions home feed knowledge personal preference also help us order home feeds pins value discovered least amount effort youre interested tackling challenges making improvements like join team chris pinchak software engineer pinterest acknowledgements technology built collaboration dan feng dmitry chechik raghavendra prabhu jeremy carroll xun liu varun sharma joe lau yuchen liu tianying chang yun park team well people across company helped make project reality technical insights invaluable feedback quick cheer standing ovation clap show much enjoyed story inventive engineers building first visual discovery engine 100 billion ideas counting httpscareerspinterestcomcareersengineering,en,"['HBase', 'Dmitry Chechik']"
36,Nikhil Dandekar,116,What makes a good data scientist/engineer? – Towards Data Science,"The term data scientist has been used lately to describe a wide variety of skills & roles. In this post I will focus on a particular flavor of data scientist. I will talk about the qualities needed to be a good data scientist-engineer who ships relevance products to users. Some examples of relevance products are:
These folks need to be strong at data science and engineering to be successful. Some places call these folks as Machine Learning engineers since most of the work they do involves Machine Learning. More generally, I feel relevance engineer is a good term to describe them.
Relevance engineers have a common set of skills that they draw upon to get their jobs done. The list below doesn’t include some of the known, obvious skills. You obviously need to be smart. You obviously need to have (or be able to learn quickly) the required “book” knowledge.
But beyond that, there are a bunch of not-so-obvious skills that you can’t learn from a book. Here are some of those, in no particular order:
This list is by no means exhaustive, but does capture some of the qualities of the smartest folks I have worked with. Happy to hear what you think.
Thanks to Peter Bailey and Andrew Hogue for feedback on the initial revisions.
*In this post, feature means a software feature, not a machine learning feature.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Engineering Manager doing Machine Learning @ Google. Previously worked on ML and search at Quora, Foursquare and Bing.
Sharing concepts, ideas, and codes.
",term data scientist used lately describe wide variety skills roles post focus particular flavor data scientist talk qualities needed good data scientistengineer ships relevance products users examples relevance products folks need strong data science engineering successful places call folks machine learning engineers since work involves machine learning generally feel relevance engineer good term describe relevance engineers common set skills draw upon get jobs done list doesnt include known obvious skills obviously need smart obviously need able learn quickly required book knowledge beyond bunch notsoobvious skills cant learn book particular order list means exhaustive capture qualities smartest folks worked happy hear think thanks peter bailey andrew hogue feedback initial revisions post feature means software feature machine learning feature quick cheer standing ovation clap show much enjoyed story engineering manager machine learning google previously worked ml search quora foursquare bing sharing concepts ideas codes,en,['Foursquare and Bing']
37,Jeff Smith,20,Modeling Madly – Data Engineering – Medium,"I recently wrapped up my second hackathon at Intent Media. You can see my summary of one of our previous hackathons here. These past two hackathons I’ve taken on some slightly different challenges than people usually go after in a hackathon: developing new machine learning models. While I‘ve been working on data science and machine learning systems for a while, I’ve found that trying to do so under extreme constraints can be a distinctly different experience. A very good data hacker can easily find themselves with a great idea at a hackathon but with little to nothing to demo at the end. Accepting that my personal experience is just my own, let me offer three tips for building new models at a hackathon.
When you’re doing a more traditional web app hack at a hackathon, you can almost run out of time and still come up with something pretty good as long as you get that last bug fixed before the demo. This is a great characteristic to build into the plan of a hack but one that simply does not apply to a machine learning hack.
Think about what happens when you do find that last bug in a machine learning project. You still need to potentially do all of the below:
That’s no “just hit refresh” workflow. Even with a well-oiled workflow, some of those tasks can take all of the time your average one-day hackathon is scheduled for. Take #3, for example. Training a production grade model using, say, Hadoop, can take a lot of time, even if you have the cash to spin up a fair-sized cluster of EC2 instances.
What that means for your hack can vary, but you’re just asking for trouble if you don’t start with that fact taken into account in the scope and goals of your project. A solid project design is absolutely crucial, if you’re going to hope to take all of the little steps involved in getting your model ready to demo.
Which leads me to my next point...
One of the best things about working in data science is all of the really smart people. But, of course, the corollary is that one of the worst things about working in data science is all of the really smart people. Sharp engineers and data scientists can take the nugget of an idea and envision a useful, powerful suite of products that would take years to build, which is not so useful when you have a day or two. Mature dataists know just how much ambition is too much and plan accordingly. I happen to be lucky enough to work with some very smart and very mature data scientists and engineers, so this has not been a problem for either of my last few hacks. But, I’m just lucky that way. You might not be so lucky.
Unrealistic ambitions are a constant danger in a machine learning hack, running along the edge of all activities like a precipice beckoning you to dive off and see where you land. If you take one thing away from this post, let it be this: don’t dive off the cliff. Just don’t do it. You won’t like where you land. You’ll wind up with more questions than answers and you’ll have nothing to show come demo time. Moreover, your fellow devs who worked on apps and not models will simply not understand what you spent your time on.
What does a precipice look like? It could be a novel distance metric. It could be a fundamental improvement to a widely used technique like SVRs. Or it could just be something really benign sounding like a longer training set. I would say that even choosing to pose the problem as a regression one instead of a classification one could qualify.
The danger originates in the intrinsic tension between the rigorous and exploratory mode of academic data science/machine learning education and the pedal-to-the-metal pace mandated by a hackathon. They are very different modes of working, and you’re just going to have suspend some of your good habits for a day or so, if you want to have something to demo.
This last point can be the trickiest to put in practice, but I think it can totally be the difference between a project that feels like a hack and one that feels like just getting warmed up on a weeklong story. If you’ve figured out how to scope your project appropriately and designed something that can really be built in a day or two, you can still actually fail to do so. I think it can the difference can easily come down to technology choices.
For example, I currently make my living writing Cascalog, Clojure, and Java on top of Hadoop to process files stored in S3. I know these tools well enough to pay my rent, but I would absolutely hesitate to use any of them in a tight-paced context. I have spent weeks trying to understand a single Cascalog bug. Seriously.
If you know the language, Python offers an unbeatable value proposition for this use case. scikit-learn has nearly everything you could imagine needing. pandas, NumPy, and SciPy are all sitting there to be brought in when appropriate. And don’t forget how awesome it can be to prototype in a purpose-built exploratory development environment like IPython.
But this is machine learning, and sometimes our data is just big. Maybe even web scale. Some people hate these phrases, but they serve a purpose. We don’t all use Hadoop out of love for horrendously complex Java applications.
Big data is not just statistics on a Mac Pro, although it can often look like that. Scale can be a real necessity even in a hackathon.
When it is, there are no easy answers. If you’re lucky, maybe you can actually work with multiple hour model learning times. If you’re really lucky, you might be using Spark and not Hadoop, in which case it might not take hours to learn your model.
My point is that, insofar as you have a choice, choose the leaner meaner tool, the one that will let you do more with less input required from you. Don’t use that C++ library that promises awesome runtime but with Python bindings that you’ve never tried. You’ll never figure out its quirks in time. Write as little data cleanup code as you can manage. Commands like dropna can save you precious minutes to hours. And if you can get your data from database or an API instead of files, then, for the love of Cthulhu, do it. Hell, even if you have to load your data from files to a database first, it might be worth your time. SQL is one of the highest productivity rapid prototyping tools I know.
And though I love to bash on the clunkiness of Hadoop, there are even ways of taking some serious pain out of using it under pressure. Depending on what you’re doing Elastic Map Reduce or PredictionIO can get you to the point of being productive much faster.
I love hackathons and their variations. They remind me of the fun old days in grad school, furiously hacking away to come up with something interesting to say about definitionally uncertain stuff.
The furious pace and the pragmatic compromises are part of the fun. Compared to things like pitch events, hackathons have way less problems (even if they have their issues as well). At their best they’re about the love of unconstrained creation. I’ve tried to do machine learning hacks because it’s just so damn cool to go from zero to having a program that makes decisions. It amazes me every time it works, and doubly so when I can manage to get something working on a deadline.
Taking on a challenge like building a new model in a hackathon is also a great learning experience, especially if you get to work as part of a strong team. Machine learning in the real world is an even larger topic than its academic cousin, and there’s always interesting things to learn. Hackathons can be great places to rapidly iterate through approaches and learn from your teammates how to build things better and faster. That’s pretty likely to come in handy sometime.
The main part of the post is over, but I wanted to make sure to leave a note for anyone who was interested in what we hack at Intent Media (or what we build for our customers). We’re hiring all sorts of smart people to build systems for machine learning and more. Please reach out if you want to hear more about how and why we do what we do.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Author of Reactive Machine Learning Systems @ManningBooks. Building AIs for fun and profit. Friend of animals.
Laying the foundation of tomorrow’s big data
",recently wrapped second hackathon intent media see summary one previous hackathons past two hackathons ive taken slightly different challenges people usually go hackathon developing new machine learning models ive working data science machine learning systems ive found trying extreme constraints distinctly different experience good data hacker easily find great idea hackathon little nothing demo end accepting personal experience let offer three tips building new models hackathon youre traditional web app hack hackathon almost run time still come something pretty good long get last bug fixed demo great characteristic build plan hack one simply apply machine learning hack think happens find last bug machine learning project still need potentially thats hit refresh workflow even welloiled workflow tasks take time average oneday hackathon scheduled take 3 example training production grade model using say hadoop take lot time even cash spin fairsized cluster ec2 instances means hack vary youre asking trouble dont start fact taken account scope goals project solid project design absolutely crucial youre going hope take little steps involved getting model ready demo leads next point one best things working data science really smart people course corollary one worst things working data science really smart people sharp engineers data scientists take nugget idea envision useful powerful suite products would take years build useful day two mature dataists know much ambition much plan accordingly happen lucky enough work smart mature data scientists engineers problem either last hacks im lucky way might lucky unrealistic ambitions constant danger machine learning hack running along edge activities like precipice beckoning dive see land take one thing away post let dont dive cliff dont wont like land youll wind questions answers youll nothing show come demo time moreover fellow devs worked apps models simply understand spent time precipice look like could novel distance metric could fundamental improvement widely used technique like svrs could something really benign sounding like longer training set would say even choosing pose problem regression one instead classification one could qualify danger originates intrinsic tension rigorous exploratory mode academic data sciencemachine learning education pedaltothemetal pace mandated hackathon different modes working youre going suspend good habits day want something demo last point trickiest put practice think totally difference project feels like hack one feels like getting warmed weeklong story youve figured scope project appropriately designed something really built day two still actually fail think difference easily come technology choices example currently make living writing cascalog clojure java top hadoop process files stored s3 know tools well enough pay rent would absolutely hesitate use tightpaced context spent weeks trying understand single cascalog bug seriously know language python offers unbeatable value proposition use case scikitlearn nearly everything could imagine needing pandas numpy scipy sitting brought appropriate dont forget awesome prototype purposebuilt exploratory development environment like ipython machine learning sometimes data big maybe even web scale people hate phrases serve purpose dont use hadoop love horrendously complex java applications big data statistics mac pro although often look like scale real necessity even hackathon easy answers youre lucky maybe actually work multiple hour model learning times youre really lucky might using spark hadoop case might take hours learn model point insofar choice choose leaner meaner tool one let less input required dont use c library promises awesome runtime python bindings youve never tried youll never figure quirks time write little data cleanup code manage commands like dropna save precious minutes hours get data database api instead files love cthulhu hell even load data files database first might worth time sql one highest productivity rapid prototyping tools know though love bash clunkiness hadoop even ways taking serious pain using pressure depending youre elastic map reduce predictionio get point productive much faster love hackathons variations remind fun old days grad school furiously hacking away come something interesting say definitionally uncertain stuff furious pace pragmatic compromises part fun compared things like pitch events hackathons way less problems even issues well best theyre love unconstrained creation ive tried machine learning hacks damn cool go zero program makes decisions amazes every time works doubly manage get something working deadline taking challenge like building new model hackathon also great learning experience especially get work part strong team machine learning real world even larger topic academic cousin theres always interesting things learn hackathons great places rapidly iterate approaches learn teammates build things better faster thats pretty likely come handy sometime main part post wanted make sure leave note anyone interested hack intent media build customers hiring sorts smart people build systems machine learning please reach want hear quick cheer standing ovation clap show much enjoyed story author reactive machine learning systems manningbooks building ais fun profit friend animals laying foundation tomorrows big data,en,"['I‘ve', 'Cascalog', 'Python', 'NumPy', 'IPython', 'SQL', 'Reactive Machine Learning Systems']"
38,Chris Jagers,45,Explaining the Wolfram Language – Chris Jagers – Medium,"Many people are already familiar with Apple’s voice search called Siri, or the search engine behind it called Wolfram Alpha. This search engine can use natural language to search vast sets of data and even compute math. However, this is just a tiny fraction of what the language can do, and I don’t even think it’s a good introduction to what’s possible. To understand the raw power of the underlying technology, you really have to understand what it is and a little about how it works.
The Wolfram website has wonderful documentation and explanations, but for the uninitiated it can seem bewildering. They have repackaged the language so many different ways, that it can be hard for the beginner to understand exactly what it is. That’s why I want to venture my own introduction.
Let’s start with it’s origins. Mathematica was designed as a desktop tool for computational research and exploration. It continued evolving and the breakthrough was realizing those symbols could be anything: images, sounds, algorithms, geometry, data-sets ... anything. So, it became more than just a language.
Stephen Wolfram calls this a knowledge-based language because it has smart-objects built in that can be computed.
The language doesn’t simply find results, it computes results into actual models, analysis and other symbolic objects. The real power is that the results remain symbolic objects that can be further manipulated symbolically (i.e. embed in another symbolic object, operate on it).
In short, anything can be computed. Pretty abstract, I know. Don’t worry, we’ll get to examples soon.
The actual syntax is a combination of Objects and Operators which are grouped and ordered by square brackets [ ]. The stuff at the center of the formula gets read first and then it expands out like a Russian doll.
Out of many potential examples, I have carefully selected one from their site to illustrate it’s simplicity and power.
Let’s say we want our system to determine the difference between poetry and prose. This would be difficult to program directly because there are so many variables and the differences are so subtle. With Wolfram Language, that hard stuff becomes easy. You can train it recognize the difference very quickly. Here’s how it works, let’s use Shakespeare for an example:
First, scan all of Hamlet and call that type of stuff prose. Then scan all of Shakespeare’s Sonnets and call that stuff poetry. Easy.
Next, train the system with machine learning: Classify and Predict are the two big functions. We want to Classify which is poetry and which is prose. Wolfram looks at our situation and instantly determines that the Markov Method is the best for differentiating among all the subtle differences between prose and poetry.
That’s it. Any system using this bit of training will automatically be able to detect the difference between poetry and prose with a high degree of accuracy. The key to this accuracy is the size of the data set. You really need at millions of data points to train it reliably. But with Wolfram, many of those data sets are already built in. Easy.
This is just one tiny example to illustrate what the language looks like and how it goes beyond symbols to work with computable objects. We could continue translating poems into interactive maps, and interactions into music, and so on.
How does Wolfram compare with other products like Apache Hadoop and others? Well, it’s a totally different thing. In those products everything is manual. The various axis (and all the variables) are manually defined. Instead, Wolfram intelligently applies formulas and makes choices to optimize results based on specific conditions. It makes the hard stuff automatic. Plus, it’s capable of much more than machine learning; that’s just one example of hundreds: sound, 3d-geometry, language, images, etc. — and a mixture of them all.
Mathematica is still the most powerful and polished way to access the Wolfram Language. Their new Programming Cloud (and other cloud offerings) signal serious intent to move to the web, but it is still early days. The language is very mature for desktop exploration, and some companies have even made Mathematica applications for small scale internal use, which can be quite useful.
Even though the Wolfram website has signaled intent to make it more broadly deployable within commercial services, I don’t think this is the proper way to use the language. Within my own company, we find Wolfram extremely handy for research, but not deployment within a web-based product. In short, it isn’t performant:
Commercial products require more than a powerful language, they are made within an ecosystem of services and vendors that all have to work together. Without machine learning built into the native cloud where data is stored, it can’t be deployed in a SaaS product in a way that lives up to expectations.
While Stephen Wolfram would love for his language to be used within commercial products, I think he resents having to play nice with lower level languages. His alternative of making API requests across the web isn’t a good way to embed intelligence within products. And I don’t think we will ever see entire SaaS products built entirely with a functional language. Programming is the art of automation.
The Wolfram Community is full of very smart people using the language for research and exploration. They represent the cutting edge of computation. Personally, I’m looking forward to when we can see intelligence woven into commercial and consumer products that solve real problems for people on a daily basis.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO of Learning Machine. www.learningmachine.com
",many people already familiar apples voice search called siri search engine behind called wolfram alpha search engine use natural language search vast sets data even compute math however tiny fraction language dont even think good introduction whats possible understand raw power underlying technology really understand little works wolfram website wonderful documentation explanations uninitiated seem bewildering repackaged language many different ways hard beginner understand exactly thats want venture introduction lets start origins mathematica designed desktop tool computational research exploration continued evolving breakthrough realizing symbols could anything images sounds algorithms geometry datasets anything became language stephen wolfram calls knowledgebased language smartobjects built computed language doesnt simply find results computes results actual models analysis symbolic objects real power results remain symbolic objects manipulated symbolically ie embed another symbolic object operate short anything computed pretty abstract know dont worry well get examples soon actual syntax combination objects operators grouped ordered square brackets stuff center formula gets read first expands like russian doll many potential examples carefully selected one site illustrate simplicity power lets say want system determine difference poetry prose would difficult program directly many variables differences subtle wolfram language hard stuff becomes easy train recognize difference quickly heres works lets use shakespeare example first scan hamlet call type stuff prose scan shakespeares sonnets call stuff poetry easy next train system machine learning classify predict two big functions want classify poetry prose wolfram looks situation instantly determines markov method best differentiating among subtle differences prose poetry thats system using bit training automatically able detect difference poetry prose high degree accuracy key accuracy size data set really need millions data points train reliably wolfram many data sets already built easy one tiny example illustrate language looks like goes beyond symbols work computable objects could continue translating poems interactive maps interactions music wolfram compare products like apache hadoop others well totally different thing products everything manual various axis variables manually defined instead wolfram intelligently applies formulas makes choices optimize results based specific conditions makes hard stuff automatic plus capable much machine learning thats one example hundreds sound 3dgeometry language images etc mixture mathematica still powerful polished way access wolfram language new programming cloud cloud offerings signal serious intent move web still early days language mature desktop exploration companies even made mathematica applications small scale internal use quite useful even though wolfram website signaled intent make broadly deployable within commercial services dont think proper way use language within company find wolfram extremely handy research deployment within webbased product short isnt performant commercial products require powerful language made within ecosystem services vendors work together without machine learning built native cloud data stored cant deployed saas product way lives expectations stephen wolfram would love language used within commercial products think resents play nice lower level languages alternative making api requests across web isnt good way embed intelligence within products dont think ever see entire saas products built entirely functional language programming art automation wolfram community full smart people using language research exploration represent cutting edge computation personally im looking forward see intelligence woven commercial consumer products solve real problems people daily basis quick cheer standing ovation clap show much enjoyed story ceo learning machine wwwlearningmachinecom,en,"['Apple', 'Wolfram', 'Objects and Operators', 'Hamlet', 'The Wolfram Community', 'Learning Machine']"
39,John Wittenauer,2,"Machine Learning Exercises In Python, Part 1 – John Wittenauer – Medium","This content originally appeared on Curious Insight
This post is part of a series covering the exercises from Andrew Ng’s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.
Part 1 — Simple Linear RegressionPart 2 — Multivariate Linear RegressionPart 3 — Logistic RegressionPart 4 — Multivariate Logistic RegressionPart 5 — Neural NetworksPart 6 — Support Vector MachinesPart 7 — K-Means Clustering & PCAPart 8 — Anomaly Detection & Recommendation
One of the pivotal moments in my professional development this year came when I discovered Coursera. I’d heard of the “MOOC” phenomenon but had not had the time to dive in and take a class. Earlier this year I finally pulled the trigger and signed up for Andrew Ng’s Machine Learning class. I completed the whole thing from start to finish, including all of the programming exercises. The experience opened my eyes to the power of this type of education platform, and I’ve been hooked ever since.
This blog post will be the first in a series covering the programming exercises from Andrew’s class. One aspect of the course that I didn’t particularly care for was the use of Octave for assignments. Although Octave/Matlab is a fine platform, most real-world “data science” is done in either R or Python (certainly there are other languages and tools being used, but these two are unquestionably at the top of the list). Since I’m trying to develop my Python skills, I decided to start working through the exercises from scratch in Python. The full source code is available at my IPython repo on Github. You’ll also find the data used in these exercises and the original exercise PDFs in sub-folders off the root directory if you’re interested.
While I can explain some of the concepts involved in this exercise along the way, it’s impossible for me to convey all the information you might need to fully comprehend it. If you’re really interested in machine learning but haven’t been exposed to it yet, I encourage you to check out the class (it’s completely free and there’s no commitment whatsoever). With that, let’s get started!
In the first part of exercise 1, we’re tasked with implementing simple linear regression to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You’d like to figure out what the expected profit of a new food truck might be given only the population of the city that it would be placed in.
Let’s start by examining the data which is in a file called “ex1data1.txt” in the “data” directory of my repository above. First we need to import a few libraries.
Now let’s get things rolling. We can use pandas to load the data into a data frame and display the first few rows using the “head” function.
(Note: Medium can’t render tables — the full example is here)
Another useful function that pandas provides out-of-the-box is the “describe” function, which calculates some basic statistics on a data set. This is helpful to get a “feel” for the data during the exploratory analysis stage of a project.
(Note: Medium can’t render tables — the full example is here)
Examining stats about your data can be helpful, but sometimes you need to find ways to visualize it too. Fortunately this data set only has one dependent variable, so we can toss it in a scatter plot to get a better idea of what it looks like. We can use the “plot” function provided by pandas for this, which is really just a wrapper for matplotlib.
It really helps to actually look at what’s going on, doesn’t it? We can clearly see that there’s a cluster of values around cities with smaller populations, and a somewhat linear trend of increasing profit as the size of the city increases. Now let’s get to the fun part — implementing a linear regression algorithm in python from scratch!
If you’re not familiar with linear regression, it’s an approach to modeling the relationship between a dependent variable and one or more independent variables (if there’s one independent variable then it’s called simple linear regression, and if there’s more than one independent variable then it’s called multiple linear regression). There are lots of different types and variances of linear regression that are outside the scope of this discussion so I won’t go into that here, but to put it simply — we’re trying to create a *linear model* of the data X, using some number of parameters theta, that describes the variance of the data such that given a new data point that’s not in X, we could accurately predict what the outcome y would be without actually knowing what y is.
In this implementation we’re going to use an optimization technique called gradient descent to find the parameters theta. If you’re familiar with linear algebra, you may be aware that there’s another way to find the optimal parameters for a linear model called the “normal equation” which basically solves the problem at once using a series of matrix calculations. However, the issue with this approach is that it doesn’t scale very well for large data sets. In contrast, we can use variants of gradient descent and other optimization methods to scale to data sets of unlimited size, so for machine learning problems this approach is more practical.
Okay, that’s enough theory. Let’s write some code. The first thing we need is a cost function. The cost function evaluates the quality of our model by calculating the error between our model’s prediction for a data point, using the model parameters, and the actual data point. For example, if the population for a given city is 4 and we predicted that it was 7, our error is (7–4)^2 = 3^2 = 9 (assuming an L2 or “least squares” loss function). We do this for each data point in X and sum the result to get the cost. Here’s the function:
Notice that there are no loops. We’re taking advantage of numpy’s linear algrebra capabilities to compute the result as a series of matrix operations. This is far more computationally efficient than an unoptimizted “for” loop.
In order to make this cost function work seamlessly with the pandas data frame we created above, we need to do some manipulating. First, we need to insert a column of 1s at the beginning of the data frame in order to make the matrix operations work correctly (I won’t go into detail on why this is needed, but it’s in the exercise text if you’re interested — basically it accounts for the intercept term in the linear equation). Second, we need to separate our data into independent variables X and our dependent variable y.
Finally, we’re going to convert our data frames to numpy matrices and instantiate a parameter matirx.
One useful trick to remember when debugging matrix operations is to look at the shape of the matrices you’re dealing with. It’s also helpful to remember when walking through the steps in your head that matrix multiplications look like (i x j) * (j x k) = (i x k), where i, j, and k are the shapes of the relative dimensions of the matrix.
((97L, 2L), (1L, 2L), (97L, 1L))
Okay, so now we can try out our cost function. Remember the parameters were initialized to 0 so the solution isn’t optimal yet, but we can see if it works.
32.072733877455676
So far so good. Now we need to define a function to perform gradient descent on the parameters *theta* using the update rules defined in the exercise text. Here’s the function for gradient descent:
The idea with gradient descent is that for each iteration, we compute the gradient of the error term in order to figure out the appropriate direction to move our parameter vector. In other words, we’re calculating the changes to make to our parameters in order to reduce the error, thus bringing our solution closer to the optimal solution (i.e best fit).
This is a fairly complex topic and I could easily devote a whole blog post just to discussing gradient descent. If you’re interested in learning more, I would recommend starting with this article and branching out from there.
Once again we’re relying on numpy and linear algebra for our solution. You may notice that my implementation is not 100% optimal. In particular, there’s a way to get rid of that inner loop and update all of the parameters at once. I’ll leave it up to the reader to figure it out for now (I’ll cover it in a later post).
Now that we’ve got a way to evaluate solutions, and a way to find a good solution, it’s time to apply this to our data set.
matrix([[-3.24140214, 1.1272942 ]])
Note that we’ve initialized a few new variables here. If you look closely at the gradient descent function, it has parameters called alpha and iters. Alpha is the learning rate — it’s a factor in the update rule for the parameters that helps determine how quickly the algorithm will converge to the optimal solution. Iters is just the number of iterations. There is no hard and fast rule for how to initialize these parameters and typically some trial-and-error is involved.
We now have a parameter vector descibing what we believe is the optimal linear model for our data set. One quick way to evaluate just how good our regression model is might be to look at the total error of our new solution on the data set:
4.5159555030789118
That’s certainly a lot better than 32, but it’s not a very intuitive way to look at it. Fortunately we have some other techniques at our disposal.
We’re now going to use matplotlib to visualize our solution. Remember the scatter plot from before? Let’s overlay a line representing our model on top of a scatter plot of the data to see how well it fits. We can use numpy’s “linspace” function to create an evenly-spaced series of points within the range of our data, and then “evaluate” those points using our model to see what the expected profit would be. We can then turn it into a line graph and plot it.
Not bad! Our solution looks like and optimal linear model of the data set. Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.
Notice that the cost always decreases — this is an example of what’s called a convex optimization problem. If you were to plot the entire solution space for the problem (i.e. plot the cost as a function of the model parameters for every possible value of the parameters) you would see that it looks like a “bowl” shape with a “basin” representing the optimal solution.
That’s all for now! In part 2 we’ll finish off the first exercise by extending this example to more than 1 variable. I’ll also show how the above solution can be reached by using a popular machine learning library called scikit-learn.
To comment on this article, check out the original post at Curious Insight
Follow me on twitter to get new post updates
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data scientist, engineer, author, investor, entrepreneur
",content originally appeared curious insight post part series covering exercises andrew ngs machine learning class coursera original code exercise text data files post available part 1 simple linear regressionpart 2 multivariate linear regressionpart 3 logistic regressionpart 4 multivariate logistic regressionpart 5 neural networkspart 6 support vector machinespart 7 kmeans clustering pcapart 8 anomaly detection recommendation one pivotal moments professional development year came discovered coursera id heard mooc phenomenon time dive take class earlier year finally pulled trigger signed andrew ngs machine learning class completed whole thing start finish including programming exercises experience opened eyes power type education platform ive hooked ever since blog post first series covering programming exercises andrews class one aspect course didnt particularly care use octave assignments although octavematlab fine platform realworld data science done either r python certainly languages tools used two unquestionably top list since im trying develop python skills decided start working exercises scratch python full source code available ipython repo github youll also find data used exercises original exercise pdfs subfolders root directory youre interested explain concepts involved exercise along way impossible convey information might need fully comprehend youre really interested machine learning havent exposed yet encourage check class completely free theres commitment whatsoever lets get started first part exercise 1 tasked implementing simple linear regression predict profits food truck suppose ceo restaurant franchise considering different cities opening new outlet chain already trucks various cities data profits populations cities youd like figure expected profit new food truck might given population city would placed lets start examining data file called ex1data1txt data directory repository first need import libraries lets get things rolling use pandas load data data frame display first rows using head function note medium cant render tables full example another useful function pandas provides outofthebox describe function calculates basic statistics data set helpful get feel data exploratory analysis stage project note medium cant render tables full example examining stats data helpful sometimes need find ways visualize fortunately data set one dependent variable toss scatter plot get better idea looks like use plot function provided pandas really wrapper matplotlib really helps actually look whats going doesnt clearly see theres cluster values around cities smaller populations somewhat linear trend increasing profit size city increases lets get fun part implementing linear regression algorithm python scratch youre familiar linear regression approach modeling relationship dependent variable one independent variables theres one independent variable called simple linear regression theres one independent variable called multiple linear regression lots different types variances linear regression outside scope discussion wont go put simply trying create linear model data x using number parameters theta describes variance data given new data point thats x could accurately predict outcome would without actually knowing implementation going use optimization technique called gradient descent find parameters theta youre familiar linear algebra may aware theres another way find optimal parameters linear model called normal equation basically solves problem using series matrix calculations however issue approach doesnt scale well large data sets contrast use variants gradient descent optimization methods scale data sets unlimited size machine learning problems approach practical okay thats enough theory lets write code first thing need cost function cost function evaluates quality model calculating error models prediction data point using model parameters actual data point example population given city 4 predicted 7 error 742 32 9 assuming l2 least squares loss function data point x sum result get cost heres function notice loops taking advantage numpys linear algrebra capabilities compute result series matrix operations far computationally efficient unoptimizted loop order make cost function work seamlessly pandas data frame created need manipulating first need insert column 1s beginning data frame order make matrix operations work correctly wont go detail needed exercise text youre interested basically accounts intercept term linear equation second need separate data independent variables x dependent variable finally going convert data frames numpy matrices instantiate parameter matirx one useful trick remember debugging matrix operations look shape matrices youre dealing also helpful remember walking steps head matrix multiplications look like x j j x k x k j k shapes relative dimensions matrix 97l 2l 1l 2l 97l 1l okay try cost function remember parameters initialized 0 solution isnt optimal yet see works 32072733877455676 far good need define function perform gradient descent parameters theta using update rules defined exercise text heres function gradient descent idea gradient descent iteration compute gradient error term order figure appropriate direction move parameter vector words calculating changes make parameters order reduce error thus bringing solution closer optimal solution ie best fit fairly complex topic could easily devote whole blog post discussing gradient descent youre interested learning would recommend starting article branching relying numpy linear algebra solution may notice implementation 100 optimal particular theres way get rid inner loop update parameters ill leave reader figure ill cover later post weve got way evaluate solutions way find good solution time apply data set matrix324140214 11272942 note weve initialized new variables look closely gradient descent function parameters called alpha iters alpha learning rate factor update rule parameters helps determine quickly algorithm converge optimal solution iters number iterations hard fast rule initialize parameters typically trialanderror involved parameter vector descibing believe optimal linear model data set one quick way evaluate good regression model might look total error new solution data set 45159555030789118 thats certainly lot better 32 intuitive way look fortunately techniques disposal going use matplotlib visualize solution remember scatter plot lets overlay line representing model top scatter plot data see well fits use numpys linspace function create evenlyspaced series points within range data evaluate points using model see expected profit would turn line graph plot bad solution looks like optimal linear model data set since gradient decent function also outputs vector cost training iteration plot well notice cost always decreases example whats called convex optimization problem plot entire solution space problem ie plot cost function model parameters every possible value parameters would see looks like bowl shape basin representing optimal solution thats part 2 well finish first exercise extending example 1 variable ill also show solution reached using popular machine learning library called scikitlearn comment article check original post curious insight follow twitter get new post updates quick cheer standing ovation clap show much enjoyed story data scientist engineer author investor entrepreneur,en,"['Curious Insight', 'Logistic RegressionPart 4', 'Multivariate Logistic RegressionPart', 'NetworksPart', 'Support Vector MachinesPart', 'K-Means Clustering & PCAPart', 'Anomaly Detection & Recommendation', 'Octave/Matlab', 'Python', 'Alpha', 'algorithm', 'Data']"
40,Pinterest Engineering,25,Building the interests platform – Pinterest Engineering – Medium,"Ningning Hu | Pinterest engineer, Discovery
The core value of Pinterest is to help people find the things they care about, by connecting them to Pins and people that relate to their interests. We’re building a service that’s powered by people, and supercharged with technology.
The interest graph — the connections that make up the Pinterest index — creates bridges between Pins, boards, and Pinners. It’s our job to build a system that helps people to collect the things they love, and connect them to communities of engaged people who share similar interests and can help them discover more. From categories like travel, fitness, and humor, to more niche areas like vintage motorcycles, craft beer, or Japanese architecture, we’re building a visual discovery tool for all interests.
The interests platform is built to support this vision. Specifically, it’s responsible for producing high quality data on interests, interest relationships, and their association with Pins, boards, and Pinners.
Figure 1: Feedback loop between machine intelligence and human curation
In contrast with conventional methods of generating such data, which rely primarily on machine learning and data mining techniques, our system relies heavily on human curation. The ultimate goal is to build a system that’s both machine and human powered, creating a feedback mechanism by which human curated data helps drive improvements in our machine algorithms, and vice versa.
Figure 2: System components
Raw input to the system includes existing data about Pins, boards, Pinners, and search queries, as well as explicit human curation signals about interests. With this data, we’re able to construct a continuously evolving interest dictionary, which provides the foundation to support other key components, such as interest feeds, interest recommendations, and related interests.
From a technology standpoint, interests are text strings that represent entities for which a group of Pinners might have a shared passion.
We generated an initial collection of interests by extracting frequently occurring n-grams from Pin and board descriptions, as well as board titles, and filtering these n-grams using custom built grammars. While this approach provided a high coverage set of interests, we found many terms to be malformed phrases. For instance, we would extract phrases such as “lamborghini yellow” instead of “yellow lamborghini”. This proved problematic because we wanted interest terms to represent how Pinners would describe them, and so, we employed a variety of methods to eliminate malformed interests terms.
We first compared terms with repeated search queries performed by a group of Pinners over a few months. Intuitively, this criterion matches well with the notion that an interest should be an entity for which a group of Pinners are passionate.
Later we filtered the candidate set through public domain ontologies like Wikipedia titles. These ontologies were primarily used to validate proper nouns as opposed to common phrases, as all available ontologies represented only a subset of possible interests. This is especially true for Pinterest, where Pinners themselves curate special interests like “mid century modern style.”
Finally, we also maintain an internal blacklist to filter abusive words and x-rated terms as well as Pinterest specific stop words, like “love”. This filtering is especially important to interest terms which might be recommended to millions of users.
We arrived at a fair quality collection of interests following the above algorithmic approaches. In order to understand the quality of our efforts, we gave a 50,000 term subset of our collection to a third party vendor which used crowdsourcing to rate our data. To be rigorous, we composed a set of four criteria by which users would evaluate candidate Interests terms:
- Is it English?
- Is it a valid phrase in grammar?
- Is it a standalone concept?
- Is it a proper name?
The crowdsourced ratings were both interesting if not somewhat expected. There was a low rate of agreement amongst raters, with especially high discrepancy in determining whether an interest’s term represented a “standalone concept.” Despite the ambiguity, we were able to confirm that 80% of the collection generated using the above algorithms satisfied our interests criteria.
This type of effort, however, is not easy to scale. The real solution is to allow Pinners to provide both implicit and explicit signals to help us determine the validity of an interest. Implicit signals behaviors like clicking and viewing, while explicit signals include asking Pinners to specifically provide information (which can be actions like a thumbs up/thumbs down, starring, or skipping recommendations).
To capture all the signals used for defining the collections of terms, we built a dictionary that stores all the data associated with each interest, including invalid interests and the reason why it’s invalid. This service plays a key role in human curation, by aggregating signals from different people. On top of this dictionary service, we can build different levels of reviewing system.
With the Interests dictionary, we can associate Pins, boards, and Pinners with representative interests. One of the initial ways we experimented with this was launching a preview of a page where Pinners can explore their interests.
Figure 3: Exploring interests
In order to match interests to Pinners, we need to aggregate all the information related with a person’s interests. At its core, our system recommends interests based upon Pins with which a Pinner interacts. Every Pin on Pinterest has been collected and given context by someone who thinks it’s important, and in doing so, is helping other people discover great content. Each individual Pin is an incredibly rich source of data. As discussed in a previous blog post on discovery data model, one Pin often has multiple copies — different people may Pin it from different sources, and the same Pin can be repinned multiple times. During this process, each Pin accumulates numerous unique textual descriptions which allows us to connect Pins with interests terms with high precision.
However, this conceptually simple process requires non-trivial engineering effort to scale to the amount of Pins and Pinners that the service has today. The data process pipeline (managed by Pinball) composes over 35 Hadoop jobs, and runs periodically to update the user-interest mapping to capture users’ latest interest information.
The initial feedback on the explore interests page has been positive, proving the capabilities of our system. We’ll continue testing different ways of exposing a person’s interests and related content, based on implicit signals, as well as explicit signals (such as the ability to create custom categories of interests).
Related interests are an important way of enabling the ability to browse interests and discover new ones. To compute related interests, we simply combine the co-occurrence relationship for interests computed at Pin and board levels.
Figure 4: Computing related interests
The quality of the related interests is surprisingly high given the simplicity of the algorithm. We attribute this effect to the cleanness of Pinterest data. Text data on Pins tend to be very concise, and contain less noise than other types of data, like web pages. Also, related interests calculation already makes use of boards, which are heavily curated by people (vs. machines) in regards to organizing related content. We find that utilizing the co-occurrence of interest terms at the level of both Pins and boards provides the best tradeoff between achieving high precision as well as recall when computing the related interests.
One of the initial ways we began showing people related content was through related Pins. When you Pin an object, you’ll see a recommendation for a related board with that same Pin so you can explore similar objects. Additionally, if you scroll beneath a Pin, you’ll see Pins from other people who’ve also Pinned that original object. At this point, 90% of all Pins have related Pins, and we’ve seen 20% growth in engagement with related Pins in the last six months.
Interests feeds provide Pinners with a continuous feed of Pins that are highly related. Our feeds are populated using a variety of sources, including search and through our annotation pipeline. A key property of the feed is flow. Only feeds with decent flow can attract Pinners to come back repeatedly, thereby maintaining high engagement. In order to optimize for our feeds, we’ve utilized a number of real-time indexing and retrieval systems, including real-time search, real-time annotating, and also human curation for some of the interests.
To ensure quality, we need to guarantee quality from all sources. For that purpose, we measure the engagement of Pins from each source and address quality issue accordingly.
Figure 5: How interest feeds are generated
Accurately capturing Pinner interests and interest relationships, and making this data understandable and actionable for tens of millions of people (collecting tens of billions of Pins), is not only an engineering challenge, but also a product design one. We’re just at the beginning, as we continue to improve the data and design ways to empower people to provide feedback that allows us to build a hybrid system combining machine and human curation to power discovery. Results of these effort will be reflected in future product releases.
If you’re interested in building new ways of helping people discover the things they care about, join our team!
Acknowledgements: The core team members for the interests backend platform are Ningning Hu, Leon Lin, Ryan Shih and Yuan Wei. Many other folks from other parts of the company, especially the discovery team and the infrastructure teams, have provided very useful feedback and help along the way to make the ongoing project successful.
Ningning Hu is an engineer at Pinterest.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering
",ningning hu pinterest engineer discovery core value pinterest help people find things care connecting pins people relate interests building service thats powered people supercharged technology interest graph connections make pinterest index creates bridges pins boards pinners job build system helps people collect things love connect communities engaged people share similar interests help discover categories like travel fitness humor niche areas like vintage motorcycles craft beer japanese architecture building visual discovery tool interests interests platform built support vision specifically responsible producing high quality data interests interest relationships association pins boards pinners figure 1 feedback loop machine intelligence human curation contrast conventional methods generating data rely primarily machine learning data mining techniques system relies heavily human curation ultimate goal build system thats machine human powered creating feedback mechanism human curated data helps drive improvements machine algorithms vice versa figure 2 system components raw input system includes existing data pins boards pinners search queries well explicit human curation signals interests data able construct continuously evolving interest dictionary provides foundation support key components interest feeds interest recommendations related interests technology standpoint interests text strings represent entities group pinners might shared passion generated initial collection interests extracting frequently occurring ngrams pin board descriptions well board titles filtering ngrams using custom built grammars approach provided high coverage set interests found many terms malformed phrases instance would extract phrases lamborghini yellow instead yellow lamborghini proved problematic wanted interest terms represent pinners would describe employed variety methods eliminate malformed interests terms first compared terms repeated search queries performed group pinners months intuitively criterion matches well notion interest entity group pinners passionate later filtered candidate set public domain ontologies like wikipedia titles ontologies primarily used validate proper nouns opposed common phrases available ontologies represented subset possible interests especially true pinterest pinners curate special interests like mid century modern style finally also maintain internal blacklist filter abusive words xrated terms well pinterest specific stop words like love filtering especially important interest terms might recommended millions users arrived fair quality collection interests following algorithmic approaches order understand quality efforts gave 50000 term subset collection third party vendor used crowdsourcing rate data rigorous composed set four criteria users would evaluate candidate interests terms english valid phrase grammar standalone concept proper name crowdsourced ratings interesting somewhat expected low rate agreement amongst raters especially high discrepancy determining whether interests term represented standalone concept despite ambiguity able confirm 80 collection generated using algorithms satisfied interests criteria type effort however easy scale real solution allow pinners provide implicit explicit signals help us determine validity interest implicit signals behaviors like clicking viewing explicit signals include asking pinners specifically provide information actions like thumbs upthumbs starring skipping recommendations capture signals used defining collections terms built dictionary stores data associated interest including invalid interests reason invalid service plays key role human curation aggregating signals different people top dictionary service build different levels reviewing system interests dictionary associate pins boards pinners representative interests one initial ways experimented launching preview page pinners explore interests figure 3 exploring interests order match interests pinners need aggregate information related persons interests core system recommends interests based upon pins pinner interacts every pin pinterest collected given context someone thinks important helping people discover great content individual pin incredibly rich source data discussed previous blog post discovery data model one pin often multiple copies different people may pin different sources pin repinned multiple times process pin accumulates numerous unique textual descriptions allows us connect pins interests terms high precision however conceptually simple process requires nontrivial engineering effort scale amount pins pinners service today data process pipeline managed pinball composes 35 hadoop jobs runs periodically update userinterest mapping capture users latest interest information initial feedback explore interests page positive proving capabilities system well continue testing different ways exposing persons interests related content based implicit signals well explicit signals ability create custom categories interests related interests important way enabling ability browse interests discover new ones compute related interests simply combine cooccurrence relationship interests computed pin board levels figure 4 computing related interests quality related interests surprisingly high given simplicity algorithm attribute effect cleanness pinterest data text data pins tend concise contain less noise types data like web pages also related interests calculation already makes use boards heavily curated people vs machines regards organizing related content find utilizing cooccurrence interest terms level pins boards provides best tradeoff achieving high precision well recall computing related interests one initial ways began showing people related content related pins pin object youll see recommendation related board pin explore similar objects additionally scroll beneath pin youll see pins people whove also pinned original object point 90 pins related pins weve seen 20 growth engagement related pins last six months interests feeds provide pinners continuous feed pins highly related feeds populated using variety sources including search annotation pipeline key property feed flow feeds decent flow attract pinners come back repeatedly thereby maintaining high engagement order optimize feeds weve utilized number realtime indexing retrieval systems including realtime search realtime annotating also human curation interests ensure quality need guarantee quality sources purpose measure engagement pins source address quality issue accordingly figure 5 interest feeds generated accurately capturing pinner interests interest relationships making data understandable actionable tens millions people collecting tens billions pins engineering challenge also product design one beginning continue improve data design ways empower people provide feedback allows us build hybrid system combining machine human curation power discovery results effort reflected future product releases youre interested building new ways helping people discover things care join team acknowledgements core team members interests backend platform ningning hu leon lin ryan shih yuan wei many folks parts company especially discovery team infrastructure teams provided useful feedback help along way make ongoing project successful ningning hu engineer pinterest quick cheer standing ovation clap show much enjoyed story inventive engineers building first visual discovery engine 100 billion ideas counting httpscareerspinterestcomcareersengineering,en,"['Implicit', 'Pinner', 'Pin']"
41,Christopher Nguyen,991,Algorithms of the Mind – Deep Learning 101 – Medium,"What Machine Learning Teaches Us About Ourselves
Originally published at blog.arimo.com.Follow me on Twitter to keep informed of interesting developments on these topics.
“Science often follows technology, because inventions give us new ways to think about the world and new phenomena in need of explanation.”
Or so Aram Harrow, an MIT physics professor, counter-intuitively argues in “Why now is the right time to study quantum computing”.
He suggests that the scientific idea of entropy could not really be conceived until steam engine technology necessitated understanding of thermodynamics. Quantum computing similarly arose from attempts to simulate quantum mechanics on ordinary computers.
So what does all this have to do with machine learning?
Much like steam engines, machine learning is a technology intended to solve specific classes of problems. Yet results from the field are indicating intriguing—possibly profound—scientific clues about how our own brains might operate, perceive, and learn. The technology of machine learning is giving us new ways to think about the science of human thought ... and imagination.
Five years ago, deep learning pioneer Geoff Hinton (who currently splits his time between the University of Toronto and Google) published the following demo.
Hinton had trained a five-layer neural network to recognize handwritten digits when given their bitmapped images. It was a form of computer vision, one that made handwriting machine-readable.
But unlike previous works on the same topic, where the main objective is simply to recognize digits, Hinton’s network could also run in reverse. That is, given the concept of a digit, it can regenerate images corresponding to that very concept.
We are seeing, quite literally, a machine imagining an image of the concept of “8”.
The magic is encoded in the layers between inputs and outputs. These layers act as a kind of associative memory, mapping back-and-forth from image and concept, from concept to image, all in one neural network.
But beyond the simplistic, brain-inspired machine vision technology here, the broader scientific question is whether this is how human imagination — visualization — works. If so, there’s a huge a-ha moment here.
After all, isn’t this something our brains do quite naturally? When we see the digit 4, we think of the concept “4”. Conversely, when someone says “8”, we can conjure up in our minds’ eye an image of the digit 8.
Is it all a kind of “running backwards” by the brain from concept to images (or sound, smell, feel, etc.) through the information encoded in the layers? Aren’t we watching this network create new pictures — and perhaps in a more advanced version, even new internal connections — as it does so?
If visual recognition and imagination are indeed just back-and-forth mapping between images and concepts, what’s happening between those layers? Do deep neural networks have some insight or analogies to offer us here?
Let’s first go back 234 years, to Immanuel Kant’s Critique of Pure Reason, in which he argues that “Intuition is nothing but the representation of phenomena”.
Kant railed against the idea that human knowledge could be explained purely as empirical and rational thought. It is necessary, he argued, to consider intuitions. In his definitions, “intuitions” are representations left in a person’s mind by sensory perceptions, where as “concepts” are descriptions of empirical objects or sensory data. Together, these make up human knowledge.
Fast forwarding two centuries later, Berkeley CS professor Alyosha Efros, who specializes in Visual Understanding, pointed out that “there are many more things in our visual world than we have words to describe them with”. Using word labels to train models, Efros argues, exposes our techniques to a language bottleneck. There are many more un-namable intuitions than we have words for.
In training deep networks, such as the seminal “cat-recognition” work led by Quoc Le at Google/Stanford, we’re discovering that the activations in successive layers appear to go from lower to higher conceptual levels. An image recognition network encodes bitmaps at the lowest layer, then apparent corners and edges at the next layer, common shapes at the next, and so on. These intermediate layers don’t necessarily have any activations corresponding to explicit high-level concepts, like “cat” or “dog”, yet they do encode a distributed representation of the sensory inputs. Only the final, output layer has such a mapping to human-defined labels, because they are constrained to match those labels.
Therefore, the above encodings and labels seem to correspond to exactly what Kant referred to as “intuitions” and “concepts”.
In yet another example of machine learning technology revealing insights about human thought, the network diagram above makes you wonder whether this is how the architecture of Intuition — albeit vastly simplified — is being expressed.
If — as Efros has pointed out — there are a lot more conceptual patterns than words can describe, then do words constrain our thoughts? This question is at the heart of the Sapir-Whorf or Linguistic Relativity Hypothesis, and the debate about whether language completely determines the boundaries of our cognition, or whether we are unconstrained to conceptualize anything — regardless of the languages we speak.
In its strongest form, the hypothesis posits that the structure and lexicon of languages constrain how one perceives and conceptualizes the world.
One of the most striking effects of this is demonstrated in the color test shown here. When asked to pick out the one square with a shade of green that’s distinct from all the others, the Himba people of northern Namibia — who have distinct words for the two shades of green — can find it almost instantly.
The rest of us, however, have a much harder time doing so.
The theory is that — once we have words to distinguish one shade from another, our brains will train itself to discriminate between the shades, so the difference would become more and more “obvious” over time. In seeing with our brain, not with our eyes, language drives perception.
With machine learning, we also observe something similar. In supervised learning, we train our models to best match images (or text, audio, etc.) against provided labels or categories. By definition, these models are trained to discriminate much more effectively between categories that have provided labels, than between other possible categories for which we have not provided labels. When viewed from the perspective of supervised machine learning, this outcome is not at all surprising. So perhaps we shouldn’t be too surprised by the results of the color experiment above, either. Language does indeed influence our perception of the world, in the same way that labels in supervised machine learning influence the model’s ability to discriminate among categories.
And yet, we also know that labels are not strictly required to discriminate between cues. In Google’s “cat-recognizing brain”, the network eventually discovers the concept of “cat”, “dog”, etc. all by itself — even without training the algorithm against explicit labels. After this unsupervised training, whenever the network is fed an image belonging to a certain category like “Cats”, the same corresponding set of “Cat” neurons always gets fired up. Simply by looking at the vast set of training images, this network has discovered the essential patterns of each category, as well as the differences of one category vs. another.
In the same way, an infant who is repeatedly shown a paper cup would soon recognize the visual pattern of such a thing, even before it ever learns the words “paper cup” to attach that pattern to a name. In this sense, the strong form of the Sapir-Whorf hypothesis cannot be entirely correct — we can, and do, discover concepts even without the words to describe them.
Supervised and unsupervised machine learning turn out to represent the two sides of the controversy’s coin. And if we recognized them as such, perhaps Sapir-Whorf would not be such a controversy, and more of a reflection of supervised and unsupervised human learning.
I find these correspondences deeply fascinating — and we’ve only scratched the surface. Philosophers, psychologists, linguists, and neuroscientists have studied these topics for a long time. The connection to machine learning and computer science is more recent, especially with the advances in big data and deep learning. When fed with huge amounts of text, images, or audio data, the latest deep learning architectures are demonstrating near or even better-than-human performance in language translation, image classification, and speech recognition.
Every new discovery in machine learning demystifies a bit more of what may be going on in our brains. We’re increasingly able to borrow from the vocabulary of machine learning to talk about our minds.
Thanks to Sonal Chokshi and Vu Pham for extensive review & edits. Also, chrisjagers, chickamade.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@arimoinc CEO & Co-Founder. Leader, Entrepreneur, Hacker, Xoogler, Executive, Professor. #DataViz #ParallelComputing #DeepLearning & former #GoogleApps.
Fundamentals and Latest Developments in #DeepLearning
",machine learning teaches us originally published blogarimocomfollow twitter keep informed interesting developments topics science often follows technology inventions give us new ways think world new phenomena need explanation aram harrow mit physics professor counterintuitively argues right time study quantum computing suggests scientific idea entropy could really conceived steam engine technology necessitated understanding thermodynamics quantum computing similarly arose attempts simulate quantum mechanics ordinary computers machine learning much like steam engines machine learning technology intended solve specific classes problems yet results field indicating intriguingpossibly profoundscientific clues brains might operate perceive learn technology machine learning giving us new ways think science human thought imagination five years ago deep learning pioneer geoff hinton currently splits time university toronto google published following demo hinton trained fivelayer neural network recognize handwritten digits given bitmapped images form computer vision one made handwriting machinereadable unlike previous works topic main objective simply recognize digits hintons network could also run reverse given concept digit regenerate images corresponding concept seeing quite literally machine imagining image concept 8 magic encoded layers inputs outputs layers act kind associative memory mapping backandforth image concept concept image one neural network beyond simplistic braininspired machine vision technology broader scientific question whether human imagination visualization works theres huge aha moment isnt something brains quite naturally see digit 4 think concept 4 conversely someone says 8 conjure minds eye image digit 8 kind running backwards brain concept images sound smell feel etc information encoded layers arent watching network create new pictures perhaps advanced version even new internal connections visual recognition imagination indeed backandforth mapping images concepts whats happening layers deep neural networks insight analogies offer us lets first go back 234 years immanuel kants critique pure reason argues intuition nothing representation phenomena kant railed idea human knowledge could explained purely empirical rational thought necessary argued consider intuitions definitions intuitions representations left persons mind sensory perceptions concepts descriptions empirical objects sensory data together make human knowledge fast forwarding two centuries later berkeley cs professor alyosha efros specializes visual understanding pointed many things visual world words describe using word labels train models efros argues exposes techniques language bottleneck many unnamable intuitions words training deep networks seminal catrecognition work led quoc le googlestanford discovering activations successive layers appear go lower higher conceptual levels image recognition network encodes bitmaps lowest layer apparent corners edges next layer common shapes next intermediate layers dont necessarily activations corresponding explicit highlevel concepts like cat dog yet encode distributed representation sensory inputs final output layer mapping humandefined labels constrained match labels therefore encodings labels seem correspond exactly kant referred intuitions concepts yet another example machine learning technology revealing insights human thought network diagram makes wonder whether architecture intuition albeit vastly simplified expressed efros pointed lot conceptual patterns words describe words constrain thoughts question heart sapirwhorf linguistic relativity hypothesis debate whether language completely determines boundaries cognition whether unconstrained conceptualize anything regardless languages speak strongest form hypothesis posits structure lexicon languages constrain one perceives conceptualizes world one striking effects demonstrated color test shown asked pick one square shade green thats distinct others himba people northern namibia distinct words two shades green find almost instantly rest us however much harder time theory words distinguish one shade another brains train discriminate shades difference would become obvious time seeing brain eyes language drives perception machine learning also observe something similar supervised learning train models best match images text audio etc provided labels categories definition models trained discriminate much effectively categories provided labels possible categories provided labels viewed perspective supervised machine learning outcome surprising perhaps shouldnt surprised results color experiment either language indeed influence perception world way labels supervised machine learning influence models ability discriminate among categories yet also know labels strictly required discriminate cues googles catrecognizing brain network eventually discovers concept cat dog etc even without training algorithm explicit labels unsupervised training whenever network fed image belonging certain category like cats corresponding set cat neurons always gets fired simply looking vast set training images network discovered essential patterns category well differences one category vs another way infant repeatedly shown paper cup would soon recognize visual pattern thing even ever learns words paper cup attach pattern name sense strong form sapirwhorf hypothesis cannot entirely correct discover concepts even without words describe supervised unsupervised machine learning turn represent two sides controversys coin recognized perhaps sapirwhorf would controversy reflection supervised unsupervised human learning find correspondences deeply fascinating weve scratched surface philosophers psychologists linguists neuroscientists studied topics long time connection machine learning computer science recent especially advances big data deep learning fed huge amounts text images audio data latest deep learning architectures demonstrating near even betterthanhuman performance language translation image classification speech recognition every new discovery machine learning demystifies bit may going brains increasingly able borrow vocabulary machine learning talk minds thanks sonal chokshi vu pham extensive review edits also chrisjagers chickamade quick cheer standing ovation clap show much enjoyed story arimoinc ceo cofounder leader entrepreneur hacker xoogler executive professor dataviz parallelcomputing deeplearning former googleapps fundamentals latest developments deeplearning,en,"['blog.arimo.com', 'MIT', 'Quantum', 'the University of Toronto', 'Google', 'Hinton', 'Berkeley CS', 'un', 'Google/Stanford', 'Linguistic Relativity Hypothesis', 'Sapir-Whorf', '@arimoinc CEO & Co-Founder']"
42,Per Harald Borgen,2100,Machine Learning in a Week – Learning New Stuff – Medium,"Getting into machine learning (ml) can seem like an unachievable task from the outside.
However, after dedicating one week to learning the basics of the subject, I found it to be much more accessible than I anticipated.
This article is intended to give others who’re interested in getting into ml a roadmap of how to get started, drawing from the experiences I made in my intro week.
Before my machine learning week, I had been reading about the subject for a while, and had gone through half of Andrew Ng’s course on Coursera and a few other theoretical courses. So I had a tiny bit of conceptual understanding of ml, though I was completely unable to transfer any of my knowledge into code. This is what I wanted to change.
I wanted to be able to solve problems with ml by the end of the week, even through this meant skipping a lot of fundamentals, and going for a top-down approach, instead of bottoms up.
After asking for advice on Hacker News, I came to the conclusion that Python’s Scikit Learn-module was the best starting point. This module gives you a wealth of algorithms to choose from, reducing the actual machine learning to a few lines of code.
I started off the week by looking for video tutorials which involved Scikit Learn. I finally landed on Sentdex’s tutorial on how to use ml for investing in stocks, which gave me the necessary knowledge to move on to the next step.
The good thing about the Sentdex tutorial is that the instructor takes you through all the steps of gathering the data. As you go along, you realize that fetching and cleaning up the data can be much more time consuming than doing the actually machine learning. So the ability to write scripts to scrape data from files or crawl the web are essential skills for aspiring machine learning geeks.
I have re-watched several of the videos later on, to help me when I’ve been stuck with problems, so I’d recommend you to do the same.
However, if you already know how to scrape data from websites, this tutorial might not be the perfect fit, as a lot of the videos evolve around data fetching. In that case, the Udacity’s Intro to Machine Learning might be a better place to start.
Tuesday I wanted to see if I could use what I had learned to solve an actual problem. As another developer in my coding cooperative was working on Bank of England’s data visualization competition, I teamed up with him to check out the datasets the bank has released. The most interesting data was their household surveys. This is an annual survey the bank perform on a few thousand households, regarding money related subjects.
The problem we decided to solve was the following:
I played around with the dataset, spent a few hours cleaning up the data, and used the Scikit Learn map to find a suitable algorithm for the problem.
We ended up with a success ratio at around 63%, which isn’t impressive at all. But the machine did at least manage to guess a little better than flipping a coin, which would have given a success rate at 50%.
Seeing results is like fuel to your motivation, so I’d recommend you doing this for yourself, once you have a basic grasp of how to use Scikit Learn.
After playing around with various Scikit Learn modules, I decided to try and write a linear regression algorithm from the ground up.
I wanted to do this, because I felt (and still feel) that I really don’t understand what’s happening on under the hood.
Luckily, the Coursera course goes into detail on how a few of the algorithms work, which came to great use at this point. More specifically, it describes the underlying concepts of using linear regression with gradient descent.
This has definitely been the most effective of learning technique, as it forces you to understand the steps that are going on ‘under the hood’. I strongly recommend you to do this at some point.
I plan to rewrite my own implementations of more complex algorithms as I go along, but I prefer doing this after I’ve played around with the respective algorithms in Scikit Learn.
On Thursday, I started doing Kaggle’s introductory tutorials. Kaggle is a platform for machine learning competitions, where you can submit solutions to problems released by companies or organizations .
I recommend you trying out Kaggle after having a little bit of a theoretical and practical understanding of machine learning. You’ll need this in order to start using Kaggle. Otherwise, it will be more frustrating than rewarding.
The Bag of Words tutorial guides you through every steps you need to take in order to enter a submission to a competition, plus gives you a brief and exciting introduction into Natural Language Processing (NLP). I ended the tutorial with much higher interest in NLP than I had when entering it.
Friday, I continued working on the Kaggle tutorials, and also started Udacity’s Intro to Machine Learning. I’m currently half ways through, and find it quite enjoyable.
It’s a lot easier the Coursera course, as it doesn’t go in depth in the algorithms. But it’s also more practical, as it teaches you Scikit Learn, which is a whole lot easier to apply to the real world than writing algorithms from the ground up in Octave, as you do in the Coursera course.
Doing it for a week hasn’t just been great fun, it has also helped my awareness of its usefulness of machine learning in society. The more I learn about it, the more I see which areas it can be used to solve problems.
Choose a top down approach if you’re not ready for the heavy stuff, and get into problem solving as quickly as possible.
Good luck!
Thanks for reading! My name is Per, I’m a co-founder of Scrimba — a better way to teach and learn code.
If you’ve read this far, I’d recommend you to check out this demo!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder of Scrimba, the next-generation platform for teaching and learning code. https://scrimba.com.
A publication about improving your technical skills.
",getting machine learning ml seem like unachievable task outside however dedicating one week learning basics subject found much accessible anticipated article intended give others whore interested getting ml roadmap get started drawing experiences made intro week machine learning week reading subject gone half andrew ngs course coursera theoretical courses tiny bit conceptual understanding ml though completely unable transfer knowledge code wanted change wanted able solve problems ml end week even meant skipping lot fundamentals going topdown approach instead bottoms asking advice hacker news came conclusion pythons scikit learnmodule best starting point module gives wealth algorithms choose reducing actual machine learning lines code started week looking video tutorials involved scikit learn finally landed sentdexs tutorial use ml investing stocks gave necessary knowledge move next step good thing sentdex tutorial instructor takes steps gathering data go along realize fetching cleaning data much time consuming actually machine learning ability write scripts scrape data files crawl web essential skills aspiring machine learning geeks rewatched several videos later help ive stuck problems id recommend however already know scrape data websites tutorial might perfect fit lot videos evolve around data fetching case udacitys intro machine learning might better place start tuesday wanted see could use learned solve actual problem another developer coding cooperative working bank englands data visualization competition teamed check datasets bank released interesting data household surveys annual survey bank perform thousand households regarding money related subjects problem decided solve following played around dataset spent hours cleaning data used scikit learn map find suitable algorithm problem ended success ratio around 63 isnt impressive machine least manage guess little better flipping coin would given success rate 50 seeing results like fuel motivation id recommend basic grasp use scikit learn playing around various scikit learn modules decided try write linear regression algorithm ground wanted felt still feel really dont understand whats happening hood luckily coursera course goes detail algorithms work came great use point specifically describes underlying concepts using linear regression gradient descent definitely effective learning technique forces understand steps going hood strongly recommend point plan rewrite implementations complex algorithms go along prefer ive played around respective algorithms scikit learn thursday started kaggles introductory tutorials kaggle platform machine learning competitions submit solutions problems released companies organizations recommend trying kaggle little bit theoretical practical understanding machine learning youll need order start using kaggle otherwise frustrating rewarding bag words tutorial guides every steps need take order enter submission competition plus gives brief exciting introduction natural language processing nlp ended tutorial much higher interest nlp entering friday continued working kaggle tutorials also started udacitys intro machine learning im currently half ways find quite enjoyable lot easier coursera course doesnt go depth algorithms also practical teaches scikit learn whole lot easier apply real world writing algorithms ground octave coursera course week hasnt great fun also helped awareness usefulness machine learning society learn see areas used solve problems choose top approach youre ready heavy stuff get problem solving quickly possible good luck thanks reading name per im cofounder scrimba better way teach learn code youve read far id recommend check demo quick cheer standing ovation clap show much enjoyed story cofounder scrimba nextgeneration platform teaching learning code httpsscrimbacom publication improving technical skills,en,"['Coursera', 'Hacker News', 'Sentdex', 'Udacity', 'Bank of England', 'algorithm', 'Kaggle', 'Natural Language Processing', 'NLP']"
43,Ahmed El Deeb,593,What to do with “small” data? – Rants on Machine Learning – Medium,"By Ahmed El Deeb
Many technology companies now have teams of smart data-scientists, versed in big-data infrastructure tools and machine learning algorithms, but every now and then, a data set with very few data points turns up and none of these algorithms seem to be working properly anymore. What the hell is happening? What can you do about it?
Most data science, relevance, and machine learning activities in technology companies have been focused around “Big Data” and scenarios with huge data sets. Sets where the rows represent documents, users, files, queries, songs, images, etc. Things that are in the thousands, hundreds of thousands, millions or even billions. The infrastructure, tools, and algorithms to deal with these kinds of data sets have been evolving very quickly and improving continuously during the last decade or so. And most data scientists and machine learning practitioners have gained experience is such situations, have grown accustomed to the appropriate algorithms, and gained good intuitions about the usual trade-offs (bias-variance, flexibility-stability, hand-crafted features vs. feature learning, etc.). But small data sets still arise in the wild every now and then, and often, they are trickier to handle, require a different set of algorithms and a different set of skills. Small data sets arise is several situations:
Problems of small-data are numerous, but mainly revolve around high variance:
1- Hire a statistician
I’m not kidding! Statisticians are the original data scientists. The field of statistics was developed when data was much harder to come by, and as such was very aware of small-sample problems. Statistical tests, parametric models, bootstrapping, and other useful mathematical tools are the domain of classical statistics, not modern machine learning. Lacking a good general-purpose statistician, get a marine-biologist, a zoologist, a psychologist, or anyone who was trained in a domain that deals with small sample experiments. The closer to your domain the better. If you don’t want to hire a statistician full time on your team, make it a temporary consultation. But hiring a classically trained statistician could be a very good investment.
2- Stick to simple models
More precisely: stick to a limited set of hypotheses. One way to look at predictive modeling is as a search problem. From an initial set of possible models, which is the most appropriate model to fit our data? In a way, each data point we use for fitting down-votes all models that make it unlikely, or up-vote models that agree with it. When you have heaps of data, you can afford to explore huge sets of models/hypotheses effectively and end up with one that is suitable. When you don’t have so many data points to begin with, you need to start from a fairly small set of possible hypotheses (e.g. the set of all linear models with 3 non-zero weights, the set of decision trees with depth <= 4, the set of histograms with 10 equally-spaced bins). This means that you rule out complex hypotheses like those that deal with non-linearity or feature interactions. This also means that you can’t afford to fit models with too many degrees of freedom (too many weights or parameters). Whenever appropriate, use strong assumptions (e.g. no negative weights, no interaction between features, specific distributions, etc.) to restrict the space of possible hypotheses.
3- Pool data when possible
Are you building a personalized spam filter? Try building it on top of a universal model trained for all users. Are you modeling GDP for a specific country? Try fitting your models on GDP for all countries for which you can get data, maybe using importance sampling to emphasize the country you’re interested in. Are you trying to predict the eruptions of a specific volcano? ... you get the idea.
4- Limit Experimentation
Don’t over-use your validation set. If you try too many different techniques, and use a hold-out set to compare between them, be aware of the statistical power of the results you are getting, and be aware that the performance you are getting on this set is not a good estimator for out of sample performance.
5- Do clean up your data
With small data sets, noise and outliers are especially troublesome. Cleaning up your data could be crucial here to get sensible models. Alternatively you can restrict your modeling to techniques especially designed to be robust to outliers. (e.g. Quantile Regression)
6- Do perform feature selection
I am not a big fan of explicit feature selection. I typically go for regularization and model averaging (next two points) to avoid over-fitting. But if the data is truly limiting, sometimes explicit feature selection is essential. Wherever possible, use domain expertise to do feature selection or elimination, as brute force approaches (e.g. all subsets or greedy forward selection) are as likely to cause over-fitting as including all features.
7- Do use Regularization
Regularization is an almost-magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model. L1 regularization produces models with fewer non-zero parameters, effectively performing implicit feature selection, which could be desirable for explainability of performance in production, while L2 regularization produces models with more conservative (closer to zero) parameters and is effectively similar to having strong zero-centered priors for the parameters (in the Bayesian world). L2 is usually better for prediction accuracy than L1.
8- Do use Model Averaging
Model averaging has similar effects to regularization is that it reduces variance and enhances generalization, but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models. The downside here is that you end up with huge collections of models, which could be slow to evaluate or awkward to deploy to a production system. Two very reasonable forms of model averaging are Bagging and Bayesian model averaging.
9- Try Bayesian Modeling and Model Averaging
Again, not a favorite technique of mine, but Bayesian inference may be well suited for dealing with smaller data sets, especially if you can use domain expertise to construct sensible priors.
10- Prefer Confidence Intervals to Point Estimates
It is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself. For regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value 95% of the time or in the case of classification it could be just a matter of producing class probabilities. This becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others. Model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression, classification and density estimation. It is also useful to do that when evaluating your models. Producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusions.
This could be a somewhat long list of things to do or try, but they all revolve around three main themes: constrained modeling, smoothing and quantification of uncertainty.
Most figures used in this post were taken from the book “Pattern Recognition and Machine Learning” by Christopher Bishop.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Relevance engineer. Machine Learning practitioner and hobbyist. Former entrepreneur.
Rants about machine learning and its future
",ahmed el deeb many technology companies teams smart datascientists versed bigdata infrastructure tools machine learning algorithms every data set data points turns none algorithms seem working properly anymore hell happening data science relevance machine learning activities technology companies focused around big data scenarios huge data sets sets rows represent documents users files queries songs images etc things thousands hundreds thousands millions even billions infrastructure tools algorithms deal kinds data sets evolving quickly improving continuously last decade data scientists machine learning practitioners gained experience situations grown accustomed appropriate algorithms gained good intuitions usual tradeoffs biasvariance flexibilitystability handcrafted features vs feature learning etc small data sets still arise wild every often trickier handle require different set algorithms different set skills small data sets arise several situations problems smalldata numerous mainly revolve around high variance 1 hire statistician im kidding statisticians original data scientists field statistics developed data much harder come aware smallsample problems statistical tests parametric models bootstrapping useful mathematical tools domain classical statistics modern machine learning lacking good generalpurpose statistician get marinebiologist zoologist psychologist anyone trained domain deals small sample experiments closer domain better dont want hire statistician full time team make temporary consultation hiring classically trained statistician could good investment 2 stick simple models precisely stick limited set hypotheses one way look predictive modeling search problem initial set possible models appropriate model fit data way data point use fitting downvotes models make unlikely upvote models agree heaps data afford explore huge sets modelshypotheses effectively end one suitable dont many data points begin need start fairly small set possible hypotheses eg set linear models 3 nonzero weights set decision trees depth 4 set histograms 10 equallyspaced bins means rule complex hypotheses like deal nonlinearity feature interactions also means cant afford fit models many degrees freedom many weights parameters whenever appropriate use strong assumptions eg negative weights interaction features specific distributions etc restrict space possible hypotheses 3 pool data possible building personalized spam filter try building top universal model trained users modeling gdp specific country try fitting models gdp countries get data maybe using importance sampling emphasize country youre interested trying predict eruptions specific volcano get idea 4 limit experimentation dont overuse validation set try many different techniques use holdout set compare aware statistical power results getting aware performance getting set good estimator sample performance 5 clean data small data sets noise outliers especially troublesome cleaning data could crucial get sensible models alternatively restrict modeling techniques especially designed robust outliers eg quantile regression 6 perform feature selection big fan explicit feature selection typically go regularization model averaging next two points avoid overfitting data truly limiting sometimes explicit feature selection essential wherever possible use domain expertise feature selection elimination brute force approaches eg subsets greedy forward selection likely cause overfitting including features 7 use regularization regularization almostmagical solution constraints model fitting reduces effective degrees freedom without reducing actual number parameters model l1 regularization produces models fewer nonzero parameters effectively performing implicit feature selection could desirable explainability performance production l2 regularization produces models conservative closer zero parameters effectively similar strong zerocentered priors parameters bayesian world l2 usually better prediction accuracy l1 8 use model averaging model averaging similar effects regularization reduces variance enhances generalization generic technique used type models even heterogeneous sets models downside end huge collections models could slow evaluate awkward deploy production system two reasonable forms model averaging bagging bayesian model averaging 9 try bayesian modeling model averaging favorite technique mine bayesian inference may well suited dealing smaller data sets especially use domain expertise construct sensible priors 10 prefer confidence intervals point estimates usually good idea get estimate confidence prediction addition producing prediction regression analysis usually takes form predicting range values calibrated cover true value 95 time case classification could matter producing class probabilities becomes crucial small data sets becomes likely certain regions feature space less represented others model averaging referred previous two points allows us pretty easily generic way regression classification density estimation also useful evaluating models producing confidence intervals metrics using compare model performance likely save jumping many wrong conclusions could somewhat long list things try revolve around three main themes constrained modeling smoothing quantification uncertainty figures used post taken book pattern recognition machine learning christopher bishop quick cheer standing ovation clap show much enjoyed story relevance engineer machine learning practitioner hobbyist former entrepreneur rants machine learning future,en,['Prefer Confidence Intervals']
44,Matt Fogel,938,The 7 Best Data Science and Machine Learning Podcasts,"Data science and machine learning have long been interests of mine, but now that I’m working on Fuzzy.ai and trying to make AI and machine learning accessible to all developers, I need to keep on top of all the news in both fields.
My preferred way to do this is through listening to podcasts. I’ve listened to a bunch of machine learning and data science podcasts in the last few months, so I thought I’d share my favorites:
A great starting point on some of the basics of data science and machine learning. Every other week, they release a 10–15 minute episode where hosts, Kyle and Linda Polich give a short primer on topics like k-means clustering, natural language processing and decision tree learning, often using analogies related to their pet parrot, Yoshi. This is the only place where you’ll learn about k-means clustering via placement of parrot droppings.
Website | iTunes
Hosted by Katie Malone and Ben Jaffe of online education startup Udacity, this weekly podcast covers diverse topics in data science and machine learning: teaching specific concepts like Hidden Markov Models and how they apply to real-world problems and datasets. They make complex topics extremely accessible.
Website | iTunes
Each week, hosts Chris Albon and Jonathon Morgan, both experienced technologists and data scientists, talk about the latest news in data science over drinks. Listening to Partially Derivative is a great way to keep up on the latest data news.
Website | iTunes
This podcast features Ben Lorica, O’Reilly Media’s Chief Data Scientist speaking with other experts about timely big data and data science topics. It can often get quite technical, but the topics of discussion are always really interesting.
Website | iTunes
Data Stories is a little more focused on data visualization than data science, but there is often some interesting overlap between the topics. Every other week, Enrico Bertini and Moritz Stefaner cover diverse topics in data with their guests. Recent episodes about smart cities and Nicholas Felton’s annual reports are particularly interesting.
Website | iTunes
Billing itself as “A Gentle Introduction to Artificial Intelligence and Machine Learning”, this podcast can still get quite technical and complex, covering topics like: “How to Reason About Uncertain Events using Fuzzy Set Theory and Fuzzy Measure Theory” and “How to Represent Knowledge using Logical Rules”.
Website | iTunes
The newest podcasts on this list, with 8 episodes released as of this writing. Every other week, hosts Katherine Gorman and Ryan Adams speak with a guest about their work, and news stories related to machine learning.
Website | iTunes
Feel I’ve unfairly left a podcast off this list? Leave me a note to let me know.
Published in Startups, Wanderlust, and Life Hacking
-
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Cofounder of @fuzzyai. Helping developers make their software smarter, faster.
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
",data science machine learning long interests mine im working fuzzyai trying make ai machine learning accessible developers need keep top news fields preferred way listening podcasts ive listened bunch machine learning data science podcasts last months thought id share favorites great starting point basics data science machine learning every week release 1015 minute episode hosts kyle linda polich give short primer topics like kmeans clustering natural language processing decision tree learning often using analogies related pet parrot yoshi place youll learn kmeans clustering via placement parrot droppings website itunes hosted katie malone ben jaffe online education startup udacity weekly podcast covers diverse topics data science machine learning teaching specific concepts like hidden markov models apply realworld problems datasets make complex topics extremely accessible website itunes week hosts chris albon jonathon morgan experienced technologists data scientists talk latest news data science drinks listening partially derivative great way keep latest data news website itunes podcast features ben lorica oreilly medias chief data scientist speaking experts timely big data data science topics often get quite technical topics discussion always really interesting website itunes data stories little focused data visualization data science often interesting overlap topics every week enrico bertini moritz stefaner cover diverse topics data guests recent episodes smart cities nicholas feltons annual reports particularly interesting website itunes billing gentle introduction artificial intelligence machine learning podcast still get quite technical complex covering topics like reason uncertain events using fuzzy set theory fuzzy measure theory represent knowledge using logical rules website itunes newest podcasts list 8 episodes released writing every week hosts katherine gorman ryan adams speak guest work news stories related machine learning website itunes feel ive unfairly left podcast list leave note let know published startups wanderlust life hacking quick cheer standing ovation clap show much enjoyed story cofounder fuzzyai helping developers make software smarter faster mediums largest publication makers subscribe receive top stories httpsgooglzhclji,en,"['Udacity', 'Hidden Markov Models', 'O’Reilly Media', 'Medium']"
45,Illia Polosukhin,255,TensorFlow Tutorial— Part 1 – Illia Polosukhin – Medium,"UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn.
Google released a machine learning framework called TensorFlow and it’s taking the world by storm. 10k+ stars on Github, a lot of publicity and general excitement in between AI researchers.
Now, but how you to use it for something regular problem Data Scientist may have? (and if you are AI researcher — we will build up to interesting problems over time).
A reasonable question, why as a Data Scientist, who already has a number of tools in your toolbox (R, Scikit Learn, etc), you care about yet another framework?
The answer is two part:
Let’s start with simple example — take Titanic dataset from Kaggle.
First, make sure you have installed TensorFlow and Scikit Learn with few helpful libs, including Scikit Flow that is simplifying a lot of work with TensorFlow:
You can get dataset and the code from http://github.com/ilblackdragon/tf_examples
Quick look at the data (use iPython or iPython notebook for ease of interactive exploration):
Let’s test how we can predict Survived class, based on float variables in Scikit Learn:
We separate dataset into features and target, fill in N/A in the data with zeros and build a logistic regression. Predicting on the training data gives us some measure of accuracy (of cause it doesn’t properly evaluate the model quality and test dataset should be used, but for simplicity we will look at train only for now).
Now using tf.learn (previously Scikit Flow):
Congratulations, you just built your first TensorFlow model!
TF.Learn is a library that wraps a lot of new APIs by TensorFlow with nice and familiar Scikit Learn API.
TensorFlow is all about a building and executing graph. This is a very powerful concept, but it is also cumbersome to start with.
Looking under the hood of TF.Learn, we just used three parts:
Even as you get more familiar with TensorFlow, pieces of Scikit Flow will be useful (like graph_actions and layers and host of other ops and tools). See future posts for examples of handling categorical variables, text and images.
Part 2 — Deep Neural Networks, Custom TensorFlow models with Scikit Flow and Digit recognition with Convolutional Networks.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-Founder @ NEAR.AI — teaching machines to code. I’m tweeting as @ilblackdragon.
",upd april 20 2016 scikit flow merged tensorflow since version 08 called tensorflow learn tflearn google released machine learning framework called tensorflow taking world storm 10k stars github lot publicity general excitement ai researchers use something regular problem data scientist may ai researcher build interesting problems time reasonable question data scientist already number tools toolbox r scikit learn etc care yet another framework answer two part lets start simple example take titanic dataset kaggle first make sure installed tensorflow scikit learn helpful libs including scikit flow simplifying lot work tensorflow get dataset code httpgithubcomilblackdragontf_examples quick look data use ipython ipython notebook ease interactive exploration lets test predict survived class based float variables scikit learn separate dataset features target fill na data zeros build logistic regression predicting training data gives us measure accuracy cause doesnt properly evaluate model quality test dataset used simplicity look train using tflearn previously scikit flow congratulations built first tensorflow model tflearn library wraps lot new apis tensorflow nice familiar scikit learn api tensorflow building executing graph powerful concept also cumbersome start looking hood tflearn used three parts even get familiar tensorflow pieces scikit flow useful like graph_actions layers host ops tools see future posts examples handling categorical variables text images part 2 deep neural networks custom tensorflow models scikit flow digit recognition convolutional networks quick cheer standing ovation clap show much enjoyed story cofounder nearai teaching machines code im tweeting ilblackdragon,en,"['UPD', 'TensorFlow', 'Google', 'Github', 'Kaggle', 'http://github.com/ilblackdragon/tf_examples', 'N/A', 'Convolutional Networks']"
46,Ahmed El Deeb,283,The Unreasonable Effectiveness of Random Forests – Rants on Machine Learning – Medium,"It’s very common for machine learning practitioners to have favorite algorithms. It’s a bit irrational, since no algorithm strictly dominates in all applications, the performance of ML algorithms varies wildly depending on the application and the dimensionality of the dataset. And even for a given problem and a given dataset, any single model will likely be beaten by an ensemble of diverse models trained by diverse algorithms anyway. But people have favorites nevertheless. Some like SVMs for the elegance of their formulation or the quality of the available implementations, some like decision rules for their simplicity and interpretability, and some are crazy about neural networks for their flexibility.
My favorite out-of-the-box algorithm is (as you might have guessed) the Random Forest, and it’s the second modeling technique I typically try on any given data set (after a linear model).
This beautiful visualization from scikit-learn illustrates the modelling capacity of a decision forest:
Here’s a paper by Leo Breiman, the inventor of the algorithms describing random forests.
Here’s another amazing paper by Rich Caruana et al. evaluating several supervised learning algorithms on many different datasets.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Relevance engineer. Machine Learning practitioner and hobbyist. Former entrepreneur.
Rants about machine learning and its future
",common machine learning practitioners favorite algorithms bit irrational since algorithm strictly dominates applications performance ml algorithms varies wildly depending application dimensionality dataset even given problem given dataset single model likely beaten ensemble diverse models trained diverse algorithms anyway people favorites nevertheless like svms elegance formulation quality available implementations like decision rules simplicity interpretability crazy neural networks flexibility favorite outofthebox algorithm might guessed random forest second modeling technique typically try given data set linear model beautiful visualization scikitlearn illustrates modelling capacity decision forest heres paper leo breiman inventor algorithms describing random forests heres another amazing paper rich caruana et al evaluating several supervised learning algorithms many different datasets quick cheer standing ovation clap show much enjoyed story relevance engineer machine learning practitioner hobbyist former entrepreneur rants machine learning future,en,['the Random Forest']
47,Christophe Bourguignat,157,6 Tricks I Learned From The OTTO Kaggle Challenge – Christophe Bourguignat – Medium,"Here are a few things I learned from the OTTO Group Kaggle competition. I had the chance to team up with great Kaggle Master Xavier Conort, and the french community as a whole has been very active.
Teaming with Xavier has been the opportunity to practice some ensembling technics.
We heavily used stacking. We added to an initial set of 93 features, new features being the predictions of N different classifiers (Random Forest, GBM, Neural Networks, ...). And then retrained P classifiers over the 93 + N features. And finally made a weighted average of the P outputs.
We tested two tricks :
This is one of the great functionalities of the last scikit-learn version (0.16). It allows to rescale the classifier predictions by taking observations predicted within a segments (e.g. 0.3–04), and comparing to the actual truth ratio of these observation (e.g. 0.23, with means that a rescaling is needed).
Here is a mini notebook explaining how to use calibration, and demonstrating how well it worked on the OTTO challenge data.
At the beginning of the competition, it appeared quickly that — once again — Gradient Boosting Trees was one of the best performing algorithm, provided that you find the right hyper parameters.
On the scikit-learn implementation, most important hyper parameters are learning_rate (the shrinkage parameter), n_estimators (the number of boosting stages), and max_depth (limits the number of nodes in the tree, the best value depends on the interaction of the input variables). min_samples_split, and min_samples_leaf can also be a way to control depth of the trees for optimal performance.
I also discovered that two other parameters were crucial for this competition. I must admit I never paid attention on it before this challenge : namely subsample (the fraction of samples to be used for fitting the individual base learners), and max_features (the number of features to consider when looking for the best split).
The problem was to find a way to quickly find the best hyperparameters combination. I first discovered GridSearchCV, that makes an exhaustive search over specified parameter ranges. As always with scikit-learn, it has a convenient programming interface, handling for example smoothly cross-validation and parallel distributing of search.
However, the number of parameters to tune, and their range, was too large to discover the best ones in the acceptable time frame I had in mind (typically while sleeping, i.e 7 to 10 hours). I had to fall back to an other option : I then used RandomizedSearchCV, that appeared in 0.14 version. With this method, search is done randomly on a subspace of parameters. It gives generally very good results, as described in this paper, and I was able to find a suitable parameter set within a few hours.
Note that some competitors, like french kaggler Amine, used Hyperopt for hyperparameters optimization.
XGBoost is a Gradient Boosting implementation heavily used by kagglers, and I now understand why. I never used it before, but it was a hot topic discussed in the forum. I decided to have a look at it, even if its main interface is in R (but there is a Python API, that I didn’t use yet). XGBoost is much faster than scikit-learn, and gave better prediction. It will remain for sure part of my toolblox.
Someone posted on the forum :
He was right. It has been for me the opportunity to play with neural networks for the first time.
Several implementations have been used by the competitors : H2O, Keras, cxxnet, ... I personally used Lasagne. Main challenges was to fine tune the number of layers, number of neurons, dropout and learning rate. Here is a notebook on what I learned.
One of the secret of the competition was to run several times the same algorithm, with random selection of observations and features, and take the average of the output.
To do that easily, I discovered the scikit-learn BaggingClassifier meta-estimator. It hides the tedious complexity of looping over model fits, random subsets selection, and averaging — and exposes easy fit() / predict_proba() entry points.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data enthusiast #BigData #DataScience #MachineLearning #FrenchData #Kaggle
",things learned otto group kaggle competition chance team great kaggle master xavier conort french community whole active teaming xavier opportunity practice ensembling technics heavily used stacking added initial set 93 features new features predictions n different classifiers random forest gbm neural networks retrained p classifiers 93 n features finally made weighted average p outputs tested two tricks one great functionalities last scikitlearn version 016 allows rescale classifier predictions taking observations predicted within segments eg 0304 comparing actual truth ratio observation eg 023 means rescaling needed mini notebook explaining use calibration demonstrating well worked otto challenge data beginning competition appeared quickly gradient boosting trees one best performing algorithm provided find right hyper parameters scikitlearn implementation important hyper parameters learning_rate shrinkage parameter n_estimators number boosting stages max_depth limits number nodes tree best value depends interaction input variables min_samples_split min_samples_leaf also way control depth trees optimal performance also discovered two parameters crucial competition must admit never paid attention challenge namely subsample fraction samples used fitting individual base learners max_features number features consider looking best split problem find way quickly find best hyperparameters combination first discovered gridsearchcv makes exhaustive search specified parameter ranges always scikitlearn convenient programming interface handling example smoothly crossvalidation parallel distributing search however number parameters tune range large discover best ones acceptable time frame mind typically sleeping ie 7 10 hours fall back option used randomizedsearchcv appeared 014 version method search done randomly subspace parameters gives generally good results described paper able find suitable parameter set within hours note competitors like french kaggler amine used hyperopt hyperparameters optimization xgboost gradient boosting implementation heavily used kagglers understand never used hot topic discussed forum decided look even main interface r python api didnt use yet xgboost much faster scikitlearn gave better prediction remain sure part toolblox someone posted forum right opportunity play neural networks first time several implementations used competitors h2o keras cxxnet personally used lasagne main challenges fine tune number layers number neurons dropout learning rate notebook learned one secret competition run several times algorithm random selection observations features take average output easily discovered scikitlearn baggingclassifier metaestimator hides tedious complexity looping model fits random subsets selection averaging exposes easy fit predict_proba entry points quick cheer standing ovation clap show much enjoyed story data enthusiast bigdata datascience machinelearning frenchdata kaggle,en,"['OTTO Group Kaggle', 'Xavier', 'N', 'Random Forest', 'GBM', 'Neural Networks', 'OTTO', 'algorithm', 'max_depth', 'the fraction of samples', 'max_features', 'XGBoost', 'Python API', 'BaggingClassifier']"
48,samim,301,Generating Stories about Images – samim – Medium,"Stories are a fundamental human tool that we use to communicate thought. Creating a stories about a image is a difficult task that many struggle with. New machine-learning experiments are enabling us to generate stories based on the content of images. This experiment explores how to generate little romantic stories about images (incl. guest star Taylor Swift).
neural-storyteller is a recently published experiment by Ryan Kiros (University of Toronto). It combines recurrent neural networks (RNN), skip-thoughts vectors and other techniques to generate little story about images. Neural-storyteller’s outputs are creative and often comedic. It is open-source.
This experiment started by running 5000 randomly selected web-images through neural-storyteller and experimenting with hyper-parameters. neural-storyteller comes with 2 pre-trained models: One trained on 14 million passages of romance novels, the other trained on Taylor Swift Lyrics. Inputs and outputs were manually filtered and recombined into two videos.
Using Romantic Novel Model. Voices generated with a Text-to-Speech.
Using Taylor Swift Model. Combined with a well known Swift instrumental.
neural-storyteller gives us a fascinating glimpse into the future of storytelling. Even though these technologies are not fully mature yet, the art of storytelling is bound to change. In the near future, authors will be training custom models, combining styles across genres and generating text with images & sounds. Exploring this exiting new medium is rewarding!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Designer & Code Magician. Working at the intersection of HCI, Machine Learning & Creativity. Building tools for Enlightenment. Narrative Engineering.
",stories fundamental human tool use communicate thought creating stories image difficult task many struggle new machinelearning experiments enabling us generate stories based content images experiment explores generate little romantic stories images incl guest star taylor swift neuralstoryteller recently published experiment ryan kiros university toronto combines recurrent neural networks rnn skipthoughts vectors techniques generate little story images neuralstorytellers outputs creative often comedic opensource experiment started running 5000 randomly selected webimages neuralstoryteller experimenting hyperparameters neuralstoryteller comes 2 pretrained models one trained 14 million passages romance novels trained taylor swift lyrics inputs outputs manually filtered recombined two videos using romantic novel model voices generated texttospeech using taylor swift model combined well known swift instrumental neuralstoryteller gives us fascinating glimpse future storytelling even though technologies fully mature yet art storytelling bound change near future authors training custom models combining styles across genres generating text images sounds exploring exiting new medium rewarding quick cheer standing ovation clap show much enjoyed story designer code magician working intersection hci machine learning creativity building tools enlightenment narrative engineering,en,"['University of Toronto', 'Taylor Swift Lyrics', 'HCI', 'Machine Learning & Creativity', 'Narrative Engineering']"
49,AirbnbEng,517,How Airbnb uses Machine Learning to Detect Host Preferences,"By Bar Ifrach
At Airbnb we seek to match people who are looking for accommodation — guests — with those looking to rent out their place — hosts. Guests reach out to hosts whose listings they wish to stay in, however a match succeeds only if the host also wants to accommodate the guest.
I first heard about Airbnb in 2012 from a friend. He offered his nice apartment on the site when he traveled to see his family during our vacations from grad school. His main goal was to fit as many booked nights as possible into the 1–2 weeks when he was away. My friend would accept or reject requests depending on whether or not the request would help him to maximize his occupancy.
About two years later, I joined Airbnb as a Data Scientist. I remembered my friend’s behavior and was curious to discover what affects hosts’ decisions to accept accommodation requests and how Airbnb could increase acceptances and matches on the platform.
What started as a small research project resulted in the development of a machine learning model that learns our hosts’ preferences for accommodation requests based on their past behavior. For each search query that a guest enters on Airbnb’s search engine, our model computes the likelihood that relevant hosts will want to accommodate the guest’s request. Then, we surface likely matches more prominently in the search results. In our A/B testing the model showed about a 3.75% increase in booking conversion, resulting in many more matches on Airbnb. In this blog post I outline the process that brought us to this model.
I kicked off my research into hosts’ acceptances by checking if other hosts maximized their occupancy like my friend. Every accommodation request falls in a sequence or in a window of available days in the calendar, such as on April 5–10 in the calendar shown below. The gray days surrounding the window are either blocked by the host or already booked. If accepted and booked, a request may leave the host with a sub-window before the check-in date (check-in gap — April 5–7) and/or a sub-window after the check-out (check-out gap — April 10).
A host looking to have a high occupancy will try to avoid such gaps. Indeed, when I plotted hosts’ tendency to accept over the sum of the check-in gap and the check-out gap (3+1= 4 in the example above), as in the next plot, I found the effect that I expected to see: hosts were more likely to accept requests that fit well in their calendar and minimize gap days.
But do all hosts try to maximize occupancy and prefer stays with short gaps? Perhaps some hosts are not interested in maximizing their occupancy and would rather host occasionally. And maybe hosts in big markets, like my friend, are different from hosts in smaller markets.
Indeed, when I looked at listings from big and small markets separately, I found that they behaved quite differently. Hosts in big markets care a lot about their occupancy — a request with no gaps is almost 6% likelier to be accepted than one with 7 gap nights. For small markets I found the opposite effect; hosts prefer to have a small number of nights between requests. So, hosts in different markets have different preferences, but it seems likely that even within a market hosts may prefer different stays.
A similar story revealed itself when I looked at hosts’ tendency to accept based on other characteristics of the accommodation request. For example, on average Airbnb hosts prefer accommodation requests that are at least a week in advance over last minute requests. But perhaps some hosts prefer short notice?
The plot below looks at the dispersion of hosts’ preferences for last minute stays (less than 7 days) versus far in advance stays (more than 7 days). Indeed, the dispersion in preferences reveals that some hosts like last minute stays better than far in advance stays — those in the bottom right — even though on average hosts prefer longer notice. I found similar dispersion in hosts’ tendency to accept other trip characteristics like the number of guests, whether it is a weekend trip etc.
All these findings pointed to the same conclusion: if we could promote in our search results hosts who would be more likely to accept an accommodation request resulting from that search query, we would expect to see happier guests and hosts and more matches that turned into fun vacations (or productive business trips).
In other words, we could personalize our search results, but not in the way you might expect. Typically personalized search results promote results that would fit the unique preferences of the searcher — the guest. At a two-sided marketplace like Airbnb, we also wanted to personalize search by the preference of the hosts whose listings would appear in the search results.
Encouraged by my findings, I joined forces with another data scientist and a software engineer to create a personalized search signal. We set out to associate hosts’ prior acceptance and decline decisions by the following characteristics of the trip: check-in date, check-out date and number of guests. By adding host preferences to our existing ranking model capturing guest preferences, we hoped to enable more and better matches.
At first glance, this seems like a perfect case for collaborative filtering — we have users (hosts) and items (trips) and we want to understand the preference for those items by combining historical ratings (accept/decline) with statistical learning from similar hosts. However, the application does not fully fit in the collaborative filtering framework for two reasons.
With these points in mind, we decided to massage the problem into something resembling collaborative filtering. We used the multiplicity of responses for the same trip to reduce the noise coming from the latent factors in the guest-host interaction. To do so, we considered hosts’ average response to a certain trip characteristic in isolation. Instead of looking at the combination of trip length, size of guest party, size of calendar gap and so on, we looked at each of these trip characteristics by itself.
With this coarser structure of preferences we were able to resolve some of the noise in our data as well as the potentially conflicting labels for the same trip. We used the mean acceptance rate for each trip characteristic as a proxy for preference. Still our data-set was relatively sparse. On average, for each trip characteristic we could not determine the preference for about 26% of hosts, because they never received an accommodation request that met those trip characteristics. As a method of imputation, we smoothed the preference using a weight function that, for each trip characteristic, averages the median preference of hosts in the region with the host’s preference. The weight on the median preference is 1 when the host has no data points and goes to 0 monotonically the more data points the host has.
Using these newly defined preferences we created predictions for host acceptances using a L-2 regularized logistic regression. Essentially, we combine the preferences for different trip characteristics into a single prediction for the probability of acceptance. The weight the preference of each trip characteristic has on the acceptance decision is the coefficient that comes out of the logistic regression. To improve the prediction, we include a few more geographic and host specific features in the logistic regression.
This flow chart summarizes the modeling technique.
We ran this model on segments of hosts on our cluster using a user-generated-function (UDF) on Hive. The UDF is written in Python; its inputs are accommodation requests, hosts’ response to them and a few other host features. Depending on the flag passed to it, the UDF either builds the preferences for the different trip characteristics or trains the logistic regression model using scikit-learn.
Our main off-line evaluation metric for the model was mean squared error (MSE), which is more appropriate in a setting when we care about the predicted probability more than about classification. In our off-line evaluation of the model we were able to get a 10% decrease in MSE over our previous model that captured host acceptance probability. This was a promising result. But, we still had to test the performance of the model live on our site.
To test the online performance of the model, we launched an experiment that used the predicted probability of host acceptance as a significant weight in our ranking algorithm that also includes many other features that capture guests’ preferences. Every time a guest in the treatment group entered a search query, our model predicted the probability of acceptance for all relevant hosts and influenced the order in which listings were presented to the guest, ranking likelier matches higher.
We evaluated the experiment by looking at multiple metrics, but the most important one was the likelihood that a guest requesting accommodation would get a booking (booking conversion). We found a 3.75% lift in our booking conversion and a significant increase in the number of successful matches between guests and hosts.
After concluding the initial experiment, we made a few more optimizations that improved conversion by approximately another 1% and then launched the experiment to 100% of users. This was an exciting outcome for our first full-fledged personalization search signal and a sizable contributor to our success.
First, this project taught us that in a two sided marketplace personalization can be effective on the buyer as well as the seller side.
Second, the project taught us that sometimes you have to roll up your sleeves and build a machine learning model tailored for your own application. In this case, the application did not quite fit in the collaborative filtering and a multilevel model with host fixed-effect was too computationally demanding and not suited for a sparse data-set. While building our own model took more time, it was a fun learning experience.
Finally, this project would not have succeeded without the fantastic work of Spencer de Mars and Lukasz Dziurzynski.
Originally published at nerds.airbnb.com on April 14, 2015.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
",bar ifrach airbnb seek match people looking accommodation guests looking rent place hosts guests reach hosts whose listings wish stay however match succeeds host also wants accommodate guest first heard airbnb 2012 friend offered nice apartment site traveled see family vacations grad school main goal fit many booked nights possible 12 weeks away friend would accept reject requests depending whether request would help maximize occupancy two years later joined airbnb data scientist remembered friends behavior curious discover affects hosts decisions accept accommodation requests airbnb could increase acceptances matches platform started small research project resulted development machine learning model learns hosts preferences accommodation requests based past behavior search query guest enters airbnbs search engine model computes likelihood relevant hosts want accommodate guests request surface likely matches prominently search results ab testing model showed 375 increase booking conversion resulting many matches airbnb blog post outline process brought us model kicked research hosts acceptances checking hosts maximized occupancy like friend every accommodation request falls sequence window available days calendar april 510 calendar shown gray days surrounding window either blocked host already booked accepted booked request may leave host subwindow checkin date checkin gap april 57 andor subwindow checkout checkout gap april 10 host looking high occupancy try avoid gaps indeed plotted hosts tendency accept sum checkin gap checkout gap 31 4 example next plot found effect expected see hosts likely accept requests fit well calendar minimize gap days hosts try maximize occupancy prefer stays short gaps perhaps hosts interested maximizing occupancy would rather host occasionally maybe hosts big markets like friend different hosts smaller markets indeed looked listings big small markets separately found behaved quite differently hosts big markets care lot occupancy request gaps almost 6 likelier accepted one 7 gap nights small markets found opposite effect hosts prefer small number nights requests hosts different markets different preferences seems likely even within market hosts may prefer different stays similar story revealed looked hosts tendency accept based characteristics accommodation request example average airbnb hosts prefer accommodation requests least week advance last minute requests perhaps hosts prefer short notice plot looks dispersion hosts preferences last minute stays less 7 days versus far advance stays 7 days indeed dispersion preferences reveals hosts like last minute stays better far advance stays bottom right even though average hosts prefer longer notice found similar dispersion hosts tendency accept trip characteristics like number guests whether weekend trip etc findings pointed conclusion could promote search results hosts would likely accept accommodation request resulting search query would expect see happier guests hosts matches turned fun vacations productive business trips words could personalize search results way might expect typically personalized search results promote results would fit unique preferences searcher guest twosided marketplace like airbnb also wanted personalize search preference hosts whose listings would appear search results encouraged findings joined forces another data scientist software engineer create personalized search signal set associate hosts prior acceptance decline decisions following characteristics trip checkin date checkout date number guests adding host preferences existing ranking model capturing guest preferences hoped enable better matches first glance seems like perfect case collaborative filtering users hosts items trips want understand preference items combining historical ratings acceptdecline statistical learning similar hosts however application fully fit collaborative filtering framework two reasons points mind decided massage problem something resembling collaborative filtering used multiplicity responses trip reduce noise coming latent factors guesthost interaction considered hosts average response certain trip characteristic isolation instead looking combination trip length size guest party size calendar gap looked trip characteristics coarser structure preferences able resolve noise data well potentially conflicting labels trip used mean acceptance rate trip characteristic proxy preference still dataset relatively sparse average trip characteristic could determine preference 26 hosts never received accommodation request met trip characteristics method imputation smoothed preference using weight function trip characteristic averages median preference hosts region hosts preference weight median preference 1 host data points goes 0 monotonically data points host using newly defined preferences created predictions host acceptances using l2 regularized logistic regression essentially combine preferences different trip characteristics single prediction probability acceptance weight preference trip characteristic acceptance decision coefficient comes logistic regression improve prediction include geographic host specific features logistic regression flow chart summarizes modeling technique ran model segments hosts cluster using usergeneratedfunction udf hive udf written python inputs accommodation requests hosts response host features depending flag passed udf either builds preferences different trip characteristics trains logistic regression model using scikitlearn main offline evaluation metric model mean squared error mse appropriate setting care predicted probability classification offline evaluation model able get 10 decrease mse previous model captured host acceptance probability promising result still test performance model live site test online performance model launched experiment used predicted probability host acceptance significant weight ranking algorithm also includes many features capture guests preferences every time guest treatment group entered search query model predicted probability acceptance relevant hosts influenced order listings presented guest ranking likelier matches higher evaluated experiment looking multiple metrics important one likelihood guest requesting accommodation would get booking booking conversion found 375 lift booking conversion significant increase number successful matches guests hosts concluding initial experiment made optimizations improved conversion approximately another 1 launched experiment 100 users exciting outcome first fullfledged personalization search signal sizable contributor success first project taught us two sided marketplace personalization effective buyer well seller side second project taught us sometimes roll sleeves build machine learning model tailored application case application quite fit collaborative filtering multilevel model host fixedeffect computationally demanding suited sparse dataset building model took time fun learning experience finally project would succeeded without fantastic work spencer de mars lukasz dziurzynski originally published nerdsairbnbcom april 14 2015 quick cheer standing ovation clap show much enjoyed story creative engineers data scientists building world belong anywhere httpairbnbio creative engineers data scientists building world belong anywhere httpairbnbio,en,"['UDF', 'Hive', 'MSE', 'algorithm', 'Spencer de Mars', 'nerds.airbnb.com']"
50,Arthur Juliani,9000,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1–3). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).
Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.
For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.
In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.
We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:
(Thanks to Praneet D for finding the optimal hyperparameters for this approach)
Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.
In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.
Below is the Tensorflow walkthrough of implementing our simple Q-Network:
While the network learns to solve the FrozenLake problem, it turns out it doesn’t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!
If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!
If you’d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on Twitter @awjliani.
More from my Simple Reinforcement Learning with Tensorflow series:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Deep Learning @Unity3D & Cognitive Neuroscience PhD student.
Exploring frontier technology through the lens of artificial intelligence, data science, and the shape of things to come
",tutorial reinforcement learning series going exploring family rl algorithms called qlearning algorithms little different policybased algorithms looked following tutorials parts 13 instead starting complex unwieldy deep neural network begin implementing simple lookuptable version algorithm show implement neuralnetwork equivalent using tensorflow given going back basics may best think part0 series hopefully give intuition really happening qlearning build going forward eventually combine policy gradient qlearning approaches build stateoftheart rl agents interested policy networks already grasp qlearning feel free start tutorial series instead unlike policy gradient methods attempt learn functions directly map observation action qlearning attempts learn value given state taking specific action approaches ultimately allow us take intelligent actions given situation means getting action differ significantly may heard deepqnetworks play atari games really larger complex implementations qlearning algorithm going discuss tutorial going attempting solve frozenlake environment openai gym unfamiliar openai gym provides easy way people experiment learning agents array provided toy games frozenlake environment consists 4x4 grid blocks one either start block goal block safe frozen block dangerous hole objective agent learn navigate start goal without moving onto hole given time agent choose move either left right catch wind occasionally blows agent onto space didnt choose perfect performance every time impossible learning avoid holes reach goal certainly still doable reward every step 0 except entering goal provides reward 1 thus need algorithm learns longterm expected rewards exactly qlearning designed provide simplest implementation qlearning table values every state row action column possible environment within cell table learn value good take given action within given state case frozenlake environment 16 possible states one block 4 possible actions four directions movement giving us 16x4 table qvalues start initializing table uniform zeros observe rewards obtain various actions update table accordingly make updates qtable using something called bellman equation states expected longterm reward given action equal immediate reward current action combined expected reward best future action taken following state way reuse qtable estimating update table future actions equation form rule looks like says qvalue given state action represent current reward r plus maximum discounted future reward expected according table next state would end discount variable allows us decide important possible future rewards compared present reward updating way table slowly begins obtain accurate measures expected future reward given action given state python walkthrough qtable algorithm implemented frozenlake environment thanks praneet finding optimal hyperparameters approach may thinking tables great dont really scale easy 16x4 table simple grid world number possible states modern game realworld environment nearly infinitely larger interesting problems tables simply dont work instead need way take description state produce qvalues actions without table neural networks come acting function approximator take number possible states represented vector learn map qvalues case frozenlake example using onelayer network takes state encoded onehot vector 1x16 produces vector 4 qvalues one action simple network acts kind like glorified table network weights serving old cells key difference easily expand tensorflow network added layers activation functions different input types whereas impossible regular table method updating little different well instead directly updating table network using backpropagation loss function loss function sumofsquares loss difference current predicted qvalues target value computed gradients passed network case qtarget chosen action equivalent qvalue computed equation 1 tensorflow walkthrough implementing simple qnetwork network learns solve frozenlake problem turns doesnt quite efficiently qtable neural networks allow greater flexibility cost stability comes qlearning number possible extensions simple qnetwork allow greater performance robust learning two tricks particular referred experience replay freezing target networks improvements tweaks key getting atariplaying deep qnetworks exploring additions future info theory behind qlearning see great post tambet matiisen hope tutorial helpful curious implement simple qlearning algorithms post valuable please consider donating help support future tutorials articles implementations contribution greatly appreciated youd like follow work deep learning ai cognitive science follow medium arthur juliani twitter awjliani simple reinforcement learning tensorflow series quick cheer standing ovation clap show much enjoyed story deep learning unity3d cognitive neuroscience phd student exploring frontier technology lens artificial intelligence data science shape things come,en,"['FrozenLake', 'Tensorflow', 'Q-Network', 'Experience Replay', 'Freezing Target Networks', 'Atari', 'Twitter @awjliani', 'Deep Learning @Unity3D & Cognitive Neuroscience']"
51,Adam Geitgey,6800,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话 , 한국어, Tiếng Việt or Русский.
Speech recognition is invading our lives. It’s built into our phones, our game consoles and our smart watches. It’s even automating our homes. For just $50, you can get an Amazon Echo Dot — a magic box that allows you to order pizza, get a weather report or even buy trash bags — just by speaking out loud:
The Echo Dot has been so popular this holiday season that Amazon can’t seem to keep them in stock!
But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments.
Andrew Ng has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between annoyingly unreliable and incredibly useful. Thanks to Deep Learning, we’re finally cresting that peak.
Let’s learn how to do speech recognition with deep learning!
If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text:
That’s the holy grail of speech recognition with deep learning, but we aren’t quite there yet (at least at the time that I wrote this — I bet that we will be in a couple of years).
The big problem is that speech varies in speed. One person might say “hello!” very quickly and another person might say “heeeelllllllllllllooooo!” very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text — “hello!” Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard.
To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let’s see how it works!
The first step in speech recognition is obvious — we need to feed sound waves into a computer.
In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition:
But sound is transmitted as waves. How do we turn sound waves into numbers? Let’s use this sound clip of me saying “Hello”:
Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let’s zoom in on one tiny part of the sound wave and take a look:
To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points:
This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is.
“CD Quality” audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech.
Lets sample our “Hello” sound wave 16,000 times per second. Here’s the first 100 samples:
You might be thinking that sampling is only creating a rough approximation of the original sound wave because it’s only taking occasional readings. There’s gaps in between our readings so we must be losing data, right?
But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples — as long as we sample at least twice as fast as the highest frequency we want to record.
I mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality. It doesn’t.
</end rant>
We now have an array of numbers with each number representing the sound wave’s amplitude at 1/16,000th of a second intervals.
We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data.
Let’s start by grouping our sampled audio into 20-millisecond-long chunks. Here’s our first 20 milliseconds of audio (i.e., our first 320 samples):
Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time:
This recording is only 1/50th of a second long. But even this short recording is a complex mish-mash of different frequencies of sound. There’s some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech.
To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it’s component parts. We’ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet.
Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes— C, E and G — all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea.
We do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one.
The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip:
But this is a lot easier to see when you draw this as a chart:
If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk):
A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network.
Now that we have our audio in a format that’s easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the letter that corresponds the sound currently being spoken.
We’ll use a recurrent neural network — that is, a neural network that has a memory that influences future predictions. That’s because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said “HEL” so far, it’s very likely we will say “LO” next to finish out the word “Hello”. It’s much less likely that we will say something unpronounceable next like “XYZ”. So having that memory of previous predictions helps the neural network make more accurate predictions going forward.
After we run our entire audio clip through the neural network (one chunk at a time), we’ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here’s what that mapping looks like for me saying “Hello”:
Our neural net is predicting that one likely thing I said was “HHHEE_LL_LLLOOO”. But it also thinks that it was possible that I said “HHHUU_LL_LLLOOO” or even “AAAUU_LL_LLLOOO”.
We have some steps we follow to clean up this output. First, we’ll replace any repeated characters a single character:
Then we’ll remove any blanks:
That leaves us with three possible transcriptions — “Hello”, “Hullo” and “Aullo”. If you say them out loud, all of these sound similar to “Hello”. Because it’s predicting one character at a time, the neural network will come up with these very sounded-out transcriptions. For example if you say “He would not go”, it might give one possible transcription as “He wud net go”.
The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic.
Of our possible transcriptions “Hello”, “Hullo” and “Aullo”, obviously “Hello” will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we’ll pick “Hello” as our final transcription instead of the others. Done!
You might be thinking “But what if someone says ‘Hullo’? It’s a valid word. Maybe ‘Hello’ is the wrong transcription!”
Of course it is possible that someone actually said “Hullo” instead of “Hello”. But a speech recognition system like this (trained on American English) will basically never produce “Hullo” as the transcription. It’s just such an unlikely thing for a user to say compared to “Hello” that it will always think you are saying “Hello” no matter how much you emphasize the ‘U’ sound.
Try it out! If your phone is set to American English, try to get your phone’s digital assistant to recognize the world “Hullo.” You can’t! It refuses! It will always understand it as “Hello.”
Not recognizing “Hullo” is a reasonable behavior, but sometimes you’ll find annoying cases where your phone just refuses to understand something valid you are saying. That’s why these speech recognition models are always being retrained with more data to fix these edge cases.
One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop’s video card... Right?
That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them.
Here’s another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise!
To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a lot of training data — far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can’t skimp on this. No one wants a voice recognition system that works 80% of the time.
For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That’s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That’s the whole game!
Don’t believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you’ve ever said into it:
So if you are looking for a start-up idea, I wouldn’t recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead.
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun! Part 7!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
",update article part series check full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 part 8 also read article tieng viet speech recognition invading lives built phones game consoles smart watches even automating homes 50 get amazon echo dot magic box allows order pizza get weather report even buy trash bags speaking loud echo dot popular holiday season amazon cant seem keep stock speech recognition around decades hitting mainstream reason deep learning finally made speech recognition accurate enough useful outside carefully controlled environments andrew ng long predicted speech recognition goes 95 accurate 99 accurate become primary way interact computers idea 4 accuracy gap difference annoyingly unreliable incredibly useful thanks deep learning finally cresting peak lets learn speech recognition deep learning know neural machine translation works might guess could simply feed sound recordings neural network train produce text thats holy grail speech recognition deep learning arent quite yet least time wrote bet couple years big problem speech varies speed one person might say hello quickly another person might say heeeelllllllllllllooooo slowly producing much longer sound file much data sound files recognized exactly text hello automatically aligning audio files various lengths fixedlength piece text turns pretty hard work around use special tricks extra precessing addition deep neural network lets see works first step speech recognition obvious need feed sound waves computer part 3 learned take image treat array numbers feed directly neural network image recognition sound transmitted waves turn sound waves numbers lets use sound clip saying hello sound waves onedimensional every moment time single value based height wave lets zoom one tiny part sound wave take look turn sound wave numbers record height wave equallyspaced points called sampling taking reading thousands times second recording number representing height sound wave point time thats basically uncompressed wav audio file cd quality audio sampled 441khz 44100 readings per second speech recognition sampling rate 16khz 16000 samples per second enough cover frequency range human speech lets sample hello sound wave 16000 times per second heres first 100 samples might thinking sampling creating rough approximation original sound wave taking occasional readings theres gaps readings must losing data right thanks nyquist theorem know use math perfectly reconstruct original sound wave spacedout samples long sample least twice fast highest frequency want record mention nearly everyone gets wrong assumes using higher sampling rates always leads better audio quality doesnt end rant array numbers number representing sound waves amplitude 116000th second intervals could feed numbers right neural network trying recognize speech patterns processing samples directly difficult instead make problem easier preprocessing audio data lets start grouping sampled audio 20millisecondlong chunks heres first 20 milliseconds audio ie first 320 samples plotting numbers simple line graph gives us rough approximation original sound wave 20 millisecond period time recording 150th second long even short recording complex mishmash different frequencies sound theres low sounds midrange sounds even highpitched sounds sprinkled taken together different frequencies mix together make complex sound human speech make data easier neural network process going break apart complex sound wave component parts well break lowpitched parts nextlowestpitchedparts adding much energy frequency bands low high create fingerprint sorts audio snippet imagine recording someone playing c major chord piano sound combination three musical notes c e g mixed together one complex sound want break apart complex sound individual notes discover c e g exact idea using mathematic operation called fourier transform breaks apart complex sound wave simple sound waves make individual sound waves add much energy contained one end result score important frequency range low pitch ie bass notes high pitch number represents much energy 50hz band 20 millisecond audio clip lot easier see draw chart repeat process every 20 millisecond chunk audio end spectrogram column lefttoright one 20ms chunk spectrogram cool actually see musical notes pitch patterns audio data neural network find patterns kind data easily raw sound waves data representation well actually feed neural network audio format thats easy process feed deep neural network input neural network 20 millisecond audio chunks little audio slice try figure letter corresponds sound currently spoken well use recurrent neural network neural network memory influences future predictions thats letter predicts affect likelihood next letter predict example said hel far likely say lo next finish word hello much less likely say something unpronounceable next like xyz memory previous predictions helps neural network make accurate predictions going forward run entire audio clip neural network one chunk time well end mapping audio chunk letters likely spoken chunk heres mapping looks like saying hello neural net predicting one likely thing said hhhee_ll_lllooo also thinks possible said hhhuu_ll_lllooo even aaauu_ll_lllooo steps follow clean output first well replace repeated characters single character well remove blanks leaves us three possible transcriptions hello hullo aullo say loud sound similar hello predicting one character time neural network come soundedout transcriptions example say would go might give one possible transcription wud net go trick combine pronunciationbased predictions likelihood scores based large database written text books news articles etc throw transcriptions seem least likely real keep transcription seems realistic possible transcriptions hello hullo aullo obviously hello appear frequently database text mention original audiobased training data thus probably correct well pick hello final transcription instead others done might thinking someone says hullo valid word maybe hello wrong transcription course possible someone actually said hullo instead hello speech recognition system like trained american english basically never produce hullo transcription unlikely thing user say compared hello always think saying hello matter much emphasize u sound try phone set american english try get phones digital assistant recognize world hullo cant refuses always understand hello recognizing hullo reasonable behavior sometimes youll find annoying cases phone refuses understand something valid saying thats speech recognition models always retrained data fix edge cases one coolest things machine learning simple sometimes seems get bunch data feed machine learning algorithm magically worldclass ai system running gaming laptops video card right sort true cases speech recognizing speech hard problem overcome almost limitless challenges bad quality microphones background noise reverb echo accent variations issues need present training data make sure neural network deal heres another example know speak loud room unconsciously raise pitch voice able talk noise humans problem understanding either way neural networks need trained handle special case need training data people yelling noise build voice recognition system performs level siri google alexa need lot training data far data likely get without hiring hundreds people record since users low tolerance poor quality voice recognition systems cant skimp one wants voice recognition system works 80 time company like google amazon hundreds thousands hours spoken audio recorded reallife situations gold thats single biggest thing separates worldclass speech recognition system hobby system whole point putting google siri every cell phone free selling 50 alexa units subscription fee get use much possible every single thing say one systems recorded forever used training data future versions speech recognition algorithms thats whole game dont believe android phone google click listen actual recordings saying every dumb thing youve ever said looking startup idea wouldnt recommend trying build speech recognition system compete google instead figure way get people give recordings talking hours data product instead liked article please consider signing machine learning fun email list ill email something new awesome share best way find write articles like also follow twitter ageitgey email directly find linkedin id love hear help team machine learning continue machine learning fun part 7 quick cheer standing ovation clap show much enjoyed story interested computers machine learning likes write,en,"['Amazon', 'Fourier', 'HEL', 'algorithm', 'Alexa', 'Google', 'Android']"
52,Adam Geitgey,5800,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话, Русский, 한국어, Tiếng Việt or Italiano.
We all know and love Google Translate, the website that can instantly translate between 100 different human languages as if by magic. It is even available on our phones and smartwatches:
The technology behind Google Translate is called Machine Translation. It has changed the world by allowing people to communicate when it wouldn’t otherwise be possible.
But we all know that high school students have been using Google Translate to... umm... assist with their Spanish homework for 15 years. Isn’t this old news?
It turns out that over the past two years, deep learning has totally rewritten our approach to machine translation. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world.
The technology behind this breakthrough is called sequence-to-sequence learning. It’s very powerful technique that be used to solve many kinds problems. After we see how it is used for translation, we’ll also learn how the exact same algorithm can be used to write AI chat bots and describe pictures.
Let’s go!
So how do we program a computer to translate human language?
The simplest approach is to replace every word in a sentence with the translated word in the target language. Here’s a simple example of translating from Spanish to English word-by-word:
This is easy to implement because all you need is a dictionary to look up each word’s translation. But the results are bad because it ignores grammar and context.
So the next thing you might do is start adding language-specific rules to improve the results. For example, you might translate common two-word phrases as a single group. And you might swap the order nouns and adjectives since they usually appear in reverse order in Spanish from how they appear in English:
That worked! If we just keep adding more rules until we can handle every part of grammar, our program should be able to translate any sentence, right?
This is how the earliest machine translation systems worked. Linguists came up with complicated rules and programmed them in one-by-one. Some of the smartest linguists in the world labored for years during the Cold War to create translation systems as a way to interpret Russian communications more easily.
Unfortunately this only worked for simple, plainly-structured documents like weather reports. It didn’t work reliably for real-world documents.
The problem is that human language doesn’t follow a fixed set of rules. Human languages are full of special cases, regional variations, and just flat out rule-breaking. The way we speak English more influenced by who invaded who hundreds of years ago than it is by someone sitting down and defining grammar rules.
After the failure of rule-based systems, new translation approaches were developed using models based on probability and statistics instead of grammar rules.
Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another.
Luckily, there’s lots of double-translated text already sitting around in strange places. For example, the European Parliament translates their proceedings into 21 languages. So researchers often use that data to help build translation systems.
The fundamental difference with statistical translation systems is that they don’t try to generate one exact translation. Instead, they generate thousands of possible translations and then they rank those translations by likely each is to be correct. They estimate how “correct” something is by how similar it is to the training data. Here’s how it works:
First, we break up our sentence into simple chunks that can each be easily translated:
Next, we will translate each of these chunks by finding all the ways humans have translated those same chunks of words in our training data.
It’s important to note that we are not just looking up these chunks in a simple translation dictionary. Instead, we are seeing how actual people translated these same chunks of words in real-world sentences. This helps us capture all of the different ways they can be used in different contexts:
Some of these possible translations are used more frequently than others. Based on how frequently each translation appears in our training data, we can give it a score.
For example, it’s much more common for someone to say “Quiero” to mean “I want” than to mean “I try.” So we can use how frequently “Quiero” was translated to “I want” in our training data to give that translation more weight than a less frequent translation.
Next, we will use every possible combination of these chunks to generate a bunch of possible sentences.
Just from the chunk translations we listed in Step 2, we can already generate nearly 2,500 different variations of our sentence by combining the chunks in different ways. Here are some examples:
But in a real-world system, there will be even more possible chunk combinations because we’ll also try different orderings of words and different ways of chunking the sentence:
Now need to scan through all of these generated sentences to find the one that is that sounds the “most human.”
To do this, we compare each generated sentence to millions of real sentences from books and news stories written in English. The more English text we can get our hands on, the better.
Take this possible translation:
It’s likely that no one has ever written a sentence like this in English, so it would not be very similar to any sentences in our data set. We’ll give this possible translation a low probability score.
But look at this possible translation:
This sentence will be similar to something in our training set, so it will get a high probability score.
After trying all possible sentences, we’ll pick the sentence that has the most likely chunk translations while also being the most similar overall to real English sentences.
Our final translation would be “I want to go to the prettiest beach.” Not bad!
Statistical machine translation systems perform much better than rule-based systems if you give them enough training data. Franz Josef Och improved on these ideas and used them to build Google Translate in the early 2000s. Machine Translation was finally available to the world.
In the early days, it was surprising to everyone that the “dumb” approach to translating based on probability worked better than rule-based systems designed by linguists. This led to a (somewhat mean) saying among researchers in the 80s:
Statistical machine translation systems work well, but they are complicated to build and maintain. Every new pair of languages you want to translate requires experts to tweak and tune a new multi-step translation pipeline.
Because it is so much work to build these different pipelines, trade-offs have to be made. If you are asking Google to translate Georgian to Telegu, it has to internally translate it into English as an intermediate step because there’s not enough Georgain-to-Telegu translations happening to justify investing heavily in that language pair. And it might do that translation using a less advanced translation pipeline than if you had asked it for the more common choice of French-to-English.
Wouldn’t it be cool if we could have the computer do all that annoying development work for us?
The holy grail of machine translation is a black box system that learns how to translate by itself— just by looking at training data. With Statistical Machine Translation, humans are still needed to build and tweak the multi-step statistical models.
In 2014, KyungHyun Cho’s team made a breakthrough. They found a way to apply deep learning to build this black box system. Their deep learning model takes in a parallel corpora and and uses it to learn how to translate between those two languages without any human intervention.
Two big ideas make this possible — recurrent neural networks and encodings. By combining these two ideas in a clever way, we can build a self-learning translation system.
We’ve already talked about recurrent neural networks in Part 2, but let’s quickly review.
A regular (non-recurrent) neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result (based on previous training). Neural networks can be used as a black box to solve lots of problems. For example, we can use a neural network to calculate the approximate value of a house based on attributes of that house:
But like most machine learning algorithms, neural networks are stateless. You pass in a list of numbers and the neural network calculates a result. If you pass in those same numbers again, it will always calculate the same result. It has no memory of past calculations. In other words, 2 + 2 always equals 4.
A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!
Why in the world would we want to do this? Shouldn’t 2 + 2 always equal 4 no matter what we last calculated?
This trick allows neural networks to learn patterns in a sequence of data. For example, you can use it to predict the next most likely word in a sentence based on the first few words:
RNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing.
If you want to learn more about RNNs, you can read Part 2 where we used one to generate a fake Ernest Hemingway book and then used another one to generate fake Super Mario Brothers levels.
The other idea we need to review is Encodings. We talked about encodings in Part 4 as part of face recognition. To explain encodings, let’s take a slight detour into how we can tell two different people apart with a computer.
When you are trying to tell two faces apart with a computer, you collect different measurements from each face and use those measurements to compare faces. For example, we might measure the size of each ear or the spacing between the eyes and compare those measurements from two pictures to see if they are the same person.
You’re probably already familiar with this idea from watching any primetime detective show like CSI:
The idea of turning a face into a list of measurements is an example of an encoding. We are taking raw data (a picture of a face) and turning it into a list of measurements that represent it (the encoding).
But like we saw in Part 4, we don’t have to come up with a specific list of facial features to measure ourselves. Instead, we can use a neural network to generate measurements from a face. The computer can do a better job than us in figuring out which measurements are best able to differentiate two similar people:
This is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images.
Guess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers:
To generate this encoding, we’ll feed the sentence into the RNN, one word at time. The final result after the last word is processed will be the values that represent the entire sentence:
Great, so now we have a way to represent an entire sentence as a set of unique numbers! We don’t know what each number in the encoding means, but it doesn’t really matter. As long as each sentence is uniquely identified by it’s own set of numbers, we don’t need to know exactly how those numbers were generated.
Ok, so we know how to use an RNN to encode a sentence into a set of unique numbers. How does that help us? Here’s where things get really cool!
What if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again:
Of course being able to encode and then decode the original sentence again isn’t very useful. But what if (and here’s the big idea!) we could train the second RNN to decode the sentence into Spanish instead of English? We could use our parallel corpora training data to train it to do that:
And just like that, we have a generic way of converting a sequence of English words into an equivalent sequence of Spanish words!
This is a powerful idea:
Note that we glossed over some things that are required to make this work with real-world data. For example, there’s additional work you have to do to deal with different lengths of input and output sentences (see bucketing and padding). There’s also issues with translating rare words correctly.
If you want to build your own language translation system, there’s a working demo included with TensorFlow that will translate between English and French. However, this is not for the faint of heart or for those with limited budgets. This technology is still new and very resource intensive. Even if you have a fast computer with a high-end video card, it might take about a month of continuous processing time to train your own language translation system.
Also, Sequence-to-sequence language translation techniques are improving so rapidly that it’s hard to keep up. Many recent improvements (like adding an attention mechanism or tracking context) are significantly improving results but these developments are so new that there aren’t even wikipedia pages for them yet. If you want to do anything serious with sequence-to-sequence learning, you’ll need to keep with new developments as they occur.
So what else can we do with sequence-to-sequence models?
About a year ago, researchers at Google showed that you can use sequence-to-sequence models to build AI bots. The idea is so simple that it’s amazing it works at all.
First, they captured chat logs between Google employees and Google’s Tech Support team. Then they trained a sequence-to-sequence model where the employee’s question was the input sentence and the Tech Support team’s response was the “translation” of that sentence.
When a user interacted with the bot, they would “translate” each of the user’s messages with this system to get the bot’s response.
The end result was a semi-intelligent bot that could (sometimes) answer real tech support questions. Here’s part of a sample conversation between a user and the bot from their paper:
They also tried building a chat bot based on millions of movie subtitles. The idea was to use conversations between movie characters as a way to train a bot to talk like a human. The input sentence is a line of dialog said by one character and the “translation” is what the next character said in response:
This produced really interesting results. Not only did the bot converse like a human, but it displayed a small bit of intelligence:
This is only the beginning of the possibilities. We aren’t limited to converting one sentence into another sentence. It’s also possible to make an image-to-sequence model that can turn an image into text!
A different team at Google did this by replacing the first RNN with a Convolutional Neural Network (like we learned about in Part 3). This allows the input to be a picture instead of a sentence. The rest works basically the same way:
And just like that, we can turn pictures into words (as long as we have lots and lots of training data)!
Andrej Karpathy expanded on these ideas to build a system capable of describing images in great detail by processing multiple regions of an image separately:
This makes it possible to build image search engines that are capable of finding images that match oddly specific search queries:
There’s even researchers working on the reverse problem, generating an entire picture based on just a text description!
Just from these examples, you can start to imagine the possibilities. So far, there have been sequence-to-sequence applications in everything from speech recognition to computer vision. I bet there will be a lot more over the next year.
If you want to learn more in depth about sequence-to-sequence models and translation, here’s some recommended resources:
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun! Part 6!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
",update article part series check full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 part 8 also read article tieng viet italiano know love google translate website instantly translate 100 different human languages magic even available phones smartwatches technology behind google translate called machine translation changed world allowing people communicate wouldnt otherwise possible know high school students using google translate umm assist spanish homework 15 years isnt old news turns past two years deep learning totally rewritten approach machine translation deep learning researchers know almost nothing language translation throwing together relatively simple machine learning solutions beating best expertbuilt language translation systems world technology behind breakthrough called sequencetosequence learning powerful technique used solve many kinds problems see used translation well also learn exact algorithm used write ai chat bots describe pictures lets go program computer translate human language simplest approach replace every word sentence translated word target language heres simple example translating spanish english wordbyword easy implement need dictionary look words translation results bad ignores grammar context next thing might start adding languagespecific rules improve results example might translate common twoword phrases single group might swap order nouns adjectives since usually appear reverse order spanish appear english worked keep adding rules handle every part grammar program able translate sentence right earliest machine translation systems worked linguists came complicated rules programmed onebyone smartest linguists world labored years cold war create translation systems way interpret russian communications easily unfortunately worked simple plainlystructured documents like weather reports didnt work reliably realworld documents problem human language doesnt follow fixed set rules human languages full special cases regional variations flat rulebreaking way speak english influenced invaded hundreds years ago someone sitting defining grammar rules failure rulebased systems new translation approaches developed using models based probability statistics instead grammar rules building statisticsbased translation system requires lots training data exact text translated least two languages doubletranslated text called parallel corpora way rosetta stone used scientists 1800s figure egyptian hieroglyphs greek computers use parallel corpora guess convert text one language another luckily theres lots doubletranslated text already sitting around strange places example european parliament translates proceedings 21 languages researchers often use data help build translation systems fundamental difference statistical translation systems dont try generate one exact translation instead generate thousands possible translations rank translations likely correct estimate correct something similar training data heres works first break sentence simple chunks easily translated next translate chunks finding ways humans translated chunks words training data important note looking chunks simple translation dictionary instead seeing actual people translated chunks words realworld sentences helps us capture different ways used different contexts possible translations used frequently others based frequently translation appears training data give score example much common someone say quiero mean want mean try use frequently quiero translated want training data give translation weight less frequent translation next use every possible combination chunks generate bunch possible sentences chunk translations listed step 2 already generate nearly 2500 different variations sentence combining chunks different ways examples realworld system even possible chunk combinations well also try different orderings words different ways chunking sentence need scan generated sentences find one sounds human compare generated sentence millions real sentences books news stories written english english text get hands better take possible translation likely one ever written sentence like english would similar sentences data set well give possible translation low probability score look possible translation sentence similar something training set get high probability score trying possible sentences well pick sentence likely chunk translations also similar overall real english sentences final translation would want go prettiest beach bad statistical machine translation systems perform much better rulebased systems give enough training data franz josef och improved ideas used build google translate early 2000s machine translation finally available world early days surprising everyone dumb approach translating based probability worked better rulebased systems designed linguists led somewhat mean saying among researchers 80s statistical machine translation systems work well complicated build maintain every new pair languages want translate requires experts tweak tune new multistep translation pipeline much work build different pipelines tradeoffs made asking google translate georgian telegu internally translate english intermediate step theres enough georgaintotelegu translations happening justify investing heavily language pair might translation using less advanced translation pipeline asked common choice frenchtoenglish wouldnt cool could computer annoying development work us holy grail machine translation black box system learns translate looking training data statistical machine translation humans still needed build tweak multistep statistical models 2014 kyunghyun chos team made breakthrough found way apply deep learning build black box system deep learning model takes parallel corpora uses learn translate two languages without human intervention two big ideas make possible recurrent neural networks encodings combining two ideas clever way build selflearning translation system weve already talked recurrent neural networks part 2 lets quickly review regular nonrecurrent neural network generic machine learning algorithm takes list numbers calculates result based previous training neural networks used black box solve lots problems example use neural network calculate approximate value house based attributes house like machine learning algorithms neural networks stateless pass list numbers neural network calculates result pass numbers always calculate result memory past calculations words 2 2 always equals 4 recurrent neural network rnn short slightly tweaked version neural network previous state neural network one inputs next calculation means previous calculations change results future calculations world would want shouldnt 2 2 always equal 4 matter last calculated trick allows neural networks learn patterns sequence data example use predict next likely word sentence based first words rnns useful time want learn patterns data human language one big complicated pattern rnns increasingly used many areas natural language processing want learn rnns read part 2 used one generate fake ernest hemingway book used another one generate fake super mario brothers levels idea need review encodings talked encodings part 4 part face recognition explain encodings lets take slight detour tell two different people apart computer trying tell two faces apart computer collect different measurements face use measurements compare faces example might measure size ear spacing eyes compare measurements two pictures see person youre probably already familiar idea watching primetime detective show like csi idea turning face list measurements example encoding taking raw data picture face turning list measurements represent encoding like saw part 4 dont come specific list facial features measure instead use neural network generate measurements face computer better job us figuring measurements best able differentiate two similar people encoding lets us represent something complicated picture face something simple 128 numbers comparing two different faces much easier compare 128 numbers face instead comparing full images guess thing sentences come encoding represents every possible different sentence series unique numbers generate encoding well feed sentence rnn one word time final result last word processed values represent entire sentence great way represent entire sentence set unique numbers dont know number encoding means doesnt really matter long sentence uniquely identified set numbers dont need know exactly numbers generated ok know use rnn encode sentence set unique numbers help us heres things get really cool took two rnns hooked endtoend first rnn could generate encoding represents sentence second rnn could take encoding logic reverse decode original sentence course able encode decode original sentence isnt useful heres big idea could train second rnn decode sentence spanish instead english could use parallel corpora training data train like generic way converting sequence english words equivalent sequence spanish words powerful idea note glossed things required make work realworld data example theres additional work deal different lengths input output sentences see bucketing padding theres also issues translating rare words correctly want build language translation system theres working demo included tensorflow translate english french however faint heart limited budgets technology still new resource intensive even fast computer highend video card might take month continuous processing time train language translation system also sequencetosequence language translation techniques improving rapidly hard keep many recent improvements like adding attention mechanism tracking context significantly improving results developments new arent even wikipedia pages yet want anything serious sequencetosequence learning youll need keep new developments occur else sequencetosequence models year ago researchers google showed use sequencetosequence models build ai bots idea simple amazing works first captured chat logs google employees googles tech support team trained sequencetosequence model employees question input sentence tech support teams response translation sentence user interacted bot would translate users messages system get bots response end result semiintelligent bot could sometimes answer real tech support questions heres part sample conversation user bot paper also tried building chat bot based millions movie subtitles idea use conversations movie characters way train bot talk like human input sentence line dialog said one character translation next character said response produced really interesting results bot converse like human displayed small bit intelligence beginning possibilities arent limited converting one sentence another sentence also possible make imagetosequence model turn image text different team google replacing first rnn convolutional neural network like learned part 3 allows input picture instead sentence rest works basically way like turn pictures words long lots lots training data andrej karpathy expanded ideas build system capable describing images great detail processing multiple regions image separately makes possible build image search engines capable finding images match oddly specific search queries theres even researchers working reverse problem generating entire picture based text description examples start imagine possibilities far sequencetosequence applications everything speech recognition computer vision bet lot next year want learn depth sequencetosequence models translation heres recommended resources liked article please consider signing machine learning fun email list ill email something new awesome share best way find write articles like also follow twitter ageitgey email directly find linkedin id love hear help team machine learning continue machine learning fun part 6 quick cheer standing ovation clap show much enjoyed story interested computers machine learning likes write,en,"['Google Translate', 'algorithm', 'the European Parliament', 'Google', 'KyungHyun Cho’s', 'house', 'Super Mario Brothers', 'Tech Support', 'Convolutional Neural Network']"
53,Tal Perry,2600,Deep Learning the Stock Market – Tal Perry – Medium,"Update 25.1.17 — Took me a while but here is an ipython notebook with a rough implementation
In the past few months I’ve been fascinated with “Deep Learning”, especially its applications to language and text. I’ve spent the bulk of my career in financial technologies, mostly in algorithmic trading and alternative data services. You can see where this is going.
I wrote this to get my ideas straight in my head. While I’ve become a “Deep Learning” enthusiast, I don’t have too many opportunities to brain dump an idea in most of its messy glory. I think that a decent indication of a clear thought is the ability to articulate it to people not from the field. I hope that I’ve succeeded in doing that and that my articulation is also a pleasurable read.
Why NLP is relevant to Stock prediction
In many NLP problems we end up taking a sequence and encoding it into a single fixed size representation, then decoding that representation into another sequence. For example, we might tag entities in the text, translate from English to French or convert audio frequencies to text. There is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance.
In my mind the biggest difference between the NLP and financial analysis is that language has some guarantee of structure, it’s just that the rules of the structure are vague. Markets, on the other hand, don’t come with a promise of a learnable structure, that such a structure exists is the assumption that this project would prove or disprove (rather it might prove or disprove if I can find that structure).
Assuming the structure is there, the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me. If that doesn’t make sense yet, keep reading. It will.
You shall know a word by the company it keeps (Firth, J. R. 1957:11)
There is tons of literature on word embeddings. Richard Socher’s lecture is a great place to start. In short, we can make a geometry of all the words in our language, and that geometry captures the meaning of words and relationships between them. You may have seen the example of “King-man +woman=Queen” or something of the sort.
Embeddings are cool because they let us represent information in a condensed way. The old way of representing words was holding a vector (a big list of numbers) that was as long as the number of words we know, and setting a 1 in a particular place if that was the current word we are looking at. That is not an efficient approach, nor does it capture any meaning. With embeddings, we can represent all of the words in a fixed number of dimensions (300 seems to be plenty, 50 works great) and then leverage their higher dimensional geometry to understand them.
The picture below shows an example. An embedding was trained on more or less the entire internet. After a few days of intensive calculations, each word was embedded in some high dimensional space. This “space” has a geometry, concepts like distance, and so we can ask which words are close together. The authors/inventors of that method made an example. Here are the words that are closest to Frog.
But we can embed more than just words. We can do, say , stock market embeddings.
Market2Vec
The first word embedding algorithm I heard about was word2vec. I want to get the same effect for the market, though I’ll be using a different algorithm. My input data is a csv, the first column is the date, and there are 4*1000 columns corresponding to the High Low Open Closing price of 1000 stocks. That is my input vector is 4000 dimensional, which is too big. So the first thing I’m going to do is stuff it into a lower dimensional space, say 300 because I liked the movie.
Taking something in 4000 dimensions and stuffing it into a 300-dimensional space my sound hard but its actually easy. We just need to multiply matrices. A matrix is a big excel spreadsheet that has numbers in every cell and no formatting problems. Imagine an excel table with 4000 columns and 300 rows, and when we basically bang it against the vector a new vector comes out that is only of size 300. I wish that’s how they would have explained it in college.
The fanciness starts here as we’re going to set the numbers in our matrix at random, and part of the “deep learning” is to update those numbers so that our excel spreadsheet changes. Eventually this matrix spreadsheet (I’ll stick with matrix from now on) will have numbers in it that bang our original 4000 dimensional vector into a concise 300 dimensional summary of itself.
We’re going to get a little fancier here and apply what they call an activation function. We’re going to take a function, and apply it to each number in the vector individually so that they all end up between 0 and 1 (or 0 and infinity, it depends). Why ? It makes our vector more special, and makes our learning process able to understand more complicated things. How?
So what? What I’m expecting to find is that that new embedding of the market prices (the vector) into a smaller space captures all the essential information for the task at hand, without wasting time on the other stuff. So I’d expect they’d capture correlations between other stocks, perhaps notice when a certain sector is declining or when the market is very hot. I don’t know what traits it will find, but I assume they’ll be useful.
Now What
Lets put aside our market vectors for a moment and talk about language models. Andrej Karpathy wrote the epic post “The Unreasonable effectiveness of Recurrent Neural Networks”. If I’d summarize in the most liberal fashion the post boils down to
And then as a punchline, he generated a bunch of text that looks like Shakespeare. And then he did it again with the Linux source code. And then again with a textbook on Algebraic geometry.
So I’ll get back to the mechanics of that magic box in a second, but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one. Where Karpathy used characters, we’re going to use our market vectors and feed them into the magic black box. We haven’t decided what we want it to predict yet, but that is okay, we won’t be feeding its output back into it either.
Going deeper
I want to point out that this is where we start to get into the deep part of deep learning. So far we just have a single layer of learning, that excel spreadsheet that condenses the market. Now we’re going to add a few more layers and stack them, to make a “deep” something. That’s the deep in deep learning.
So Karpathy shows us some sample output from the Linux source code, this is stuff his black box wrote.
Notice that it knows how to open and close parentheses, and respects indentation conventions; The contents of the function are properly indented and the multi-line printk statement has an inner indentation. That means that this magic box understands long range dependencies. When it’s indenting within the print statement it knows it’s in a print statement and also remembers that it’s in a function( Or at least another indented scope). That’s nuts. It’s easy to gloss over that but an algorithm that has the ability to capture and remember long term dependencies is super useful because... We want to find long term dependencies in the market.
Inside the magical black box
What’s inside this magical black box? It is a type of Recurrent Neural Network (RNN) called an LSTM. An RNN is a deep learning algorithm that operates on sequences (like sequences of characters). At every step, it takes a representation of the next character (Like the embeddings we talked about before) and operates on the representation with a matrix, like we saw before. The thing is, the RNN has some form of internal memory, so it remembers what it saw previously. It uses that memory to decide how exactly it should operate on the next input. Using that memory, the RNN can “remember” that it is inside of an intended scope and that is how we get properly nested output text.
A fancy version of an RNN is called a Long Short Term Memory (LSTM). LSTM has cleverly designed memory that allows it to
So an LSTM can see a “{“ and say to itself “Oh yeah, that’s important I should remember that” and when it does, it essentially remembers an indication that it is in a nested scope. Once it sees the corresponding “}” it can decide to forget the original opening brace and thus forget that it is in a nested scope.
We can have the LSTM learn more abstract concepts by stacking a few of them on top of each other, that would make us “Deep” again. Now each output of the previous LSTM becomes the inputs of the next LSTM, and each one goes on to learn higher abstractions of the data coming in. In the example above (and this is just illustrative speculation), the first layer of LSTMs might learn that characters separated by a space are “words”. The next layer might learn word types like (static void action_new_function).The next layer might learn the concept of a function and its arguments and so on. It’s hard to tell exactly what each layer is doing, though Karpathy’s blog has a really nice example of how he did visualize exactly that.
Connecting Market2Vec and LSTMs
The studious reader will notice that Karpathy used characters as his inputs, not embeddings (Technically a one-hot encoding of characters). But, Lars Eidnes actually used word embeddings when he wrote Auto-Generating Clickbait With Recurrent Neural Network
The figure above shows the network he used. Ignore the SoftMax part (we’ll get to it later). For the moment, check out how on the bottom he puts in a sequence of words vectors at the bottom and each one. (Remember, a “word vector” is a representation of a word in the form of a bunch of numbers, like we saw in the beginning of this post). Lars inputs a sequence of Word Vectors and each one of them:
We’re going to do the same thing with one difference, instead of word vectors we’ll input “MarketVectors”, those market vectors we described before. To recap, the MarketVectors should contain a summary of what’s happening in the market at a given point in time. By putting a sequence of them through LSTMs I hope to capture the long term dynamics that have been happening in the market. By stacking together a few layers of LSTMs I hope to capture higher level abstractions of the market’s behavior.
What Comes out
Thus far we haven’t talked at all about how the algorithm actually learns anything, we just talked about all the clever transformations we’ll do on the data. We’ll defer that conversation to a few paragraphs down, but please keep this part in mind as it is the se up for the punch line that makes everything else worthwhile.
In Karpathy’s example, the output of the LSTMs is a vector that represents the next character in some abstract representation. In Eidnes’ example, the output of the LSTMs is a vector that represents what the next word will be in some abstract space. The next step in both cases is to change that abstract representation into a probability vector, that is a list that says how likely each character or word respectively is likely to appear next. That’s the job of the SoftMax function. Once we have a list of likelihoods we select the character or word that is the most likely to appear next.
In our case of “predicting the market”, we need to ask ourselves what exactly we want to market to predict? Some of the options that I thought about were:
1 and 2 are regression problems, where we have to predict an actual number instead of the likelihood of a specific event (like the letter n appearing or the market going up). Those are fine but not what I want to do.
3 and 4 are fairly similar, they both ask to predict an event (In technical jargon — a class label). An event could be the letter n appearing next or it could be Moved up 5% while not going down more than 3% in the last 10 minutes. The trade-off between 3 and 4 is that 3 is much more common and thus easier to learn about while 4 is more valuable as not only is it an indicator of profit but also has some constraint on risk.
5 is the one we’ll continue with for this article because it’s similar to 3 and 4 but has mechanics that are easier to follow. The VIX is sometimes called the Fear Index and it represents how volatile the stocks in the S&P500 are. It is derived by observing the implied volatility for specific options on each of the stocks in the index.
Sidenote — Why predict the VIX
What makes the VIX an interesting target is that
Back to our LSTM outputs and the SoftMax
How do we use the formulations we saw before to predict changes in the VIX a few minutes in the future? For each point in our dataset, we’ll look what happened to the VIX 5 minutes later. If it went up by more than 1% without going down more than 0.5% during that time we’ll output a 1, otherwise a 0. Then we’ll get a sequence that looks like:
We want to take the vector that our LSTMs output and squish it so that it gives us the probability of the next item in our sequence being a 1. The squishing happens in the SoftMax part of the diagram above. (Technically, since we only have 1 class now, we use a sigmoid ).
So before we get into how this thing learns, let’s recap what we’ve done so far
How does this thing learn?
Now the fun part. Everything we did until now was called the forward pass, we’d do all of those steps while we train the algorithm and also when we use it in production. Here we’ll talk about the backward pass, the part we do only while in training that makes our algorithm learn.
So during training, not only did we prepare years worth of historical data, we also prepared a sequence of prediction targets, that list of 0 and 1 that showed if the VIX moved the way we want it to or not after each observation in our data.
To learn, we’ll feed the market data to our network and compare its output to what we calculated. Comparing in our case will be simple subtraction, that is we’ll say that our model’s error is
Or in English, the square root of the square of the difference between what actually happened and what we predicted.
Here’s the beauty. That’s a differential function, that is, we can tell by how much the error would have changed if our prediction would have changed a little. Our prediction is the outcome of a differentiable function, the SoftMax The inputs to the softmax, the LSTMs are all mathematical functions that are differentiable. Now all of these functions are full of parameters, those big excel spreadsheets I talked about ages ago. So at this stage what we do is take the derivative of the error with respect to every one of the millions of parameters in all of those excel spreadsheets we have in our model. When we do that we can see how the error will change when we change each parameter, so we’ll change each parameter in a way that will reduce the error.
This procedure propagates all the way to the beginning of the model. It tweaks the way we embed the inputs into MarketVectors so that our MarketVectors represent the most significant information for our task.
It tweaks when and what each LSTM chooses to remember so that their outputs are the most relevant to our task.
It tweaks the abstractions our LSTMs learn so that they learn the most important abstractions for our task.
Which in my opinion is amazing because we have all of this complexity and abstraction that we never had to specify anywhere. It’s all inferred MathaMagically from the specification of what we consider to be an error.
What’s next
Now that I’ve laid this out in writing and it still makes sense to me I want
So, if you’ve come this far please point out my errors and share your inputs.
Other thoughts
Here are some mostly more advanced thoughts about this project, what other things I might try and why it makes sense to me that this may actually work.
Liquidity and efficient use of capital
Generally the more liquid a particular market is the more efficient that is. I think this is due to a chicken and egg cycle, whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself. As a market becomes more liquid and more capital can be used in it, you’ll find more sophisticated players moving in. This is because it is expensive to be sophisticated, so you need to make returns on a large chunk of capital in order to justify your operational costs.
A quick corollary is that in less liquid markets the competition isn’t quite as sophisticated and so the opportunities a system like this can bring may not have been traded away. The point being were I to try and trade this I would try and trade it on less liquid segments of the market, that is maybe the TASE 100 instead of the S&P 500.
This stuff is new
The knowledge of these algorithms, the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average Joe such as myself. I’d assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but, as I mention in the above paragraph, they are likely executing in liquid markets that can support their size. The next tier of market participants, I assume, have a slower velocity of technological assimilation and in that sense, there is or soon will be a race to execute on this in as yet untapped markets.
Multiple Time Frames
While I mentioned a single stream of inputs in the above, I imagine that a more efficient way to train would be to train market vectors (at least) on multiple time frames and feed them in at the inference stage. That is, my lowest time frame would be sampled every 30 seconds and I’d expect the network to learn dependencies that stretch hours at most.
I don’t know if they are relevant or not but I think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model. I’m still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with.
MarketVectors
When using word vectors in NLP we usually start with a pretrained model and continue adjusting the embeddings during training of our model. In my case, there are no pretrained market vector available nor is tehre a clear algorithm for training them.
My original consideration was to use an auto-encoder like in this paper but end to end training is cooler.
A more serious consideration is the success of sequence to sequence models in translation and speech recognition, where a sequence is eventually encoded as a single vector and then decoded into a different representation (Like from speech to text or from English to French). In that view, the entire architecture I described is essentially the encoder and I haven’t really laid out a decoder.
But, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. I want it to find correlations or relations between various stocks and compose features about them.
The alternative is to run each input through an LSTM, perhaps concatenate all of the output vectors and consider that output of the encoder stage. I think this will be inefficient as the interactions and correlations between instruments and their features will be lost, and thre will be 10x more computation required. On the other hand, such an architecture could naively be paralleled across multiple GPUs and hosts which is an advantage.
CNNs
Recently there has been a spur of papers on character level machine translation. This paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an RNN. I haven’t given it more than a brief read but I think that a modification where I’d treat each stock as a channel and convolve over channels first (like in RGB images) would be another way to capture the market dynamics, in the same way that they essentially encode semantic meaning from characters.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of https://LightTag.io, platform to annotate text for NLP. Google developer expert in ML. I do deep learning on text for a living and for fun.
",update 25117 took ipython notebook rough implementation past months ive fascinated deep learning especially applications language text ive spent bulk career financial technologies mostly algorithmic trading alternative data services see going wrote get ideas straight head ive become deep learning enthusiast dont many opportunities brain dump idea messy glory think decent indication clear thought ability articulate people field hope ive succeeded articulation also pleasurable read nlp relevant stock prediction many nlp problems end taking sequence encoding single fixed size representation decoding representation another sequence example might tag entities text translate english french convert audio frequencies text torrent work coming areas lot results achieving state art performance mind biggest difference nlp financial analysis language guarantee structure rules structure vague markets hand dont come promise learnable structure structure exists assumption project would prove disprove rather might prove disprove find structure assuming structure idea summarizing current state market way encode semantics paragraph seems plausible doesnt make sense yet keep reading shall know word company keeps firth j r 195711 tons literature word embeddings richard sochers lecture great place start short make geometry words language geometry captures meaning words relationships may seen example kingman womanqueen something sort embeddings cool let us represent information condensed way old way representing words holding vector big list numbers long number words know setting 1 particular place current word looking efficient approach capture meaning embeddings represent words fixed number dimensions 300 seems plenty 50 works great leverage higher dimensional geometry understand picture shows example embedding trained less entire internet days intensive calculations word embedded high dimensional space space geometry concepts like distance ask words close together authorsinventors method made example words closest frog embed words say stock market embeddings market2vec first word embedding algorithm heard word2vec want get effect market though ill using different algorithm input data csv first column date 41000 columns corresponding high low open closing price 1000 stocks input vector 4000 dimensional big first thing im going stuff lower dimensional space say 300 liked movie taking something 4000 dimensions stuffing 300dimensional space sound hard actually easy need multiply matrices matrix big excel spreadsheet numbers every cell formatting problems imagine excel table 4000 columns 300 rows basically bang vector new vector comes size 300 wish thats would explained college fanciness starts going set numbers matrix random part deep learning update numbers excel spreadsheet changes eventually matrix spreadsheet ill stick matrix numbers bang original 4000 dimensional vector concise 300 dimensional summary going get little fancier apply call activation function going take function apply number vector individually end 0 1 0 infinity depends makes vector special makes learning process able understand complicated things im expecting find new embedding market prices vector smaller space captures essential information task hand without wasting time stuff id expect theyd capture correlations stocks perhaps notice certain sector declining market hot dont know traits find assume theyll useful lets put aside market vectors moment talk language models andrej karpathy wrote epic post unreasonable effectiveness recurrent neural networks id summarize liberal fashion post boils punchline generated bunch text looks like shakespeare linux source code textbook algebraic geometry ill get back mechanics magic box second let remind want predict future market based past like predicted next word based previous one karpathy used characters going use market vectors feed magic black box havent decided want predict yet okay wont feeding output back either going deeper want point start get deep part deep learning far single layer learning excel spreadsheet condenses market going add layers stack make deep something thats deep deep learning karpathy shows us sample output linux source code stuff black box wrote notice knows open close parentheses respects indentation conventions contents function properly indented multiline printk statement inner indentation means magic box understands long range dependencies indenting within print statement knows print statement also remembers function least another indented scope thats nuts easy gloss algorithm ability capture remember long term dependencies super useful want find long term dependencies market inside magical black box whats inside magical black box type recurrent neural network rnn called lstm rnn deep learning algorithm operates sequences like sequences characters every step takes representation next character like embeddings talked operates representation matrix like saw thing rnn form internal memory remembers saw previously uses memory decide exactly operate next input using memory rnn remember inside intended scope get properly nested output text fancy version rnn called long short term memory lstm lstm cleverly designed memory allows lstm see say oh yeah thats important remember essentially remembers indication nested scope sees corresponding decide forget original opening brace thus forget nested scope lstm learn abstract concepts stacking top would make us deep output previous lstm becomes inputs next lstm one goes learn higher abstractions data coming example illustrative speculation first layer lstms might learn characters separated space words next layer might learn word types like static void action_new_functionthe next layer might learn concept function arguments hard tell exactly layer though karpathys blog really nice example visualize exactly connecting market2vec lstms studious reader notice karpathy used characters inputs embeddings technically onehot encoding characters lars eidnes actually used word embeddings wrote autogenerating clickbait recurrent neural network figure shows network used ignore softmax part well get later moment check bottom puts sequence words vectors bottom one remember word vector representation word form bunch numbers like saw beginning post lars inputs sequence word vectors one going thing one difference instead word vectors well input marketvectors market vectors described recap marketvectors contain summary whats happening market given point time putting sequence lstms hope capture long term dynamics happening market stacking together layers lstms hope capture higher level abstractions markets behavior comes thus far havent talked algorithm actually learns anything talked clever transformations well data well defer conversation paragraphs please keep part mind se punch line makes everything else worthwhile karpathys example output lstms vector represents next character abstract representation eidnes example output lstms vector represents next word abstract space next step cases change abstract representation probability vector list says likely character word respectively likely appear next thats job softmax function list likelihoods select character word likely appear next case predicting market need ask exactly want market predict options thought 1 2 regression problems predict actual number instead likelihood specific event like letter n appearing market going fine want 3 4 fairly similar ask predict event technical jargon class label event could letter n appearing next could moved 5 going 3 last 10 minutes tradeoff 3 4 3 much common thus easier learn 4 valuable indicator profit also constraint risk 5 one well continue article similar 3 4 mechanics easier follow vix sometimes called fear index represents volatile stocks sp500 derived observing implied volatility specific options stocks index sidenote predict vix makes vix interesting target back lstm outputs softmax use formulations saw predict changes vix minutes future point dataset well look happened vix 5 minutes later went 1 without going 05 time well output 1 otherwise 0 well get sequence looks like want take vector lstms output squish gives us probability next item sequence 1 squishing happens softmax part diagram technically since 1 class use sigmoid get thing learns lets recap weve done far thing learn fun part everything called forward pass wed steps train algorithm also use production well talk backward pass part training makes algorithm learn training prepare years worth historical data also prepared sequence prediction targets list 0 1 showed vix moved way want observation data learn well feed market data network compare output calculated comparing case simple subtraction well say models error english square root square difference actually happened predicted heres beauty thats differential function tell much error would changed prediction would changed little prediction outcome differentiable function softmax inputs softmax lstms mathematical functions differentiable functions full parameters big excel spreadsheets talked ages ago stage take derivative error respect every one millions parameters excel spreadsheets model see error change change parameter well change parameter way reduce error procedure propagates way beginning model tweaks way embed inputs marketvectors marketvectors represent significant information task tweaks lstm chooses remember outputs relevant task tweaks abstractions lstms learn learn important abstractions task opinion amazing complexity abstraction never specify anywhere inferred mathamagically specification consider error whats next ive laid writing still makes sense want youve come far please point errors share inputs thoughts mostly advanced thoughts project things might try makes sense may actually work liquidity efficient use capital generally liquid particular market efficient think due chicken egg cycle whereas market becomes liquid able absorb capital moving without capital hurting market becomes liquid capital used youll find sophisticated players moving expensive sophisticated need make returns large chunk capital order justify operational costs quick corollary less liquid markets competition isnt quite sophisticated opportunities system like bring may traded away point try trade would try trade less liquid segments market maybe tase 100 instead sp 500 stuff new knowledge algorithms frameworks execute computing power train new least sense available average joe id assume top players figured stuff years ago capacity execute long mention paragraph likely executing liquid markets support size next tier market participants assume slower velocity technological assimilation sense soon race execute yet untapped markets multiple time frames mentioned single stream inputs imagine efficient way train would train market vectors least multiple time frames feed inference stage lowest time frame would sampled every 30 seconds id expect network learn dependencies stretch hours dont know relevant think patterns multiple time frames cost computation brought low enough worthwhile incorporate model im still wrestling best represent computational graph perhaps mandatory start marketvectors using word vectors nlp usually start pretrained model continue adjusting embeddings training model case pretrained market vector available tehre clear algorithm training original consideration use autoencoder like paper end end training cooler serious consideration success sequence sequence models translation speech recognition sequence eventually encoded single vector decoded different representation like speech text english french view entire architecture described essentially encoder havent really laid decoder want achieve something specific first layer one takes input 4000 dimensional vector outputs 300 dimensional one want find correlations relations various stocks compose features alternative run input lstm perhaps concatenate output vectors consider output encoder stage think inefficient interactions correlations instruments features lost thre 10x computation required hand architecture could naively paralleled across multiple gpus hosts advantage cnns recently spur papers character level machine translation paper caught eye manage capture long range dependencies convolutional layer rather rnn havent given brief read think modification id treat stock channel convolve channels first like rgb images would another way capture market dynamics way essentially encode semantic meaning characters quick cheer standing ovation clap show much enjoyed story founder httpslighttagio platform annotate text nlp google developer expert ml deep learning text living fun,en,"['NLP', 'Recurrent Neural Network', 'Auto-Generating Clickbait With Recurrent Neural Network', 'SoftMax', 'MarketVectors', 'VIX', 'algorithm', 'Multiple Time Frames', 'RGB']"
54,Andrej Karpathy,9200,Yes you should understand backprop – Andrej Karpathy – Medium,"When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:
This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:
> The problem with Backpropagation is that it is a leaky abstraction.
In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.
We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.
Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.
TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.
Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.
TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.
Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):
This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.
What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b...)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.
TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.
Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:
If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.
The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.
The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:
It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.
I submitted an issue on the DQN repo and this was promptly fixed.
Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.
The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.
That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Director of AI at Tesla. Previously Research Scientist at OpenAI and PhD student at Stanford. I like to train deep neural nets on large datasets.
",offered cs231n deep learning class stanford intentionally designed programming assignments include explicit calculations involved backpropagation lowest level students implement forward backward pass layer raw numpy inevitably students complained class message boards seemingly perfectly sensible appeal youre never going write backward passes class practice writing torturing students amusement easy answers could make arguments along lines worth knowing whats hood intellectual curiosity perhaps might want improve core algorithm later much stronger practical argument wanted devote whole post problem backpropagation leaky abstraction words easy fall trap abstracting away learning process believing simply stack arbitrary layers together backprop magically make work data lets look explicit examples case quite unintuitive ways starting easy one point fashionable use sigmoid tanh nonlinearities fully connected layers tricky part people might realize think backward pass sloppy weight initialization data preprocessing nonlinearities saturate entirely stop learning training loss flat refuse go example fully connected layer sigmoid nonlinearity computes using raw numpy weight matrix w initialized large output matrix multiply could large range eg numbers 400 400 make outputs vector z almost binary either 1 0 case z1z local gradient sigmoid nonlinearity cases become zero vanish making gradient x w zero rest backward pass come zero point due multiplication chain rule another nonobvious fun fact sigmoid local gradient z1z achieves maximum 025 z 05 means every time gradient signal flows sigmoid gate magnitude always diminishes one quarter youre using basic sgd would make lower layers network train much slower higher ones tldr youre using sigmoids tanh nonlinearities network understand backpropagation always nervous making sure initialization doesnt cause fully saturated see longer explanation cs231n lecture video another fun nonlinearity relu thresholds neurons zero forward backward pass fully connected layer uses relu would core include stare youll see neuron gets clamped zero forward pass ie z0 doesnt fire weights get zero gradient lead called dead relu problem relu neuron unfortunately initialized never fires neurons weights ever get knocked large update training regime neuron remain permanently dead like permanent irrecoverable brain damage sometimes forward entire training set trained network find large fraction eg 40 neurons zero entire time tldr understand backpropagation network relus youre always nervous dead relus neurons never turn example entire training set remain permanently dead neurons also die training usually symptom aggressive learning rates see longer explanation cs231n lecture video vanilla rnns feature another good example unintuitive effects backpropagation ill copy paste slide cs231n simplified rnn take input x computes recurrence hidden state equivalently input x could always zero rnn unrolled time steps stare backward pass youll see gradient signal going backwards time hidden states always multiplied matrix recurrence matrix whh interspersed nonlinearity backprop happens take one number start multiplying number b ie abbbbbb sequence either goes zero b 1 explodes infinity b1 thing happens backward pass rnn except b matrix number reason largest eigenvalue instead tldr understand backpropagation youre using rnns nervous gradient clipping prefer use lstm see longer explanation cs231n lecture video lets look one one actually inspired post yesterday browsing deep q learning implementation tensorflow see others deal computing numpy equivalent q integer vector turns trivial operation supported tf anyway searched dqn tensorflow clicked first link found core code excerpt youre familiar dqn see target_q_t reward gamma argmax_a qsa q_acted qsa action taken authors subtract two variable delta want minimize line 295 l2 loss tfreduce_meantfsquare far good problem line 291 authors trying robust outliers delta large clip tfclip_by_value wellintentioned looks sensible perspective forward pass introduces major bug think backward pass clip_by_value function local gradient zero outside range min_delta max_delta whenever delta minmax_delta gradient becomes exactly zero backprop authors clipping raw q delta likely trying clip gradient added robustness case correct thing use huber loss place tfsquare bit gross tensorflow want clip gradient threshold since cant meddle gradients directly roundabout way defining huber loss torch would much simple submitted issue dqn repo promptly fixed backpropagation leaky abstraction credit assignment scheme nontrivial consequences try ignore works hood tensorflow automagically makes networks learn ready wrestle dangers presents much less effective building debugging neural networks good news backpropagation difficult understand presented properly relatively strong feelings topic seems 95 backpropagation materials present wrong filling pages mechanical math instead would recommend cs231n lecture backprop emphasizes intuition yay shameless selfadvertising spare time bonus work cs231n assignments get write backprop manually help solidify understanding thats hope youll much suspicious backpropagation going forward think carefully backward pass also im aware post unintentionally turned several cs231n ads apologies quick cheer standing ovation clap show much enjoyed story director ai tesla previously research scientist openai phd student stanford like train deep neural nets large datasets,en,"['Stanford', 'CS231n', 'Vanilla', 'DQN', 'Q(s', 'min_delta', 'max_delta', 'min/max_delta', 'Torch', 'Tesla']"
55,Per Harald Borgen,4800,Machine Learning in a Year – Learning New Stuff – Medium,"This is a follow up to an article I wrote last year, Machine Learning in a Week, on how I kickstarted my way into machine learning (ml) by devoting five days to the subject.
After this highly effective introduction, I continued learning on my spare time and almost exactly one year later I did my first ml project at work, which involved using various ml and natural language processing (nlp) techniques to qualify sales leads at Xeneta.
This felt like a blessing: getting paid to do something I normally did for fun!
It also ripped me out of the delusion that only people with masters degrees or Ph.D’s work with ml professionally.
In this post I want to share my journey, as it might inspire others to do the same.
My interest in ml stems back to 2014 when I started reading articles about it on Hacker News. I simply found the idea of teaching machines stuff by looking at data appealing. At the time I wasn’t even a professional developer, but a hobby coder who’d done a couple of small projects.
So I began watching the first few chapters of Udacity’s Supervised Learning course, while also reading all articles I came across on the subject.
This gave me a little bit of conceptual understanding, though no practical skills. I also didn’t finish it, as I rarely do with MOOC’s.
In January 2015 I joined the Founders and Coders (FAC) bootcamp in London in order to become a developer. A few weeks in, I wanted to learn how to actually code machine learning algorithms, so I started a study group with a few of my peers. Every Tuesday evening, we’d watch lectures from Coursera’s Machine Learning course.
It’s a fantastic course, and I learned a hell of a lot. But it’s tough for a beginner. I had to watch the lectures over and over again before grasping the concepts. The Octave coding task are challenging as well, especially if you don’t know Octave. As a result of the difficulty, one by one fell off the study group as the weeks passed. Eventually, I fell off it myself as well.
In hindsight, I should have started with a course that either used ml libraries for the coding tasks — as opposed to building the algorithms from scratch — or at least used a programming language I knew.
If I could go back in time, I’d choose Udacity’s Intro to Machine Learning, as it’s easier and uses Python and Scikit Learn. This way, we would have gotten our hands dirty as soon as possible, gained confidence, and had more fun.
One of the last things I did at FAC was the ml week stunt. My goal was to be able to apply machine learning to actual problems at the end of the week, which I managed to do.
Throughout the week I did the following:
It’s by far the steepest ml learning curve I’ve ever experienced. Go ahead and read the article if you want a more detailed overview.
After I finished FAC in London and moved back to Norway, I tried to repeat the success from the ml week, but for neural networks instead.
This failed.
There were simply too many distractions to spend 10 hours of coding and learning every day. I had underestimated how important it was to be surrounded by peers at FAC.
However, I got started with neural nets at least, and slowly started to grasp the concept. By July I managed to code my first net. It’s probably the crappiest implementation ever created, and I actually find it embarrassing to show off. But it did the trick; I proved to myself that I understood concepts like backpropagation and gradient descent.
In the second half of the year, my progression slowed down, as I started a new job. The most important takeaway from this period was the leap from non-vectorized to vectorized implementations of neural networks, which involved repeating linear algebra from university.
By the end of the year I wrote an article as a summary of how I learned this:
During the christmas vacation of 2015, I got a motivational boost again and decided try out Kaggle. So I spent quite some time experimenting with various algorithms for their Homesite Quote Conversion, Otto Group Product Classification and Bike Sharing Demand contests.
The main takeaway from this was the experience of iteratively improving the results by experimenting with the algorithms and the data.
I learned to trust my logic when doing machine learning.
If tweaking a parameter or engineering a new feature seems like a good idea logically, it’s quite likely that it actually will help.
Back at work in January 2016 I wanted to continue in the flow I’d gotten into during Christmas. So I asked my manager if I could spend some time learning stuff during my work hours as well, which he happily approved.
Having gotten a basic understanding of neural networks at this point, I wanted to move on to deep learning.
My first attempt was Udacity’s Deep Learning course, which ended up as a big disappointment. The contents of the video lectures are good, but they are too short and shallow to me.
And the IPython Notebook assignments ended up being too frustrating, as I spent most of my time debugging code errors, which is the most effective way to kill motivation. So after doing that for a couple of sessions at work, I simply gave up.
To their defense, I’m a total noob when it comes to IPython Notebooks, so it might not be as bad for you as it was for me. So it might be that I simply wasn’t ready for the course.
Luckily, I then discovered Stanford’s CS224D and decided to give it a shot. It is a fantastic course. And though it’s difficult, I never end up debugging when doing the problem sets.
Secondly, they actually give you the solution code as well, which I often look at when I’m stuck, so that I can work my way backwards to understand the steps needed to reach a solution.
Though I’ve haven’t finished it yet, it has significantly boosted my knowledge in nlp and neural networks so far.
However it’s been tough. Really tough. At one point, I realized I needed help from someone better than me, so I came in touch with a Ph.D student who was willing to help me out for 40 USD per hour, both with the problem sets as well as the overall understanding. This has been critical for me in order to move on, as he has uncovered a lot of black holes in my knowledge.
In addition to this, Xeneta also hired a data scientist recently. He’s got a masters degree in math, so I often ask him for help when I’m stuck with various linear algebra an calculus tasks, or ml in general. So be sure to check out which resources you have internally in your company as well.
After doing all this, I finally felt ready to do a ml project at work. It basically involved training an algorithm to qualify sales leads by reading company descriptions, and has actually proven to be a big time saver for the sales guys using the tool.
Check out out article I wrote about it below or head over to GitHub to dive straight into the code.
Getting to this point has surely been a long journey. But also a fast one; when I started my machine learning in a week project, I certainly didn’t have any hopes of actually using it professionally within a year.
But it’s 100 percent possible. And if I can do it, so can anybody else.
Thanks for reading! My name is Per, I’m a co-founder of Scrimba — a better way to teach and learn code.
If you’ve read this far, I’d recommend you to check out this demo!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder of Scrimba, the next-generation platform for teaching and learning code. https://scrimba.com.
A publication about improving your technical skills.
",follow article wrote last year machine learning week kickstarted way machine learning ml devoting five days subject highly effective introduction continued learning spare time almost exactly one year later first ml project work involved using various ml natural language processing nlp techniques qualify sales leads xeneta felt like blessing getting paid something normally fun also ripped delusion people masters degrees phds work ml professionally post want share journey might inspire others interest ml stems back 2014 started reading articles hacker news simply found idea teaching machines stuff looking data appealing time wasnt even professional developer hobby coder whod done couple small projects began watching first chapters udacitys supervised learning course also reading articles came across subject gave little bit conceptual understanding though practical skills also didnt finish rarely moocs january 2015 joined founders coders fac bootcamp london order become developer weeks wanted learn actually code machine learning algorithms started study group peers every tuesday evening wed watch lectures courseras machine learning course fantastic course learned hell lot tough beginner watch lectures grasping concepts octave coding task challenging well especially dont know octave result difficulty one one fell study group weeks passed eventually fell well hindsight started course either used ml libraries coding tasks opposed building algorithms scratch least used programming language knew could go back time id choose udacitys intro machine learning easier uses python scikit learn way would gotten hands dirty soon possible gained confidence fun one last things fac ml week stunt goal able apply machine learning actual problems end week managed throughout week following far steepest ml learning curve ive ever experienced go ahead read article want detailed overview finished fac london moved back norway tried repeat success ml week neural networks instead failed simply many distractions spend 10 hours coding learning every day underestimated important surrounded peers fac however got started neural nets least slowly started grasp concept july managed code first net probably crappiest implementation ever created actually find embarrassing show trick proved understood concepts like backpropagation gradient descent second half year progression slowed started new job important takeaway period leap nonvectorized vectorized implementations neural networks involved repeating linear algebra university end year wrote article summary learned christmas vacation 2015 got motivational boost decided try kaggle spent quite time experimenting various algorithms homesite quote conversion otto group product classification bike sharing demand contests main takeaway experience iteratively improving results experimenting algorithms data learned trust logic machine learning tweaking parameter engineering new feature seems like good idea logically quite likely actually help back work january 2016 wanted continue flow id gotten christmas asked manager could spend time learning stuff work hours well happily approved gotten basic understanding neural networks point wanted move deep learning first attempt udacitys deep learning course ended big disappointment contents video lectures good short shallow ipython notebook assignments ended frustrating spent time debugging code errors effective way kill motivation couple sessions work simply gave defense im total noob comes ipython notebooks might bad might simply wasnt ready course luckily discovered stanfords cs224d decided give shot fantastic course though difficult never end debugging problem sets secondly actually give solution code well often look im stuck work way backwards understand steps needed reach solution though ive havent finished yet significantly boosted knowledge nlp neural networks far however tough really tough one point realized needed help someone better came touch phd student willing help 40 usd per hour problem sets well overall understanding critical order move uncovered lot black holes knowledge addition xeneta also hired data scientist recently hes got masters degree math often ask help im stuck various linear algebra calculus tasks ml general sure check resources internally company well finally felt ready ml project work basically involved training algorithm qualify sales leads reading company descriptions actually proven big time saver sales guys using tool check article wrote head github dive straight code getting point surely long journey also fast one started machine learning week project certainly didnt hopes actually using professionally within year 100 percent possible anybody else thanks reading name per im cofounder scrimba better way teach learn code youve read far id recommend check demo quick cheer standing ovation clap show much enjoyed story cofounder scrimba nextgeneration platform teaching learning code httpsscrimbacom publication improving technical skills,en,"['nlp', 'Hacker News', 'Udacity', 'Coursera', 'FAC', 'Kaggle', 'Otto Group Product Classification', 'IPython Notebooks', 'Stanford', 'Xeneta', 'GitHub']"
56,Tim Anglade,7000,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native","The HBO show Silicon Valley released a real AI app that identifies hotdogs — and not hotdogs — like the one shown on season 4’s 4th episode (the app is now available on Android as well as iOS!)
To achieve this, we designed a bespoke neural architecture that runs directly on your phone, and trained it with Tensorflow, Keras & Nvidia GPUs.
While the use-case is farcical, the app is an approachable example of both deep learning, and edge computing. All AI work is powered 100% by the user’s device, and images are processed without ever leaving their phone. This provides users with a snappier experience (no round trip to the cloud), offline availability, and better privacy. This also allows us to run the app at a cost of $0, even under the load of a million users, providing significant savings compared to traditional cloud-based AI approaches.
The app was developed in-house by the show, by a single developer, running on a single laptop & attached GPU, using hand-curated data. In that respect, it may provide a sense of what can be achieved today, with a limited amount of time & resources, by non-technical companies, individual developers, and hobbyists alike. In that spirit, this article attempts to give a detailed overview of steps involved to help others build their own apps.
If you haven’t seen the show or tried the app (you should!), the app lets you snap a picture and then tells you whether it thinks that image is of a hotdog or not. It’s a straightforward use-case, that pays homage to recent AI research and applications, in particular ImageNet.
While we’ve probably dedicated more engineering resources to recognizing hotdogs than anyone else, the app still fails in horrible and/or subtle ways.
Conversely, it’s also sometimes able to recognize hotdogs in complex situations... According to Engadget, “It’s incredible. I’ve had more success identifying food with the app in 20 minutes than I have had tagging and identifying songs with Shazam in the past two years.”
Have you ever found yourself reading Hacker News, thinking “they raised a 10M series A for that? I could build it in one weekend!” This app probably feels a lot like that, and the initial prototype was indeed built in a single weekend using Google Cloud Platform’s Vision API, and React Native. But the final app we ended up releasing on the app store required months of additional (part-time) work, to deliver meaningful improvements that would be difficult for an outsider to appreciate. We spent weeks optimizing overall accuracy, training time, inference time, iterating on our setup & tooling so we could have a faster development iterations, and spent a whole weekend optimizing the user experience around iOS & Android permissions (don’t even get me started on that one).
All too often technical blog posts or academic papers skip over this part, preferring to present the final chosen solution. In the interest of helping others learn from our mistake & choices, we will present an abridged view of the approaches that didn’t work for us, before we describe the final architecture we ended up shipping in the next section.
We chose React Native to build the prototype as it would give us an easy sandbox to experiment with, and would help us quickly support many devices. The experience ended up being a good one and we kept React Native for the remainder of the project: it didn’t always make things easy, and the design for the app was purposefully limited, but in the end React Native got the job done.
The other main component we used for the prototype — Google Cloud’s Vision API was quickly abandoned. There were 3 main factors:
For these reasons, we started experimenting with what’s trendily called “edge computing”, which for our purposes meant that after training our neural network on our laptop, we would export it and embed it directly into our mobile app, so that the neural network execution phase (or inference) would run directly inside the user’s phone.
Through a chance encounter with Pete Warden of the TensorFlow team, we had become aware of its ability to run TensorFlow directly embedded on an iOS device, and started exploring that path. After React Native, TensorFlow became the second fixed part of our stack.
It only took a day of work to integrate TensorFlow’s Objective-C++ camera example in our React Native shell. It took slightly longer to use their transfer learning script, which helps you retrain the Inception architecture to deal with a more specific image problem. Inception is the name of a family of neural architectures built by Google to deal with image recognition problems. Inception is available “pre-trained” which means the training phase has been completed and the weights are set. Most often for image recognition networks, they have been trained on ImageNet, a dataset containing over 20,000 different types of objects (hotdogs are one of them). However, much like Google Cloud’s Vision API, ImageNet training rewards breadth as much as depth here, and out-of-the-box accuracy on a single one of the 20,000+ categories can be lacking. As such, retraining (also called “transfer learning”) aims to take a full-trained neural net, and retrain it to perform better on the specific problem you’d like to handle. This usually involves some degree of “forgetting”, either by excising entire layers from the stack, or by slowly erasing the network’s ability to distinguish a type of object (e.g. chairs) in favor of better accuracy at recognizing the one you care about (i.e. hotdogs).
While the network (Inception in this case) may have been trained on the 14M images contained in ImageNet, we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition.
The big advantage of transfer learning are you will get better results much faster, and with less data than if you train from scratch. A full training might take months on multiple GPUs and require millions of images, while retraining can conceivably be done in hours on a laptop with a couple thousand images.
One of the biggest challenges we encountered was understanding exactly what should count as a hotdog and what should not. Defining what a “hotdog” is ends up being surprisingly difficult (do cut up sausages count, and if so, which kinds?) and subject to cultural interpretation.
Similarly, the “open world” nature of our problem meant we had to deal with an almost infinite number of inputs. While certain computer-vision problems have relatively limited inputs (say, x-rays of bolts with or without a mechanical default), we had to prepare the app to be fed selfies, nature shots and any number of foods.
Suffice to say, this approach was promising, and did lead to some improved results, however, it had to be abandoned for a couple of reasons.
First The nature of our problem meant a strong imbalance in training data: there are many more examples of things that are not hotdogs, than things that are hotdogs. In practice this means that if you train your algorithm on 3 hotdog images and 97 non-hotdog images, and it recognizes 0% of the former but 100% of the latter, it will still score 97% accuracy by default! This was not straightforward to solve out of the box using TensorFlow’s retrain tool, and basically necessitated setting up a deep learning model from scratch, import weights, and train in a more controlled manner.
At this point we decided to bite the bullet and get something started with Keras, a deep learning library that provides nicer, easier-to-use abstractions on top of TensorFlow, including pretty awesome training tools, and a class_weights option which is ideal to deal with this sort of dataset imbalance we were dealing with.
We used that opportunity to try other popular neural architectures like VGG, but one problem remained. None of them could comfortably fit on an iPhone. They consumed too much memory, which led to app crashes, and would sometime takes up to 10 seconds to compute, which was not ideal from a UX standpoint. Many things were attempted to mitigate that, but in the end it these architectures were just too big to run efficiently on mobile.
To give you a context out of time, this was roughly the mid-way point of the project. By that time, the UI was 90%+ done and very little of it was going to change. But in hindsight, the neural net was at best 20% done. We had a good sense of challenges & a good dataset, but 0 lines of the final neural architecture had been written, none of our neural code could reliably run on mobile, and even our accuracy was going to improve drastically in the weeks to come.
The problem directly ahead of us was simple: if Inception and VGG were too big, was there a simpler, pre-trained neural network we could retrain? At the suggestion of the always excellent Jeremy P. Howard (where has that guy been all our life?), we explored Xception, Enet and SqueezeNet. We quickly settled on SqueezeNet due to its explicit positioning as a solution for embedded deep learning, and the availability of a pre-trained Keras model on GitHub (yay open-source).
So how big of a difference does this make? An architecture like VGG uses about 138 million parameters (essentially the number of numbers necessary to model the neurons and values between them). Inception is already a massive improvement, requiring only 23 million parameters. SqueezeNet, in comparison only requires 1.25 million.
This has two advantages:
There are tradeoffs of course:
During this phase, we started experimenting with tuning the neural network architecture. In particular, we started using Batch Normalization and trying different activation functions.
After adding Batch Normalization and ELU to SqueezeNet, we were able to train neural network that achieve 90%+ accuracy when training from scratch, however, they were relatively brittle meaning the same network would overfit in some cases, or underfit in others when confronted to real-life testing. Even adding more examples to the dataset and playing with data augmentation failed to deliver a network that met expectations.
So while this phase was promising, and for the first time gave us a functioning app that could work entirely on an iPhone, in less than a second, we eventually moved to our 4th & final architecture.
Our final architecture was spurred in large part by the publication on April 17 of Google’s MobileNets paper, promising a new neural architecture with Inception-like accuracy on simple problems like ours, with only 4M or so parameters. This meant it sat in an interesting sweet spot between a SqueezeNet that had maybe been overly simplistic for our purposes, and the possibly overwrought elephant-trying-to-squeeze-in-a-tutu of using Inception or VGG on Mobile. The paper introduced some capacity to tune the size & complexity of network specifically to trade memory/CPU consumption against accuracy, which was very much top of mind for us at the time.
With less than a month to go before the app had to launch we endeavored to reproduce the paper’s results. This was entirely anticlimactic as within a day of the paper being published a Keras implementation was already offered publicly on GitHub by Refik Can Malli, a student at Istanbul Technical University, whose work we had already benefitted from when we took inspiration from his excellent Keras SqueezeNet implementation. The depth & openness of the deep learning community, and the presence of talented minds like R.C. is what makes deep learning viable for applications today — but they also make working in this field more thrilling than any tech trend we’ve been involved with.
Our final architecture ended up making significant departures from the MobileNets architecture or from convention, in particular:
So how does this stack work exactly? Deep Learning often gets a bad rap for being a “black box”, and while it’s true many components of it can be mysterious, the networks we use often leak information about how some of their magic work. We can look at the layers of this stack and how they activate on specific input images, giving us a sense of each layer’s ability to recognize sausage, buns, or other particularly salient hotdog features.
Data quality was of the utmost importance. A neural network can only be as good as the data that trained it, and improving training set quality was probably one of the top 3 things we spent time on during this project. The key things we did to improve this were:
The final composition of our dataset was 150k images, of which only 3k were hotdogs: there are only so many hotdogs you can look at, but there are many not hotdogs to look at. The 49:1 imbalance was dealt with by saying a Keras class weight of 49:1 in favor of hotdogs. Of the remaining 147k images, most were of food, with just 3k photos of non-food items, to help the network generalize a bit more and not get tricked into seeing a hotdog if presented with an image of a human in a red outfit.
Our data augmentation rules were as follows:
These numbers were derived intuitively, based on experiments and our understanding of the real-life usage of our app, as opposed to careful experimentation.
The final key to our data pipeline was using Patrick Rodriguez’s multiprocess image data generator for Keras. While Keras does have a built-in multi-threaded and multiprocess implementation, we found Patrick’s library to be consistently faster in our experiments, for reasons we did not have time to investigate. This library cut our training time to a third of what it used to be.
The network was trained using a 2015 MacBook Pro and attached external GPU (eGPU), specifically an Nvidia GTX 980 Ti (we’d probably buy a 1080 Ti if we were starting today). We were able to train the network on batches of 128 images at a time. The network was trained for a total of 240 epochs, meaning we ran all 150k images through the network 240 times. This took about 80 hours.
We trained the network in 3 phases:
While learning rates were identified by running the linear experiment recommended by the CLR paper, they seem to intuitively make sense, in that the max for each phase is within a factor of 2 of the previous minimum, which is aligned with the industry standard recommendation of halving your learning rate if your accuracy plateaus during training.
In the interest of time we performed some training runs on a Paperspace P5000 instance running Ubuntu. In those cases, we were able to double the batch size, and found that optimal learning rates for each phase were roughly double as well.
Even having designed a relatively compact neural architecture, and having trained it to handle situations it may find in a mobile context, we had a lot of work left to make it run properly. Trying to run a top-of-the-line neural net architecture out of the box can quickly burns hundreds megabytes of RAM, which few mobile devices can spare today. Beyond network optimizations, it turns out the way you handle images or even load TensorFlow itself can have a huge impact on how quickly your network runs, how little RAM it uses, and how crash-free the experience will be for your users.
This was maybe the most mysterious part of this project. Relatively little information can be found about it, possibly due to the dearth of production deep learning applications running on mobile devices as of today. However, we must commend the Tensorflow team, and particularly Pete Warden, Andrew Harp and Chad Whipkey for the existing documentation and their kindness in answering our inquiries.
Instead of using TensorFlow on iOS, we looked at using Apple’s built-in deep learning libraries instead (BNNS, MPSCNN and later on, CoreML). We would have designed the network in Keras, trained it with TensorFlow, exported all the weight values, re-implemented the network with BNNS or MPSCNN (or imported it via CoreML), and loaded the parameters into that new implementation. However, the biggest obstacle was that these new Apple libraries are only available on iOS 10+, and we wanted to support older versions of iOS. As iOS 10+ adoption and these frameworks continue to improve, there may not be a case for using TensorFlow on device in the near future.
If you think injecting JavaScript into your app on the fly is cool, try injecting neural nets into your app! The last production trick we used was to leverage CodePush and Apple’s relatively permissive terms of service, to live-inject new versions of our neural networks after submission to the app store. While this was mostly done to help us quickly deliver accuracy improvements to our users after release, you could conceivably use this approach to drastically expand or alter the feature set of your app without going through an app store review again.
There are a lot of things that didn’t work or we didn’t have time to do, and these are the ideas we’d investigate in the future:
Finally, we’d be remiss not to mention the obvious and important influence of User Experience, Developer Experience and built-in biases in developing an AI app. Each probably deserve their own post (or their own book) but here are the very concrete impacts of these 3 things in our experience.
UX (User Experience) is arguably more critical at every stage of the development of an AI app than for a traditional application. There are no Deep Learning algorithms that will give you perfect results right now, but there are many situations where the right mix of Deep Learning + UX will lead to results that are indistinguishable from perfect. Proper UX expectations are irreplaceable when it comes to setting developers on the right path to design their neural networks, setting the proper expectations for users when they use the app, and gracefully handling the inevitable AI failures. Building AI apps without a UX-first mindset is like training a neural net without Stochastic Gradient Descent: you will end up stuck in the local minima of the Uncanny Valley on your way to building the perfect AI use-case.
DX (Developer Experience) is extremely important as well, because deep learning training time is the new horsing around while waiting for your program to compile. We suggest you heavily favor DX first (hence Keras), as it’s always possible to optimize runtime for later runs (manual GPU parallelization, multi-process data augmentation, TensorFlow pipeline, even re-implementing for caffe2 / pyTorch).
Even projects with relatively obtuse APIs & documentation like TensorFlow greatly improve DX by providing a highly-tested, highly-used, well-maintained environment for training & running neural networks.
For the same reason, it’s hard to beat both the cost as well as the flexibility of having your own local GPU for development. Being able to look at / edit images locally, edit code with your preferred tool without delays greatly improves the development quality & speed of building AI projects.
Most AI apps will hit more critical cultural biases than ours, but as an example, even our straightforward use-case, caught us flat-footed with built-in biases in our initial dataset, that made the app unable to recognize French-style hotdogs, Asian hotdogs, and more oddities we did not have immediate personal experience with. It’s critical to remember that AI do not make “better” decisions than humans — they are infected by the same human biases we fall prey to, via the training sets humans provide.
Thanks to: Mike Judge, Alec Berg, Clay Tarver, Todd Silverstein, Jonathan Dotan, Lisa Schomas, Amy Solomon, Dorothy Street & Rich Toyon, and all the writers of the show — the app would simply not exist without them.Meaghan, Dana, David, Jay, and everyone at HBO. Scale Venture Partners & GitLab. Rachel Thomas and Jeremy Howard & Fast AI for all that they have taught me, and for kindly reviewing a draft of this post. Check out their free online Deep Learning course, it’s awesome! JP Simard for his help on iOS. And finally, the TensorFlow team & r/MachineLearning for their help & inspiration.
... And thanks to everyone who used & shared the app! It made staring at pictures of hotdogs for months on end totally worth it 😅
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
A.I., Startups & HBO’s Silicon Valley. Get in touch: timanglade@gmail.com
",hbo show silicon valley released real ai app identifies hotdogs hotdogs like one shown season 4s 4th episode app available android well ios achieve designed bespoke neural architecture runs directly phone trained tensorflow keras nvidia gpus usecase farcical app approachable example deep learning edge computing ai work powered 100 users device images processed without ever leaving phone provides users snappier experience round trip cloud offline availability better privacy also allows us run app cost 0 even load million users providing significant savings compared traditional cloudbased ai approaches app developed inhouse show single developer running single laptop attached gpu using handcurated data respect may provide sense achieved today limited amount time resources nontechnical companies individual developers hobbyists alike spirit article attempts give detailed overview steps involved help others build apps havent seen show tried app app lets snap picture tells whether thinks image hotdog straightforward usecase pays homage recent ai research applications particular imagenet weve probably dedicated engineering resources recognizing hotdogs anyone else app still fails horrible andor subtle ways conversely also sometimes able recognize hotdogs complex situations according engadget incredible ive success identifying food app 20 minutes tagging identifying songs shazam past two years ever found reading hacker news thinking raised 10m series could build one weekend app probably feels lot like initial prototype indeed built single weekend using google cloud platforms vision api react native final app ended releasing app store required months additional parttime work deliver meaningful improvements would difficult outsider appreciate spent weeks optimizing overall accuracy training time inference time iterating setup tooling could faster development iterations spent whole weekend optimizing user experience around ios android permissions dont even get started one often technical blog posts academic papers skip part preferring present final chosen solution interest helping others learn mistake choices present abridged view approaches didnt work us describe final architecture ended shipping next section chose react native build prototype would give us easy sandbox experiment would help us quickly support many devices experience ended good one kept react native remainder project didnt always make things easy design app purposefully limited end react native got job done main component used prototype google clouds vision api quickly abandoned 3 main factors reasons started experimenting whats trendily called edge computing purposes meant training neural network laptop would export embed directly mobile app neural network execution phase inference would run directly inside users phone chance encounter pete warden tensorflow team become aware ability run tensorflow directly embedded ios device started exploring path react native tensorflow became second fixed part stack took day work integrate tensorflows objectivec camera example react native shell took slightly longer use transfer learning script helps retrain inception architecture deal specific image problem inception name family neural architectures built google deal image recognition problems inception available pretrained means training phase completed weights set often image recognition networks trained imagenet dataset containing 20000 different types objects hotdogs one however much like google clouds vision api imagenet training rewards breadth much depth outofthebox accuracy single one 20000 categories lacking retraining also called transfer learning aims take fulltrained neural net retrain perform better specific problem youd like handle usually involves degree forgetting either excising entire layers stack slowly erasing networks ability distinguish type object eg chairs favor better accuracy recognizing one care ie hotdogs network inception case may trained 14m images contained imagenet able retrain thousand hotdog images get drastically enhanced hotdog recognition big advantage transfer learning get better results much faster less data train scratch full training might take months multiple gpus require millions images retraining conceivably done hours laptop couple thousand images one biggest challenges encountered understanding exactly count hotdog defining hotdog ends surprisingly difficult cut sausages count kinds subject cultural interpretation similarly open world nature problem meant deal almost infinite number inputs certain computervision problems relatively limited inputs say xrays bolts without mechanical default prepare app fed selfies nature shots number foods suffice say approach promising lead improved results however abandoned couple reasons first nature problem meant strong imbalance training data many examples things hotdogs things hotdogs practice means train algorithm 3 hotdog images 97 nonhotdog images recognizes 0 former 100 latter still score 97 accuracy default straightforward solve box using tensorflows retrain tool basically necessitated setting deep learning model scratch import weights train controlled manner point decided bite bullet get something started keras deep learning library provides nicer easiertouse abstractions top tensorflow including pretty awesome training tools class_weights option ideal deal sort dataset imbalance dealing used opportunity try popular neural architectures like vgg one problem remained none could comfortably fit iphone consumed much memory led app crashes would sometime takes 10 seconds compute ideal ux standpoint many things attempted mitigate end architectures big run efficiently mobile give context time roughly midway point project time ui 90 done little going change hindsight neural net best 20 done good sense challenges good dataset 0 lines final neural architecture written none neural code could reliably run mobile even accuracy going improve drastically weeks come problem directly ahead us simple inception vgg big simpler pretrained neural network could retrain suggestion always excellent jeremy p howard guy life explored xception enet squeezenet quickly settled squeezenet due explicit positioning solution embedded deep learning availability pretrained keras model github yay opensource big difference make architecture like vgg uses 138 million parameters essentially number numbers necessary model neurons values inception already massive improvement requiring 23 million parameters squeezenet comparison requires 125 million two advantages tradeoffs course phase started experimenting tuning neural network architecture particular started using batch normalization trying different activation functions adding batch normalization elu squeezenet able train neural network achieve 90 accuracy training scratch however relatively brittle meaning network would overfit cases underfit others confronted reallife testing even adding examples dataset playing data augmentation failed deliver network met expectations phase promising first time gave us functioning app could work entirely iphone less second eventually moved 4th final architecture final architecture spurred large part publication april 17 googles mobilenets paper promising new neural architecture inceptionlike accuracy simple problems like 4m parameters meant sat interesting sweet spot squeezenet maybe overly simplistic purposes possibly overwrought elephanttryingtosqueezeinatutu using inception vgg mobile paper introduced capacity tune size complexity network specifically trade memorycpu consumption accuracy much top mind us time less month go app launch endeavored reproduce papers results entirely anticlimactic within day paper published keras implementation already offered publicly github refik malli student istanbul technical university whose work already benefitted took inspiration excellent keras squeezenet implementation depth openness deep learning community presence talented minds like rc makes deep learning viable applications today also make working field thrilling tech trend weve involved final architecture ended making significant departures mobilenets architecture convention particular stack work exactly deep learning often gets bad rap black box true many components mysterious networks use often leak information magic work look layers stack activate specific input images giving us sense layers ability recognize sausage buns particularly salient hotdog features data quality utmost importance neural network good data trained improving training set quality probably one top 3 things spent time project key things improve final composition dataset 150k images 3k hotdogs many hotdogs look many hotdogs look 491 imbalance dealt saying keras class weight 491 favor hotdogs remaining 147k images food 3k photos nonfood items help network generalize bit get tricked seeing hotdog presented image human red outfit data augmentation rules follows numbers derived intuitively based experiments understanding reallife usage app opposed careful experimentation final key data pipeline using patrick rodriguezs multiprocess image data generator keras keras builtin multithreaded multiprocess implementation found patricks library consistently faster experiments reasons time investigate library cut training time third used network trained using 2015 macbook pro attached external gpu egpu specifically nvidia gtx 980 ti wed probably buy 1080 ti starting today able train network batches 128 images time network trained total 240 epochs meaning ran 150k images network 240 times took 80 hours trained network 3 phases learning rates identified running linear experiment recommended clr paper seem intuitively make sense max phase within factor 2 previous minimum aligned industry standard recommendation halving learning rate accuracy plateaus training interest time performed training runs paperspace p5000 instance running ubuntu cases able double batch size found optimal learning rates phase roughly double well even designed relatively compact neural architecture trained handle situations may find mobile context lot work left make run properly trying run topoftheline neural net architecture box quickly burns hundreds megabytes ram mobile devices spare today beyond network optimizations turns way handle images even load tensorflow huge impact quickly network runs little ram uses crashfree experience users maybe mysterious part project relatively little information found possibly due dearth production deep learning applications running mobile devices today however must commend tensorflow team particularly pete warden andrew harp chad whipkey existing documentation kindness answering inquiries instead using tensorflow ios looked using apples builtin deep learning libraries instead bnns mpscnn later coreml would designed network keras trained tensorflow exported weight values reimplemented network bnns mpscnn imported via coreml loaded parameters new implementation however biggest obstacle new apple libraries available ios 10 wanted support older versions ios ios 10 adoption frameworks continue improve may case using tensorflow device near future think injecting javascript app fly cool try injecting neural nets app last production trick used leverage codepush apples relatively permissive terms service liveinject new versions neural networks submission app store mostly done help us quickly deliver accuracy improvements users release could conceivably use approach drastically expand alter feature set app without going app store review lot things didnt work didnt time ideas wed investigate future finally wed remiss mention obvious important influence user experience developer experience builtin biases developing ai app probably deserve post book concrete impacts 3 things experience ux user experience arguably critical every stage development ai app traditional application deep learning algorithms give perfect results right many situations right mix deep learning ux lead results indistinguishable perfect proper ux expectations irreplaceable comes setting developers right path design neural networks setting proper expectations users use app gracefully handling inevitable ai failures building ai apps without uxfirst mindset like training neural net without stochastic gradient descent end stuck local minima uncanny valley way building perfect ai usecase dx developer experience extremely important well deep learning training time new horsing around waiting program compile suggest heavily favor dx first hence keras always possible optimize runtime later runs manual gpu parallelization multiprocess data augmentation tensorflow pipeline even reimplementing caffe2 pytorch even projects relatively obtuse apis documentation like tensorflow greatly improve dx providing highlytested highlyused wellmaintained environment training running neural networks reason hard beat cost well flexibility local gpu development able look edit images locally edit code preferred tool without delays greatly improves development quality speed building ai projects ai apps hit critical cultural biases example even straightforward usecase caught us flatfooted builtin biases initial dataset made app unable recognize frenchstyle hotdogs asian hotdogs oddities immediate personal experience critical remember ai make better decisions humans infected human biases fall prey via training sets humans provide thanks mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street rich toyon writers show app would simply exist without themmeaghan dana david jay everyone hbo scale venture partners gitlab rachel thomas jeremy howard fast ai taught kindly reviewing draft post check free online deep learning course awesome jp simard help ios finally tensorflow team rmachinelearning help inspiration thanks everyone used shared app made staring pictures hotdogs months end totally worth quick cheer standing ovation clap show much enjoyed story ai startups hbos silicon valley get touch timangladegmailcom,en,"['HBO', 'app', 'Android', 'Tensorflow, Keras & Nvidia', 'GPU', 'time & resources', 'ImageNet', 'Hacker News', 'Google Cloud Platform’s', 'Vision API', 'React Native', 'iOS & Android', 'TensorFlow', 'Google', 'Keras', 'VGG', 'iPhone', 'UI', 'SqueezeNet', 'GitHub', 'Batch Normalization', 'ELU', 'Istanbul Technical University', 'MobileNets', 'MacBook Pro', 'CLR', 'Ubuntu', 'RAM', 'Apple', 'MPSCNN', 'Deep Learning + UX', 'DX', 'Dorothy Street & Rich Toyon', 'Scale Venture Partners & GitLab', 'Jeremy Howard & Fast AI', 'TensorFlow team & r/MachineLearning', '&', 'A.I.', 'Startups & HBO']"
57,Dhruv Parthasarathy,4300,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,"At Athelas, we use Convolutional Neural Networks(CNNs) for a lot more than just classification! In this post, we’ll see how CNNs can be used, with great results, in image instance segmentation.
Ever since Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!
While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.
In classification, there’s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.
We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!
Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.
Through this post, we’ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they’ve evolved from one implementation to the next. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, we’ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:
Inspired by the research of Hinton’s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:
Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name we’ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevsky’s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,
Let’s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.
Understanding R-CNN
The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.
But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - propose a bunch of boxes in the image and see if any of them actually correspond to an object.
R-CNN creates these bounding boxes, or region proposals, using a process called Selective Search which you can read about here. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.
Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.
On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.
Improving the Bounding Boxes
Now, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:
So, to summarize, R-CNN is just the following steps:
R-CNN works really well, but is really quite slow for a few simple reasons:
In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Let’s now go over its main insights.
Fast R-CNN Insight 1: RoI (Region of Interest) Pooling
For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple — Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?
This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!
Fast R-CNN Insight 2: Combine All Models into One Network
The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three.
You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:
Even with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process — the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using Selective Search, a fairly slow process that was found to be the bottleneck of the overall process.
In the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.
The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?
Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, only one CNN needs to be trained and we get region proposals almost for free! The authors write:
Here are the inputs and outputs of their model:
How the Regions are Generated
Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating what’s known as the Region Proposal Network.
The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting k potential bounding boxes and scores for how good each of those boxes is expected to be. What do these k boxes represent?
Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we won’t see many boxes that are very very thin. In such a way, we create k such common aspect ratios we call anchor boxes. For each such anchor box, we output one bounding box and score per position in the image.
With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:
We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.
So far, we’ve seen how we’ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.
Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as Mask R-CNN.
Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?
Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:
But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.
RoiAlign - Realigning RoIPool to be More Accurate
When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.
The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.
Imagine we have an image of size 128x128 and a feature map of size 25x25. Let’s imagine we want features the region corresponding to the top-left 15x15 pixels in the original image (see above). How might we select these pixels from the feature map?
We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 * 25/128 ~= 2.93 pixels.
In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, we avoid such rounding. Instead, we use bilinear interpolation to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.
Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:
If you’re interested in trying out these algorithms yourselves, here are relevant repositories:
Faster R-CNN
Mask R-CNN
In just 3 years, we’ve seen how the research community has progressed from Krizhevsky et. al’s original result to R-CNN, and finally all the way to such powerful results as Mask R-CNN. Seen in isolation, results like Mask R-CNN seem like incredible leaps of genius that would be unapproachable. Yet, through this post, I hope you’ve seen how such advancements are really the sum of intuitive, incremental improvements through years of hard work and collaboration. Each of the ideas proposed by R-CNN, Fast R-CNN, Faster R-CNN, and finally Mask R-CNN were not necessarily quantum leaps, yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight.
What particularly excites me, is that the time between R-CNN and Mask R-CNN was just three years! With continued funding, focus, and support, how much further can Computer Vision improve over the next three years?
If you see any errors or issues in this post, please contact me at dhruv@getathelas.com and I”ll immediately correct them!
If you’re interested in applying such techniques, come join us at Athelas where we apply Computer Vision to blood diagnostics daily:
Other posts we’ve written:
Thanks to Bharath Ramsundar, Pranav Ramkrishnan, Tanay Tandon, and Oliver Cameron for help with this post!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@dhruvp. VP Eng @Athelas. MIT Math and CS Undergrad ’13. MIT CS Masters ’14. Previously: Director of AI Programs @ Udacity.
Blood Diagnostics through Deep Learning http://athelas.com
",athelas use convolutional neural networkscnns lot classification post well see cnns used great results image instance segmentation ever since alex krizhevsky geoff hinton ilya sutskever imagenet 2012 convolutional neural networkscnns become gold standard image classification fact since cnns improved point outperform humans imagenet challenge results impressive image classification far simpler complexity diversity true human visual understanding classification theres generally image single object focus task say image see look world around us carry far complex tasks see complicated sights multiple overlapping objects different backgrounds classify different objects also identify boundaries differences relations one another cnns help us complex tasks namely given complicated image use cnns identify different objects image boundaries shown ross girshick peers last years answer conclusively yes post well cover intuition behind main techniques used object detection segmentation see theyve evolved one implementation next particular well cover rcnn regional cnn original application cnns problem along descendants fast rcnn faster rcnn finally well cover mask rcnn paper released recently facebook research extends object detection techniques provide pixel level segmentation papers referenced post inspired research hintons lab university toronto small team uc berkeley led professor jitendra malik asked today seems like inevitable question object detection task finding different objects image classifying seen image team comprised ross girshick name well see jeff donahue trevor darrel found problem solved krizhevskys results testing pascal voc challenge popular object detection challenge akin imagenet write lets take moment understand architecture regions cnns rcnn works understanding rcnn goal rcnn take image correctly identify main objects via bounding box image find bounding boxes rcnn might intuitively well propose bunch boxes image see actually correspond object rcnn creates bounding boxes region proposals using process called selective search read high level selective search shown image looks image windows different sizes size tries group together adjacent pixels texture color intensity identify objects proposals created rcnn warps region standard square size passes modified version alexnet winning submission imagenet 2012 inspired rcnn shown final layer cnn rcnn adds support vector machine svm simply classifies whether object object step 4 image improving bounding boxes found object box tighten box fit true dimensions object final step rcnn rcnn runs simple linear regression region proposal generate tighter bounding box coordinates get final result inputs outputs regression model summarize rcnn following steps rcnn works really well really quite slow simple reasons 2015 ross girshick first author rcnn solved problems leading second algorithm short history fast rcnn lets go main insights fast rcnn insight 1 roi region interest pooling forward pass cnn girshick realized image lot proposed regions image invariably overlapped causing us run cnn computation 2000 times insight simple run cnn per image find way share computation across 2000 proposals exactly fast rcnn using technique known roipool region interest pooling core roipool shares forward pass cnn image across subregions image notice cnn features region obtained selecting corresponding region cnns feature map features region pooled usually using max pooling takes us one pass original image opposed 2000 fast rcnn insight 2 combine models one network second insight fast rcnn jointly train cnn classifier bounding box regressor single model earlier different models extract image features cnn classify svm tighten bounding boxes regressor fast rcnn instead used single network compute three see done image fast rcnn replaced svm classifier softmax layer top cnn output classification also added linear regression layer parallel softmax layer output bounding box coordinates way outputs needed came one single network inputs outputs overall model even advancements still one remaining bottleneck fast rcnn process region proposer saw first step detecting locations objects generating bunch potential bounding boxes regions interest test fast rcnn proposals created using selective search fairly slow process found bottleneck overall process middle 2015 team microsoft research composed shaoqing ren kaiming ross girshick jian sun found way make region proposal step almost cost free architecture creatively named faster rcnn insight faster rcnn region proposals depended features image already calculated forward pass cnn first step classification reuse cnn results region proposals instead running separate selective search algorithm indeed faster rcnn team achieved image see single cnn used carry region proposals classification way one cnn needs trained get region proposals almost free authors write inputs outputs model regions generated lets take moment see faster rcnn generates region proposals cnn features faster rcnn adds fully convolutional network top features cnn creating whats known region proposal network region proposal network works passing sliding window cnn feature map window outputting k potential bounding boxes scores good boxes expected k boxes represent intuitively know objects image fit certain common aspect ratios sizes instance know want rectangular boxes resemble shapes humans likewise know wont see many boxes thin way create k common aspect ratios call anchor boxes anchor box output one bounding box score per position image anchor boxes mind lets take look inputs outputs region proposal network pass bounding box likely object fast rcnn generate classification tightened bounding boxes far weve seen weve able use cnn features many interesting ways effectively locate different objects image bounding boxes extend techniques go one step locate exact pixels object instead bounding boxes problem known image segmentation kaiming team researchers including girshick explored facebook ai using architecture known mask rcnn much like fast rcnn faster rcnn mask rcnns underlying intuition straight forward given faster rcnn works well object detection could extend also carry pixel level segmentation mask rcnn adding branch faster rcnn outputs binary mask says whether given pixel part object branch white image fully convolutional network top cnn based feature map inputs outputs mask rcnn authors make one small adjustment make pipeline work expected roialign realigning roipool accurate run without modifications original faster rcnn architecture mask rcnn authors realized regions feature map selected roipool slightly misaligned regions original image since image segmentation requires pixel level specificity unlike bounding boxes naturally led inaccuracies authors able solve problem cleverly adjusting roipool precisely aligned using method known roialign imagine image size 128x128 feature map size 25x25 lets imagine want features region corresponding topleft 15x15 pixels original image see might select pixels feature map know pixel original image corresponds 25128 pixels feature map select 15 pixels original image select 15 25128 293 pixels roipool would round select 2 pixels causing slight misalignment however roialign avoid rounding instead use bilinear interpolation get precise idea would pixel 293 high level allows us avoid misalignments caused roipool masks generated mask rcnn combines classifications bounding boxes faster rcnn generate wonderfully precise segmentations youre interested trying algorithms relevant repositories faster rcnn mask rcnn 3 years weve seen research community progressed krizhevsky et als original result rcnn finally way powerful results mask rcnn seen isolation results like mask rcnn seem like incredible leaps genius would unapproachable yet post hope youve seen advancements really sum intuitive incremental improvements years hard work collaboration ideas proposed rcnn fast rcnn faster rcnn finally mask rcnn necessarily quantum leaps yet sum products led really remarkable results bring us closer human level understanding sight particularly excites time rcnn mask rcnn three years continued funding focus support much computer vision improve next three years see errors issues post please contact dhruvgetathelascom ill immediately correct youre interested applying techniques come join us athelas apply computer vision blood diagnostics daily posts weve written thanks bharath ramsundar pranav ramkrishnan tanay tandon oliver cameron help post quick cheer standing ovation clap show much enjoyed story dhruvp vp eng athelas mit math cs undergrad 13 mit cs masters 14 previously director ai programs udacity blood diagnostics deep learning httpathelascom,en,"['Ilya Sutskever', 'ImageNet', 'Convolutional Neural', 'CNN', 'Facebook Research', 'Hinton', 'the University of Toronto', 'UC Berkeley', 'the PASCAL VOC Challenge', 'Selective Search', 'Support Vector Machine', 'R-CNN', 'Microsoft Research', 'algorithm', 'the Region Proposal Network', 'The Region Proposal Network', 'this Region Proposal Network', 'Facebook AI', 'Computer Vision', 'VP Eng @Athelas', 'CS', 'MIT CS Masters', 'AI Programs @ Udacity']"
58,Sebastian Heinz,4400,A simple deep learning model for stock price prediction using TensorFlow,"For a recent hackathon that we did at STATWORX, some of our team members scraped minutely S&P 500 data from the Google Finance API. The data consisted of index as well as stock prices of the S&P’s 500 constituents. Having this data at hand, the idea of developing a deep learning model for predicting the S&P 500 index based on the 500 constituents prices one minute ago came immediately on my mind.
Playing around with the data and building the deep learning model with TensorFlow was fun and so I decided to write my first Medium.com story: a little TensorFlow tutorial on predicting S&P 500 stock prices. What you will read is not an in-depth tutorial, but more a high-level introduction to the important building blocks and concepts of TensorFlow models. The Python code I’ve created is not optimized for efficiency but understandability. The dataset I’ve used can be downloaded from here (40MB).
Our team exported the scraped stock data from our scraping server as a csv file. The dataset contains n = 41266 minutes of data ranging from April to August 2017 on 500 stocks as well as the total S&P 500 index price. Index and stocks are arranged in wide format.
The data was already cleaned and prepared, meaning missing stock and index prices were LOCF’ed (last observation carried forward), so that the file did not contain any missing values.
A quick look at the S&P time series using pyplot.plot(data['SP500']):
Note: This is actually the lead of the S&P 500 index, meaning, its value is shifted 1 minute into the future. This operation is necessary since we want to predict the next minute of the index and not the current minute.
The dataset was split into training and test data. The training data contained 80% of the total dataset. The data was not shuffled but sequentially sliced. The training data ranges from April to approx. end of July 2017, the test data ends end of August 2017.
There are a lot of different approaches to time series cross validation, such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling. The latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values.
Most neural network architectures benefit from scaling the inputs (sometimes also the output). Why? Because most common activation functions of the network’s neurons such as tanh or sigmoid are defined on the [-1, 1] or [0, 1] interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets anyway. Scaling can be easily accomplished in Python using sklearn’s MinMaxScaler.
Remark: Caution must be undertaken regarding what part of the data is scaled and when. A common mistake is to scale the whole dataset before training and test split are being applied. Why is this a mistake? Because scaling invokes the calculation of statistics e.g. the min/max of a variable. When performing time series forecasting in real life, you do not have information from future observations at the time of forecasting. Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data. Otherwise, you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction.
TensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework. It is based on a C++ low level backend but is usually controlled via Python (there is also a neat TensorFlow library for R, maintained by RStudio). TensorFlow operates on a graph representation of the underlying computational task. This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. Check out this simple example (stolen from our deep learning introduction from our blog):
In the figure above, two numbers are supposed to be added. Those numbers are stored in two variables, a and b. The two values are flowing through the graph and arrive at the square node, where they are being added. The result of the addition is stored into another variable, c. Actually, a, b and c can be considered as placeholders. Any numbers that are fed into a and b get added and are stored into c. This is exactly how TensorFlow works. The user defines an abstract representation of the model (neural network) through placeholders and variables. Afterwards, the placeholders get ""filled"" with real data and the actual computations take place. The following code implements the toy example from above in TensorFlow:
After having imported the TensorFlow library, two placeholders are defined using tf.placeholder(). They correspond to the two blue circles on the left of the image above. Afterwards, the mathematical addition is defined via tf.add(). The result of the computation is c = 9. With placeholders set up, the graph can be executed with any integer value for a and b. Of course, the former problem is just a toy example. The required graphs and computations in a neural network are much more complex.
As mentioned before, it all starts with placeholders. We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1).
The shape of the placeholders correspond to [None, n_stocks] with [None] meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly.
The None argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch, so we keep if flexible. We will later define the variable batch_size that controls the number of observations per training batch.
Besides placeholders, variables are another cornerstone of the TensorFlow universe. While placeholders are used to store input and target data in the graph, variables are used as flexible containers within the graph that are allowed to change during graph execution. Weights and biases are represented as variables in order to adapt during training. Variables need to be initialized, prior to model training. We will get into that a litte later in more detail.
The model consists of four hidden layers. The first layer contains 1024 neurons, slightly more than double the size of the inputs. Subsequent hidden layers are always half the size of the previous layer, which means 512, 256 and finally 128 neurons. A reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers. Of course, other network architectures and neuron configurations are possible but are out of scope for this introduction level article.
It is important to understand the required variable dimensions between input, hidden and output layers. As a rule of thumb in multilayer perceptrons (MLPs, the type of networks used here), the second dimension of the previous layer is the first dimension in the current layer for weight matrices. This might sound complicated but is essentially just each layer passing its output as input to the next layer. The biases dimension equals the second dimension of the current layer’s weight matrix, which corresponds the number of neurons in this layer.
After definition of the required weight and bias variables, the network topology, the architecture of the network, needs to be specified. Hereby, placeholders (data) and variables (weighs and biases) need to be combined into a system of sequential matrix multiplications.
Furthermore, the hidden layers of the network are transformed by activation functions. Activation functions are important elements of the network architecture since they introduce non-linearity to the system. There are dozens of possible activation functions out there, one of the most common is the rectified linear unit (ReLU) which will also be used in this model.
The image below illustrates the network architecture. The model consists of three major building blocks. The input layer, the hidden layers and the output layer. This architecture is called a feedforward network. Feedforward indicates that the batch of data solely flows from left to right. Other network architectures, such as recurrent neural networks, also allow data flowing “backwards” in the network.
The cost function of the network is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets. Basically, any differentiable function can be implemented in order to compute a deviation measure between predictions and targets.
However, the MSE exhibits certain properties that are advantageous for the general optimization problem to be solved.
The optimizer takes care of the necessary computations that are used to adapt the network’s weight and bias variables during training. Those computations invoke the calculation of so called gradients, that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network’s cost function. The development of stable and speedy optimizers is a major field in neural network an deep learning research.
Here the Adam Optimizer is used, which is one of the current default optimizers in deep learning development. Adam stands for “Adaptive Moment Estimation” and can be considered as a combination between two other popular optimizers AdaGrad and RMSProp.
Initializers are used to initialize the network’s variables before training. Since neural networks are trained using numerical optimization techniques, the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem. There are different initializers available in TensorFlow, each with different initialization approaches. Here, I use the tf.variance_scaling_initializer(), which is one of the default initialization strategies.
Note, that with TensorFlow it is possible to define multiple initialization functions for different variables within the graph. However, in most cases, a unified initialization is sufficient.
After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by minibatch training. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The training dataset gets divided into n / batch_size batches that are sequentially fed into the network. At this point the placeholders X and Y come into play. They store the input and target data and present them to the network as inputs and targets.
A sampled data batch of X flows through the network until it reaches the output layer. There, TensorFlow compares the models predictions against the actual observed targets Y in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the networks parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself. The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.
The training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies.
During the training, we evaluate the networks predictions on the test set — the data which is not learned, but set aside — for every 5th batch and visualize it. Additionally, the images are exported to disk and later combined into a video animation of the training process (see below). The model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs. Nice!
One can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data. This also corresponds to the Adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum. After 10 epochs, we have a pretty close fit to the test data! The final test MSE equals 0.00078 (it is very low, because the target is scaled). The mean absolute percentage error of the forecast on the test set is equal to 5.31% which is pretty good. Note, that this is just a fit to the test data, no actual out of sample metrics in a real world scenario.
Please note that there are tons of ways of further improving this result: design of layers and neurons, choosing different initialization and activation schemes, introduction of dropout layers of neurons, early stopping and so on. Furthermore, different types of deep learning models, such as recurrent neural networks might achieve better performance on this task. However, this is not the scope of this introductory post.
The release of TensorFlow was a landmark event in deep learning research. Its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ML algorithms. However, flexibility comes at the cost of longer time-to-model cycles compared to higher level APIs such as Keras or MxNet. Nonetheless, I am sure that TensorFlow will make its way to the de-facto standard in neural network and deep learning development in research and practical applications. Many of our customers are already using TensorFlow or start developing projects that employ TensorFlow models. Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. Let’s see what Google has planned for the future of TensorFlow. One thing that is missing, at least in my opinion, is a neat graphical user interface for designing and developing neural net architectures with TensorFlow backend. Maybe, this is something Google is already working on ;)
If you have any comments or questions on my first Medium story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice.
Update: I’ve added both the Python script as well as a (zipped) dataset to a Github repository. Feel free to clone and fork.
Lastly, follow me on: Twitter | LinkedIn
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO @ STATWORX. Doing data science, stats and ML for over a decade. Food, wine and cocktail enthusiast. Check our website: https://www.statworx.com
Highlights from Machine Learning Research, Projects and Learning Materials. From and For ML Scientists, Engineers an Enthusiasts.
",recent hackathon statworx team members scraped minutely sp 500 data google finance api data consisted index well stock prices sps 500 constituents data hand idea developing deep learning model predicting sp 500 index based 500 constituents prices one minute ago came immediately mind playing around data building deep learning model tensorflow fun decided write first mediumcom story little tensorflow tutorial predicting sp 500 stock prices read indepth tutorial highlevel introduction important building blocks concepts tensorflow models python code ive created optimized efficiency understandability dataset ive used downloaded 40mb team exported scraped stock data scraping server csv file dataset contains n 41266 minutes data ranging april august 2017 500 stocks well total sp 500 index price index stocks arranged wide format data already cleaned prepared meaning missing stock index prices locfed last observation carried forward file contain missing values quick look sp time series using pyplotplotdatasp500 note actually lead sp 500 index meaning value shifted 1 minute future operation necessary since want predict next minute index current minute dataset split training test data training data contained 80 total dataset data shuffled sequentially sliced training data ranges april approx end july 2017 test data ends end august 2017 lot different approaches time series cross validation rolling forecasts without refitting elaborate concepts time series bootstrap resampling latter involves repeated samples remainder seasonal decomposition time series order simulate samples follow seasonal pattern original time series exact copies values neural network architectures benefit scaling inputs sometimes also output common activation functions networks neurons tanh sigmoid defined 1 1 0 1 interval respectively nowadays rectified linear unit relu activations commonly used activations unbounded axis possible activation values however scale inputs targets anyway scaling easily accomplished python using sklearns minmaxscaler remark caution must undertaken regarding part data scaled common mistake scale whole dataset training test split applied mistake scaling invokes calculation statistics eg minmax variable performing time series forecasting real life information future observations time forecasting therefore calculation scaling statistics conducted training data must applied test data otherwise use future information time forecasting commonly biases forecasting metrics positive direction tensorflow great piece software currently leading deep learning neural network computation framework based c low level backend usually controlled via python also neat tensorflow library r maintained rstudio tensorflow operates graph representation underlying computational task approach allows user specify mathematical operations elements graph data variables operators since neural networks actually graphs data mathematical operations tensorflow perfect neural networks deep learning check simple example stolen deep learning introduction blog figure two numbers supposed added numbers stored two variables b two values flowing graph arrive square node added result addition stored another variable c actually b c considered placeholders numbers fed b get added stored c exactly tensorflow works user defines abstract representation model neural network placeholders variables afterwards placeholders get filled real data actual computations take place following code implements toy example tensorflow imported tensorflow library two placeholders defined using tfplaceholder correspond two blue circles left image afterwards mathematical addition defined via tfadd result computation c 9 placeholders set graph executed integer value b course former problem toy example required graphs computations neural network much complex mentioned starts placeholders need two placeholders order fit model x contains networks inputs stock prices sp 500 constituents time networks outputs index value sp 500 time 1 shape placeholders correspond none n_stocks none meaning inputs 2dimensional matrix outputs 1dimensional vector crucial understand input output dimensions neural net needs order design properly none argument indicates point yet know number observations flow neural net graph batch keep flexible later define variable batch_size controls number observations per training batch besides placeholders variables another cornerstone tensorflow universe placeholders used store input target data graph variables used flexible containers within graph allowed change graph execution weights biases represented variables order adapt training variables need initialized prior model training get litte later detail model consists four hidden layers first layer contains 1024 neurons slightly double size inputs subsequent hidden layers always half size previous layer means 512 256 finally 128 neurons reduction number neurons subsequent layer compresses information network identifies previous layers course network architectures neuron configurations possible scope introduction level article important understand required variable dimensions input hidden output layers rule thumb multilayer perceptrons mlps type networks used second dimension previous layer first dimension current layer weight matrices might sound complicated essentially layer passing output input next layer biases dimension equals second dimension current layers weight matrix corresponds number neurons layer definition required weight bias variables network topology architecture network needs specified hereby placeholders data variables weighs biases need combined system sequential matrix multiplications furthermore hidden layers network transformed activation functions activation functions important elements network architecture since introduce nonlinearity system dozens possible activation functions one common rectified linear unit relu also used model image illustrates network architecture model consists three major building blocks input layer hidden layers output layer architecture called feedforward network feedforward indicates batch data solely flows left right network architectures recurrent neural networks also allow data flowing backwards network cost function network used generate measure deviation networks predictions actual observed training targets regression problems mean squared error mse function commonly used mse computes average squared deviation predictions targets basically differentiable function implemented order compute deviation measure predictions targets however mse exhibits certain properties advantageous general optimization problem solved optimizer takes care necessary computations used adapt networks weight bias variables training computations invoke calculation called gradients indicate direction weights biases changed training order minimize networks cost function development stable speedy optimizers major field neural network deep learning research adam optimizer used one current default optimizers deep learning development adam stands adaptive moment estimation considered combination two popular optimizers adagrad rmsprop initializers used initialize networks variables training since neural networks trained using numerical optimization techniques starting point optimization problem one key factors find good solutions underlying problem different initializers available tensorflow different initialization approaches use tfvariance_scaling_initializer one default initialization strategies note tensorflow possible define multiple initialization functions different variables within graph however cases unified initialization sufficient defined placeholders variables initializers cost functions optimizers network model needs trained usually done minibatch training minibatch training random data samples n batch_size drawn training data fed network training dataset gets divided n batch_size batches sequentially fed network point placeholders x come play store input target data present network inputs targets sampled data batch x flows network reaches output layer tensorflow compares models predictions actual observed targets current batch afterwards tensorflow conducts optimization step updates networks parameters corresponding selected learning scheme updated weights biases next batch sampled process repeats procedure continues batches presented network one full sweep batches called epoch training network stops maximum number epochs reached another stopping criterion defined user applies training evaluate networks predictions test set data learned set aside every 5th batch visualize additionally images exported disk later combined video animation training process see model quickly learns shape und location time series test data able produce accurate prediction epochs nice one see networks rapidly adapts basic shape time series continues learn finer patterns data also corresponds adam learning scheme lowers learning rate model training order overshoot optimization minimum 10 epochs pretty close fit test data final test mse equals 000078 low target scaled mean absolute percentage error forecast test set equal 531 pretty good note fit test data actual sample metrics real world scenario please note tons ways improving result design layers neurons choosing different initialization activation schemes introduction dropout layers neurons early stopping furthermore different types deep learning models recurrent neural networks might achieve better performance task however scope introductory post release tensorflow landmark event deep learning research flexibility performance allows researchers develop kinds sophisticated neural network architectures well ml algorithms however flexibility comes cost longer timetomodel cycles compared higher level apis keras mxnet nonetheless sure tensorflow make way defacto standard neural network deep learning development research practical applications many customers already using tensorflow start developing projects employ tensorflow models also data science consultants statworx heavily using tensorflow deep learning neural net research development lets see google planned future tensorflow one thing missing least opinion neat graphical user interface designing developing neural net architectures tensorflow backend maybe something google already working comments questions first medium story feel free comment try answer also feel free use code share story peers social platforms choice update ive added python script well zipped dataset github repository feel free clone fork lastly follow twitter linkedin quick cheer standing ovation clap show much enjoyed story ceo statworx data science stats ml decade food wine cocktail enthusiast check website httpswwwstatworxcom highlights machine learning research projects learning materials ml scientists engineers enthusiasts,en,"['STATWORX', 'S&P', 'TensorFlow', 'Medium.com', 'Python', 'ReLU', 'fed', 'the Adam Optimizer', 'AdaGrad', 'RMSProp', 'n / batch_size', 'Keras', 'Google', 'Github', 'Machine Learning Research', 'Learning Materials']"
59,Blaise Aguera y Arcas,8700,Do algorithms reveal sexual orientation or just expose our stereotypes?,"by Blaise Agüera y Arcas, Alexander Todorov and Margaret Mitchell
A study claiming that artificial intelligence can infer sexual orientation from facial images caused a media uproar in the Fall of 2017. The Economist featured this work on the cover of their September 9th magazine; on the other hand two major LGBTQ organizations, The Human Rights Campaign and GLAAD, immediately labeled it “junk science”. Michal Kosinski, who co-authored the study with fellow researcher Yilun Wang, initially expressed surprise, calling the critiques “knee-jerk” reactions. However, he then proceeded to make even bolder claims: that such AI algorithms will soon be able to measure the intelligence, political orientation, and criminal inclinations of people from their facial images alone.
Kosinski’s controversial claims are nothing new. Last year, two computer scientists from China posted a non-peer-reviewed paper online in which they argued that their AI algorithm correctly categorizes “criminals” with nearly 90% accuracy from a government ID photo alone. Technology startups had also begun to crop up, claiming that they can profile people’s character from their facial images. These developments had prompted the three of us to collaborate earlier in the year on a Medium essay, Physiognomy’s New Clothes, to confront claims that AI face recognition reveals deep character traits. We described how the junk science of physiognomy has roots going back into antiquity, with practitioners in every era resurrecting beliefs based on prejudice using the new methodology of the age. In the 19th century this included anthropology and psychology; in the 20th, genetics and statistical analysis; and in the 21st, artificial intelligence.
In late 2016, the paper motivating our physiognomy essay seemed well outside the mainstream in tech and academia, but as in other areas of discourse, what recently felt like a fringe position must now be addressed head on. Kosinski is a faculty member of Stanford’s Graduate School of Business, and this new study has been accepted for publication in the respected Journal of Personality and Social Psychology. Much of the ensuing scrutiny has focused on ethics, implicitly assuming that the science is valid. We will focus on the science.
The authors trained and tested their “sexual orientation detector” using 35,326 images from public profiles on a US dating website. Composite images of the lesbian, gay, and straight men and women in the sample reveal a great deal about the information available to the algorithm:
Clearly there are differences between these four composite faces. Wang and Kosinski assert that the key differences are in physiognomy, meaning that a sexual orientation tends to go along with a characteristic facial structure. However, we can immediately see that some of these differences are more superficial. For example, the “average” straight woman appears to wear eyeshadow, while the “average” lesbian does not. Glasses are clearly visible on the gay man, and to a lesser extent on the lesbian, while they seem absent in the heterosexual composites. Might it be the case that the algorithm’s ability to detect orientation has little to do with facial structure, but is due rather to patterns in grooming, presentation and lifestyle?
We conducted a survey of 8,000 Americans using Amazon’s Mechanical Turk crowdsourcing platform to see if we could independently confirm these patterns, asking 77 yes/no questions such as “Do you wear eyeshadow?”, “Do you wear glasses?”, and “Do you have a beard?”, as well as questions about gender and sexual orientation. The results show that lesbians indeed use eyeshadow much less than straight women do, gay men and women do both wear glasses more, and young opposite-sex-attracted men are considerably more likely to have prominent facial hair than their gay or same-sex-attracted peers.
Breaking down the answers by the age of the respondent can provide a richer and clearer view of the data than any single statistic. In the following figures, we show the proportion of women who answer “yes” to “Do you ever use makeup?” (top) and “Do you wear eyeshadow?” (bottom), averaged over 6-year age intervals:
The blue curves represent strictly opposite-sex attracted women (a nearly identical set to those who answered “yes” to “Are you heterosexual or straight?”); the cyan curve represents women who answer “yes” to either or both of “Are you sexually attracted to women?” and “Are you romantically attracted to women?”; and the red curve represents women who answer “yes” to “Are you homosexual, gay or lesbian?”. [1] The shaded regions around each curve show 68% confidence intervals. [2] The patterns revealed here are intuitive; it won’t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same-sex attracted and (even more so) lesbian-identifying women. On the other hand these curves also show us how often these stereotypes are violated.
That same-sex attracted men of most ages wear glasses significantly more than exclusively opposite-sex attracted men do might be a bit less obvious, but this trend is equally clear: [3]
A proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men. However, asking the question “Do you like how you look in glasses?” reveals that this is likely more of a stylistic choice:
Same-sex attracted women also report wearing glasses more, as well as liking how they look in glasses more, across a range of ages:
One can also see how opposite-sex attracted women under the age of 40 wear contact lenses significantly more than same-sex attracted women, despite reporting that they have a vision defect at roughly the same rate, further illustrating how the difference is driven by an aesthetic preference: [4]
Similar analysis shows that young same-sex attracted men are much less likely to have hairy faces than opposite-sex attracted men (“serious facial hair” in our plots is defined as answering “yes” to having a goatee, beard, or moustache, but “no” to stubble). Overall, opposite-sex attracted men in our sample are 35% more likely to have serious facial hair than same-sex attracted men, and for men under the age of 31 (who are overrepresented on dating websites), this rises to 75%.
Wang and Kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connected with prenatal underexposure to androgens (male hormones), resulting in a feminizing effect, hence sparser facial hair. The fact that we see a cohort of same-sex attracted men in their 40s who have just as much facial hair as opposite-sex attracted men suggests a different story, in which fashion trends and cultural norms play the dominant role in choices about facial hair among men, not differing exposure to hormones early in development.
The authors of the paper additionally note that the heterosexual male composite appears to have darker skin than the other three composites. Our survey confirms that opposite-sex attracted men consistently self-report having a tan face (“Yes” to “Is your face tan?”) slightly more often than same-sex attracted men:
Once again Wang and Kosinski reach for a hormonal explanation, writing: “While the brightness of the facial image might be driven by many factors, previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin”. However, a simpler answer is suggested by the responses to the question “Do you work outdoors?”:
Overall, opposite-sex attracted men are 29% more likely to work outdoors, and among men under 31, this rises to 39%. Previous research has found that increased exposure to sunlight leads to darker skin! [5]
None of these results prove that there is no physiological basis for sexual orientation; in fact ample evidence shows us that orientation runs much deeper than a choice or a “lifestyle”. In a critique aimed in part at fraudulent “conversion therapy” programs, United States Surgeon General David Satcher wrote in a 2001 report, “Sexual orientation is usually determined by adolescence, if not earlier [...], and there is no valid scientific evidence that sexual orientation can be changed”. It follows that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlates and maybe even the origins of sexual orientation. In our survey we also find some evidence of outwardly visible correlates of orientation that are not cultural: perhaps most strikingly, very tall women are overrepresented among lesbian-identifying respondents. [6] However, while this is interesting, it’s very far from a good predictor of women’s sexual orientation. Makeup and eyeshadow do much better.
The way Wang and Kosinski measure the efficacy of their “AI gaydar” is equivalent to choosing a straight and a gay or lesbian face image, both from data “held out” during the training process, and asking how often the algorithm correctly guesses which is which. 50% performance would be no better than random chance. For women, guessing that the taller of the two is the lesbian achieves only 51% accuracy — barely above random chance. This is because, despite the statistically meaningful overrepresentation of tall women among the lesbian population, the great majority of lesbians are not unusually tall.
By contrast, the performance measures in the paper, 81% for gay men and 71% for lesbian women, seem impressive. [7] Consider, however, that we can achieve comparable results with trivial models based only on a handful of yes/no survey questions about presentation. For example, for pairs of women, one of whom is lesbian, the following not-exactly-superhuman algorithm is on average 63% accurate: if neither or both women wear eyeshadow, flip a coin; otherwise guess that the one who wears eyeshadow is straight, and the other lesbian. Adding six more yes/no questions about presentation (“Do you ever use makeup?”, “Do you have long hair?”, “Do you have short hair?”, “Do you ever use colored lipstick?”, “Do you like how you look in glasses?”, and “Do you work outdoors?”) as additional signals raises the performance to 70%. [8] Given how many more details about presentation are available in a face image, 71% performance no longer seems so impressive.
Several studies, including a recent one in the Journal of Sex Research, have shown that human judges’ “gaydar” is no more reliable than a coin flip when the judgement is based on pictures taken under well-controlled conditions (head pose, lighting, glasses, makeup, etc.). It’s better than chance if these variables are not controlled for, because a person’s presentation — especially if that person is out — involves social signaling. We signal our orientation and many other kinds of status, presumably in order to attract the kind of attention we want and to fit in with people like us. [9]
Wang and Kosinski argue against this interpretation on the grounds that their algorithm works on Facebook selfies of openly gay men as well as dating website selfies. The issue, however, is not whether the images come from a dating website or Facebook, but whether they are self-posted or taken under standardized conditions. Most people present themselves in ways that have been calibrated over many years of media consumption, observing others, looking in the mirror, and gauging social reactions. In one of the earliest “gaydar” studies using social media, participants could categorize gay men with about 58% accuracy; but when the researchers used Facebook images of gay and heterosexual men posted by their friends (still far from a perfect control), the accuracy dropped to 52%.
If subtle biases in image quality, expression, and grooming can be picked up on by humans, these biases can also be detected by an AI algorithm. While Wang and Kosinski acknowledge grooming and style, they believe that the chief differences between their composite images relate to face shape, arguing that gay men’s faces are more “feminine” (narrower jaws, longer noses, larger foreheads) while lesbian faces are more “masculine” (larger jaws, shorter noses, smaller foreheads). As with less facial hair on gay men and darker skin on straight men, they suggest that the mechanism is gender-atypical hormonal exposure during development. This echoes a widely discredited 19th century model of homosexuality, “sexual inversion”.
More likely, heterosexual men tend to take selfies from slightly below, which will have the apparent effect of enlarging the chin, shortening the nose, shrinking the forehead, and attenuating the smile (see our selfies below). This view emphasizes dominance — or, perhaps more benignly, an expectation that the viewer will be shorter. On the other hand, as a wedding photographer notes in her blog, “when you shoot from above, your eyes look bigger, which is generally attractive — especially for women.” This may be a heteronormative assessment.
When a face is photographed from below, the nostrils are prominent, while higher shooting angles de-emphasize and eventually conceal them altogether. Looking again at the composite images, we can see that the heterosexual male face has more pronounced dark spots corresponding to the nostrils than the gay male, while the opposite is true for the female faces. This is consistent with a pattern of heterosexual men on average shooting from below, heterosexual women from above as the wedding photographer suggests, and gay men and lesbian women from directly in front. A similar pattern is evident in the eyebrows: shooting from above makes them look more V-shaped, but their apparent shape becomes flatter, and eventually caret-shaped (^) as the camera is lowered. Shooting from below also makes the outer corners of the eyes appear lower. In short, the changes in the average positions of facial landmarks are consistent with what we would expect to see from differing selfie angles.
The ambiguity between shooting angle and the real physical sizes of facial features is hard to fully disentangle from a two-dimensional image, both for a human viewer and for an algorithm. Although the authors are using face recognition technology designed to try to cancel out all effects of head pose, lighting, grooming, and other variables not intrinsic to the face, we can confirm that this doesn’t work perfectly; that’s why multiple distinct images of a person help when grouping photos by subject in Google Photos, and why a person may initially appear in more than one group.
Tom White, a researcher at Victoria University in New Zealand, has experimented with the same facial recognition engine Kosinski and Wang use (VGG Face), and has found that its output varies systematically based on variables like smiling and head pose. When he trains a classifier based on VGG Face’s output to distinguish a happy expression from a neutral one, it gets the answer right 92% of the time — which is significant, given that the heterosexual female composite has a much more pronounced smile. Changes in head pose might be even more reliably detectable; for 576 test images, a classifier is able to pick out the ones facing to the right with 100% accuracy.
In summary, we have shown how the obvious differences between lesbian or gay and straight faces in selfies relate to grooming, presentation, and lifestyle — that is, differences in culture, not in facial structure. These differences include:
We’ve demonstrated that just a handful of yes/no questions about these variables can do nearly as good a job at guessing orientation as supposedly sophisticated facial recognition AI. Further, the current generation of facial recognition remains sensitive to head pose and facial expression. Therefore — at least at this point — it’s hard to credit the notion that this AI is in some way superhuman at “outing” us based on subtle but unalterable details of our facial structure.
This doesn’t negate the privacy concerns the authors and various commentators have raised, but it emphasizes that such concerns relate less to AI per se than to mass surveillance, which is troubling regardless of the technologies used (even when, as in the days of the Stasi in East Germany, these were nothing but paper files and audiotapes). Like computers or the internal combustion engine, AI is a general-purpose technology that can be used to automate a great many tasks, including ones that should not be undertaken in the first place.
We are hopeful about the confluence of new, powerful AI technologies with social science, but not because we believe in reviving the 19th century research program of inferring people’s inner character from their outer appearance. Rather, we believe AI is an essential tool for understanding patterns in human culture and behavior. It can expose stereotypes inherent in everyday language. It can reveal uncomfortable truths, as in Google’s work with the Geena Davis Institute, where our face gender classifier established that men are seen and heard nearly twice as often as women in Hollywood movies (yet female-led films outperform others at the box office!). Making social progress and holding ourselves to account is more difficult without such hard evidence, even when it only confirms our suspicions.
Two of us (Margaret Mitchell and Blaise Agüera y Arcas) are research scientists specializing in machine learning and AI at Google; Agüera y Arcas leads a team that includes deep learning applied to face recognition, and powers face grouping in Google Photos. Alex Todorov is a professor in the Psychology Department at Princeton, where he directs the social perception lab. He is the author of Face Value: The Irresistible Influence of First Impressions.
[1] This wording is based on several large national surveys, which we were able to use to sanity-check our numbers. About 6% of respondents identified as “homosexual, gay or lesbian” and 85% as “heterosexual”. About 4% (of all genders) were exclusively same-sex attracted. Of the men, 10% were either sexually or romantically same-sex attracted, and of the women, 20%. Just under 1% of respondents were trans, and about 2% identified with both or neither of the pronouns “she” and “he”. These numbers are broadly consistent with other surveys, especially when considered as a function of age. The Mechanical Turk population skews somewhat younger than the overall population of the US, and consistent with other studies, our data show that younger people are far more likely to identify non-heteronormatively.
[2] These are wider for same-sex attracted and lesbian women because they are minority populations, resulting in a larger sampling error. The same holds for older people in our sample.
[3] For the remainder of the plots we stick to opposite-sex attracted and same-sex attracted, as the counts are higher and the error bars therefore smaller; these categories are also somewhat less culturally freighted, since they rely on questions about attraction rather than identity. As with eyeshadow and makeup, the effects are similar and often even larger when comparing heterosexual-identifying with lesbian- or gay-identifying people.
[4] Although we didn’t test this explicitly, slightly different rates of laser correction surgery seem a likely cause of the small but growing disparity between opposite-sex attracted and same-sex attracted women who answer “yes” to the vision defect questions as they age.
[5] This finding may prompt the further question, “Why do more opposite-sex attracted men work outdoors?” This is not addressed by any of our survey questions, but hopefully the other evidence presented here will discourage an essentialist assumption such as “straight men are just more outdoorsy” without the evidence of a controlled study that can support the leap from correlation to cause. Such explanations are a form of logical fallacy sometimes called a just-so story: “an unverifiable narrative explanation for a cultural practice”.
[6] Of the 253 lesbian-identified women in the sample, 5, or 2%, were over six feet, and 25, or 10%, were over 5’9”. Out of 3,333 heterosexual women (women who answered “yes” to “Are you heterosexual or straight?”), only 16, or 0.5%, were over six feet, and 152, or 5%, were over 5’9”.
[7] They note that these figures rise to 91% for men and 83% for women if 5 images are considered.
[8] These results are based on the simplest possible machine learning technique, a linear classifier. The classifier is trained on a randomly chosen 70% of the data, with the remaining 30% of the data held out for testing. Over 500 repetitions of this procedure, the error is 69.53% ± 2.98%. With the same number of repetitions and holdout, basing the decision on height alone gives an error of 51.08% ± 3.27%, and basing it on eyeshadow alone yields 62.96% ± 2.39%.
[9] A longstanding body of work, e.g. Goffman’s The Presentation of Self in Everyday Life (1959) and Jones and Pittman’s Toward a General Theory of Strategic Self-Presentation (1982), delves more deeply into why we present ourselves the way we do, both for instrumental reasons (status, power, attraction) and because our presentation informs and is informed by how we conceive of our social selves.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Blaise Aguera y Arcas leads Google’s AI group in Seattle. He founded Seadragon, and was one of the creators of Photosynth at Microsoft.
",blaise aguera arcas alexander todorov margaret mitchell study claiming artificial intelligence infer sexual orientation facial images caused media uproar fall 2017 economist featured work cover september 9th magazine hand two major lgbtq organizations human rights campaign glaad immediately labeled junk science michal kosinski coauthored study fellow researcher yilun wang initially expressed surprise calling critiques kneejerk reactions however proceeded make even bolder claims ai algorithms soon able measure intelligence political orientation criminal inclinations people facial images alone kosinskis controversial claims nothing new last year two computer scientists china posted nonpeerreviewed paper online argued ai algorithm correctly categorizes criminals nearly 90 accuracy government id photo alone technology startups also begun crop claiming profile peoples character facial images developments prompted three us collaborate earlier year medium essay physiognomys new clothes confront claims ai face recognition reveals deep character traits described junk science physiognomy roots going back antiquity practitioners every era resurrecting beliefs based prejudice using new methodology age 19th century included anthropology psychology 20th genetics statistical analysis 21st artificial intelligence late 2016 paper motivating physiognomy essay seemed well outside mainstream tech academia areas discourse recently felt like fringe position must addressed head kosinski faculty member stanfords graduate school business new study accepted publication respected journal personality social psychology much ensuing scrutiny focused ethics implicitly assuming science valid focus science authors trained tested sexual orientation detector using 35326 images public profiles us dating website composite images lesbian gay straight men women sample reveal great deal information available algorithm clearly differences four composite faces wang kosinski assert key differences physiognomy meaning sexual orientation tends go along characteristic facial structure however immediately see differences superficial example average straight woman appears wear eyeshadow average lesbian glasses clearly visible gay man lesser extent lesbian seem absent heterosexual composites might case algorithms ability detect orientation little facial structure due rather patterns grooming presentation lifestyle conducted survey 8000 americans using amazons mechanical turk crowdsourcing platform see could independently confirm patterns asking 77 yesno questions wear eyeshadow wear glasses beard well questions gender sexual orientation results show lesbians indeed use eyeshadow much less straight women gay men women wear glasses young oppositesexattracted men considerably likely prominent facial hair gay samesexattracted peers breaking answers age respondent provide richer clearer view data single statistic following figures show proportion women answer yes ever use makeup top wear eyeshadow bottom averaged 6year age intervals blue curves represent strictly oppositesex attracted women nearly identical set answered yes heterosexual straight cyan curve represents women answer yes either sexually attracted women romantically attracted women red curve represents women answer yes homosexual gay lesbian 1 shaded regions around curve show 68 confidence intervals 2 patterns revealed intuitive wont breaking news straight women tend wear makeup eyeshadow samesex attracted even lesbianidentifying women hand curves also show us often stereotypes violated samesex attracted men ages wear glasses significantly exclusively oppositesex attracted men might bit less obvious trend equally clear 3 proponent physiognomy might tempted guess somehow related differences visual acuity populations men however asking question like look glasses reveals likely stylistic choice samesex attracted women also report wearing glasses well liking look glasses across range ages one also see oppositesex attracted women age 40 wear contact lenses significantly samesex attracted women despite reporting vision defect roughly rate illustrating difference driven aesthetic preference 4 similar analysis shows young samesex attracted men much less likely hairy faces oppositesex attracted men serious facial hair plots defined answering yes goatee beard moustache stubble overall oppositesex attracted men sample 35 likely serious facial hair samesex attracted men men age 31 overrepresented dating websites rises 75 wang kosinski speculate paper faintness beard moustache gay male composite might connected prenatal underexposure androgens male hormones resulting feminizing effect hence sparser facial hair fact see cohort samesex attracted men 40s much facial hair oppositesex attracted men suggests different story fashion trends cultural norms play dominant role choices facial hair among men differing exposure hormones early development authors paper additionally note heterosexual male composite appears darker skin three composites survey confirms oppositesex attracted men consistently selfreport tan face yes face tan slightly often samesex attracted men wang kosinski reach hormonal explanation writing brightness facial image might driven many factors previous research found testosterone stimulates melanocyte structure function leading darker skin however simpler answer suggested responses question work outdoors overall oppositesex attracted men 29 likely work outdoors among men 31 rises 39 previous research found increased exposure sunlight leads darker skin 5 none results prove physiological basis sexual orientation fact ample evidence shows us orientation runs much deeper choice lifestyle critique aimed part fraudulent conversion therapy programs united states surgeon general david satcher wrote 2001 report sexual orientation usually determined adolescence earlier valid scientific evidence sexual orientation changed follows dig deeply enough human physiology neuroscience eventually find reliable correlates maybe even origins sexual orientation survey also find evidence outwardly visible correlates orientation cultural perhaps strikingly tall women overrepresented among lesbianidentifying respondents 6 however interesting far good predictor womens sexual orientation makeup eyeshadow much better way wang kosinski measure efficacy ai gaydar equivalent choosing straight gay lesbian face image data held training process asking often algorithm correctly guesses 50 performance would better random chance women guessing taller two lesbian achieves 51 accuracy barely random chance despite statistically meaningful overrepresentation tall women among lesbian population great majority lesbians unusually tall contrast performance measures paper 81 gay men 71 lesbian women seem impressive 7 consider however achieve comparable results trivial models based handful yesno survey questions presentation example pairs women one lesbian following notexactlysuperhuman algorithm average 63 accurate neither women wear eyeshadow flip coin otherwise guess one wears eyeshadow straight lesbian adding six yesno questions presentation ever use makeup long hair short hair ever use colored lipstick like look glasses work outdoors additional signals raises performance 70 8 given many details presentation available face image 71 performance longer seems impressive several studies including recent one journal sex research shown human judges gaydar reliable coin flip judgement based pictures taken wellcontrolled conditions head pose lighting glasses makeup etc better chance variables controlled persons presentation especially person involves social signaling signal orientation many kinds status presumably order attract kind attention want fit people like us 9 wang kosinski argue interpretation grounds algorithm works facebook selfies openly gay men well dating website selfies issue however whether images come dating website facebook whether selfposted taken standardized conditions people present ways calibrated many years media consumption observing others looking mirror gauging social reactions one earliest gaydar studies using social media participants could categorize gay men 58 accuracy researchers used facebook images gay heterosexual men posted friends still far perfect control accuracy dropped 52 subtle biases image quality expression grooming picked humans biases also detected ai algorithm wang kosinski acknowledge grooming style believe chief differences composite images relate face shape arguing gay mens faces feminine narrower jaws longer noses larger foreheads lesbian faces masculine larger jaws shorter noses smaller foreheads less facial hair gay men darker skin straight men suggest mechanism genderatypical hormonal exposure development echoes widely discredited 19th century model homosexuality sexual inversion likely heterosexual men tend take selfies slightly apparent effect enlarging chin shortening nose shrinking forehead attenuating smile see selfies view emphasizes dominance perhaps benignly expectation viewer shorter hand wedding photographer notes blog shoot eyes look bigger generally attractive especially women may heteronormative assessment face photographed nostrils prominent higher shooting angles deemphasize eventually conceal altogether looking composite images see heterosexual male face pronounced dark spots corresponding nostrils gay male opposite true female faces consistent pattern heterosexual men average shooting heterosexual women wedding photographer suggests gay men lesbian women directly front similar pattern evident eyebrows shooting makes look vshaped apparent shape becomes flatter eventually caretshaped camera lowered shooting also makes outer corners eyes appear lower short changes average positions facial landmarks consistent would expect see differing selfie angles ambiguity shooting angle real physical sizes facial features hard fully disentangle twodimensional image human viewer algorithm although authors using face recognition technology designed try cancel effects head pose lighting grooming variables intrinsic face confirm doesnt work perfectly thats multiple distinct images person help grouping photos subject google photos person may initially appear one group tom white researcher victoria university new zealand experimented facial recognition engine kosinski wang use vgg face found output varies systematically based variables like smiling head pose trains classifier based vgg faces output distinguish happy expression neutral one gets answer right 92 time significant given heterosexual female composite much pronounced smile changes head pose might even reliably detectable 576 test images classifier able pick ones facing right 100 accuracy summary shown obvious differences lesbian gay straight faces selfies relate grooming presentation lifestyle differences culture facial structure differences include weve demonstrated handful yesno questions variables nearly good job guessing orientation supposedly sophisticated facial recognition ai current generation facial recognition remains sensitive head pose facial expression therefore least point hard credit notion ai way superhuman outing us based subtle unalterable details facial structure doesnt negate privacy concerns authors various commentators raised emphasizes concerns relate less ai per se mass surveillance troubling regardless technologies used even days stasi east germany nothing paper files audiotapes like computers internal combustion engine ai generalpurpose technology used automate great many tasks including ones undertaken first place hopeful confluence new powerful ai technologies social science believe reviving 19th century research program inferring peoples inner character outer appearance rather believe ai essential tool understanding patterns human culture behavior expose stereotypes inherent everyday language reveal uncomfortable truths googles work geena davis institute face gender classifier established men seen heard nearly twice often women hollywood movies yet femaleled films outperform others box office making social progress holding account difficult without hard evidence even confirms suspicions two us margaret mitchell blaise aguera arcas research scientists specializing machine learning ai google aguera arcas leads team includes deep learning applied face recognition powers face grouping google photos alex todorov professor psychology department princeton directs social perception lab author face value irresistible influence first impressions 1 wording based several large national surveys able use sanitycheck numbers 6 respondents identified homosexual gay lesbian 85 heterosexual 4 genders exclusively samesex attracted men 10 either sexually romantically samesex attracted women 20 1 respondents trans 2 identified neither pronouns numbers broadly consistent surveys especially considered function age mechanical turk population skews somewhat younger overall population us consistent studies data show younger people far likely identify nonheteronormatively 2 wider samesex attracted lesbian women minority populations resulting larger sampling error holds older people sample 3 remainder plots stick oppositesex attracted samesex attracted counts higher error bars therefore smaller categories also somewhat less culturally freighted since rely questions attraction rather identity eyeshadow makeup effects similar often even larger comparing heterosexualidentifying lesbian gayidentifying people 4 although didnt test explicitly slightly different rates laser correction surgery seem likely cause small growing disparity oppositesex attracted samesex attracted women answer yes vision defect questions age 5 finding may prompt question oppositesex attracted men work outdoors addressed survey questions hopefully evidence presented discourage essentialist assumption straight men outdoorsy without evidence controlled study support leap correlation cause explanations form logical fallacy sometimes called justso story unverifiable narrative explanation cultural practice 6 253 lesbianidentified women sample 5 2 six feet 25 10 59 3333 heterosexual women women answered yes heterosexual straight 16 05 six feet 152 5 59 7 note figures rise 91 men 83 women 5 images considered 8 results based simplest possible machine learning technique linear classifier classifier trained randomly chosen 70 data remaining 30 data held testing 500 repetitions procedure error 6953 298 number repetitions holdout basing decision height alone gives error 5108 327 basing eyeshadow alone yields 6296 239 9 longstanding body work eg goffmans presentation self everyday life 1959 jones pittmans toward general theory strategic selfpresentation 1982 delves deeply present way instrumental reasons status power attraction presentation informs informed conceive social selves quick cheer standing ovation clap show much enjoyed story blaise aguera arcas leads googles ai group seattle founded seadragon one creators photosynth microsoft,en,"['Blaise Agüera', 'LGBTQ', 'The Human Rights Campaign', 'Kosinski', 'Stanford', 'Graduate School of Business', 'Journal of Personality and Social Psychology', 'Wang', 'Amazon', 'Mechanical Turk', 'United States Surgeon', 'algorithm', 'the Journal of Sex Research', 'Facebook', 'Victoria University', 'VGG Face', 'Google', 'the Geena Davis Institute', 'the Psychology Department', 'Seadragon', 'Microsoft']"
60,James Le,18400,A Tour of The Top 10 Algorithms for Machine Learning Newbies,"In machine learning, there’s something called the “No Free Lunch” theorem. In a nutshell, it states that no one algorithm works best for every problem, and it’s especially relevant for supervised learning (i.e. predictive modeling).
For example, you can’t say that neural networks are always better than decision trees or vice-versa. There are many factors at play, such as the size and structure of your dataset.
As a result, you should try many different algorithms for your problem, while using a hold-out “test set” of data to evaluate performance and select the winner.
Of course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in. As an analogy, if you need to clean your house, you might use a vacuum, a broom, or a mop, but you wouldn’t bust out a shovel and start digging.
However, there is a common principle that underlies all supervised machine learning algorithms for predictive modeling.
This is a general learning task where we would like to make predictions in the future (Y) given new examples of input variables (X). We don’t know what the function (f) looks like or its form. If we did, we would use it directly and we would not need to learn it from data using machine learning algorithms.
The most common type of machine learning is to learn the mapping Y = f(X) to make predictions of Y for new X. This is called predictive modeling or predictive analytics and our goal is to make the most accurate predictions possible.
For machine learning newbies who are eager to understand the basic of machine learning, here is a quick tour on the top 10 machine learning algorithms used by data scientists.
Linear regression is perhaps one of the most well-known and well-understood algorithms in statistics and machine learning.
Predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. We will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.
The representation of linear regression is an equation that describes a line that best fits the relationship between the input variables (x) and the output variables (y), by finding specific weightings for the input variables called coefficients (B).
For example: y = B0 + B1 * x
We will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients B0 and B1.
Different techniques can be used to learn the linear regression model from data, such as a linear algebra solution for ordinary least squares and gradient descent optimization.
Linear regression has been around for more than 200 years and has been extensively studied. Some good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data, if possible. It is a fast and simple technique and good first algorithm to try.
Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).
Logistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable. Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.
The logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. IF less than 0.5 then output 1) and predict a class value.
Because of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. This can be useful for problems where you need to give more rationale for a prediction.
Like linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. It’s a fast model to learn and effective on binary classification problems.
Logistic Regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then the Linear Discriminant Analysis algorithm is the preferred linear classification technique.
The representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class. For a single input variable this includes:
Predictions are made by calculating a discriminate value for each class and making a prediction for the class with the largest value. The technique assumes that the data has a Gaussian distribution (bell curve), so it is a good idea to remove outliers from your data before hand. It’s a simple and powerful method for classification predictive modeling problems.
Decision Trees are an important type of algorithm for predictive modeling machinelearning.
The representation of the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).
The leaf nodes of the tree contain an output variable (y) which is used to make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.
Trees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.
Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.
The model is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.
Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.
The KNN algorithm is very simple and very effective. The model representation for KNN is the entire training dataset. Simple right?
Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable, for classification problems this might be the mode (or most common) class value.
The trick is in how to determine the similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance, a number you can calculate directly based on the differences between each input variable.
KNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed, just in time. You can also update and curate your training instances over time to keep predictions accurate.
The idea of distance or closeness can break down in very high dimensions (lots of input variables) which can negatively affect the performance of the algorithm on your problem. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.
A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset. The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.
The representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm. After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction. Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.
If you discover that KNN gives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset.
Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.
A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line and let’s assume that all of our input points can be completely separated by this line. The SVM learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane.
The distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points are relevant in defining the hyperplane and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane. In practice, an optimization algorithm is used to find the values for the coefficients that maximizes the margin.
SVM might be one of the most powerful out-of-the-box classifiers and worth trying on your dataset.
Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.
The bootstrap is a powerful statistical method for estimating a quantity from a data sample. Such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value.
In bagging, the same approach is used, but instead for estimating entire statistical models, most commonly decision trees. Multiple samples of your training data are taken then models are constructed for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.
Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.
The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.
If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.
Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.
AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.
AdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more weight, whereas easy to predict instances are given less weight. Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data.
Because so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed.
A typical question asked by a beginner, when facing a wide variety of machine learning algorithms, is “which algorithm should I use?” The answer to the question varies depending on many factors, including: (1) The size, quality, and nature of data; (2) The available computational time; (3) The urgency of the task; and (4) What you want to do with the data.
Even an experienced data scientist cannot tell which algorithm will perform the best before trying different algorithms. Although there are many other Machine Learning algorithms, these are the most popular ones. If you’re a newbie to Machine Learning, these would be a good starting point to learn.
— —
If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Blue Ocean Thinker (https://jameskle.com/)
Sharing concepts, ideas, and codes.
",machine learning theres something called free lunch theorem nutshell states one algorithm works best every problem especially relevant supervised learning ie predictive modeling example cant say neural networks always better decision trees viceversa many factors play size structure dataset result try many different algorithms problem using holdout test set data evaluate performance select winner course algorithms try must appropriate problem picking right machine learning task comes analogy need clean house might use vacuum broom mop wouldnt bust shovel start digging however common principle underlies supervised machine learning algorithms predictive modeling general learning task would like make predictions future given new examples input variables x dont know function f looks like form would use directly would need learn data using machine learning algorithms common type machine learning learn mapping fx make predictions new x called predictive modeling predictive analytics goal make accurate predictions possible machine learning newbies eager understand basic machine learning quick tour top 10 machine learning algorithms used data scientists linear regression perhaps one wellknown wellunderstood algorithms statistics machine learning predictive modeling primarily concerned minimizing error model making accurate predictions possible expense explainability borrow reuse steal algorithms many different fields including statistics use towards ends representation linear regression equation describes line best fits relationship input variables x output variables finding specific weightings input variables called coefficients b example b0 b1 x predict given input x goal linear regression learning algorithm find values coefficients b0 b1 different techniques used learn linear regression model data linear algebra solution ordinary least squares gradient descent optimization linear regression around 200 years extensively studied good rules thumb using technique remove variables similar correlated remove noise data possible fast simple technique good first algorithm try logistic regression another technique borrowed machine learning field statistics goto method binary classification problems problems two class values logistic regression like linear regression goal find values coefficients weight input variable unlike linear regression prediction output transformed using nonlinear function called logistic function logistic function looks like big transform value range 0 1 useful apply rule output logistic function snap values 0 1 eg less 05 output 1 predict class value way model learned predictions made logistic regression also used probability given data instance belonging class 0 class 1 useful problems need give rationale prediction like linear regression logistic regression work better remove attributes unrelated output variable well attributes similar correlated fast model learn effective binary classification problems logistic regression classification algorithm traditionally limited twoclass classification problems two classes linear discriminant analysis algorithm preferred linear classification technique representation lda pretty straight forward consists statistical properties data calculated class single input variable includes predictions made calculating discriminate value class making prediction class largest value technique assumes data gaussian distribution bell curve good idea remove outliers data hand simple powerful method classification predictive modeling problems decision trees important type algorithm predictive modeling machinelearning representation decision tree model binary tree binary tree algorithms data structures nothing fancy node represents single input variable x split point variable assuming variable numeric leaf nodes tree contain output variable used make prediction predictions made walking splits tree arriving leaf node output class value leaf node trees fast learn fast making predictions also often accurate broad range problems require special preparation data naive bayes simple surprisingly powerful algorithm predictive modeling model comprised two types probabilities calculated directly training data 1 probability class 2 conditional probability class given x value calculated probability model used make predictions new data using bayes theorem data realvalued common assume gaussian distribution bell curve easily estimate probabilities naive bayes called naive assumes input variable independent strong assumption unrealistic real data nevertheless technique effective large range complex problems knn algorithm simple effective model representation knn entire training dataset simple right predictions made new data point searching entire training set k similar instances neighbors summarizing output variable k instances regression problems might mean output variable classification problems might mode common class value trick determine similarity data instances simplest technique attributes scale inches example use euclidean distance number calculate directly based differences input variable knn require lot memory space store data performs calculation learn prediction needed time also update curate training instances time keep predictions accurate idea distance closeness break high dimensions lots input variables negatively affect performance algorithm problem called curse dimensionality suggests use input variables relevant predicting output variable downside knearest neighbors need hang entire training dataset learning vector quantization algorithm lvq short artificial neural network algorithm allows choose many training instances hang onto learns exactly instances look like representation lvq collection codebook vectors selected randomly beginning adapted best summarize training dataset number iterations learning algorithm learned codebook vectors used make predictions like knearest neighbors similar neighbor best matching codebook vector found calculating distance codebook vector new data instance class value real value case regression best matching unit returned prediction best results achieved rescale data range 0 1 discover knn gives good results dataset try using lvq reduce memory requirements storing entire training dataset support vector machines perhaps one popular talked machine learning algorithms hyperplane line splits input variable space svm hyperplane selected best separate points input variable space class either class 0 class 1 twodimensions visualize line lets assume input points completely separated line svm learning algorithm finds coefficients results best separation classes hyperplane distance hyperplane closest data points referred margin best optimal hyperplane separate two classes line largest margin points relevant defining hyperplane construction classifier points called support vectors support define hyperplane practice optimization algorithm used find values coefficients maximizes margin svm might one powerful outofthebox classifiers worth trying dataset random forest one popular powerful machine learning algorithms type ensemble machine learning algorithm called bootstrap aggregation bagging bootstrap powerful statistical method estimating quantity data sample mean take lots samples data calculate mean average mean values give better estimation true mean value bagging approach used instead estimating entire statistical models commonly decision trees multiple samples training data taken models constructed data sample need make prediction new data model makes prediction predictions averaged give better estimate true output value random forest tweak approach decision trees created rather selecting optimal split points suboptimal splits made introducing randomness models created sample data therefore different otherwise would still accurate unique different ways combining predictions results better estimate true underlying output value get good results algorithm high variance like decision trees often get better results bagging algorithm boosting ensemble technique attempts create strong classifier number weak classifiers done building model training data creating second model attempts correct errors first model models added training set predicted perfectly maximum number models added adaboost first really successful boosting algorithm developed binary classification best starting point understanding boosting modern boosting methods build adaboost notably stochastic gradient boosting machines adaboost used short decision trees first tree created performance tree training instance used weight much attention next tree created pay attention training instance training data hard predict given weight whereas easy predict instances given less weight models created sequentially one updating weights training instances affect learning performed next tree sequence trees built predictions made new data performance tree weighted accurate training data much attention put correcting mistakes algorithm important clean data outliers removed typical question asked beginner facing wide variety machine learning algorithms algorithm use answer question varies depending many factors including 1 size quality nature data 2 available computational time 3 urgency task 4 want data even experienced data scientist cannot tell algorithm perform best trying different algorithms although many machine learning algorithms popular ones youre newbie machine learning would good starting point learn enjoyed piece id love hit clap button others might stumble upon find code github writing projects httpsjamesklecom also follow twitter email directly find linkedin quick cheer standing ovation clap show much enjoyed story blue ocean thinker httpsjamesklecom sharing concepts ideas codes,en,"['Linear', 'algorithm', 'LDA', 'Bayes Theorem', 'KNN', 'K', 'K-Nearest Neighbors', 'The Learning Vector Quantization', 'LVQ', 'Support Vector Machines', 'SVM', 'Random Forest', 'Bootstrap Aggregation', 'AdaBoost', 'GitHub', 'LinkedIn']"
61,Emmanuel Ameisen,12800,How to solve 90% of NLP problems: a step-by-step guide,"For more content like this, follow Insight and Emmanuel on Twitter.
Whether you are an established company or working to launch a new service, you can always leverage text data to validate, improve, and expand the functionalities of your product. The science of extracting meaning and learning from text data is an active topic of research called Natural Language Processing (NLP).
NLP produces new and exciting results on a daily basis, and is a very large field. However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other:
While many NLP papers and tutorials exist online, we have found it hard to find guidelines and tips on how to approach these problems efficiently from the ground up.
After leading hundreds of projects a year and gaining advice from top teams all over the United States, we wrote this post to explain how to build Machine Learning solutions to solve problems like the ones mentioned above. We’ll begin with the simplest method that could work, and then move on to more nuanced solutions, such as feature engineering, word vectors, and deep learning.
After reading this article, you’ll know how to:
We wrote this post as a step-by-step guide; it can also serve as a high level overview of highly effective standard approaches.
This post is accompanied by an interactive notebook demonstrating and applying all these techniques. Feel free to run the code and follow along!
Every Machine Learning problem starts with data, such as a list of emails, posts, or tweets. Common sources of textual information include:
“Disasters on Social Media” dataset
For this post, we will use a dataset generously provided by CrowdFlower, called “Disasters on Social Media”, where:
Our task will be to detect which tweets are about a disastrous event as opposed to an irrelevant topic such as a movie. Why? A potential application would be to exclusively notify law enforcement officials about urgent emergencies while ignoring reviews of the most recent Adam Sandler film. A particular challenge with this task is that both classes contain the same search terms used to find the tweets, so we will have to use subtler differences to distinguish between them.
In the rest of this post, we will refer to tweets that are about disasters as “disaster”, and tweets about anything else as “irrelevant”.
We have labeled data and so we know which tweets belong to which categories. As Richard Socher outlines below, it is usually faster, simpler, and cheaper to find and label enough data to train a model on, rather than trying to optimize a complex unsupervised method.
One of the key skills of a data scientist is knowing whether the next step should be working on the model or the data. A good rule of thumb is to look at the data first and then clean it up. A clean dataset will allow a model to learn meaningful features and not overfit on irrelevant noise.
Here is a checklist to use to clean your data: (see the code for more details):
After following these steps and checking for additional errors, we can start using the clean, labelled data to train models!
Machine Learning models take numerical values as input. Models working on images, for example, take in a matrix representing the intensity of each pixel in each color channel.
Our dataset is a list of sentences, so in order for our algorithm to extract patterns from the data, we first need to find a way to represent it in a way that our algorithm can understand, i.e. as a list of numbers.
A natural way to represent text for computers is to encode each character individually as a number (ASCII for example). If we were to feed this simple representation into a classifier, it would have to learn the structure of words from scratch based only on our data, which is impossible for most datasets. We need to use a higher level approach.
For example, we can build a vocabulary of all the unique words in our dataset, and associate a unique index to each word in the vocabulary. Each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary. At each index in this list, we mark how many times the given word appears in our sentence. This is called a Bag of Words model, since it is a representation that completely ignores the order of words in our sentence. This is illustrated below.
We have around 20,000 words in our vocabulary in the “Disasters of Social Media” example, which means that every sentence will be represented as a vector of length 20,000. The vector will contain mostly 0s because each sentence contains only a very small subset of our vocabulary.
In order to see whether our embeddings are capturing information that is relevant to our problem (i.e. whether the tweets are about disasters or not), it is a good idea to visualize them and see if the classes look well separated. Since vocabularies are usually very large and visualizing data in 20,000 dimensions is impossible, techniques like PCA will help project the data down to two dimensions. This is plotted below.
The two classes do not look very well separated, which could be a feature of our embeddings or simply of our dimensionality reduction. In order to see whether the Bag of Words features are of any use, we can train a classifier based on them.
When first approaching a problem, a general best practice is to start with the simplest tool that could solve the job. Whenever it comes to classifying data, a common favorite for its versatility and explainability is Logistic Regression. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.
We split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data. After training, we get an accuracy of 75.4%. Not too shabby! Guessing the most frequent class (“irrelevant”) would give us only 57%. However, even if 75% precision was good enough for our needs, we should never ship a model without trying to understand it.
A first step is to understand the types of errors our model makes, and which kind of errors are least desirable. In our example, false positives are classifying an irrelevant tweet as a disaster, and false negatives are classifying a disaster as an irrelevant tweet. If the priority is to react to every potential event, we would want to lower our false negatives. If we are constrained in resources however, we might prioritize a lower false positive rate to reduce false alarms. A good way to visualize this information is using a Confusion Matrix, which compares the predictions our model makes with the true label. Ideally, the matrix would be a diagonal line from top left to bottom right (our predictions match the truth perfectly).
Our classifier creates more false negatives than false positives (proportionally). In other words, our model’s most common error is inaccurately classifying disasters as irrelevant. If false positives represent a high cost for law enforcement, this could be a good bias for our classifier to have.
To validate our model and interpret its predictions, it is important to look at which words it is using to make decisions. If our data is biased, our classifier will make accurate predictions in the sample data, but the model would not generalize well in the real world. Here we plot the most important words for both the disaster and irrelevant class. Plotting word importance is simple with Bag of Words and Logistic Regression, since we can just extract and rank the coefficients that the model used for its predictions.
Our classifier correctly picks up on some patterns (hiroshima, massacre), but clearly seems to be overfitting on some meaningless terms (heyoo, x1392). Right now, our Bag of Words model is dealing with a huge vocabulary of different words and treating all words equally. However, some of these words are very frequent, and are only contributing noise to our predictions. Next, we will try a way to represent sentences that can account for the frequency of words, to see if we can pick up more signal from our data.
In order to help our model focus more on meaningful words, we can use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise. Here is the PCA projection of our new embeddings.
We can see above that there is a clearer distinction between the two colors. This should make it easier for our classifier to separate both groups. Let’s see if this leads to better performance. Training another Logistic Regression on our new embeddings, we get an accuracy of 76.2%.
A very slight improvement. Has our model started picking up on more important words? If we are getting a better result while preventing our model from “cheating” then we can truly consider this model an upgrade.
The words it picked up look much more relevant! Although our metrics on our test set only increased slightly, we have much more confidence in the terms our model is using, and thus would feel more comfortable deploying it in a system that would interact with customers.
Our latest model managed to pick up on high signal words. However, it is very likely that if we deploy this model, we will encounter words that we have not seen in our training set before. The previous model will not be able to accurately classify these tweets, even if it has seen very similar words during training.
To solve this problem, we need to capture the semantic meaning of words, meaning we need to understand that words like ‘good’ and ‘positive’ are closer than ‘apricot’ and ‘continent.’ The tool we will use to help us capture meaning is called Word2Vec.
Using pre-trained words
Word2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.
The authors of the paper open sourced a model that was pre-trained on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model. The pre-trained vectors can be found in the repository associated with this post.
A quick way to get a sentence embedding for our classifier is to average Word2Vec scores of all words in our sentence. This is a Bag of Words approach just like before, but this time we only lose the syntax of our sentence, while keeping some semantic information.
Here is a visualization of our new embeddings using previous techniques:
The two groups of colors look even more separated here, our new embeddings should help our classifier find the separation between both classes. After training the same model a third time (a Logistic Regression), we get an accuracy score of 77.7%, our best result yet! Time to inspect our model.
Since our embeddings are not represented as a vector with one dimension per word as in our previous models, it’s harder to see which words are the most relevant to our classification. While we still have access to the coefficients of our Logistic Regression, they relate to the 300 dimensions of our embeddings rather than the indices of words.
For such a low gain in accuracy, losing all explainability seems like a harsh trade-off. However, with more complex models we can leverage black box explainers such as LIME in order to get some insight into how our classifier works.
LIME
LIME is available on Github through an open-sourced package. A black-box explainer allows users to explain the decisions of any classifier on one particular example by perturbing the input (in our case removing words from the sentence) and seeing how the prediction changes.
Let’s see a couple explanations for sentences from our dataset.
However, we do not have time to explore the thousands of examples in our dataset. What we’ll do instead is run LIME on a representative sample of test cases and see which words keep coming up as strong contributors. Using this approach we can get word importance scores like we had for previous models and validate our model’s predictions.
Looks like the model picks up highly relevant words implying that it appears to make understandable decisions. These seem like the most relevant words out of all previous models and therefore we’re more comfortable deploying in to production.
We’ve covered quick and efficient approaches to generate compact sentence embeddings. However, by omitting the order of words, we are discarding all of the syntactic information of our sentences. If these methods do not provide sufficient results, you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation. A common way to do that is to treat a sentence as a sequence of individual word vectors using either Word2Vec or more recent approaches such as GloVe or CoVe. This is what we will do below.
Convolutional Neural Networks for Sentence Classification train very quickly and work well as an entry level deep learning architecture. While Convolutional Neural Networks (CNN) are mainly known for their performance on image data, they have been providing excellent results on text related tasks, and are usually much quicker to train than most complex NLP approaches (e.g. LSTMs and Encoder/Decoder architectures). This model preserves the order of words and learns valuable information on which sequences of words are predictive of our target classes. Contrary to previous models, it can tell the difference between “Alex eats plants” and “Plants eat Alex.”
Training this model does not require much more work than previous approaches (see code for details) and gives us a model that is much better than the previous ones, getting 79.5% accuracy! As with the models above, the next step should be to explore and explain the predictions using the methods we described to validate that it is indeed the best model to deploy to users. By now, you should feel comfortable tackling this on your own.
Here is a quick recap of the approach we’ve successfully used:
These approaches were applied to a particular example case using models tailored towards understanding and leveraging short text such as tweets, but the ideas are widely applicable to a variety of problems. I hope this helped you, we’d love to hear your comments and questions! Feel free to comment below or reach out to @EmmanuelAmeisen here or on Twitter.
Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Lead at Insight AI @EmmanuelAmeisen
Insight Fellows Program - Your bridge to a career in data
",content like follow insight emmanuel twitter whether established company working launch new service always leverage text data validate improve expand functionalities product science extracting meaning learning text data active topic research called natural language processing nlp nlp produces new exciting results daily basis large field however worked hundreds companies insight team seen key practical applications come much frequently many nlp papers tutorials exist online found hard find guidelines tips approach problems efficiently ground leading hundreds projects year gaining advice top teams united states wrote post explain build machine learning solutions solve problems like ones mentioned well begin simplest method could work move nuanced solutions feature engineering word vectors deep learning reading article youll know wrote post stepbystep guide also serve high level overview highly effective standard approaches post accompanied interactive notebook demonstrating applying techniques feel free run code follow along every machine learning problem starts data list emails posts tweets common sources textual information include disasters social media dataset post use dataset generously provided crowdflower called disasters social media task detect tweets disastrous event opposed irrelevant topic movie potential application would exclusively notify law enforcement officials urgent emergencies ignoring reviews recent adam sandler film particular challenge task classes contain search terms used find tweets use subtler differences distinguish rest post refer tweets disasters disaster tweets anything else irrelevant labeled data know tweets belong categories richard socher outlines usually faster simpler cheaper find label enough data train model rather trying optimize complex unsupervised method one key skills data scientist knowing whether next step working model data good rule thumb look data first clean clean dataset allow model learn meaningful features overfit irrelevant noise checklist use clean data see code details following steps checking additional errors start using clean labelled data train models machine learning models take numerical values input models working images example take matrix representing intensity pixel color channel dataset list sentences order algorithm extract patterns data first need find way represent way algorithm understand ie list numbers natural way represent text computers encode character individually number ascii example feed simple representation classifier would learn structure words scratch based data impossible datasets need use higher level approach example build vocabulary unique words dataset associate unique index word vocabulary sentence represented list long number distinct words vocabulary index list mark many times given word appears sentence called bag words model since representation completely ignores order words sentence illustrated around 20000 words vocabulary disasters social media example means every sentence represented vector length 20000 vector contain mostly 0s sentence contains small subset vocabulary order see whether embeddings capturing information relevant problem ie whether tweets disasters good idea visualize see classes look well separated since vocabularies usually large visualizing data 20000 dimensions impossible techniques like pca help project data two dimensions plotted two classes look well separated could feature embeddings simply dimensionality reduction order see whether bag words features use train classifier based first approaching problem general best practice start simplest tool could solve job whenever comes classifying data common favorite versatility explainability logistic regression simple train results interpretable easily extract important coefficients model split data training set used fit model test set see well generalizes unseen data training get accuracy 754 shabby guessing frequent class irrelevant would give us 57 however even 75 precision good enough needs never ship model without trying understand first step understand types errors model makes kind errors least desirable example false positives classifying irrelevant tweet disaster false negatives classifying disaster irrelevant tweet priority react every potential event would want lower false negatives constrained resources however might prioritize lower false positive rate reduce false alarms good way visualize information using confusion matrix compares predictions model makes true label ideally matrix would diagonal line top left bottom right predictions match truth perfectly classifier creates false negatives false positives proportionally words models common error inaccurately classifying disasters irrelevant false positives represent high cost law enforcement could good bias classifier validate model interpret predictions important look words using make decisions data biased classifier make accurate predictions sample data model would generalize well real world plot important words disaster irrelevant class plotting word importance simple bag words logistic regression since extract rank coefficients model used predictions classifier correctly picks patterns hiroshima massacre clearly seems overfitting meaningless terms heyoo x1392 right bag words model dealing huge vocabulary different words treating words equally however words frequent contributing noise predictions next try way represent sentences account frequency words see pick signal data order help model focus meaningful words use tfidf score term frequency inverse document frequency top bag words model tfidf weighs words rare dataset discounting words frequent add noise pca projection new embeddings see clearer distinction two colors make easier classifier separate groups lets see leads better performance training another logistic regression new embeddings get accuracy 762 slight improvement model started picking important words getting better result preventing model cheating truly consider model upgrade words picked look much relevant although metrics test set increased slightly much confidence terms model using thus would feel comfortable deploying system would interact customers latest model managed pick high signal words however likely deploy model encounter words seen training set previous model able accurately classify tweets even seen similar words training solve problem need capture semantic meaning words meaning need understand words like good positive closer apricot continent tool use help us capture meaning called word2vec using pretrained words word2vec technique find continuous embeddings words learns reading massive amounts text memorizing words tend appear similar contexts trained enough data generates 300dimension vector word vocabulary words similar meaning closer authors paper open sourced model pretrained large corpus leverage include knowledge semantic meaning model pretrained vectors found repository associated post quick way get sentence embedding classifier average word2vec scores words sentence bag words approach like time lose syntax sentence keeping semantic information visualization new embeddings using previous techniques two groups colors look even separated new embeddings help classifier find separation classes training model third time logistic regression get accuracy score 777 best result yet time inspect model since embeddings represented vector one dimension per word previous models harder see words relevant classification still access coefficients logistic regression relate 300 dimensions embeddings rather indices words low gain accuracy losing explainability seems like harsh tradeoff however complex models leverage black box explainers lime order get insight classifier works lime lime available github opensourced package blackbox explainer allows users explain decisions classifier one particular example perturbing input case removing words sentence seeing prediction changes lets see couple explanations sentences dataset however time explore thousands examples dataset well instead run lime representative sample test cases see words keep coming strong contributors using approach get word importance scores like previous models validate models predictions looks like model picks highly relevant words implying appears make understandable decisions seem like relevant words previous models therefore comfortable deploying production weve covered quick efficient approaches generate compact sentence embeddings however omitting order words discarding syntactic information sentences methods provide sufficient results utilize complex model take whole sentences input predict labels without need build intermediate representation common way treat sentence sequence individual word vectors using either word2vec recent approaches glove cove convolutional neural networks sentence classification train quickly work well entry level deep learning architecture convolutional neural networks cnn mainly known performance image data providing excellent results text related tasks usually much quicker train complex nlp approaches eg lstms encoderdecoder architectures model preserves order words learns valuable information sequences words predictive target classes contrary previous models tell difference alex eats plants plants eat alex training model require much work previous approaches see code details gives us model much better previous ones getting 795 accuracy models next step explore explain predictions using methods described validate indeed best model deploy users feel comfortable tackling quick recap approach weve successfully used approaches applied particular example case using models tailored towards understanding leveraging short text tweets ideas widely applicable variety problems hope helped wed love hear comments questions feel free comment reach emmanuelameisen twitter want learn applied artificial intelligence top professionals silicon valley new york learn artificial intelligence program company working ai would like get involved insight ai fellows program feel free get touch quick cheer standing ovation clap show much enjoyed story ai lead insight ai emmanuelameisen insight fellows program bridge career data,en,"['Insight', 'Natural Language Processing', 'NLP', 'CrowdFlower', 'algorithm', 'ASCII', 'PCA', 'Bag of Words and Logistic Regression', 'TF-IDF', 'LIME', 'CNN', 'Encoder/Decoder', '@EmmanuelAmeisen', 'Artificial Intelligence', 'the Insight AI Fellows Program', 'Insight AI @EmmanuelAmeisen', 'Insight Fellows Program - Your']"
62,Mybridge,10100,30 Amazing Machine Learning Projects for the Past Year (v.2018),"For the past year, we’ve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance).
This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017. Mybridge AI evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of Github stars is 3,558.
Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year.
<Recommended Learning>
A) Neural Networks
Deep Learning A-ZTM: Hands-On Artificial Neural Networks
[68,745 recommends, 4.5/5 stars]
B) TensorFlow
Complete Guide to TensorFlow for Deep Learning with Python
[17,834 recommends, 4.6/5 stars]
(Click the numbers below. Credit given to the biggest contributor.)
FastText: Library for fast text representation and classification. [11786 stars on Github]. Courtesy of Facebook Research
........... [ Muse: Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github]
Deep-photo-styletransfer: Code and data for paper “Deep Photo Style Transfer” [9747 stars on Github]. Courtesy of Fujun Luan, Ph.D. at Cornell University
The world’s simplest facial recognition api for Python and the command line [8672 stars on Github]. Courtesy of Adam Geitgey
Magenta: Music and Art Generation with Machine Intelligence [8113 stars on Github].
Sonnet: TensorFlow-based neural network library [5731 stars on Github]. Courtesy of Malcolm Reynolds at Deepmind
deeplearn.js: A hardware-accelerated machine intelligence library for the web [5462 stars on Github]. Courtesy of Nikhil Thorat at Google Brain
Fast Style Transfer in TensorFlow [4843 stars on Github]. Courtesy of Logan Engstrom at MIT
Pysc2: StarCraft II Learning Environment [3683 stars on Github]. Courtesy of Timo Ewalds at DeepMind
AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research [3861 stars on Github]. Courtesy of Shital Shah at Microsoft
Facets: Visualizations for machine learning datasets [3371 stars on Github]. Courtesy of Google Brain
Style2Paints: AI colorization of images [3310 stars on Github].
Tensor2Tensor: A library for generalized sequence to sequence models — Google Research [3087 stars on Github]. Courtesy of Ryan Sepassi at Google Brain
Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more) [2847 stars on Github]. Courtesy of Jun-Yan Zhu, Ph.D at Berkeley
Faiss: A library for efficient similarity search and clustering of dense vectors. [2629 stars on Github]. Courtesy of Facebook Research
Fashion-mnist: A MNIST-like fashion product database [2780 stars on Github]. Courtesy of Han Xiao, Research Scientist Zalando Tech
ParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets [2578 stars on Github]. Courtesy of Alexander Miller at Facebook Research
Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit [2571 stars on Github].
Pyro: Deep universal probabilistic programming with Python and PyTorch [2387 stars on Github]. Courtesy of Uber AI Labs
iGAN: Interactive Image Generation powered by GAN [2369 stars on Github].
Deep-image-prior: Image restoration with neural networks but without learning [2188 stars on Github]. Courtesy of Dmitry Ulyanov, Ph.D at Skoltech
Face_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV. [1967 stars on Github].
Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind’s WaveNet and tensorflow [1961 stars on Github]. Courtesy of Namju Kim at Kakao Brain
StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation [1954 stars on Github]. Courtesy of Yunjey Choi at Korea University
Ml-agents: Unity Machine Learning Agents [1658 stars on Github]. Courtesy of Arthur Juliani, Deep Learning at Unity3D
DeepVideoAnalytics: A distributed visual search and visual data analytics platform [1494 stars on Github]. Courtesy of Akshay Bhat, Ph.D at Cornell University
OpenNMT: Open-Source Neural Machine Translation in Torch [1490 stars on Github].
Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs [1283 stars on Github]. Courtesy of Ming-Yu Liu at AI Research Scientist at Nvidia
Horovod: Distributed training framework for TensorFlow. [1188 stars on Github]. Courtesy of Uber Engineering
AI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models [899 stars on Github].
Deep neural networks for voice conversion (voice style transfer) in Tensorflow [845 stars on Github]. Courtesy of Dabi Ahn, AI Research at Kakao Brain
That’s it for Machine Learning open source projects. If you like this curation, read best daily articles based on your programming skills on our website.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
We rank articles for professionals
Read more and achieve more
",past year weve compared nearly 8800 open source machine learning projects pick top 30 03 chance extremely competitive list carefully picks best open source machine learning libraries datasets apps published january december 2017 mybridge ai evaluates quality considering popularity engagement recency give idea quality average number github stars 3558 open source projects useful data scientists learn reading source code build something top existing projects give plenty time play around machine learning projects may missed past year recommended learning neural networks deep learning aztm handson artificial neural networks 68745 recommends 455 stars b tensorflow complete guide tensorflow deep learning python 17834 recommends 465 stars click numbers credit given biggest contributor fasttext library fast text representation classification 11786 stars github courtesy facebook research muse multilingual unsupervised supervised word embeddings based fast text 695 stars github deepphotostyletransfer code data paper deep photo style transfer 9747 stars github courtesy fujun luan phd cornell university worlds simplest facial recognition api python command line 8672 stars github courtesy adam geitgey magenta music art generation machine intelligence 8113 stars github sonnet tensorflowbased neural network library 5731 stars github courtesy malcolm reynolds deepmind deeplearnjs hardwareaccelerated machine intelligence library web 5462 stars github courtesy nikhil thorat google brain fast style transfer tensorflow 4843 stars github courtesy logan engstrom mit pysc2 starcraft ii learning environment 3683 stars github courtesy timo ewalds deepmind airsim open source simulator based unreal engine autonomous vehicles microsoft ai research 3861 stars github courtesy shital shah microsoft facets visualizations machine learning datasets 3371 stars github courtesy google brain style2paints ai colorization images 3310 stars github tensor2tensor library generalized sequence sequence models google research 3087 stars github courtesy ryan sepassi google brain imagetoimage translation pytorch eg horse2zebra edges2cats 2847 stars github courtesy junyan zhu phd berkeley faiss library efficient similarity search clustering dense vectors 2629 stars github courtesy facebook research fashionmnist mnistlike fashion product database 2780 stars github courtesy han xiao research scientist zalando tech parlai framework training evaluating ai models variety openly available dialog datasets 2578 stars github courtesy alexander miller facebook research fairseq facebook ai research sequencetosequence toolkit 2571 stars github pyro deep universal probabilistic programming python pytorch 2387 stars github courtesy uber ai labs igan interactive image generation powered gan 2369 stars github deepimageprior image restoration neural networks without learning 2188 stars github courtesy dmitry ulyanov phd skoltech face_classification realtime face detection emotiongender classification using fer2013imdb datasets keras cnn model opencv 1967 stars github speechtotextwavenet endtoend sentence level english speech recognition using deepminds wavenet tensorflow 1961 stars github courtesy namju kim kakao brain stargan unified generative adversarial networks multidomain imagetoimage translation 1954 stars github courtesy yunjey choi korea university mlagents unity machine learning agents 1658 stars github courtesy arthur juliani deep learning unity3d deepvideoanalytics distributed visual search visual data analytics platform 1494 stars github courtesy akshay bhat phd cornell university opennmt opensource neural machine translation torch 1490 stars github pix2pixhd synthesizing manipulating 2048x1024 images conditional gans 1283 stars github courtesy mingyu liu ai research scientist nvidia horovod distributed training framework tensorflow 1188 stars github courtesy uber engineering aiblocks powerful intuitive wysiwyg interface allows anyone create machine learning models 899 stars github deep neural networks voice conversion voice style transfer tensorflow 845 stars github courtesy dabi ahn ai research kakao brain thats machine learning open source projects like curation read best daily articles based programming skills website quick cheer standing ovation clap show much enjoyed story rank articles professionals read achieve,en,"['Mybridge AI', 'Github', 'Facebook Research', 'Cornell University', 'Nikhil Thorat', 'Google Brain', 'TensorFlow', 'MIT', 'Unreal Engine', 'Microsoft AI & Research', 'Shital Shah', 'Microsoft', 'Google Research', 'PyTorch (e.g. horse2zebra', 'Berkeley\n', 'MNIST', 'Han Xiao', 'Uber AI Labs', 'Interactive Image Generation', 'GAN', 'CNN', 'WaveNet', 'Namju Kim', 'Multi-Domain Image', 'Korea University', 'Unity Machine Learning Agents', 'Torch', 'AI Research Scientist', 'Uber Engineering\n', 'WYSIWYG']"
63,David Foster,12800,How to build your own AlphaZero AI using Python and Keras,"In this article I’ll attempt to cover three things:
In March 2016, Deepmind’s AlphaGo beat 18 times world champion Go player Lee Sedol 4–1 in a series watched by over 200 million people. A machine had learnt a super-human strategy for playing Go, a feat previously thought impossible, or at the very least, at least a decade away from being accomplished.
This in itself, was a remarkable achievement. However, on 18th October 2017, DeepMind took a giant leap further.
The paper ‘Mastering the Game of Go without Human Knowledge’ unveiled a new variant of the algorithm, AlphaGo Zero, that had defeated AlphaGo 100–0. Incredibly, it had done so by learning solely through self-play, starting ‘tabula rasa’ (blank state) and gradually finding strategies that would beat previous incarnations of itself. No longer was a database of human expert games required to build a super-human AI .
A mere 48 days later, on 5th December 2017, DeepMind released another paper ‘Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm’ showing how AlphaGo Zero could be adapted to beat the world-champion programs StockFish and Elmo at chess and shogi. The entire learning process, from being shown the games for the first time, to becoming the best computer program in the world, had taken under 24 hours.
With this, AlphaZero was born — the general algorithm for getting good at something, quickly, without any prior knowledge of human expert strategy.
There are two amazing things about this achievement:
It cannot be overstated how important this is. This means that the underlying methodology of AlphaGo Zero can be applied to ANY game with perfect information (the game state is fully known to both players at all times) because no prior expertise is required beyond the rules of the game.
This is how it was possible for DeepMind to publish the chess and shogi papers only 48 days after the original AlphaGo Zero paper. Quite literally, all that needed to change was the input file that describes the mechanics of the game and to tweak the hyper-parameters relating to the neural network and Monte Carlo tree search.
If AlphaZero used super-complex algorithms that only a handful of people in the world understood, it would still be an incredible achievement. What makes it extraordinary is that a lot of the ideas in the paper are actually far less complex than previous versions. At its heart, lies the following beautifully simple mantra for learning:
Doesn’t that sound a lot like how you learn to play games? When you play a bad move, it’s either because you misjudged the future value of resulting positions, or you misjudged the likelihood that your opponent would play a certain move, so didn’t think to explore that possibility. These are exactly the two aspects of gameplay that AlphaZero is trained to learn.
Firstly, check out the AlphaGo Zero cheat sheet for a high level understanding of how AlphaGo Zero works. It’s worth having that to refer to as we walk through each part of the code. There’s also a great article here that explains how AlphaZero works in more detail.
Clone this Git repository, which contains the code I’ll be referencing.
To start the learning process, run the top two panels in the run.ipynb Jupyter notebook. Once it’s built up enough game positions to fill its memory the neural network will begin training. Through additional self-play and training, it will gradually get better at predicting the game value and next moves from any position, resulting in better decision making and smarter overall play.
We’ll now have a look at the code in more detail, and show some results that demonstrate the AI getting stronger over time.
N.B — This is my own understanding of how AlphaZero works based on the information available in the papers referenced above. If any of the below is incorrect, apologies and I’ll endeavour to correct it!
The game that our algorithm will learn to play is Connect4 (or Four In A Row). Not quite as complex as Go... but there are still 4,531,985,219,092 game positions in total.
The game rules are straightforward. Players take it in turns to enter a piece of their colour in the top of any available column. The first player to get four of their colour in a row — each vertically, horizontally or diagonally, wins. If the entire grid is filled without a four-in-a-row being created, the game is drawn.
Here’s a summary of the key files that make up the codebase:
This file contains the game rules for Connect4.
Each squares is allocated a number from 0 to 41, as follows:
The game.py file gives the logic behind moving from one game state to another, given a chosen action. For example, given the empty board and action 38, the takeAction method return a new game state, with the starting player’s piece at the bottom of the centre column.
You can replace the game.py file with any game file that conforms to the same API and the algorithm will in principal, learn strategy through self play, based on the rules you have given it.
This contains the code that starts the learning process. It loads the game rules and then iterates through the main loop of the algorithm, which consist of three stages:
There are two agents involved in this loop, the best_player and the current_player.
The best_player contains the best performing neural network and is used to generate the self play memories. The current_player then retrains its neural network on these memories and is then pitched against the best_player. If it wins, the neural network inside the best_player is switched for the neural network inside the current_player, and the loop starts again.
This contains the Agent class (a player in the game). Each player is initialised with its own neural network and Monte Carlo Search Tree.
The simulate method runs the Monte Carlo Tree Search process. Specifically, the agent moves to a leaf node of the tree, evaluates the node with its neural network and then backfills the value of the node up through the tree.
The act method repeats the simulation multiple times to understand which move from the current position is most favourable. It then returns the chosen action to the game, to enact the move.
The replay method retrains the neural network, using memories from previous games.
This file contains the Residual_CNN class, which defines how to build an instance of the neural network.
It uses a condensed version of the neural network architecture in the AlphaGoZero paper — i.e. a convolutional layer, followed by many residual layers, then splitting into a value and policy head.
The depth and number of convolutional filters can be specified in the config file.
The Keras library is used to build the network, with a backend of Tensorflow.
To view individual convolutional filters and densely connected layers in the neural network, run the following inside the the run.ipynb notebook:
This contains the Node, Edge and MCTS classes, that constitute a Monte Carlo Search Tree.
The MCTS class contains the moveToLeaf and backFill methods previously mentioned, and instances of the Edge class store the statistics about each potential move.
This is where you set the key parameters that influence the algorithm.
Adjusting these variables will affect that running time, neural network accuracy and overall success of the algorithm. The above parameters produce a high quality Connect4 player, but take a long time to do so. To speed the algorithm up, try the following parameters instead.
Contains the playMatches and playMatchesBetweenVersions functions that play matches between two agents.
To play against your creation, run the following code (it’s also in the run.ipynb notebook)
When you run the algorithm, all model and memory files are saved in the run folder, in the root directory.
To restart the algorithm from this checkpoint later, transfer the run folder to the run_archive folder, attaching a run number to the folder name. Then, enter the run number, model version number and memory version number into the initialise.py file, corresponding to the location of the relevant files in the run_archive folder. Running the algorithm as usual will then start from this checkpoint.
An instance of the Memory class stores the memories of previous games, that the algorithm uses to retrain the neural network of the current_player.
This file contains a custom loss function, that masks predictions from illegal moves before passing to the cross entropy loss function.
The locations of the run and run_archive folders.
Log files are saved to the log folder inside the run folder.
To turn on logging, set the values of the logger_disabled variables to False inside this file.
Viewing the log files will help you to understand how the algorithm works and see inside its ‘mind’. For example, here is a sample from the logger.mcts file.
Equally from the logger.tourney file, you can see the probabilities attached to each move, during the evaluation phase:
Training over a couple of days produces the following chart of loss against mini-batch iteration number:
The top line is the error in the policy head (the cross entropy of the MCTS move probabilities, against the output from the neural network). The bottom line is the error in the value head (the mean squared error between the actual game value and the neural network predict of the value). The middle line is an average of the two.
Clearly, the neural network is getting better at predicting the value of each game state and the likely next moves. To show how this results in stronger and stronger play, I ran a league between 17 players, ranging from the 1st iteration of the neural network, up to the 49th. Each pairing played twice, with both players having a chance to play first.
Here are the final standings:
Clearly, the later versions of the neural network are superior to the earlier versions, winning most of their games. It also appears that the learning hasn’t yet saturated — with further training time, the players would continue to get stronger, learning more and more intricate strategies.
As an example, one clear strategy that the neural network has favoured over time is grabbing the centre column early. Observe the difference between the first version of the algorithm and say, the 30th version:
1st neural network version
30th neural network version
This is a good strategy as many lines require the centre column — claiming this early ensures your opponent cannot take advantage of this. This has been learnt by the neural network, without any human input.
There is a game.py file for a game called ‘Metasquares’ in the games folder. This involves placing X and O markers in a grid to try to form squares of different sizes. Larger squares score more points than smaller squares and the player with the most points when the grid is full wins.
If you switch the Connect4 game.py file for the Metasquares game.py file, the same algorithm will learn how to play Metasquares instead.
Hopefully you find this article useful — let me know in the comments below if you find any typos or have questions about anything in the codebase or article and I’ll get back to you as soon as possible.
If you would like to learn more about how our company, Applied Data Science develops innovative data science solutions for businesses, feel free to get in touch through our website or directly through LinkedIn.
... and if you like this, feel free to leave a few hearty claps :)
Applied Data Science is a London based consultancy that implements end-to-end data science solutions for businesses, delivering measurable value. If you’re looking to do more with your data, let’s talk.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder of Applied Data Science
Cutting edge data science news and projects
",article ill attempt cover three things march 2016 deepminds alphago beat 18 times world champion go player lee sedol 41 series watched 200 million people machine learnt superhuman strategy playing go feat previously thought impossible least least decade away accomplished remarkable achievement however 18th october 2017 deepmind took giant leap paper mastering game go without human knowledge unveiled new variant algorithm alphago zero defeated alphago 1000 incredibly done learning solely selfplay starting tabula rasa blank state gradually finding strategies would beat previous incarnations longer database human expert games required build superhuman ai mere 48 days later 5th december 2017 deepmind released another paper mastering chess shogi selfplay general reinforcement learning algorithm showing alphago zero could adapted beat worldchampion programs stockfish elmo chess shogi entire learning process shown games first time becoming best computer program world taken 24 hours alphazero born general algorithm getting good something quickly without prior knowledge human expert strategy two amazing things achievement cannot overstated important means underlying methodology alphago zero applied game perfect information game state fully known players times prior expertise required beyond rules game possible deepmind publish chess shogi papers 48 days original alphago zero paper quite literally needed change input file describes mechanics game tweak hyperparameters relating neural network monte carlo tree search alphazero used supercomplex algorithms handful people world understood would still incredible achievement makes extraordinary lot ideas paper actually far less complex previous versions heart lies following beautifully simple mantra learning doesnt sound lot like learn play games play bad move either misjudged future value resulting positions misjudged likelihood opponent would play certain move didnt think explore possibility exactly two aspects gameplay alphazero trained learn firstly check alphago zero cheat sheet high level understanding alphago zero works worth refer walk part code theres also great article explains alphazero works detail clone git repository contains code ill referencing start learning process run top two panels runipynb jupyter notebook built enough game positions fill memory neural network begin training additional selfplay training gradually get better predicting game value next moves position resulting better decision making smarter overall play well look code detail show results demonstrate ai getting stronger time nb understanding alphazero works based information available papers referenced incorrect apologies ill endeavour correct game algorithm learn play connect4 four row quite complex go still 4531985219092 game positions total game rules straightforward players take turns enter piece colour top available column first player get four colour row vertically horizontally diagonally wins entire grid filled without fourinarow created game drawn heres summary key files make codebase file contains game rules connect4 squares allocated number 0 41 follows gamepy file gives logic behind moving one game state another given chosen action example given empty board action 38 takeaction method return new game state starting players piece bottom centre column replace gamepy file game file conforms api algorithm principal learn strategy self play based rules given contains code starts learning process loads game rules iterates main loop algorithm consist three stages two agents involved loop best_player current_player best_player contains best performing neural network used generate self play memories current_player retrains neural network memories pitched best_player wins neural network inside best_player switched neural network inside current_player loop starts contains agent class player game player initialised neural network monte carlo search tree simulate method runs monte carlo tree search process specifically agent moves leaf node tree evaluates node neural network backfills value node tree act method repeats simulation multiple times understand move current position favourable returns chosen action game enact move replay method retrains neural network using memories previous games file contains residual_cnn class defines build instance neural network uses condensed version neural network architecture alphagozero paper ie convolutional layer followed many residual layers splitting value policy head depth number convolutional filters specified config file keras library used build network backend tensorflow view individual convolutional filters densely connected layers neural network run following inside runipynb notebook contains node edge mcts classes constitute monte carlo search tree mcts class contains movetoleaf backfill methods previously mentioned instances edge class store statistics potential move set key parameters influence algorithm adjusting variables affect running time neural network accuracy overall success algorithm parameters produce high quality connect4 player take long time speed algorithm try following parameters instead contains playmatches playmatchesbetweenversions functions play matches two agents play creation run following code also runipynb notebook run algorithm model memory files saved run folder root directory restart algorithm checkpoint later transfer run folder run_archive folder attaching run number folder name enter run number model version number memory version number initialisepy file corresponding location relevant files run_archive folder running algorithm usual start checkpoint instance memory class stores memories previous games algorithm uses retrain neural network current_player file contains custom loss function masks predictions illegal moves passing cross entropy loss function locations run run_archive folders log files saved log folder inside run folder turn logging set values logger_disabled variables false inside file viewing log files help understand algorithm works see inside mind example sample loggermcts file equally loggertourney file see probabilities attached move evaluation phase training couple days produces following chart loss minibatch iteration number top line error policy head cross entropy mcts move probabilities output neural network bottom line error value head mean squared error actual game value neural network predict value middle line average two clearly neural network getting better predicting value game state likely next moves show results stronger stronger play ran league 17 players ranging 1st iteration neural network 49th pairing played twice players chance play first final standings clearly later versions neural network superior earlier versions winning games also appears learning hasnt yet saturated training time players would continue get stronger learning intricate strategies example one clear strategy neural network favoured time grabbing centre column early observe difference first version algorithm say 30th version 1st neural network version 30th neural network version good strategy many lines require centre column claiming early ensures opponent cannot take advantage learnt neural network without human input gamepy file game called metasquares games folder involves placing x markers grid try form squares different sizes larger squares score points smaller squares player points grid full wins switch connect4 gamepy file metasquares gamepy file algorithm learn play metasquares instead hopefully find article useful let know comments find typos questions anything codebase article ill get back soon possible would like learn company applied data science develops innovative data science solutions businesses feel free get touch website directly linkedin like feel free leave hearty claps applied data science london based consultancy implements endtoend data science solutions businesses delivering measurable value youre looking data lets talk quick cheer standing ovation clap show much enjoyed story cofounder applied data science cutting edge data science news projects,en,"['AlphaGo', 'AlphaZero', 'Clone', 'algorithm', 'API', 'Residual_CNN', 'Node', 'run_archive', 'initialise.py', 'Applied Data Science', 'LinkedIn']"
64,George Seif,11400,The 5 Clustering Algorithms Data Scientists Need to Know,"Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.
In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we’re going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!
K-Means is probably the most well know clustering algorithm. It’s taught in a lot of introductory data science and machine learning classes. It’s easy to understand and implement in code! Check out the graphic below for an illustration.
K-Means has the advantage that it’s pretty fast, as all we’re really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity O(n).
On the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn’t always trivial and ideally with a clustering algorithm we’d want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.
K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.
Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.
An illustration of the entire process from end-to-end with all of the sliding windows is show below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.
In contrast to K-means clustering there is no need to select the number of clusters as mean-shift automatically discovers this. That’s a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius “r” can be non-trivial.
DBSCAN is a density based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let’s get started!
DBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it is able to find arbitrarily sized and arbitrarily shaped clusters quite well.
The main drawback of DBSCAN is that it doesn’t perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold ε and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold ε becomes challenging to estimate.
One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn’t the best way of doing things by looking at the image below. On the left hand side it looks quite obvious to the human eye that there are two circular clusters with different radius’ centered at the same mean. K-Means can’t handle this because the mean values of the clusters are a very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.
Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.
In order to find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation) we will use an optimization algorithm called Expectation–Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed on to the process of Expectation–Maximization clustering using GMMs.
There are really 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster’s covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.
Hierarchical clustering algorithms actually fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm steps
Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can’t do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n3), unlike the linear complexity of K-Means and GMM.
There are your top 5 clustering algorithms that a data scientist should know! We’ll end off with an awesome visualization of how well these algorithms and a few others perform, courtesy of Scikit Learn!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Certified Nerd. AI / Machine Learning Engineer.
Sharing concepts, ideas, and codes.
",clustering machine learning technique involves grouping data points given set data points use clustering algorithm classify data point specific group theory data points group similar properties andor features data points different groups highly dissimilar properties andor features clustering method unsupervised learning common technique statistical data analysis used many fields data science use clustering analysis gain valuable insights data seeing groups data points fall apply clustering algorithm today going look 5 popular clustering algorithms data scientists need know pros cons kmeans probably well know clustering algorithm taught lot introductory data science machine learning classes easy understand implement code check graphic illustration kmeans advantage pretty fast really computing distances points group centers computations thus linear complexity hand kmeans couple disadvantages firstly select many groupsclasses isnt always trivial ideally clustering algorithm wed want figure us point gain insight data kmeans also starts random choice cluster centers therefore may yield different clustering results different runs algorithm thus results may repeatable lack consistency cluster methods consistent kmedians another clustering algorithm related kmeans except instead recomputing group center points using mean use median vector group method less sensitive outliers using median much slower larger datasets sorting required iteration computing median vector mean shift clustering slidingwindowbased algorithm attempts find dense areas data points centroidbased algorithm meaning goal locate center points groupclass works updating candidates center points mean points within slidingwindow candidate windows filtered postprocessing stage eliminate nearduplicates forming final set center points corresponding groups check graphic illustration illustration entire process endtoend sliding windows show black dot represents centroid sliding window gray dot data point contrast kmeans clustering need select number clusters meanshift automatically discovers thats massive advantage fact cluster centers converge towards points maximum density also quite desirable quite intuitive understand fits well naturally datadriven sense drawback selection window sizeradius r nontrivial dbscan density based clustered algorithm similar meanshift couple notable advantages check another fancy graphic lets get started dbscan poses great advantages clustering algorithms firstly require peset number clusters also identifies outliers noises unlike meanshift simply throws cluster even data point different additionally able find arbitrarily sized arbitrarily shaped clusters quite well main drawback dbscan doesnt perform well others clusters varying density setting distance threshold minpoints identifying neighborhood points vary cluster cluster density varies drawback also occurs highdimensional data since distance threshold becomes challenging estimate one major drawbacks kmeans naive use mean value cluster center see isnt best way things looking image left hand side looks quite obvious human eye two circular clusters different radius centered mean kmeans cant handle mean values clusters close together kmeans also fails cases clusters circular result using mean cluster center gaussian mixture models gmms give us flexibility kmeans gmms assume data points gaussian distributed less restrictive assumption saying circular using mean way two parameters describe shape clusters mean standard deviation taking example two dimensions means clusters take kind elliptical shape since standard deviation x directions thus gaussian distribution assigned single cluster order find parameters gaussian cluster eg mean standard deviation use optimization algorithm called expectationmaximization em take look graphic illustration gaussians fitted clusters proceed process expectationmaximization clustering using gmms really 2 key advantages using gmms firstly gmms lot flexible terms cluster covariance kmeans due standard deviation parameter clusters take ellipse shape rather restricted circles kmeans actually special case gmm clusters covariance along dimensions approaches 0 secondly since gmms use probabilities multiple clusters per data point data point middle two overlapping clusters simply define class saying belongs xpercent class 1 ypercent class 2 ie gmms support mixed membership hierarchical clustering algorithms actually fall 2 categories topdown bottomup bottomup algorithms treat data point single cluster outset successively merge agglomerate pairs clusters clusters merged single cluster contains data points bottomup hierarchical clustering therefore called hierarchical agglomerative clustering hac hierarchy clusters represented tree dendrogram root tree unique cluster gathers samples leaves clusters one sample check graphic illustration moving algorithm steps hierarchical clustering require us specify number clusters even select number clusters looks best since building tree additionally algorithm sensitive choice distance metric tend work equally well whereas clustering algorithms choice distance metric critical particularly good use case hierarchical clustering methods underlying data hierarchical structure want recover hierarchy clustering algorithms cant advantages hierarchical clustering come cost lower efficiency time complexity on3 unlike linear complexity kmeans gmm top 5 clustering algorithms data scientist know well end awesome visualization well algorithms others perform courtesy scikit learn quick cheer standing ovation clap show much enjoyed story certified nerd ai machine learning engineer sharing concepts ideas codes,en,"['K-Means', 'algorithm', 'K-means', 'DBSCAN', 'minPoints', 'Gaussian Mixture Models', 'GMM', 'Hierarchical', 'HAC', 'Certified Nerd']"
65,Mybridge,6600,30 Amazing Python Projects for the Past Year (v.2018),"For the past year, we’ve compared nearly 15,000 open source Python projects to pick Top 30 (0.2% chance).
This is an extremely competitive list and it carefully picks the best open source Python libraries, tools and programs published between January and December 2017. Mybridge AI evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of Github stars is 3,707.
Open source projects can be useful for programmers. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Python projects you may have missed for the past year.
<Recommended Learning>
A) Beginner
The Python Bible: Build 11 Projects and Go from Beginner to Pro
[27,672 recommends, 4.7/5 stars]
B) Data Science
Python for Data Science and Machine Learning Bootcamp: Use NumPy, Pandas, Seaborn , Matplotlib , Plotly
[90,212 recommends, 4.6/5 stars]
(Click the numbers below. Credit given to the biggest contributor.)
Home-assistant (v0.6+): Open-source home automation platform running on Python 3 [11357 stars on Github]. Courtesy of Paulus Schoutsen
Pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration [11019 stars on Github]. Courtesy of Adam Paszke and others at PyTorch Team
Grumpy: A Python to Go source code transcompiler and runtime. [8367 stars on Github]. Courtesy of Dylan Trotter and others at Google
Sanic: Async Python 3.5+ web server that’s written to go fast [8028 stars on Github]. Courtesy of Channel Cat and Eli Uriegas
Python-fire: A library for automatically generating command line interfaces (CLIs) from absolutely any Python object. [7775 stars on Github]. Courtesy of David Bieber and others at Google Brain.
spaCy (v2.0): Industrial-strength Natural Language Processing (NLP) with Python and Cython [7633 stars on Github]. Courtesy of Matthew Honnibal
Pipenv: Python Development Workflow for Humans [7273 stars on Github]. Courtesy of Kenneth Reitz
MicroPython: A lean and efficient Python implementation for microcontrollers and constrained systems [5728 stars on Github].
Prophet: Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth [4369 stars on Github]. Courtesy of Facebook
SerpentAI: Game Agent Framework in Python. Helping you create AIs / Bots to play any game [3411 stars on Github]. Courtesy of Nicholas Brochu
Dash: Interactive, reactive web apps in pure python [3281 stars on Github]. Courtesy of Chris P
InstaPy: Instagram Bot. Like/Comment/Follow Automation Script. [3179 stars on Github]. Courtesy of TimG
Apistar: A fast and expressive API framework. For Python [3024 stars on Github]. Courtesy of Tom Christie
Faiss: A library for efficient similarity search and clustering of dense vectors [2717 stars on Github]. Courtesy of Matthijs Douze and others at Facebook Research
MechanicalSoup: A Python library for automating interaction with websites [2244 stars on Github].
Better-exceptions: Pretty and useful exceptions in Python, automatically [2121 stars on Github]. Courtesy of Qix
Flashtext: Extract Keywords from sentence or Replace keywords in sentences [2019 stars on Github]. Courtesy of Vikash Singh
Maya: Datetime for Humans in Python [1828 stars on Github]. Kenneth Reitz
Mimesis (v1.0): Python library, which helps generate mock data in different languages for various purposes. These data can be especially useful at various stages of software development and testing [1732 stars on Github]. Courtesy of Líkið Geimfari
Open-paperless: Scan, index, and archive all of your paper documents. A document management system. [1717 stars on Github]. Courtesy of Tina Zhou
Fsociety: Hacking Tools Pack. A Penetration Testing Framework. [1585 stars on Github]. Courtesy of Manis Manisso
LivePython: Visually trace Python code in real-time [1577 stars on Github]. Courtesy of Anastasis Germanidis
Hatch: A modern project, package, and virtual env manager for Python [1537 stars on Github]. Courtesy of Ofek Lev
Tangent: Source-to-Source Debuggable Derivatives in Pure Python [1433 stars on Github]. Courtesy of Alex Wiltschko and others at Google Brain
Clairvoyant: A Python program that identifies and monitors historical cues for short term stock movement [1159 stars on Github]. Courtesy of Anthony Federico
MonkeyType: A system for Python that generates static type annotations by collecting runtime types. [1143 stars on Github]. Courtesy of Carl Meyer at Instagram Engineering
Eel: A little Python library for making simple Electron-like HTML/JS GUI apps [1137 stars on Github].
Surprise v1.0: A Python scikit for building and analyzing recommender systems [1103 stars on Github].
Gain: Web crawling framework for everyone. [1009 stars on Github]. Courtesy of 高久力
PDFTabExtract: A set of tools for extracting tables from PDF files helping to do data mining on scanned documents. [722 stars on Github].
That’s it for Python Open Source of the Year. If you like this curation, read best daily articles based on your programming skills on our website.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
We rank articles for professionals
Read more and achieve more
",past year weve compared nearly 15000 open source python projects pick top 30 02 chance extremely competitive list carefully picks best open source python libraries tools programs published january december 2017 mybridge ai evaluates quality considering popularity engagement recency give idea quality average number github stars 3707 open source projects useful programmers learn reading source code build something top existing projects give plenty time play around python projects may missed past year recommended learning beginner python bible build 11 projects go beginner pro 27672 recommends 475 stars b data science python data science machine learning bootcamp use numpy pandas seaborn matplotlib plotly 90212 recommends 465 stars click numbers credit given biggest contributor homeassistant v06 opensource home automation platform running python 3 11357 stars github courtesy paulus schoutsen pytorch tensors dynamic neural networks python strong gpu acceleration 11019 stars github courtesy adam paszke others pytorch team grumpy python go source code transcompiler runtime 8367 stars github courtesy dylan trotter others google sanic async python 35 web server thats written go fast 8028 stars github courtesy channel cat eli uriegas pythonfire library automatically generating command line interfaces clis absolutely python object 7775 stars github courtesy david bieber others google brain spacy v20 industrialstrength natural language processing nlp python cython 7633 stars github courtesy matthew honnibal pipenv python development workflow humans 7273 stars github courtesy kenneth reitz micropython lean efficient python implementation microcontrollers constrained systems 5728 stars github prophet tool producing high quality forecasts time series data multiple seasonality linear nonlinear growth 4369 stars github courtesy facebook serpentai game agent framework python helping create ais bots play game 3411 stars github courtesy nicholas brochu dash interactive reactive web apps pure python 3281 stars github courtesy chris p instapy instagram bot likecommentfollow automation script 3179 stars github courtesy timg apistar fast expressive api framework python 3024 stars github courtesy tom christie faiss library efficient similarity search clustering dense vectors 2717 stars github courtesy matthijs douze others facebook research mechanicalsoup python library automating interaction websites 2244 stars github betterexceptions pretty useful exceptions python automatically 2121 stars github courtesy qix flashtext extract keywords sentence replace keywords sentences 2019 stars github courtesy vikash singh maya datetime humans python 1828 stars github kenneth reitz mimesis v10 python library helps generate mock data different languages various purposes data especially useful various stages software development testing 1732 stars github courtesy liki geimfari openpaperless scan index archive paper documents document management system 1717 stars github courtesy tina zhou fsociety hacking tools pack penetration testing framework 1585 stars github courtesy manis manisso livepython visually trace python code realtime 1577 stars github courtesy anastasis germanidis hatch modern project package virtual env manager python 1537 stars github courtesy ofek lev tangent sourcetosource debuggable derivatives pure python 1433 stars github courtesy alex wiltschko others google brain clairvoyant python program identifies monitors historical cues short term stock movement 1159 stars github courtesy anthony federico monkeytype system python generates static type annotations collecting runtime types 1143 stars github courtesy carl meyer instagram engineering eel little python library making simple electronlike htmljs gui apps 1137 stars github surprise v10 python scikit building analyzing recommender systems 1103 stars github gain web crawling framework everyone 1009 stars github courtesy pdftabextract set tools extracting tables pdf files helping data mining scanned documents 722 stars github thats python open source year like curation read best daily articles based programming skills website quick cheer standing ovation clap show much enjoyed story rank articles professionals read achieve,en,"['Python', 'Mybridge AI', 'Github', 'Data Science and Machine Learning', 'Paulus Schoutsen\n', 'GPU', 'PyTorch Team', 'Google', 'Channel Cat', 'Google Brain', 'Natural Language Processing', 'NLP', 'Cython', 'Python Development Workflow', 'MicroPython', 'Matthijs Douze', 'Facebook Research', 'Replace', 'Electron']"
66,Simon Greenman,10200,Who Is Going To Make Money In AI? Part I – Towards Data Science,"We are in the midst of a gold rush in AI. But who will reap the economic benefits? The mass of startups who are all gold panning? The corporates who have massive gold mining operations? The technology giants who are supplying the picks and shovels? And which nations have the richest seams of gold?
We are currently experiencing another gold rush in AI. Billions are being invested in AI startups across every imaginable industry and business function. Google, Amazon, Microsoft and IBM are in a heavyweight fight investing over $20 billion in AI in 2016. Corporates are scrambling to ensure they realise the productivity benefits of AI ahead of their competitors while looking over their shoulders at the startups. China is putting its considerable weight behind AI and the European Union is talking about a $22 billion AI investment as it fears losing ground to China and the US.
AI is everywhere. From the 3.5 billion daily searches on Google to the new Apple iPhone X that uses facial recognition to Amazon Alexa that cutely answers our questions. Media headlines tout the stories of how AI is helping doctors diagnose diseases, banks better assess customer loan risks, farmers predict crop yields, marketers target and retain customers, and manufacturers improve quality control. And there are think tanks dedicated to studying the physical, cyber and political risks of AI.
AI and machine learning will become ubiquitous and woven into the fabric of society. But as with any gold rush the question is who will find gold? Will it just be the brave, the few and the large? Or can the snappy upstarts grab their nuggets? Will those providing the picks and shovel make most of the money? And who will hit pay dirt?
As I started thinking about who was going to make money in AI I ended up with seven questions. Who will make money across the (1) chip makers, (2) platform and infrastructure providers, (3) enabling models and algorithm providers, (4) enterprise solution providers, (5) industry vertical solution providers, (6) corporate users of AI and (7) nations? While there are many ways to skin the cat of the AI landscape, hopefully below provides a useful explanatory framework — a value chain of sorts. The companies noted are representative of larger players in each category but in no way is this list intended to be comprehensive or predictive.
Even though the price of computational power has fallen exponentially, demand is rising even faster. AI and machine learning with its massive datasets and its trillions of vector and matrix calculations has a ferocious and insatiable appetite. Bring on the chips.
NVIDIA’s stock is up 1500% in the past two years benefiting from the fact that their graphical processing unit (GPU) chips that were historically used to render beautiful high speed flowing games graphics were perfect for machine learning. Google recently launched its second generation of Tensor Processing Units (TPUs). And Microsoft is building its own Brainwave AI machine learning chips. At the same time startups such as Graphcore, who has raised over $110M, is looking to enter the market. Incumbents chip providers such as IBM, Intel, Qualcomm and AMD are not standing still. Even Facebook is rumoured to be building a team to design its own AI chips. And the Chinese are emerging as serious chip players with Cambricon Technology announcing the first cloud AI chip this past week.
What is clear is that the cost of designing and manufacturing chips then sustaining a position as a global chip leader is very high. It requires extremely deep pockets and a world class team of silicon and software engineers. This means that there will be very few new winners. Just like the gold rush days those that provide the cheapest and most widely used picks and shovels will make a lot of money.
The AI race is now also taking place in the cloud. Amazon realised early that startups would much rather rent computers and software than buy it. And so it launched Amazon Web Services (AWS) in 2006. Today AI is demanding so much compute power that companies are increasingly turning to the cloud to rent hardware through Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) offerings.
The fight is on among the tech giants. Microsoft is offering their hybrid public and private Azure cloud service that allegedly has over one million computers. And in the past few weeks they announced that their Brainwave hardware solutionsdramatically accelerate machine learning with their own Bing search engine performance improving by a factor of ten. Google is rushing to play catchup with its own GoogleCloud offering. And we are seeing the Chinese Alibaba starting to take global share.
Amazon — Microsoft — Google and IBM are going to continue to duke this one out. And watch out for the massively scaled cloud players from China. The big picks and shovels guys will win again.
Today Google is the world’s largest AI company attracting the best AI minds, spending small country size GDP budgets on R&D, and sitting on the best datasets gleamed from the billions of users of their services. AI is powering Google’s search, autonomous vehicles, speech recognition, intelligent reasoning, massive search and even its own work on drug discovery and disease diangosis.
And the incredible AI machine learning software and algorithms that are powering all of Google’s AI activity — TensorFlow — is now being given away for free. Yes for free! TensorFlow is now an open source software project available to the world. And why are they doing this? As Jeff Dean, head of Google Brain, recently said there are 20 million organisations in the world that could benefit from machine learning today. If millions of companies use this best in class free AI software then they are likely to need lots of computing power. And who is better served to offer that? Well Google Cloud is of course optimised for TensorFlow and related AI services. And once you become reliant on their software and their cloud you become a very sticky customer for many years to come. No wonder it is a brutal race for global AI algorithm dominance with Amazon — Microsoft — IBM also offering their own cheap or free AI software services.
We are also seeing a fight for not only machine learning algorithms but cognitive algorithms that offer services for conversational agents and bots, speech, natural language processing (NLP) and semantics, vision, and enhanced core algorithms. One startup in this increasingly contested space is Clarifai who provides advanced image recognition systems for businesses to detect near-duplicates and visual searches. It has raised nearly $40M over the past three years. The market for vision related algorithms and services is estimated to be a cumulative $8 billion in revenue between 2016 and 2025.
The giants are not standing still. IBM, for example, is offering its Watson cognitive products and services. They have twenty or so APIs for chatbots, vision, speech, language, knowledge management and empathy that can be simply be plugged into corporate software to create AI enabled applications. Cognitive APIs are everywhere. KDnuggets lists here over 50 of the top cognitive services from the giants and startups. These services are being put into the cloud as AI as a Service (AIaaS) to make them more accessible. Just recently Microsoft’s CEO Satya Nadella claimed that a million developers are using their AI APIs, services and tools for building AI-powered apps and nearly 300,000 developers are using their tools for chatbots. I wouldn’t want to be a startup competing with these Goliaths.
The winners in this space are likely to favour the heavyweights again. They can hire the best research and engineering talent, spend the most money, and have access to the largest datasets. To flourish startups are going to have to be really well funded, supported by leading researchers with a whole battery of IP patents and published papers, deep domain expertise, and have access to quality datasets. And they should have excellent navigational skills to sail ahead of the giants or sail different races. There will many startup casualties, but those that can scale will find themselves as global enterprises or quickly acquired by the heavyweights. And even if a startup has not found a path to commercialisation, then they could become acquihires (companies bought for their talent) if they are working on enabling AI algorithms with a strong research oriented team. We saw this in 2014 when DeepMind, a two year old London based company that developed unique reinforcement machine learning algorithms, was acquired by Google for $400M.
Enterprise software has been dominated by giants such as Salesforce, IBM, Oracle and SAP. They all recognise that AI is a tool that needs to be integrated into their enterprise offerings. But many startups are rushing to become the next generation of enterprise services filling in gaps where the incumbents don’t currently tread or even attempting to disrupt them.
We analysed over two hundred use cases in the enterprise space ranging from customer management to marketing to cybersecurity to intelligence to HR to the hot area of Cognitive Robotic Process Automation (RPA). The enterprise field is much more open than previous spaces with a veritable medley of startups providing point solutions for these use cases. Today there are over 200 AI powered companies just in the recruitment space, many of them AI startups. Cybersecurity leader DarkTrace and RPA leader UiPathhave war chests in the $100 millions. The incumbents also want to make sure their ecosystems stay on the forefront and are investing in startups that enhance their offering. Salesforce has invested in Digital Genius a customer management solution and similarly Unbable that offers enterprise translation services. Incumbents also often have more pressing problems. SAP, for example, is rushing to play catchup in offering a cloud solution, let alone catchup in AI. We are also seeing tools providers trying to simplify the tasks required to create, deploy and manage AI services in the enterprise. Machine learning training, for example, is a messy business where 80% of time can be spent on data wrangling. And an inordinate amount of time is spent on testing and tuning of what is called hyperparameters. Petuum, a tools provider based in Pittsburgh in the US, has raised over $100M to help accelerate and optimise the deployment of machine learning models.
Many of these enterprise startup providers can have a healthy future if they quickly demonstrate that they are solving and scaling solutions to meet real world enterprise needs. But as always happens in software gold rushes there will be a handful of winners in each category. And for those AI enterprise category winners they are likely to be snapped up, along with the best in-class tool providers, by the giants if they look too threatening.
AI is driving a race for the best vertical industry solutions. There are a wealth of new AI powered startups providing solutions to corporate use cases in the healthcare, financial services, agriculture, automative, legal and industrial sectors. And many startups are taking the ambitious path to disrupt the incumbent corporate players by offering a service directly to the same customers.
It is clear that many startups are providing valuable point solutions and can succeed if they have access to (1) large and proprietary data training sets, (2) domain knowledge that gives them deep insights into the opportunities within a sector, (3) a deep pool of talent around applied AI and (4) deep pockets of capital to fund rapid growth. Those startups that are doing well generally speak the corporate commercial language of customers, business efficiency and ROI in the form of well developed go-to-market plans.
For example, ZestFinance has raised nearly $300M to help improve credit decision making that will provide fair and transparent credit to everyone. They claim they have the world’s best data scientists. But they would, wouldn’t they? For those startups that are looking to disrupt existing corporate players they need really deep pockets. For example, Affirm, that offers loans to consumers at the point of sale, has raised over $700M. These companies quickly need to create a defensible moat to ensure they remain competitive. This can come from data network effects where more data begets better AI based services and products that gets more revenue and customers that gets more data. And so the flywheel effect continues.
And while corporates might look to new vendors in their industry for AI solutions that could enhance their top and bottom line, they are not going to sit back and let upstarts muscle in on their customers. And they are not going to sit still and let their corporate competitors gain the first advantage through AI. There is currently a massive race for corporate innovation. Large companies have their own venture groups investing in startups, running accelerators and building their own startups to ensure that they are leaders in AI driven innovation.
Large corporates are in a strong position against the startups and smaller companies due to their data assets. Data is the fuel for AI and machine learning. Who is better placed to take advantage of AI than the insurance company that has reams of historic data on underwriting claims? The financial services company that knows everything about consumer financial product buying behaviour? Or the search company that sees more user searches for information than any other?
Corporates large and small are well positioned to extract value from AI. In fact Gartner research predicts AI-derived business value is projected to reach up to $3.9 trillion by 2022. There are hundreds if not thousands of valuable use cases that AI can addresses across organisations. Corporates can improve their customer experience, save costs, lower prices, drive revenues and sell better products and services powered by AI. AI will help the big get bigger often at the expense of smaller companies. But they will need to demonstrate strong visionary leadership, an ability to execute, and a tolerance for not always getting technology enabled projects right on the first try.
Countries are also also in a battle for AI supremacy. China has not been shy about its call to arms around AI. It is investing massively in growing technical talent and developing startups. Its more lax regulatory environment, especially in data privacy, helps China lead in AI sectors such as security and facial recognition. Just recently there was an example of Chinese police picking out one most wanted face in a crowd of 50,000 at a music concert. And SenseTime Group Ltd, that analyses faces and images on a massive scale, reported it raised $600M becoming the most valuable global AI startup. The Chinese point out that their mobile market is 3x the size of the US and there are 50x more mobile payments taking place — this is a massive data advantage. The European focus on data privacy regulation could put them at a disadvantage in certain areas of AI even if the Union is talking about a $22B investment in AI.
The UK, Germany, France and Japan have all made recent announcements about their nation state AI strategies. For example, President Macron said the French government will spend $1.85 billion over the next five years to support the AI ecosystem including the creation of large public datasets. Companies such as Google’s DeepMind and Samsung have committed to open new Paris labs and Fujitsu is expanding its Paris research centre. The British just announced a $1.4 billion push into AI including funding of 1000 AI PhDs. But while nations are investing in AI talent and the ecosystem, the question is who will really capture the value. Will France and the UK simply be subsidising PhDs who will be hired by Google? And while payroll and income taxes will be healthy on those six figure machine learning salaries, the bulk of the economic value created could be with this American company, its shareholders, and the smiling American Treasury.
AI will increase productivity and wealth in companies and countries. But how will that wealth be distributed when the headlines suggest that 30 to 40% of our jobs will be taken by the machines? Economists can point to lessons from hundreds of years of increasing technology automation. Will there be net job creation or net job loss? The public debate often cites Geoffrey Hinton, the godfather of machine learning, who suggested radiologists will lose their jobs by the dozen as machines diagnose diseases from medical images. But then we can look to the Chinese who are using AI to assist radiologists in managing the overwhelming demand to review 1.4 billion CT scans annually for lung cancer. The result is not job losses but an expanded market with more efficient and accurate diagnosis. However there is likely to be a period of upheaval when much of the value will go to those few companies and countries that control AI technology and data. And lower skilled countries whose wealth depends on jobs that are targets of AI automation will likely suffer. AI will favour the large and the technologically skilled.
In examining the landscape of AI it has became clear that we are now entering a truly golden era for AI. And there are few key themes appearing as to where the economic value will migrate:
In short it looks like the AI gold rush will favour the companies and countries with control and scale over the best AI tools and technology, the data, the best technical workers, the most customers and the strongest access to capital. Those with scale will capture the lion’s share of the economic value from AI. In some ways ‘plus ça change, plus c’est la même chose.’ But there will also be large golden nuggets that will be found by a few choice brave startups. But like any gold rush many startups will hit pay dirt. And many individuals and societies will likely feel like they have not seen the benefits of the gold rush.
This is the first part in a series of articles I intend to write on the topic of the economics of AI. I welcome your feedback.
Written by Simon Greenman
I am a lover of technology and how it can be applied in the business world. I run my own advisory firm Best Practice AI helping executives of enterprises and startups accelerate the adoption of ROI based AI applications . Please get in touch to discuss this. If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it. And please post your comments or you can email me directly or find me on LinkedIn or twitter or follow me at Simon Greenman.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI guy. MapQuest guy. Grow, innovate and transform companies with tech. Start-up investor, mentor and geek.
Sharing concepts, ideas, and codes.
",midst gold rush ai reap economic benefits mass startups gold panning corporates massive gold mining operations technology giants supplying picks shovels nations richest seams gold currently experiencing another gold rush ai billions invested ai startups across every imaginable industry business function google amazon microsoft ibm heavyweight fight investing 20 billion ai 2016 corporates scrambling ensure realise productivity benefits ai ahead competitors looking shoulders startups china putting considerable weight behind ai european union talking 22 billion ai investment fears losing ground china us ai everywhere 35 billion daily searches google new apple iphone x uses facial recognition amazon alexa cutely answers questions media headlines tout stories ai helping doctors diagnose diseases banks better assess customer loan risks farmers predict crop yields marketers target retain customers manufacturers improve quality control think tanks dedicated studying physical cyber political risks ai ai machine learning become ubiquitous woven fabric society gold rush question find gold brave large snappy upstarts grab nuggets providing picks shovel make money hit pay dirt started thinking going make money ai ended seven questions make money across 1 chip makers 2 platform infrastructure providers 3 enabling models algorithm providers 4 enterprise solution providers 5 industry vertical solution providers 6 corporate users ai 7 nations many ways skin cat ai landscape hopefully provides useful explanatory framework value chain sorts companies noted representative larger players category way list intended comprehensive predictive even though price computational power fallen exponentially demand rising even faster ai machine learning massive datasets trillions vector matrix calculations ferocious insatiable appetite bring chips nvidias stock 1500 past two years benefiting fact graphical processing unit gpu chips historically used render beautiful high speed flowing games graphics perfect machine learning google recently launched second generation tensor processing units tpus microsoft building brainwave ai machine learning chips time startups graphcore raised 110m looking enter market incumbents chip providers ibm intel qualcomm amd standing still even facebook rumoured building team design ai chips chinese emerging serious chip players cambricon technology announcing first cloud ai chip past week clear cost designing manufacturing chips sustaining position global chip leader high requires extremely deep pockets world class team silicon software engineers means new winners like gold rush days provide cheapest widely used picks shovels make lot money ai race also taking place cloud amazon realised early startups would much rather rent computers software buy launched amazon web services aws 2006 today ai demanding much compute power companies increasingly turning cloud rent hardware infrastructure service iaas platform service paas offerings fight among tech giants microsoft offering hybrid public private azure cloud service allegedly one million computers past weeks announced brainwave hardware solutionsdramatically accelerate machine learning bing search engine performance improving factor ten google rushing play catchup googlecloud offering seeing chinese alibaba starting take global share amazon microsoft google ibm going continue duke one watch massively scaled cloud players china big picks shovels guys win today google worlds largest ai company attracting best ai minds spending small country size gdp budgets rd sitting best datasets gleamed billions users services ai powering googles search autonomous vehicles speech recognition intelligent reasoning massive search even work drug discovery disease diangosis incredible ai machine learning software algorithms powering googles ai activity tensorflow given away free yes free tensorflow open source software project available world jeff dean head google brain recently said 20 million organisations world could benefit machine learning today millions companies use best class free ai software likely need lots computing power better served offer well google cloud course optimised tensorflow related ai services become reliant software cloud become sticky customer many years come wonder brutal race global ai algorithm dominance amazon microsoft ibm also offering cheap free ai software services also seeing fight machine learning algorithms cognitive algorithms offer services conversational agents bots speech natural language processing nlp semantics vision enhanced core algorithms one startup increasingly contested space clarifai provides advanced image recognition systems businesses detect nearduplicates visual searches raised nearly 40m past three years market vision related algorithms services estimated cumulative 8 billion revenue 2016 2025 giants standing still ibm example offering watson cognitive products services twenty apis chatbots vision speech language knowledge management empathy simply plugged corporate software create ai enabled applications cognitive apis everywhere kdnuggets lists 50 top cognitive services giants startups services put cloud ai service aiaas make accessible recently microsofts ceo satya nadella claimed million developers using ai apis services tools building aipowered apps nearly 300000 developers using tools chatbots wouldnt want startup competing goliaths winners space likely favour heavyweights hire best research engineering talent spend money access largest datasets flourish startups going really well funded supported leading researchers whole battery ip patents published papers deep domain expertise access quality datasets excellent navigational skills sail ahead giants sail different races many startup casualties scale find global enterprises quickly acquired heavyweights even startup found path commercialisation could become acquihires companies bought talent working enabling ai algorithms strong research oriented team saw 2014 deepmind two year old london based company developed unique reinforcement machine learning algorithms acquired google 400m enterprise software dominated giants salesforce ibm oracle sap recognise ai tool needs integrated enterprise offerings many startups rushing become next generation enterprise services filling gaps incumbents dont currently tread even attempting disrupt analysed two hundred use cases enterprise space ranging customer management marketing cybersecurity intelligence hr hot area cognitive robotic process automation rpa enterprise field much open previous spaces veritable medley startups providing point solutions use cases today 200 ai powered companies recruitment space many ai startups cybersecurity leader darktrace rpa leader uipathhave war chests 100 millions incumbents also want make sure ecosystems stay forefront investing startups enhance offering salesforce invested digital genius customer management solution similarly unbable offers enterprise translation services incumbents also often pressing problems sap example rushing play catchup offering cloud solution let alone catchup ai also seeing tools providers trying simplify tasks required create deploy manage ai services enterprise machine learning training example messy business 80 time spent data wrangling inordinate amount time spent testing tuning called hyperparameters petuum tools provider based pittsburgh us raised 100m help accelerate optimise deployment machine learning models many enterprise startup providers healthy future quickly demonstrate solving scaling solutions meet real world enterprise needs always happens software gold rushes handful winners category ai enterprise category winners likely snapped along best inclass tool providers giants look threatening ai driving race best vertical industry solutions wealth new ai powered startups providing solutions corporate use cases healthcare financial services agriculture automative legal industrial sectors many startups taking ambitious path disrupt incumbent corporate players offering service directly customers clear many startups providing valuable point solutions succeed access 1 large proprietary data training sets 2 domain knowledge gives deep insights opportunities within sector 3 deep pool talent around applied ai 4 deep pockets capital fund rapid growth startups well generally speak corporate commercial language customers business efficiency roi form well developed gotomarket plans example zestfinance raised nearly 300m help improve credit decision making provide fair transparent credit everyone claim worlds best data scientists would wouldnt startups looking disrupt existing corporate players need really deep pockets example affirm offers loans consumers point sale raised 700m companies quickly need create defensible moat ensure remain competitive come data network effects data begets better ai based services products gets revenue customers gets data flywheel effect continues corporates might look new vendors industry ai solutions could enhance top bottom line going sit back let upstarts muscle customers going sit still let corporate competitors gain first advantage ai currently massive race corporate innovation large companies venture groups investing startups running accelerators building startups ensure leaders ai driven innovation large corporates strong position startups smaller companies due data assets data fuel ai machine learning better placed take advantage ai insurance company reams historic data underwriting claims financial services company knows everything consumer financial product buying behaviour search company sees user searches information corporates large small well positioned extract value ai fact gartner research predicts aiderived business value projected reach 39 trillion 2022 hundreds thousands valuable use cases ai addresses across organisations corporates improve customer experience save costs lower prices drive revenues sell better products services powered ai ai help big get bigger often expense smaller companies need demonstrate strong visionary leadership ability execute tolerance always getting technology enabled projects right first try countries also also battle ai supremacy china shy call arms around ai investing massively growing technical talent developing startups lax regulatory environment especially data privacy helps china lead ai sectors security facial recognition recently example chinese police picking one wanted face crowd 50000 music concert sensetime group ltd analyses faces images massive scale reported raised 600m becoming valuable global ai startup chinese point mobile market 3x size us 50x mobile payments taking place massive data advantage european focus data privacy regulation could put disadvantage certain areas ai even union talking 22b investment ai uk germany france japan made recent announcements nation state ai strategies example president macron said french government spend 185 billion next five years support ai ecosystem including creation large public datasets companies googles deepmind samsung committed open new paris labs fujitsu expanding paris research centre british announced 14 billion push ai including funding 1000 ai phds nations investing ai talent ecosystem question really capture value france uk simply subsidising phds hired google payroll income taxes healthy six figure machine learning salaries bulk economic value created could american company shareholders smiling american treasury ai increase productivity wealth companies countries wealth distributed headlines suggest 30 40 jobs taken machines economists point lessons hundreds years increasing technology automation net job creation net job loss public debate often cites geoffrey hinton godfather machine learning suggested radiologists lose jobs dozen machines diagnose diseases medical images look chinese using ai assist radiologists managing overwhelming demand review 14 billion ct scans annually lung cancer result job losses expanded market efficient accurate diagnosis however likely period upheaval much value go companies countries control ai technology data lower skilled countries whose wealth depends jobs targets ai automation likely suffer ai favour large technologically skilled examining landscape ai became clear entering truly golden era ai key themes appearing economic value migrate short looks like ai gold rush favour companies countries control scale best ai tools technology data best technical workers customers strongest access capital scale capture lions share economic value ai ways plus ca change plus cest la meme chose also large golden nuggets found choice brave startups like gold rush many startups hit pay dirt many individuals societies likely feel like seen benefits gold rush first part series articles intend write topic economics ai welcome feedback written simon greenman lover technology applied business world run advisory firm best practice ai helping executives enterprises startups accelerate adoption roi based ai applications please get touch discuss enjoyed piece id love hit clap button others might stumble upon please post comments email directly find linkedin twitter follow simon greenman quick cheer standing ovation clap show much enjoyed story ai guy mapquest guy grow innovate transform companies tech startup investor mentor geek sharing concepts ideas codes,en,"['Google', 'Amazon', 'Microsoft', 'IBM', 'the European Union', 'Apple', 'Amazon Alexa', 'algorithm providers', 'GPU', 'Intel', 'AMD', 'Cambricon Technology', 'Amazon Web Services', 'AWS', 'Platform', 'Brainwave', 'GoogleCloud', 'Alibaba', 'TensorFlow', 'Google Brain', 'NLP', 'Watson', 'Goliaths', 'IP', 'Oracle', 'SAP', 'Cognitive Robotic Process Automation', 'RPA', 'Digital Genius', 'Incumbents', 'ROI', 'AI', 'Gartner', 'SenseTime Group Ltd', 'Samsung', 'Fujitsu', 'PhDs', 'American Treasury', 'Best Practice AI', 'LinkedIn', 'MapQuest']"
67,Eugenio Culurciello,6400,The fall of RNN / LSTM – Towards Data Science,"We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. Now it is time to drop them!
It is the year 2014 and LSTM and RNN make a great come-back from the dead. We all read Colah’s blog and Karpathy’s ode to RNN. But we were all young and unexperienced. For a few years this was the way to solve sequence learning, sequence translation (seq2seq), which also resulted in amazing results in speech to text comprehension and the raise of Siri, Cortana, Google voice assistant, Alexa. Also let us not forget machine translation, which resulted in the ability to translate documents into different languages or neural machine translation, but also translate images into text, text into images, and captioning video, and ... well you got the idea.
Then in the following years (2015–16) came ResNet and Attention. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by averaging networks influenced by a context vector. More on this later.
It only took 2 more years, but today we can definitely say:
But do not take our words for it, also see evidence that Attention based networks are used more and more by Google, Facebook, Salesforce, to name a few. All these companies have replaced RNN and variants for attention based models, and it is just the beginning. RNN have the days counted in all applications, because they require more resources to train and run than attention-based models. See this post for more info.
Remember RNN and LSTM and derivatives use mainly sequential processing over time. See the horizontal arrow in the diagram below:
This arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell. This means it can be easily corrupted by being multiplied many time by small numbers < 0. This is the cause of vanishing gradients.
To the rescue, came the LSTM module, which today can be seen as multiple switch gates, and a bit like ResNet it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.
But not all of it, as you can see from the figure above. Still we have a sequential path from older past cells to the current one. In fact the path is now even more complicated, because it has additive and forget branches attached to it. No question LSTM and GRU and derivatives are able to learn a lot of longer term information! See results here; but they can remember sequences of 100s, not 1000s or 10,000s or more.
And one issue of RNN is that they are not hardware friendly. Let me explain: it takes a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable. We will need to process at the edge, right into the Amazon Echo! See note below for more details.
If sequential processing is to be avoided, then we can find units that “look-ahead” or better “look-back”, since most of the time we deal with real-time causal data where we know the past and want to affect future decisions. Not so in translating sentences, or analyzing recorded videos, for example, where we have all data and can reason on it more time. Such look-back/ahead units are neural attention modules, which we previously explained here.
To the rescue, and combining multiple neural attention modules, comes the “hierarchical neural attention encoder”, shown in the figure below:
A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.
Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. This is also similar to Temporal convolutional network (TCN), reported in Note 3 below.
In the hierarchical neural attention encoder multiple layers of attention can look at a small portion of recent past, say 100 vectors, while layers above can look at 100 of these attention modules, effectively integrating the information of 100 x 100 vectors. This extends the ability of the hierarchical neural attention encoder to 10,000 past vectors.
But more importantly look at the length of the path needed to propagate a representation vector to the output of the network: in hierarchical networks it is proportional to log(N) where N are the number of hierarchy layers. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T >> N.
This architecture is similar to a neural Turing machine, but lets the neural network decide what is read out from memory via attention. This means an actual neural network will decide which vectors from the past are important for future decisions.
But what about storing to memory? The architecture above stores all previous representation in memory, unlike neural Turning machines. This can be rather inefficient: think about storing the representation of every frame in a video — most times the representation vector does not change frame-to-frame, so we really are storing too much of the same! What can we do is add another unit to prevent correlated data to be stored. For example by not storing vectors too similar to previously stored ones. But this is really a hack, the best would be to be let the application guide what vectors should be saved or not. This is the focus of current research studies. Stay tuned for more information.
Tell your friends! It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. Please tell them about this post.
About training RNN/LSTM: RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. In short, LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units. And it is easy to add more computational units, but hard to add more memory bandwidth (note enough lines on a chip, long wires from processors to memory, etc). As a result, RNN/LSTM and variants are not a good match for hardware acceleration, and we talked about this issue before here and here. A solution will be compute in memory-devices like the ones we work on at FWDNXT.
See this repository for a simple example of these techniques.
Note 1: Hierarchical neural attention is similar to the ideas in WaveNet. But instead of a convolutional neural network we use hierarchical attention modules. Also: Hierarchical neural attention can be also bi-directional.
Note 2: RNN and LSTM are memory-bandwidth limited problems (see this for details). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them! The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).
Note 3: Here is a paper comparing CNN to RNN. Temporal convolutional network (TCN) “outperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory”.
Note 4: Related to this topic, is the fact that we know little of how our human brain learns and remembers sequences. “We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks” — these chunks remind me of small convolutional or attention like networks on smaller sequences, that then are hierarchically strung together like in the hierarchical neural attention encoder and Temporal convolutional network (TCN). More studies make me think that working memory is similar to RNN networks that uses recurrent real neuron networks, and their capacity is very low. On the other hand both the cortex and hippocampus give us the ability to remember really long sequences of steps (like: where did I park my car at airport 5 days ago), suggesting that more parallel pathways may be involved to recall long sequences, where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task.
Note 5: The above evidence shows we do not read sequentially, in fact we interpret characters, words and sentences as a group. An attention-based or convolutional module perceives the sequence and projects a representation in our mind. We would not be misreading this if we processed this information sequentially! We would stop and notice the inconsistencies!
I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more...
If you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I dream and build new technology
Sharing concepts, ideas, and codes.
",fell recurrent neural networks rnn longshort term memory lstm variants time drop year 2014 lstm rnn make great comeback dead read colahs blog karpathys ode rnn young unexperienced years way solve sequence learning sequence translation seq2seq also resulted amazing results speech text comprehension raise siri cortana google voice assistant alexa also let us forget machine translation resulted ability translate documents different languages neural machine translation also translate images text text images captioning video well got idea following years 201516 came resnet attention one could better understand lstm clever bypass technique also attention showed mlp network could replaced averaging networks influenced context vector later took 2 years today definitely say take words also see evidence attention based networks used google facebook salesforce name companies replaced rnn variants attention based models beginning rnn days counted applications require resources train run attentionbased models see post info remember rnn lstm derivatives use mainly sequential processing time see horizontal arrow diagram arrow means longterm information sequentially travel cells getting present processing cell means easily corrupted multiplied many time small numbers 0 cause vanishing gradients rescue came lstm module today seen multiple switch gates bit like resnet bypass units thus remember longer time steps lstm thus way remove vanishing gradients problems see figure still sequential path older past cells current one fact path even complicated additive forget branches attached question lstm gru derivatives able learn lot longer term information see results remember sequences 100s 1000s 10000s one issue rnn hardware friendly let explain takes lot resources train network fast also takes much resources run model cloud given demand speechtotext growing rapidly cloud scalable need process edge right amazon echo see note details sequential processing avoided find units lookahead better lookback since time deal realtime causal data know past want affect future decisions translating sentences analyzing recorded videos example data reason time lookbackahead units neural attention modules previously explained rescue combining multiple neural attention modules comes hierarchical neural attention encoder shown figure better way look past use attention modules summarize past encoded vectors context vector ct notice hierarchy attention modules similar hierarchy neural networks also similar temporal convolutional network tcn reported note 3 hierarchical neural attention encoder multiple layers attention look small portion recent past say 100 vectors layers look 100 attention modules effectively integrating information 100 x 100 vectors extends ability hierarchical neural attention encoder 10000 past vectors importantly look length path needed propagate representation vector output network hierarchical networks proportional logn n number hierarchy layers contrast steps rnn needs maximum length sequence remembered n architecture similar neural turing machine lets neural network decide read memory via attention means actual neural network decide vectors past important future decisions storing memory architecture stores previous representation memory unlike neural turning machines rather inefficient think storing representation every frame video times representation vector change frametoframe really storing much add another unit prevent correlated data stored example storing vectors similar previously stored ones really hack best would let application guide vectors saved focus current research studies stay tuned information tell friends surprising us see many companies still use rnnlstm speech text many unaware networks inefficient scalable please tell post training rnnlstm rnn lstm difficult train require memorybandwidthbound computation worst nightmare hardware designer ultimately limits applicability neural networks solutions short lstm require 4 linear layer mlp layer per cell run sequence timestep linear layers require large amounts memory bandwidth computed fact cannot use many compute unit often system enough memory bandwidth feed computational units easy add computational units hard add memory bandwidth note enough lines chip long wires processors memory etc result rnnlstm variants good match hardware acceleration talked issue solution compute memorydevices like ones work fwdnxt see repository simple example techniques note 1 hierarchical neural attention similar ideas wavenet instead convolutional neural network use hierarchical attention modules also hierarchical neural attention also bidirectional note 2 rnn lstm memorybandwidth limited problems see details processing units need much memory bandwidth number operationss provide making impossible fully utilize external bandwidth never going enough way slightly ameliorate problem use internal fast caches high bandwidth best way use techniques require large amount parameters moved back forth memory reused multiple computation per byte transferred high arithmetic intensity note 3 paper comparing cnn rnn temporal convolutional network tcn outperform canonical recurrent networks lstms across diverse range tasks datasets demonstrating longer effective memory note 4 related topic fact know little human brain learns remembers sequences often learn recall long sequences smaller segments phone number 858 534 22 30 memorized four segments behavioral experiments suggest humans animals employ strategy breaking cognitive behavioral sequences chunks wide variety tasks chunks remind small convolutional attention like networks smaller sequences hierarchically strung together like hierarchical neural attention encoder temporal convolutional network tcn studies make think working memory similar rnn networks uses recurrent real neuron networks capacity low hand cortex hippocampus give us ability remember really long sequences steps like park car airport 5 days ago suggesting parallel pathways may involved recall long sequences attention mechanism gate important chunks force hops parts sequence relevant final goal task note 5 evidence shows read sequentially fact interpret characters words sentences group attentionbased convolutional module perceives sequence projects representation mind would misreading processed information sequentially would stop notice inconsistencies almost 20 years experience neural networks hardware software rare combination see medium webpage scholar linkedin found article useful please consider donation support tutorials blogs contribution make difference quick cheer standing ovation clap show much enjoyed story dream build new technology sharing concepts ideas codes,en,"['Google', 'Alexa', 'MLP', 'Attention', 'Facebook, Salesforce', 'GRU', 'RNN', 'the hierarchical neural attention encoder', 'N', 'T', 'Linear', 'FWDNXT', 'WaveNet', 'CNN', 'LinkedIn']"
68,WiseWolf Fund,14200,GAME-CHANGING TRENDS TO LOOK OUT FOR WITH AI – WiseWolf Fund – Medium,"Artificial Intelligence is a state-of-the-art technological trend that many companies are trying to integrate into their business. A recent report by McKinsey states that Baidu, the Chinese equivalent of Alphabet, invested $20 billion in AI last year. At the same time, Alphabet invested roughly $30 billion in developing AI technologies. The Chinese government has been actively pursuing AI technology in an attempt to control a future cornerstone innovation. Companies in the US are also investing time, money and energy into advancing AI technology.
The reason for such interest towards artificial intelligence is that artificial intelligence can enhance any product or function. This is why companies and governments make considerable investments in the research and development of this technology. Its role in increasing the production performance while simultaneously reducing the costs cannot be underestimated.
Since some of the largest entities in the world are focused on promoting the AI technology, it would be wise to understand and follow the trend. AI is already shaping the economy, and in the near future, its effect may be even more significant. Ignoring the new technology and its influence on the global economic situation is a recipe for failure.
Despite the huge public interest and attention towards AI, its evolution is still somewhat halted by the objective causes. As any new and fast-developing industry, AI is quickly outgrowing its environment. According to Adam Temper, an author of many creative researches on artificial intelligence, the development of AI is mostly limited by the “lack of employees with relevant expertise, very few mature standard industry tools, limited high quality training material available, few options for easy access to preconfigured machine learning environments, and the general focus in the industry on implementation rather than design”.
With any new complex technology, the learning curve is steep. Our educational institutions are several steps behind the commercial applications of this technology. It is important that AI scientists work collaboratively, sharing knowledge and best practice, to address this deficiency. AI is rapidly increasing its impact on society; we need to ensure that the power of AI doesn’t remain with the elite few.
Another factor that may be hindering the progress of AI is the cautious stance that people tend to take towards it. Artificial intelligence is still too sci-fi, too strange and, therefore, sometimes scary. When people learn to trust AI, it will make a true quantum leap in the way of general adoption and application. Adam Temper supports this point, too, describing the possible ways for AI technology to gain public trust as
At the same time, if we analyze the primary purpose of AI, we will see it for what it really is — a tool to perform the routine tasks relieving humans for something more creative or innovative. When asked about the current trends and opportunities of AI, Aaron Edell, CEO and co-founder of Machine Box, and one of the top writers on AI, described them as follows:
AI has also become a political talking point in recent years. There have been arguments that AI will help to create jobs, but that it will also cause certain workers to lose their jobs. For example, estimations prove that self-driving vehicles will cause 25,000 truck drivers to lose their jobs each month. Also, as much as 1 million pickers and packers working in US warehouses could be out of a job. This is due to the fact that by implementing AI, factories can operate with as few as a dozen of workers.
Naturally, companies gladly implement artificial intelligence, as it ensures considerable savings. At the same time, governments are concerned about the current employment situation as well as the short-term and long-term predictions. Some countries have already begun to plan measures about the new AI technology that are intended to keep the economy stable.
In fact, it would not be fair to say that artificial intelligence causes people to lose jobs. True, the whole point of automation is making machines do what people used to do before. However, it would be more correct if we said that artificial intelligence reshapes the employment situation. Together with taking over human functions, it creates other jobs, forces people to master new skills, encourages workers to increase productivity. But it is obvious that AI is going to turn the regular sequence of events upside down.
Therefore, the best approach is not to wait until AI leaves you unemployed, but rather proactively embrace it and learn to live with it. As we said already, AI can also create jobs, so a wise move would be to learn to manage AI-based tools. With the advance of AI products, learning to work with them may secure you a job and even promote your career.
Your future largely depends on your current and expected income. However, another important factor is the way you manage your finances. Of course, investing in your own or your children’s knowledge is one of the best investments you can ever make. At the same time, if you need some financial cushion to secure your family’s welfare, you should look at the available investment opportunities.
And this is where artificial intelligence may become your best friend, professional consultant and investment manager. In the recent years, in addition to the traditional banks and financial institutions, we have witnessed the appearance of a totally new and innovative investment system.
We are talking about the blockchain technology and the cryptocurrencies that it supports. Millions of people all over the world have already appreciated the transparency and flexibility of the blockchain networks. By watching the cryptocurrency trends carefully and trading wisely, individual investors have made fortunes within a very short time.
Nowadays, the cryptocurrency opportunities are open for everyone, not only for the industry experts. There are investment funds running on artificial intelligence that are available for individual investors.
With such funds, you are, on one hand, protected by the blockchain technology. It ensures proper safety of your funds and the security of your transactions. On the other hand, you do not need to be an investment expert to make wise decisions. This is where artificial intelligence is at your service. It analyzes the existing trends on the extremely volatile cryptocurrency market and shows you the best opportunities.
The main point is that we should not regard AI as a threat to our careers and a danger to our well-being. Instead, we should analyze the investment openings created by AI technology that can secure our prosperity. For example, Wolf Coin is using AI technology to create a seamless investment channel for savvy individuals. This robust channel opens great opportunities that investors can use to become new rich kids on the block. Most noteworthy, the low entry cost of $10 has made it one offer that will enjoy a huge buzz. The focus on this new market opening will help people build a solid financial nest egg that will keep them safe even in the face of the storm.
Wisewolf Fund launching the Wolf Coin focused its effort on creating a great opportunity for people who wish to benefit from cryptocurrency trading but are new to this trend. With artificial intelligence and advanced analytical algorithms, the fund arranges the most favorable conditions for individual investors.
Mainstream manufacturers, companies, and factories are embracing AI technology to change the mode of their operations. Therefore, it is critical to keep tabs on this reality as it can bring many benefits that cannot be found elsewhere. AI is one of the hottest topics of discussion, however, it is now clear that AI is here to stay. So, people should accept the obvious in order to create the future that they desire. The wisest strategy is to embrace artificial intelligence and let it work to maintain our well-being.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The WiseWolf Crypto Fund provides an easy way to enter the cryptocurrency market even for non-techies.
",artificial intelligence stateoftheart technological trend many companies trying integrate business recent report mckinsey states baidu chinese equivalent alphabet invested 20 billion ai last year time alphabet invested roughly 30 billion developing ai technologies chinese government actively pursuing ai technology attempt control future cornerstone innovation companies us also investing time money energy advancing ai technology reason interest towards artificial intelligence artificial intelligence enhance product function companies governments make considerable investments research development technology role increasing production performance simultaneously reducing costs cannot underestimated since largest entities world focused promoting ai technology would wise understand follow trend ai already shaping economy near future effect may even significant ignoring new technology influence global economic situation recipe failure despite huge public interest attention towards ai evolution still somewhat halted objective causes new fastdeveloping industry ai quickly outgrowing environment according adam temper author many creative researches artificial intelligence development ai mostly limited lack employees relevant expertise mature standard industry tools limited high quality training material available options easy access preconfigured machine learning environments general focus industry implementation rather design new complex technology learning curve steep educational institutions several steps behind commercial applications technology important ai scientists work collaboratively sharing knowledge best practice address deficiency ai rapidly increasing impact society need ensure power ai doesnt remain elite another factor may hindering progress ai cautious stance people tend take towards artificial intelligence still scifi strange therefore sometimes scary people learn trust ai make true quantum leap way general adoption application adam temper supports point describing possible ways ai technology gain public trust time analyze primary purpose ai see really tool perform routine tasks relieving humans something creative innovative asked current trends opportunities ai aaron edell ceo cofounder machine box one top writers ai described follows ai also become political talking point recent years arguments ai help create jobs also cause certain workers lose jobs example estimations prove selfdriving vehicles cause 25000 truck drivers lose jobs month also much 1 million pickers packers working us warehouses could job due fact implementing ai factories operate dozen workers naturally companies gladly implement artificial intelligence ensures considerable savings time governments concerned current employment situation well shortterm longterm predictions countries already begun plan measures new ai technology intended keep economy stable fact would fair say artificial intelligence causes people lose jobs true whole point automation making machines people used however would correct said artificial intelligence reshapes employment situation together taking human functions creates jobs forces people master new skills encourages workers increase productivity obvious ai going turn regular sequence events upside therefore best approach wait ai leaves unemployed rather proactively embrace learn live said already ai also create jobs wise move would learn manage aibased tools advance ai products learning work may secure job even promote career future largely depends current expected income however another important factor way manage finances course investing childrens knowledge one best investments ever make time need financial cushion secure familys welfare look available investment opportunities artificial intelligence may become best friend professional consultant investment manager recent years addition traditional banks financial institutions witnessed appearance totally new innovative investment system talking blockchain technology cryptocurrencies supports millions people world already appreciated transparency flexibility blockchain networks watching cryptocurrency trends carefully trading wisely individual investors made fortunes within short time nowadays cryptocurrency opportunities open everyone industry experts investment funds running artificial intelligence available individual investors funds one hand protected blockchain technology ensures proper safety funds security transactions hand need investment expert make wise decisions artificial intelligence service analyzes existing trends extremely volatile cryptocurrency market shows best opportunities main point regard ai threat careers danger wellbeing instead analyze investment openings created ai technology secure prosperity example wolf coin using ai technology create seamless investment channel savvy individuals robust channel opens great opportunities investors use become new rich kids block noteworthy low entry cost 10 made one offer enjoy huge buzz focus new market opening help people build solid financial nest egg keep safe even face storm wisewolf fund launching wolf coin focused effort creating great opportunity people wish benefit cryptocurrency trading new trend artificial intelligence advanced analytical algorithms fund arranges favorable conditions individual investors mainstream manufacturers companies factories embracing ai technology change mode operations therefore critical keep tabs reality bring many benefits cannot found elsewhere ai one hottest topics discussion however clear ai stay people accept obvious order create future desire wisest strategy embrace artificial intelligence let work maintain wellbeing quick cheer standing ovation clap show much enjoyed story wisewolf crypto fund provides easy way enter cryptocurrency market even nontechies,en,"['Artificial Intelligence', 'McKinsey', 'Baidu', 'AI', 'Alphabet', 'Wisewolf Fund', 'The WiseWolf Crypto Fund']"
69,Michael Solana,680,Artificial Intelligence Is Humanity's Rorschach Test,"Member Feature Story
Slime Sunday / Founders Fund
Slime Sunday / Founders Fund
I don’t fear artificial intelligence, I fear people who fear artificial intelligence.
It’s the 1960s. A psychologist stares at his patient — a balding, middle-aged foreman with a cigarette in his hand, and a curl of smoke around him like a halo on an acid trip. The psychologist holds up an inkblot, an ambiguous, black splatter on a white flashcard, and asks his patient what he sees. The thinking is his patient, not willing or otherwise able to express his feelings, his thoughts, his motivations, might inadvertently reveal some piece of his inner self while describing the ambiguous. The foreman doesn’t see a nondescript swiggle, or stain. He sees a man and woman making love, perhaps violently. He sees a mother holding her child. He sees a grisly murder. While the descriptions of these inkblots reveal very little about the world, they reveal a great deal about the man describing them, because when faced with an inscrutable abstract he projects himself onto the ambiguous.
Let’s look at this in the context of artificial intelligence. I’m not talking about self-driving cars, or algorithms serving ads for wallpaper and nice leather boots on Gmail. I’m not talking about the stuff we call artificial intelligence to raise money from bewildered venture capitalists on Sand Hill Road. I’m talking about general artificial intelligence, which is a computer that wants stuff, and chiefly to live. I’m talking about building a conscious machine just smart enough to make itself smarter. From here, the thought experiment runs like this: the conscious machine does make itself smarter, and once it’s smarter, it learns how to make itself smarter, which it does for good measure. The smarter the machine becomes, the faster this pattern repeats itself, and the intelligence of the machine begins to increase exponentially. In this way, a conscious artificial intelligence born on a Tuesday morning might be twice as smart as the smartest man who ever lived by Wednesday afternoon, and omnipotent by Friday. This is how we invent the thing that invents God. In nerd lore, it’s known as the Singularity. The question — the only question that could possibly matter to a human no longer at the top of the intellectual food chain — is what does an exponential intelligence want? Conventional wisdom: it extremely wants to murder you.
The dystopian version of superintelligence is illustrated with frequency by leaders in the technology industry, and is famously depicted by Hollywood in films like Terminator, or more recently Ex Machina, and even the Avengers. The “angry god A.I.” is a story you know, because it is the story you are constantly told: we build the thinking machine, it surpasses our abilities in every way, and it destroys us for one of any number of reasons. Maybe it perceives us as a threat. Maybe we’re just in its way, and it hardly perceives us at all — humanity, a disposable insect race. There are of course many arguments in opposition to the now ubiquitous concept of our apocalypse by artificial intelligence. I myself have called into question the logic of such dystopian arguments in Anatomy of Next. But our subject here is less pertaining to the nature of the conscious machine than it is to the way we talk about this subject, and what it means. First, consider that most of the artificial intelligence depicted in culture looks human, a representation with no basis in technological reality. Then, the true scope of the Singularity is almost impossible to predict, which begs a question: where are these opinions about the broadly unknowable coming from?
There’s an obvious difficulty in trying to understand the hypothetical motivations of a hypothetically god-like intelligence. To your beloved labradoodle, you are a being of immense magic with near unfathomable motivations. You summon light and sound from inanimate matter, soar through the streets on angry metal, cast fire from your hands! The labradoodle’s conception of man is distorted because there is a vast difference between the intelligence of a dog, and the intelligence of a human. Let us name this difference ‘x.’ Now, as we try and understand the difference between the most intelligent human who has ever lived and a hypothetical god-like intelligence born of the Singularity, let us set our difference in intelligence at a conservative ‘1000x.’
How does one even begin to conceive of a being this smart?
Here we approach our inscrutable abstract, and our robot Rorschach test. But in this contemporary version of the famous psychological prompts, what we are observing is not even entirely ambiguous. We are attempting to imagine a greatly-amplified mind. Here, each of us has a particularly relevant data point — our own. In trying to imagine the amplified intelligence, it is natural to imagine our own intelligence amplified. In imagining the motivations of this amplified intelligence, we naturally imagine ourselves. If, as you try to conceive of a future with machine intelligence, a monster comes to mind, it is likely you aren’t afraid of something alien at all. You’re afraid of something exactly like you. What would you do with unlimited power?
Psychological projection seems to work in several contexts outside of general artificial intelligence. In the technology industry the concept of “meritocracy” is now hotly debated. How much of your life is determined by luck, and how much by chance? There’s no answer here we know for sure, but has there ever been a better Rorschach test for separating high-achievers from people who were given what they have? Questions pertaining to human nature are almost open self-reflection. Are we basically good, with some exceptions, or are humans basically beasts, with an animal nature just barely contained by a set of slowly-eroding stories we tell ourselves — law, faith, society. The inner workings of a mind can’t be fully shared, and they can’t be observed by a neutral party. We therefore do not — can not, currently — know anything of the inner workings of people in general. But we can know ourselves. So in the face of large abstractions concerning intelligence, we hold up a mirror.
Not everyone who fears general artificial intelligence would cause harm to others. There are many people who haven’t thought deeply about these questions at all. They look to their neighbors for cues on what to think, and there is no shortage of people willing to tell them. The media has ads to sell, after all, and historically they have found great success in doing this with horror stories. But as we try to understand the people who have thought about these questions with some depth — with the depth required of a thoughtful screenplay, for example, or a book, or a company — it’s worth considering the inkblot.
technology, liberty, teenagers with superpowers. vp @foundersfund. creator + producer #anatomyofnext.
Welcome to a place where words matter. On Medium, smart voices and original ideas take center stage — with no ads in sight. Watch
Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore
Get unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade
",member feature story slime sunday founders fund slime sunday founders fund dont fear artificial intelligence fear people fear artificial intelligence 1960s psychologist stares patient balding middleaged foreman cigarette hand curl smoke around like halo acid trip psychologist holds inkblot ambiguous black splatter white flashcard asks patient sees thinking patient willing otherwise able express feelings thoughts motivations might inadvertently reveal piece inner self describing ambiguous foreman doesnt see nondescript swiggle stain sees man woman making love perhaps violently sees mother holding child sees grisly murder descriptions inkblots reveal little world reveal great deal man describing faced inscrutable abstract projects onto ambiguous lets look context artificial intelligence im talking selfdriving cars algorithms serving ads wallpaper nice leather boots gmail im talking stuff call artificial intelligence raise money bewildered venture capitalists sand hill road im talking general artificial intelligence computer wants stuff chiefly live im talking building conscious machine smart enough make smarter thought experiment runs like conscious machine make smarter smarter learns make smarter good measure smarter machine becomes faster pattern repeats intelligence machine begins increase exponentially way conscious artificial intelligence born tuesday morning might twice smart smartest man ever lived wednesday afternoon omnipotent friday invent thing invents god nerd lore known singularity question question could possibly matter human longer top intellectual food chain exponential intelligence want conventional wisdom extremely wants murder dystopian version superintelligence illustrated frequency leaders technology industry famously depicted hollywood films like terminator recently ex machina even avengers angry god ai story know story constantly told build thinking machine surpasses abilities every way destroys us one number reasons maybe perceives us threat maybe way hardly perceives us humanity disposable insect race course many arguments opposition ubiquitous concept apocalypse artificial intelligence called question logic dystopian arguments anatomy next subject less pertaining nature conscious machine way talk subject means first consider artificial intelligence depicted culture looks human representation basis technological reality true scope singularity almost impossible predict begs question opinions broadly unknowable coming theres obvious difficulty trying understand hypothetical motivations hypothetically godlike intelligence beloved labradoodle immense magic near unfathomable motivations summon light sound inanimate matter soar streets angry metal cast fire hands labradoodles conception man distorted vast difference intelligence dog intelligence human let us name difference x try understand difference intelligent human ever lived hypothetical godlike intelligence born singularity let us set difference intelligence conservative 1000x one even begin conceive smart approach inscrutable abstract robot rorschach test contemporary version famous psychological prompts observing even entirely ambiguous attempting imagine greatlyamplified mind us particularly relevant data point trying imagine amplified intelligence natural imagine intelligence amplified imagining motivations amplified intelligence naturally imagine try conceive future machine intelligence monster comes mind likely arent afraid something alien youre afraid something exactly like would unlimited power psychological projection seems work several contexts outside general artificial intelligence technology industry concept meritocracy hotly debated much life determined luck much chance theres answer know sure ever better rorschach test separating highachievers people given questions pertaining human nature almost open selfreflection basically good exceptions humans basically beasts animal nature barely contained set slowlyeroding stories tell law faith society inner workings mind cant fully shared cant observed neutral party therefore currently know anything inner workings people general know face large abstractions concerning intelligence hold mirror everyone fears general artificial intelligence would cause harm others many people havent thought deeply questions look neighbors cues think shortage people willing tell media ads sell historically found great success horror stories try understand people thought questions depth depth required thoughtful screenplay example book company worth considering inkblot technology liberty teenagers superpowers vp foundersfund creator producer anatomyofnext welcome place words matter medium smart voices original ideas take center stage ads sight watch follow topics care well deliver best stories homepage inbox explore get unlimited access best stories medium support writers youre 5month upgrade,en,"['Singularity', 'Terminator']"
70,Lance Ulanoff,15100,Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium,"I think it was the first “Um.” That was the moment when I realized I was hearing something extraordinary: A computer carrying out a completely natural and very human-sounding conversation with a real person. And it wasn’t just a random talk. This conversation had a purpose, a destination: to make an appointment at a hair salon.
The entity making the call and appointment was Google Assistant running Duplex, Google’s still experimental AI voice system and the venue was Google I/O, Google’s yearly developer conference, which this year focused heavily on the latest developments in AI, Machine- and Deep-Learning.
Google CEO Sundar Pichai explained that what we were hearing was a real phone call made to a hair salon that didn’t know it was part of an experiment or that they were talking to a computer. He launched Duplex by asking Google Assistant to book a haircut appointment for Tuesday morning. The AI did the rest.
Duplex made the call and, when someone at the salon picked up, the voice AI started the conversation with:
“Hi, I’m calling to book a woman’s hair cut appointment for a client, um, I’m looking for something on May third?”
When the attendant asked Duplex to give her one second, Duplex responded with:
“Mmm-hmm.”
The conversation continued as the salon representative presented various dates and times and the AI asked about other options. Eventually, the AI and the salon worker agreed on an appointment date and time.
What I heard was so convincing I had trouble discerning who was the salon worker and who (what) was the Duplex AI. It was stunning and somewhat disconcerting. I liken it to the feeling you’d get if a store mannequin suddenly smiled at you.
It was easily the most remarkable human-computer conversation I’d ever heard and the closest thing I’ve seen a voice AI passing the Turing Test, which is the AI threshold suggested by Computer Scientist Alan Turing in the 1950s. Turing posited that by 2000 computers would be able to fool humans into thinking they were conversing with other humans at least 30% of the time.
He was right. In 2014, a chatbot named Eugene Goostman successfully impersonated a wise-ass 14-year old programmer during lengthy text-based chats with unsuspecting humans.
Turing, however hadn’t necessarily considered voice-based systems and, for obvious reasons, talking computers are somewhat less adept at fooling humans. Spend a few minutes conversing with your voice assistant of choice and you’ll soon discover their limitations.
Their speech can be stilted, pronunciations off and response times can be slow (especially if they’re trying to access a cloud-based server) and forget about conversations. Most can handle two consecutive queries at most and they virtually all require a trigger phrase like “Alexa” or “Hey Siri.” (Google is working on removing unnecessary “Okay Googles” in short back and forth convos with the digital assistant).
Google Assistant running Duplex didn’t exhibit any of those short comings. It sounded like a young female assistant carefully scheduling her boss’s haircut. In addition to the natural cadence, Google added speech disfluencies (the verbal ticks, “ums,” “uhs,” and “mm-hmms”) and latency or pauses that naturally occur when people are speaking. The result is a perfectly human voice produced entirely by a computer.
The second call demonstration, where a male-voiced Duplex tried to make restaurant reservations, was even more remarkable. The human call participant didn’t entirely understand Duplex’s verbal requests and then told Duplex that, for the number of people it wanted to bring to the restaurant, they didn’t need a reservation. Duplex handled all this without missing a beat.
“The amazing thing is that the assistant can actually understand the nuances of conversation,” said Pichai during the keynote. That ability comes by way of neural network technology and intensive machine learning,
For as accomplished as Duplex is in making hair appointments and restaurant reservations, it might stumble in deeper or more abstract conversations. In a blog post on Duplex development, Google engineers explained that they constrained Duplex’s training to “closed domains” or well-defined topics (like dinner reservations and hair appointments) This gave them the ability to perform intense exploration of the topics and focus training. Duplex was guided during training within the domain by “experienced operators” who could keep track of mistakes and worked with engineers to improve responses.
In short, this means that while Duplex has your hair and dining-out options covered, it could stumble in movie reservations and negotiations with your cable provider.
Even so, Duplex fooled two humans. I heard no hesitation or confusion. In the hair salon call, there was no indication that the salon worker thought something was amiss. She wanted to help this young woman make an appointment. What will she think when she learns she was duped by Duplex?
Obviously, Duplex’s conversations were also short, each lasting less than a minute, putting them well-short of the Turing Test benchmark. I would’ve enjoyed hearing the conversations devolve as they extended a few minutes or more.
I’m sure Duplex will soon tackle more domains and longer conversations, and it will someday pass the Turing Test.
It’s only a matter of time before Duplex is handling other mundane or difficult calls for us, like calling our parents with our own voices (see Wavenet technology). Eventually, we’ll have our Duplex voices call each other, handling pleasantries and making plans, which Google Assistant can then drop in our Google Calendar.
But that’s the future.
For now, Duplex’s performance stands as a powerful proof of concept for our long-imagined future of conversational AI’s capable of helping, entertaining and engaging with us. It’s the first major step on the path to the AI depicted in the movie Her where Joaquin Phoenix starred as a man who falls in love with his chatty voice assistant played by the disembodied voice of Scarlett Johansson.
So, no, Duplex didn’t pass the Turing test, but I do wonder what Alan Turing would think of it.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Tech expert, journalist, social media commentator, amateur cartoonist and robotics fan.
",think first um moment realized hearing something extraordinary computer carrying completely natural humansounding conversation real person wasnt random talk conversation purpose destination make appointment hair salon entity making call appointment google assistant running duplex googles still experimental ai voice system venue google io googles yearly developer conference year focused heavily latest developments ai machine deeplearning google ceo sundar pichai explained hearing real phone call made hair salon didnt know part experiment talking computer launched duplex asking google assistant book haircut appointment tuesday morning ai rest duplex made call someone salon picked voice ai started conversation hi im calling book womans hair cut appointment client um im looking something may third attendant asked duplex give one second duplex responded mmmhmm conversation continued salon representative presented various dates times ai asked options eventually ai salon worker agreed appointment date time heard convincing trouble discerning salon worker duplex ai stunning somewhat disconcerting liken feeling youd get store mannequin suddenly smiled easily remarkable humancomputer conversation id ever heard closest thing ive seen voice ai passing turing test ai threshold suggested computer scientist alan turing 1950s turing posited 2000 computers would able fool humans thinking conversing humans least 30 time right 2014 chatbot named eugene goostman successfully impersonated wiseass 14year old programmer lengthy textbased chats unsuspecting humans turing however hadnt necessarily considered voicebased systems obvious reasons talking computers somewhat less adept fooling humans spend minutes conversing voice assistant choice youll soon discover limitations speech stilted pronunciations response times slow especially theyre trying access cloudbased server forget conversations handle two consecutive queries virtually require trigger phrase like alexa hey siri google working removing unnecessary okay googles short back forth convos digital assistant google assistant running duplex didnt exhibit short comings sounded like young female assistant carefully scheduling bosss haircut addition natural cadence google added speech disfluencies verbal ticks ums uhs mmhmms latency pauses naturally occur people speaking result perfectly human voice produced entirely computer second call demonstration malevoiced duplex tried make restaurant reservations even remarkable human call participant didnt entirely understand duplexs verbal requests told duplex number people wanted bring restaurant didnt need reservation duplex handled without missing beat amazing thing assistant actually understand nuances conversation said pichai keynote ability comes way neural network technology intensive machine learning accomplished duplex making hair appointments restaurant reservations might stumble deeper abstract conversations blog post duplex development google engineers explained constrained duplexs training closed domains welldefined topics like dinner reservations hair appointments gave ability perform intense exploration topics focus training duplex guided training within domain experienced operators could keep track mistakes worked engineers improve responses short means duplex hair diningout options covered could stumble movie reservations negotiations cable provider even duplex fooled two humans heard hesitation confusion hair salon call indication salon worker thought something amiss wanted help young woman make appointment think learns duped duplex obviously duplexs conversations also short lasting less minute putting wellshort turing test benchmark wouldve enjoyed hearing conversations devolve extended minutes im sure duplex soon tackle domains longer conversations someday pass turing test matter time duplex handling mundane difficult calls us like calling parents voices see wavenet technology eventually well duplex voices call handling pleasantries making plans google assistant drop google calendar thats future duplexs performance stands powerful proof concept longimagined future conversational ais capable helping entertaining engaging us first major step path ai depicted movie joaquin phoenix starred man falls love chatty voice assistant played disembodied voice scarlett johansson duplex didnt pass turing test wonder alan turing would think quick cheer standing ovation clap show much enjoyed story tech expert journalist social media commentator amateur cartoonist robotics fan,en,"['Google', 'Duplex', 'AI', 'Computer', 'Wavenet']"
71,Matt Schlicht,5000,The Complete Beginner’s Guide To Chatbots – Chatbots Magazine,"What are chatbots? Why are they such a big opportunity? How do they work? How can I build one? How can I meet other people interested in chatbots?
These are the questions we’re going to answer for you right now.
Ready? Let’s do this.
(Do you work in ecommerce? Stop reading and click here, we made something for you.)
(p.s. here is where I believe the future of bots is headed, you will probably disagree with me at first.)
(p.p.s. My newest guide about conversational commerce is up, I think you’ll find it super interesting.)
A chatbot is a service, powered by rules and sometimes artificial intelligence, that you interact with via a chat interface. The service could be any number of things, ranging from functional to fun, and it could live in any major chat product (Facebook Messenger, Slack, Telegram, Text Messages, etc.).
If you haven’t wrapped your head around it yet, don’t worry. Here’s an example to help you visualize a chatbot.
If you wanted to buy shoes from Nordstrom online, you would go to their website, look around until you find the shoes you wanted, and then you would purchase them.
If Nordstrom makes a bot, which I’m sure they will, you would simply be able to message Nordstrom on Facebook. It would ask you what you’re looking for and you would simply... tell it.
Instead of browsing a website, you will have a conversation with the Nordstrom bot, mirroring the type of experience you would get when you go into the retail store.
Watch this video from Facebook’s recent F8 conference (where they make their major announcements). At the 7:30 mark, David Marcus, the Vice President of Messaging Products at Facebook, explains what it looks like to buy shoes in a Facebook Messenger bot.
Buying shoes isn’t the only thing chatbots can be used for. Here are a couple of other examples:
See? With bots, the possibilities are endless. You can build anything imaginable, and I encourage you to do just that.
But why make a bot? Sure, it looks cool, it’s using some super advanced technology, but why should someone spend their time and energy on it?
It’s a huge opportunity. HUGE. Scroll down and I’ll explain.
You are probably wondering “Why does anyone care about chatbots? They look like simple text based services... what’s the big deal?”
Great question. I’ll tell you why people care about chatbots.
It’s because for the first time ever people are using messenger apps more than they are using social networks.
Let that sink in for a second.
People are using messenger apps more than they are using social networks.
So, logically, if you want to build a business online, you want to build where the people are. That place is now inside messenger apps.
This is why chatbots are such a big deal. It’s potentially a huge business opportunity for anyone willing to jump headfirst and build something people want.
But, how do these bots work? How do they know how to talk to people and answer questions? Isn’t that artificial intelligence and isn’t that insanely hard to do?
Yes, you are correct, it is artificial intelligence, but it’s something that you can totally do yourself.
Let me explain.
There are two types of chatbots, one functions based on a set of rules, and the other more advanced version uses machine learning.
What does this mean?
Chatbot that functions based on rules:
Chatbot that functions using machine learning:
Bots are created with a purpose. A store will likely want to create a bot that helps you purchase something, where someone like Comcast might create a bot that can answer customer support questions.
You start to interact with a chatbot by sending it a message. Click here to try sending a message to the CNN chatbot on Facebook.
So, if these bots use artificial intelligence to make them work well... isn’t that really hard to do? Don’t I need to be an expert at artificial intelligence to be able to build something that has artificial intelligence?
Short answer? No, you don’t have to be an expert at artificial intelligence to create an awesome chatbot that has artificial intelligence. Just make sure to not over promise on your application’s abilities. If you can’t make the product good with artificial intelligence right now, it might be best to not put it in yet.
However, over the past decade quite a bit of advancements have been made in the area of artificial intelligence, so much in fact that anyone who knows how to code can incorporate some level of artificial intelligence into their products.
How do you build artificial intelligence into your bot? Don’t worry, I’ve got you covered, I’ll tell you how to do it in the next section of this post.
Building a chatbot can sound daunting, but it’s totally doable. You’ll be creating an artificial intelligence powered chatting machine in no time (or, of course, you can always build a basic chat bot that doesn’t have a fancy AI brain and strictly follows rules).
You will need to figure out what problem you are going to solve with your bot, choose which platform your bot will live on (Facebook, Slack, etc), set up a server to run your bot from, and choose which service you will use to build your bot.
Here are a ton of resources to get you started.
Platform documentation:
Other Resources:
Don’t want to build your own?
Now that you’ve got your chatbot and artificial intelligence resources, maybe it’s time you met other people who are also interested in chatbots.
Chatbots have been around for decades, but because of the recent advancements in artificial intelligence and machine learning, there is a big opportunity for people to create bots that are better, faster, and stronger.
If you’re reading this, you probably fall into one of these categories:
Wouldn’t it be awesome if you had a place to meet, learn, and share information with other people interested in chatbots? Yeah, we thought so too.
That’s why I created a forum called “Chatbot News”, and it has quickly become the largest community related to Chatbots.
The members of the Chatbots group are investors who manage well over $2 billion in capital, employees at Facebook, Instagram, Fitbit, Nike, and Ycombinator companies, and hackers from around the world.
We would love if you joined. Click here to request an invite private chatbots community.
I have also created the Silicon Valley Chatbots Meetup, register here to be notified when we schedule our first event.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO of Octane AI, Founder of Chatbots Magazine, YC Alum, Forbes 30 Under 30, product at Ustream for 4 years (sold for $130mil), did digital for Lil Wayne.
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.
",chatbots big opportunity work build one meet people interested chatbots questions going answer right ready lets work ecommerce stop reading click made something ps believe future bots headed probably disagree first pps newest guide conversational commerce think youll find super interesting chatbot service powered rules sometimes artificial intelligence interact via chat interface service could number things ranging functional fun could live major chat product facebook messenger slack telegram text messages etc havent wrapped head around yet dont worry heres example help visualize chatbot wanted buy shoes nordstrom online would go website look around find shoes wanted would purchase nordstrom makes bot im sure would simply able message nordstrom facebook would ask youre looking would simply tell instead browsing website conversation nordstrom bot mirroring type experience would get go retail store watch video facebooks recent f8 conference make major announcements 730 mark david marcus vice president messaging products facebook explains looks like buy shoes facebook messenger bot buying shoes isnt thing chatbots used couple examples see bots possibilities endless build anything imaginable encourage make bot sure looks cool using super advanced technology someone spend time energy huge opportunity huge scroll ill explain probably wondering anyone care chatbots look like simple text based services whats big deal great question ill tell people care chatbots first time ever people using messenger apps using social networks let sink second people using messenger apps using social networks logically want build business online want build people place inside messenger apps chatbots big deal potentially huge business opportunity anyone willing jump headfirst build something people want bots work know talk people answer questions isnt artificial intelligence isnt insanely hard yes correct artificial intelligence something totally let explain two types chatbots one functions based set rules advanced version uses machine learning mean chatbot functions based rules chatbot functions using machine learning bots created purpose store likely want create bot helps purchase something someone like comcast might create bot answer customer support questions start interact chatbot sending message click try sending message cnn chatbot facebook bots use artificial intelligence make work well isnt really hard dont need expert artificial intelligence able build something artificial intelligence short answer dont expert artificial intelligence create awesome chatbot artificial intelligence make sure promise applications abilities cant make product good artificial intelligence right might best put yet however past decade quite bit advancements made area artificial intelligence much fact anyone knows code incorporate level artificial intelligence products build artificial intelligence bot dont worry ive got covered ill tell next section post building chatbot sound daunting totally doable youll creating artificial intelligence powered chatting machine time course always build basic chat bot doesnt fancy ai brain strictly follows rules need figure problem going solve bot choose platform bot live facebook slack etc set server run bot choose service use build bot ton resources get started platform documentation resources dont want build youve got chatbot artificial intelligence resources maybe time met people also interested chatbots chatbots around decades recent advancements artificial intelligence machine learning big opportunity people create bots better faster stronger youre reading probably fall one categories wouldnt awesome place meet learn share information people interested chatbots yeah thought thats created forum called chatbot news quickly become largest community related chatbots members chatbots group investors manage well 2 billion capital employees facebook instagram fitbit nike ycombinator companies hackers around world would love joined click request invite private chatbots community also created silicon valley chatbots meetup register notified schedule first event quick cheer standing ovation clap show much enjoyed story ceo octane ai founder chatbots magazine yc alum forbes 30 30 product ustream 4 years sold 130mil digital lil wayne chatbots ai nlp facebook messenger slack telegram,en,"['p.p.s', 'Facebook', 'Messaging Products at Facebook', 'CNN', 'Chatbots', 'Nike', 'the Silicon Valley Chatbots Meetup', 'Octane AI', 'Founder of Chatbots Magazine', 'YC Alum', 'Forbes', 'Ustream', 'NLP']"
72,Milo Spencer-Harper,7800,How to build a simple neural network in 9 lines of Python code,"As part of my quest to learn about AI, I set myself the goal of building a simple neural network in Python. To ensure I truly understand it, I had to build it from scratch without using a neural network library. Thanks to an excellent blog post by Andrew Trask I achieved my goal. Here it is in just 9 lines of code:
In this blog post, I’ll explain how I did it, so you can build your own. I’ll also provide a longer, but more beautiful version of the source code.
But first, what is a neural network? The human brain consists of 100 billion cells called neurons, connected together by synapses. If sufficient synaptic inputs to a neuron fire, that neuron will also fire. We call this process “thinking”.
We can model this process by creating a neural network on a computer. It’s not necessary to model the biological complexity of the human brain at a molecular level, just its higher level rules. We use a mathematical technique called matrices, which are grids of numbers. To make it really simple, we will just model a single neuron, with three inputs and one output.
We’re going to train the neuron to solve the problem below. The first four examples are called a training set. Can you work out the pattern? Should the ‘?’ be 0 or 1?
You might have noticed, that the output is always equal to the value of the leftmost input column. Therefore the answer is the ‘?’ should be 1.
Training process
But how do we teach our neuron to answer the question correctly? We will give each input a weight, which can be a positive or negative number. An input with a large positive weight or a large negative weight, will have a strong effect on the neuron’s output. Before we start, we set each weight to a random number. Then we begin the training process:
Eventually the weights of the neuron will reach an optimum for the training set. If we allow the neuron to think about a new situation, that follows the same pattern, it should make a good prediction.
This process is called back propagation.
Formula for calculating the neuron’s output
You might be wondering, what is the special formula for calculating the neuron’s output? First we take the weighted sum of the neuron’s inputs, which is:
Next we normalise this, so the result is between 0 and 1. For this, we use a mathematically convenient function, called the Sigmoid function:
If plotted on a graph, the Sigmoid function draws an S shaped curve.
So by substituting the first equation into the second, the final formula for the output of the neuron is:
You might have noticed that we’re not using a minimum firing threshold, to keep things simple.
Formula for adjusting the weights
During the training cycle (Diagram 3), we adjust the weights. But how much do we adjust the weights by? We can use the “Error Weighted Derivative” formula:
Why this formula? First we want to make the adjustment proportional to the size of the error. Secondly, we multiply by the input, which is either a 0 or a 1. If the input is 0, the weight isn’t adjusted. Finally, we multiply by the gradient of the Sigmoid curve (Diagram 4). To understand this last one, consider that:
The gradient of the Sigmoid curve, can be found by taking the derivative:
So by substituting the second equation into the first equation, the final formula for adjusting the weights is:
There are alternative formulae, which would allow the neuron to learn more quickly, but this one has the advantage of being fairly simple.
Constructing the Python code
Although we won’t use a neural network library, we will import four methods from a Python mathematics library called numpy. These are:
For example we can use the array() method to represent the training set shown earlier:
The ‘.T’ function, transposes the matrix from horizontal to vertical. So the computer is storing the numbers like this.
Ok. I think we’re ready for the more beautiful version of the source code. Once I’ve given it to you, I’ll conclude with some final thoughts.
I have added comments to my source code to explain everything, line by line. Note that in each iteration we process the entire training set simultaneously. Therefore our variables are matrices, which are grids of numbers. Here is a complete working example written in Python:
Also available here: https://github.com/miloharper/simple-neural-network
Final thoughts
Try running the neural network using this Terminal command:
python main.py
You should get a result that looks like:
We did it! We built a simple neural network using Python!
First the neural network assigned itself random weights, then trained itself using the training set. Then it considered a new situation [1, 0, 0] and predicted 0.99993704. The correct answer was 1. So very close!
Traditional computer programs normally can’t learn. What’s amazing about neural networks is that they can learn, adapt and respond to new situations. Just like the human mind.
Of course that was just 1 neuron performing a very simple task. But what if we hooked millions of these neurons together? Could we one day create something conscious?
I’ve been inspired by the huge response this article has received. I’m considering creating an online course. Click here to tell me what topic to cover. I’d love to hear your feedback.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Economics at Oxford University. Founder of www.moju.io. Interested in politics and AI.
Technology trends and New Invention? Follow this collection to update the latest trend! [UPDATE] As a collection editor, I don’t have any permission to add your articles in the wild. Please submit your article and I will approve. Also, follow this collection, please.
",part quest learn ai set goal building simple neural network python ensure truly understand build scratch without using neural network library thanks excellent blog post andrew trask achieved goal 9 lines code blog post ill explain build ill also provide longer beautiful version source code first neural network human brain consists 100 billion cells called neurons connected together synapses sufficient synaptic inputs neuron fire neuron also fire call process thinking model process creating neural network computer necessary model biological complexity human brain molecular level higher level rules use mathematical technique called matrices grids numbers make really simple model single neuron three inputs one output going train neuron solve problem first four examples called training set work pattern 0 1 might noticed output always equal value leftmost input column therefore answer 1 training process teach neuron answer question correctly give input weight positive negative number input large positive weight large negative weight strong effect neurons output start set weight random number begin training process eventually weights neuron reach optimum training set allow neuron think new situation follows pattern make good prediction process called back propagation formula calculating neurons output might wondering special formula calculating neurons output first take weighted sum neurons inputs next normalise result 0 1 use mathematically convenient function called sigmoid function plotted graph sigmoid function draws shaped curve substituting first equation second final formula output neuron might noticed using minimum firing threshold keep things simple formula adjusting weights training cycle diagram 3 adjust weights much adjust weights use error weighted derivative formula formula first want make adjustment proportional size error secondly multiply input either 0 1 input 0 weight isnt adjusted finally multiply gradient sigmoid curve diagram 4 understand last one consider gradient sigmoid curve found taking derivative substituting second equation first equation final formula adjusting weights alternative formulae would allow neuron learn quickly one advantage fairly simple constructing python code although wont use neural network library import four methods python mathematics library called numpy example use array method represent training set shown earlier function transposes matrix horizontal vertical computer storing numbers like ok think ready beautiful version source code ive given ill conclude final thoughts added comments source code explain everything line line note iteration process entire training set simultaneously therefore variables matrices grids numbers complete working example written python also available httpsgithubcommiloharpersimpleneuralnetwork final thoughts try running neural network using terminal command python mainpy get result looks like built simple neural network using python first neural network assigned random weights trained using training set considered new situation 1 0 0 predicted 099993704 correct answer 1 close traditional computer programs normally cant learn whats amazing neural networks learn adapt respond new situations like human mind course 1 neuron performing simple task hooked millions neurons together could one day create something conscious ive inspired huge response article received im considering creating online course click tell topic cover id love hear feedback quick cheer standing ovation clap show much enjoyed story studied economics oxford university founder wwwmojuio interested politics ai technology trends new invention follow collection update latest trend update collection editor dont permission add articles wild please submit article approve also follow collection please,en,"['Python', 'Python mathematics', 'Studied Economics', 'Oxford University', 'New Invention', 'UPDATE']"
73,Greg Fish,1,looking for a ghost in the machine – [ weird things ],"A short while ago, I wrote about some of the challenges involved in creating artificial intelligence and raised the question of how exactly a machine would spontaneously attain self-awareness. While I’ve gotten plenty of feedback about how far technology has come so far and how it’s imminent that machines will become much smarter than us, I never got any specifics as to how exactly this would happen. To me, it’s not a philosophical question because I’m used to looking at technology from a design and development standpoint. When I ask for specifics, I’m talking about functional requirements. So far, the closest thing to outlining the requirements for a super-intelligent computer is a paper by University of Oxford philosopher and futurist Nick Bostrom.
The first thing Bostrom tries to do is to establish a benchmark by how to grade what he calls a super-intellect and qualifying his definition. According to him, this super-intellect would be smarter than any human mind in every capacity from the scientific to the creative. It’s a pretty lofty goal because designing something smarter than yourself requires that you build something you don’t fully understand. You might have a sudden stroke of luck and succeed, but it’s more than likely that you’ll build a defective product instead. Imagine building a DNA helix from scratch and with no detailed manual to go by. Even if you have all the tools and know where to find some bits of information to guide you, when you don’t know exactly what you’re doing, the task becomes very challenging and you end up making a lot of mistakes along the way.
There’s also the question of how exactly we evaluate what the term smarter means. In Bostrom’s projections, when you have an intelligent machine become fully proficient in a certain area of expertise like say, medicine, it could combine with another machine which has an excellent understanding of physics and so on until all this consolidation leads to a device that knows all that we know and can use all that cross-disciplinary knowledge to gain insights we just don’t have yet. Technologically that should be possible, but the question is whether a machine like that would really be smarter than humans per se. It would be far more knowledgeable than any individual human, granted. But it’s not as if there aren’t experts in particular fields coming together to make all sorts of cross-disciplinary connections and discoveries. What Bostrom calls a super-intellect is actually just a massive knowledge base that can mine itself for information.
The paper was last revised in 1998 when we didn’t have the enormous digital libraries we take for granted in today’s world. Those libraries seem a fair bit like Bostrom’s super-intellect in their function and if we were to combine them to mine their depths with sophisticated algorithms which look for cross-disciplinary potential, we’d bring his concept to life. But there’s not a whole lot of intelligence there. Just a lot of data, much of which would be subject to change or revision as research and discovery continue. Just like Bostrom says, it would be a very useful tool for scientists and researchers. However, it wouldn’t be thinking on its own and giving the humans advice, even if we put all this data on supercomputers which could live up to the paper’s ambitious hardware requirements. Rev it up to match the estimated capacity of our brain, it says, and watch a new kind of intellect start waking up and take shape with the proper software.
According to Bostrom, the human brain operates at 100 teraflops, or 100 trillion floating point operations per second. Now, as he predicted, computers have reached this speed by 2004 and went far beyond that. In fact, we have supercomputers which are as much as ten times faster. Supposedly, at these operating speeds, we should be able to write software which allows supercomputers to learn by interacting with humans and sifting through our digitized knowledge. But the reality is that we’d be trying to teach an intimate object made of metal and plastic how to think and solve problems, something we’re already born with and hone over our lifetimes. You can teach someone how to ride a bike and how to balance, but how exactly would you teach someone to understand the purpose of riding a bike? How would you tell someone with no emotion, no desires, no wants and no needs why he should go anywhere? That deep layer of motivation and wiring has taken several billion years to appear and was honed over a 600 million additional years of evolution. When we start trying to make an AI system comparable to ours, we’re effectively way behind from the get-go.
To truly create an intelligent computer which doesn’t just act as if it’s thinking or do mechanical actions which are easy to predict and program, we’d need to impart in all that information in trillions of lines of code and trick circuitry into deducing it needs to behave like a living being. And that’s a job that couldn’t be done in less than century, much less in the next 20 to 30 years as projected by Ray Kurzweil and his fans.
[ eerie illustration by Neil Blevins ]
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
techie, Rantt staff writer and editor, computer lobotomist
science, tech, and other oddities
",short ago wrote challenges involved creating artificial intelligence raised question exactly machine would spontaneously attain selfawareness ive gotten plenty feedback far technology come far imminent machines become much smarter us never got specifics exactly would happen philosophical question im used looking technology design development standpoint ask specifics im talking functional requirements far closest thing outlining requirements superintelligent computer paper university oxford philosopher futurist nick bostrom first thing bostrom tries establish benchmark grade calls superintellect qualifying definition according superintellect would smarter human mind every capacity scientific creative pretty lofty goal designing something smarter requires build something dont fully understand might sudden stroke luck succeed likely youll build defective product instead imagine building dna helix scratch detailed manual go even tools know find bits information guide dont know exactly youre task becomes challenging end making lot mistakes along way theres also question exactly evaluate term smarter means bostroms projections intelligent machine become fully proficient certain area expertise like say medicine could combine another machine excellent understanding physics consolidation leads device knows know use crossdisciplinary knowledge gain insights dont yet technologically possible question whether machine like would really smarter humans per se would far knowledgeable individual human granted arent experts particular fields coming together make sorts crossdisciplinary connections discoveries bostrom calls superintellect actually massive knowledge base mine information paper last revised 1998 didnt enormous digital libraries take granted todays world libraries seem fair bit like bostroms superintellect function combine mine depths sophisticated algorithms look crossdisciplinary potential wed bring concept life theres whole lot intelligence lot data much would subject change revision research discovery continue like bostrom says would useful tool scientists researchers however wouldnt thinking giving humans advice even put data supercomputers could live papers ambitious hardware requirements rev match estimated capacity brain says watch new kind intellect start waking take shape proper software according bostrom human brain operates 100 teraflops 100 trillion floating point operations per second predicted computers reached speed 2004 went far beyond fact supercomputers much ten times faster supposedly operating speeds able write software allows supercomputers learn interacting humans sifting digitized knowledge reality wed trying teach intimate object made metal plastic think solve problems something already born hone lifetimes teach someone ride bike balance exactly would teach someone understand purpose riding bike would tell someone emotion desires wants needs go anywhere deep layer motivation wiring taken several billion years appear honed 600 million additional years evolution start trying make ai system comparable effectively way behind getgo truly create intelligent computer doesnt act thinking mechanical actions easy predict program wed need impart information trillions lines code trick circuitry deducing needs behave like living thats job couldnt done less century much less next 20 30 years projected ray kurzweil fans eerie illustration neil blevins quick cheer standing ovation clap show much enjoyed story techie rantt staff writer editor computer lobotomist science tech oddities,en,"['University of Oxford', 'Rantt']"
74,Greg Fish,1,the technical trouble with humanoid robots – [ weird things ],"If you’ve been reading this blog long enough, you may recall that I’m not a big fan of humanoid robots. There’s no need to invoke the uncanny valley effect, even though some attempts to build humanoid robots managed to produce rather creepy entities which try to look as human as possible to goad future users into some kind of social bond with them, presumably to gain their trust and get into a perfect position to kill the inferior things made of flesh. No, the reason why I’m not sure that humanoid robots will be invaluable to us in the future is a very pragmatic one. Simply put, emulating bipedalism is a huge computational overhead as well as a major, and unavoidable engineering and maintenance headache. And with the limits on size and weight of would be robot butlers, as well as the patience of its users, humanoid bot designers may be aiming a bit too high...
We walk, run, and perform complicated tasks with our hands and feet so easily, we only notice the amount of effort and coordination this takes after an injury that limits our mobility. The reason why we can do that lies in a small, squishy mass of neurons coordinating a firestorm of constant activity. Unlike old-standing urban myths imply, we actually use all of our brainpower, and we need it to help coordinate and execute the same motions that robots struggle to repeat. Of course our brains are cheating when compared to a computer because with tens of billions of neurons and trillions of synapses, our brains are like screaming fast supercomputers. They can calculate what it will take to catch a ball in mid-air in less than a few hundred milliseconds and make the most minute adjustments to our muscles in order to keep us balanced and upright just as quickly. Likewise, our bodies can heal the constant wear and tear on our joints, wear and tear we will accumulate from walking, running, and bumping into things. Bipedal robots navigating our world wouldn’t have these assets.
Humanoid machines would need to be constantly maintained just to keep up with us in a mechanical sense, and carry the equivalent of Red Storm in their heads, or at least be linked to something like it, to even hope to coordinate themselves as quickly as we do cognitively and physically. Academically, this is a lofty goal which could yield new algorithms and robotic designs. Practically? Not so much. While last month’s feature in Pop Sci bemoaned the lack of interest in humanoid robots in the U.S., it also failed to demonstrate why such an incredibly complicated machine would be needed for basic household chores that could be done by robotic systems functioning independently, and without the need to move on two legs. Instead, we got the standard Baby Boomers’ caretaker argument which goes somewhat like this...
Or, alternatively, a computer could book your appointments via e-mail, or a system that lets patients make an appointment with their doctors on the web, a smart dispenser that gives you the right amount of pills, checks for potential interactions based on public medical databases, and beeps to remind you to take your medicine, and a programmable walker with actuators and a few buttons could do these jobs while costing far less than the tens of millions a humanoid robot would cost by 2025, and requiring much less coordination or learning than a programmable humanoid. Why wouldn’t we want to pursue immediate fixes to what’s being described as a looming caretaker shortage choosing instead to invest billions of dollars into E-Jeeves, which may take an entire decade or two just to learn how to go about daily human life, ready to tackle the problem only after it was no longer an issue, even if we started right now? If anything, harping on the need for a robotic hand for Baby Boomers’ future medical woes would only prompt more R&D cash into immediate solutions and rules- based intelligent agents we already employ rather than long-term academic research.
There’s a huge gap between human abilities and machinery because we have the benefit of having evolved over hundreds of millions of years of trial and error. Machines, even though they’re advancing at an ever faster pace, only had a few decades by comparison. It will take decades more to build self-repairing machines and computer chips that can boast the same performance as a supercomputer while being small enough to fit in human-sized robots’ heads before robotic butlers become practical and feasible. And even then, we might go with distinctly robotic versions because they’d be cheaper to maintain and operate.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
techie, Rantt staff writer and editor, computer lobotomist
science, tech, and other oddities
",youve reading blog long enough may recall im big fan humanoid robots theres need invoke uncanny valley effect even though attempts build humanoid robots managed produce rather creepy entities try look human possible goad future users kind social bond presumably gain trust get perfect position kill inferior things made flesh reason im sure humanoid robots invaluable us future pragmatic one simply put emulating bipedalism huge computational overhead well major unavoidable engineering maintenance headache limits size weight would robot butlers well patience users humanoid bot designers may aiming bit high walk run perform complicated tasks hands feet easily notice amount effort coordination takes injury limits mobility reason lies small squishy mass neurons coordinating firestorm constant activity unlike oldstanding urban myths imply actually use brainpower need help coordinate execute motions robots struggle repeat course brains cheating compared computer tens billions neurons trillions synapses brains like screaming fast supercomputers calculate take catch ball midair less hundred milliseconds make minute adjustments muscles order keep us balanced upright quickly likewise bodies heal constant wear tear joints wear tear accumulate walking running bumping things bipedal robots navigating world wouldnt assets humanoid machines would need constantly maintained keep us mechanical sense carry equivalent red storm heads least linked something like even hope coordinate quickly cognitively physically academically lofty goal could yield new algorithms robotic designs practically much last months feature pop sci bemoaned lack interest humanoid robots us also failed demonstrate incredibly complicated machine would needed basic household chores could done robotic systems functioning independently without need move two legs instead got standard baby boomers caretaker argument goes somewhat like alternatively computer could book appointments via email system lets patients make appointment doctors web smart dispenser gives right amount pills checks potential interactions based public medical databases beeps remind take medicine programmable walker actuators buttons could jobs costing far less tens millions humanoid robot would cost 2025 requiring much less coordination learning programmable humanoid wouldnt want pursue immediate fixes whats described looming caretaker shortage choosing instead invest billions dollars ejeeves may take entire decade two learn go daily human life ready tackle problem longer issue even started right anything harping need robotic hand baby boomers future medical woes would prompt rd cash immediate solutions rules based intelligent agents already employ rather longterm academic research theres huge gap human abilities machinery benefit evolved hundreds millions years trial error machines even though theyre advancing ever faster pace decades comparison take decades build selfrepairing machines computer chips boast performance supercomputer small enough fit humansized robots heads robotic butlers become practical feasible even might go distinctly robotic versions theyd cheaper maintain operate quick cheer standing ovation clap show much enjoyed story techie rantt staff writer editor computer lobotomist science tech oddities,en,['Baby Boomers']
75,Frank Diana,50,The Evolving Role of Business Analytics – Frank Diana – Medium,"An older post that seems to be getting a lot of attention. Appreciation for analytics rising?
Business Analytics refers to the skills, technologies, applications and practices for the continuous exploration of data to gain insight that drive business decisions. Business Analytics is multi-faceted. It combines multiple forms of analytics and applies the right method to deliver expected results. It focuses on developing new insights using techniques including, data mining, predictive analytics, natural language processing, artificial intelligence, statistical analysis and quantitative analysis. In addition, domain knowledge is a key component of the business analytics portfolio. Business Analytics can then be viewed as the combination of domain knowledge and all forms of analytics in a way that creates analytic applications focused on enabling specific business outcomes.
Analytic applications have a set of business outcomes that they must enable. For fraud, its reducing loss, for quality & safety, it might be avoiding expensive recalls. Understanding how to enable these outcomes is the first step in determining the make-up of each specific application. For example, in the case of insurance fraud, it’s not enough to use statistical analysis to predict fraud. You need a strong focus on text, domain expertise, and the ability to visually portray organized crime rings. Insight gained through this analysis may be used as input for human decisions or may drive fully automated decisions. Database capacity, processor speeds and software enhancements will continue to drive even more sophisticated analytic applications. The key components of business analytics are:
There is a massive explosion of data occurring on a number of levels. The notion of data overload was echoed in a previous 2010 IBM CEO study titled “Capitalizing on Complexity”. In this study, a large number of CEOs described their organizations as data rich, but insight poor. Many voiced frustration over their inability to transform available data into feasible action plans. This notion of turning data into insight, and insight to action is a common and growing theme.
According to Pricewaterhouse-Coopers, there are approximately 75 to 100 million blogs and 10–20 million Internet discussion boards and forums in the English language alone. As the Forrester diagram describes, more consumers are moving up the ladder and becoming creators of content. In addition, estimates show the volume of unstructured data (email, audio, video, Web pages, etc.) doubles every three months. Effectively managing and harnessing this vast amount of information presents both a great challenge and a great opportunity.
Data is flowing through medical devices, scientific devices, sensors, monitors, detectors, other supply chain devices, instrumented cars and roads, instrumented domestic appliances, etc. Everything will be instrumented and from this instrumentation comes data. This data will be analyzed to find insight that drives smarter decisions. The utility sector provides a great example of the growing need for analytics. The smart grid and the gradual installation of intelligent endpoints, smart meters and other devices will generate volumes of data. Smart grid utilities are evolving into brokers of information. The data tsunami that will wash over utilities in the coming years is a formidable IT challenge, but it is also a huge opportunity to move beyond simple meter-to-cash functions and into real-time optimization of their operations. This type of instrumentation is playing out in many industries. As this occurs, industry players will be challenged to leverage the data generated by these devices.
Inside the enterprise, consider the increasing volumes of emails, Word documents, PDFs, Excel worksheets and free form text fields that contain everything from budgets and forecasts to customer proposals, contracts, call center notes and expense reports. Outside the enterprise, the growth of web-based content, which is primarily unstructured, continues to accelerate –everything from social media, comments in blogs, forums and social networks, to survey verbatim and wiki pages. Most industry analysts estimate more than 80% of the intelligence required to make smarter decisions is contained in unstructured data or text.
The survey results in a recent MIT Sloan report support both an aggressive adoption of analytics and a shift in the analytic footprint. According to the report, many traditional forms of analytics will be surpassed in the next 24 months. The authors produced a very effective visual that shows this shift from today’s analytic footprint to the future footprint. Although listed as number one, the authors describe visualization as dashboards and scorecards — the traditional methods of visualization. New and emerging methods help accelerate time-to-insight. These new approaches help us absorb insight from large volumes of data in rapid fashion. The analytics identified as creating the most value in 24 months are:
Companies and organizations continue to invest millions of dollars capturing, storing and maintaining all types of business data to drive sales and revenue, optimize operations, manage risk and ensure compliance. Most of this investment has been in technologies and applications that manage structured data — coded information residing in relational data base management systems in the form of rows and columns. Current methods such as traditional business intelligence (BI) are more about querying and reporting and focus on answering questions such as what happened, how many, how often, and what actions are needed. New forms of advanced analytics are required to address the business imperatives described earlier.
Business Analytics focuses on answering questions such as why is this happening, what if these trends continue, what will happen next (predict), what is the best that can happen (optimize). There is a growing view that prescribing outcomes is the ultimate role of analytics; that is, identifying those actions that deliver the right business outcomes. Organizations should first define the insights needed to meet business objectives, and then identify data that provides that insight. Too often, companies start with data.
The previously mentioned IBM study also revealed that analytics-driven organizations had 33 percent more revenue growth with 32 percent more return on capital invested. Organizations expect value from emerging analytic techniques to soar. The growth of innovative analytic applications will serve as a means to help individuals across the organization consume and act upon insights derived through complex analysis. Some examples of innovative use:
A recent MIT Sloan report effectively uses the maturity model concept to describe how organizations typically evolve to analytic excellence. The authors point out that organizations begin with efficiency goals and then address growth objectives after experience is gained. The authors believe this is a common practice, but not necessarily a best practice. They see the traditional analytic adoption path starting in data-intensive areas like financial management, operations, and sales and marketing. As companies move up the maturity curve, they branch out into new functions, such as strategy, product research, customer service, and customer experience. In the opinion of the authors, these patterns suggest that success in one area stimulates adoption in others. They suggest that this allows organizations to increase their level of sophistication.
The authors of the MIT Sloan special report through their analysis of survey results have created three levels of analytic capabilities:
The report provides a very nice matrix that describes these levels in the context of a maturity model. In reviewing business challenges outlined in the matrix, there is one very interesting dynamic: the transition from cost and efficiencies to revenue growth, customer retention and customer acquisition.
The authors of the report found that as the value of analytics grows, organizations are likely to seek a wider range of capabilities — and more advanced use of existing ones. The survey indicated that this dynamic is leading some organizations to create a centralized analytics unit that makes it possible to share analytic resources efficiently and effectively. These centralized enterprise units are the primary source of analytics, providing a home for more advanced skills within the organization. This same dynamic will lead to the appointment of Chief Analytics Officers (CAO) starting in 2011.
The availability of strong business-focused analytical talent will be the greatest constraint on a company’s analytical capabilities. The Outsourcing of analytics will become an attractive alternative as the need for specialized skills will lead organizations to look for outside help. Outsourcing analytics allows a company to focus on taking action based on insights delivered by the outsourcer. The outsourcer can leverage these specialized resources across multiple clients. As the importance of analytics grows, organizations will have an option to outsource. Expect to see more of this in 2011.
We will see more organizations establish enterprise data management functions to coordinate data across business units. We will also see smarter approaches such as information lifecycle management as opposed to the common approach of throwing more hardware at the growing data problem. The information management challenge will grow as millions of next-generation tech-savvy users use feeds and mash-ups to bring data together into usable parts so they can answer their own questions. This gives rise to new challenges, including data security and governance.
Originally published at frankdiana.net on March 20, 2011.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
TCS Executive focused on the rapid evolution of society and business. Fascinated by the view of the world in the next decade and beyond https://frankdiana.net/
",older post seems getting lot attention appreciation analytics rising business analytics refers skills technologies applications practices continuous exploration data gain insight drive business decisions business analytics multifaceted combines multiple forms analytics applies right method deliver expected results focuses developing new insights using techniques including data mining predictive analytics natural language processing artificial intelligence statistical analysis quantitative analysis addition domain knowledge key component business analytics portfolio business analytics viewed combination domain knowledge forms analytics way creates analytic applications focused enabling specific business outcomes analytic applications set business outcomes must enable fraud reducing loss quality safety might avoiding expensive recalls understanding enable outcomes first step determining makeup specific application example case insurance fraud enough use statistical analysis predict fraud need strong focus text domain expertise ability visually portray organized crime rings insight gained analysis may used input human decisions may drive fully automated decisions database capacity processor speeds software enhancements continue drive even sophisticated analytic applications key components business analytics massive explosion data occurring number levels notion data overload echoed previous 2010 ibm ceo study titled capitalizing complexity study large number ceos described organizations data rich insight poor many voiced frustration inability transform available data feasible action plans notion turning data insight insight action common growing theme according pricewaterhousecoopers approximately 75 100 million blogs 1020 million internet discussion boards forums english language alone forrester diagram describes consumers moving ladder becoming creators content addition estimates show volume unstructured data email audio video web pages etc doubles every three months effectively managing harnessing vast amount information presents great challenge great opportunity data flowing medical devices scientific devices sensors monitors detectors supply chain devices instrumented cars roads instrumented domestic appliances etc everything instrumented instrumentation comes data data analyzed find insight drives smarter decisions utility sector provides great example growing need analytics smart grid gradual installation intelligent endpoints smart meters devices generate volumes data smart grid utilities evolving brokers information data tsunami wash utilities coming years formidable challenge also huge opportunity move beyond simple metertocash functions realtime optimization operations type instrumentation playing many industries occurs industry players challenged leverage data generated devices inside enterprise consider increasing volumes emails word documents pdfs excel worksheets free form text fields contain everything budgets forecasts customer proposals contracts call center notes expense reports outside enterprise growth webbased content primarily unstructured continues accelerate everything social media comments blogs forums social networks survey verbatim wiki pages industry analysts estimate 80 intelligence required make smarter decisions contained unstructured data text survey results recent mit sloan report support aggressive adoption analytics shift analytic footprint according report many traditional forms analytics surpassed next 24 months authors produced effective visual shows shift todays analytic footprint future footprint although listed number one authors describe visualization dashboards scorecards traditional methods visualization new emerging methods help accelerate timetoinsight new approaches help us absorb insight large volumes data rapid fashion analytics identified creating value 24 months companies organizations continue invest millions dollars capturing storing maintaining types business data drive sales revenue optimize operations manage risk ensure compliance investment technologies applications manage structured data coded information residing relational data base management systems form rows columns current methods traditional business intelligence bi querying reporting focus answering questions happened many often actions needed new forms advanced analytics required address business imperatives described earlier business analytics focuses answering questions happening trends continue happen next predict best happen optimize growing view prescribing outcomes ultimate role analytics identifying actions deliver right business outcomes organizations first define insights needed meet business objectives identify data provides insight often companies start data previously mentioned ibm study also revealed analyticsdriven organizations 33 percent revenue growth 32 percent return capital invested organizations expect value emerging analytic techniques soar growth innovative analytic applications serve means help individuals across organization consume act upon insights derived complex analysis examples innovative use recent mit sloan report effectively uses maturity model concept describe organizations typically evolve analytic excellence authors point organizations begin efficiency goals address growth objectives experience gained authors believe common practice necessarily best practice see traditional analytic adoption path starting dataintensive areas like financial management operations sales marketing companies move maturity curve branch new functions strategy product research customer service customer experience opinion authors patterns suggest success one area stimulates adoption others suggest allows organizations increase level sophistication authors mit sloan special report analysis survey results created three levels analytic capabilities report provides nice matrix describes levels context maturity model reviewing business challenges outlined matrix one interesting dynamic transition cost efficiencies revenue growth customer retention customer acquisition authors report found value analytics grows organizations likely seek wider range capabilities advanced use existing ones survey indicated dynamic leading organizations create centralized analytics unit makes possible share analytic resources efficiently effectively centralized enterprise units primary source analytics providing home advanced skills within organization dynamic lead appointment chief analytics officers cao starting 2011 availability strong businessfocused analytical talent greatest constraint companys analytical capabilities outsourcing analytics become attractive alternative need specialized skills lead organizations look outside help outsourcing analytics allows company focus taking action based insights delivered outsourcer outsourcer leverage specialized resources across multiple clients importance analytics grows organizations option outsource expect see 2011 see organizations establish enterprise data management functions coordinate data across business units also see smarter approaches information lifecycle management opposed common approach throwing hardware growing data problem information management challenge grow millions nextgeneration techsavvy users use feeds mashups bring data together usable parts answer questions gives rise new challenges including data security governance originally published frankdiananet march 20 2011 quick cheer standing ovation clap show much enjoyed story tcs executive focused rapid evolution society business fascinated view world next decade beyond httpsfrankdiananet,en,"['Business Analytics', 'Insight', 'IBM', 'Pricewaterhouse-Coopers', 'MIT Sloan', 'CAO', 'TCS Executive']"
76,Wildcat2030,5,Becoming a Cyborg should be taken gently: Of Modern Bio-Paleo-Machines — Cyborgology,"We are on the edge of a Paleolithic Machine intelligence world. A world oscillating between that which is already historical, and that which is barely recognizable. Some of us, teetering on this bio-electronic borderline, have this ghostly sensation that a new horizon is on the verge of being revealed, still misty yet glowing with some inner light, eerie but compelling.
The metaphor I used for bridging, seemingly contrasting, on first sight paradoxical, between such a futuristic concept as machine intelligence and the Paleolithic age is apt I think. For though advances in computation, with fractional AI, appearing almost everywhere are becoming nearly casual, the truth of the matter is that Machines are still tribal and dispersed. It is a dawn all right, but a dawn is still only a hint of the day that is about to shine, a dawn of hyperconnected machines, interweaved with biological organisms, cybernetically info-related and semi independent.
The modern Paleo-machines do not recognize borders; do not concern themselves with values and morality and do not philosophize about the meaning of it all, not yet that is. As in our own Paleo past the needs of the machines do not yet contain passions for individuation, desire for emotional recognition or indeed feelings of dismay or despair, uncontrollable urges or dreams of far worlds.
Also this will change, eventually. But not yet.
The paleo machinic world is in its experimentation stage, probing it boundaries, surveying the landscape of the infoverse, mapping the hyperconnected situation, charting a trajectory for its own evolution, all this unconsciously.
We, the biological part of the machine, are providing the tools for its uplift, we embed cameras everywhere so it can see, we implant sensors all over the planet so it may feel, but above all we nudge and we push towards a greater connectivity, all this unaware.
Together we form a weird cohabitation of biomechanical, electro-organic, planetary OS that is changing its environment, no more human, not mechanical, but a combined interactive intelligence, that journey on, oblivious to its past, blind to its future, irreverent to the moment of its conception, already lost to its parenthood agreement.
And yet, it evolves.
Unconscious on the machine part, unaware on the biological part, the almost sentient operating system of the global planetary infosphere, is emerging, wild eyed, complex in its arrangement of co-existence, it reaches to comprehend its unexpected growth.
The quid pro quo: we give the machines the platform to evolve; the machines in turn give us advantages of fitness and manipulation. We give the machines a space to turn our dreams into reality; the machines in turn serve our needs and acquire sapience in the process.
In this hypercomplex state of affairs, there is no judgment and no inherent morality; there is motion, inevitable, inexorable, inescapable, and mesmerizing.
The embodiment is cybernetic, though there be no pilot. Cyborgian and enhanced we play the game, not of thrones but of the commons. Connected and networked the machines follow in our footsteps, catalyzing our universality, providing for us in turn a meaning we cannot yet understand or realize.
The hybridization process is in full swing, reaching to cohere tribes of machines with tribes of humans, each providing for the other a non-designed direction for which neither has a plan, or projected outcome; both mingling and weaving a reality for which there is no ontos, expecting no Telos.
All this leads us to remember that only retrospectively do we recognize the move from the paleo tribes to the Neolithic status, we did not know that it happened then, and had no control over the motion, on the same token, we scarcely see the motion now and have no control over its directionality.
There is however a small difference, some will say it is insignificant, I do not think it so, for we are, some of us, to some extent at least, aware of the motion, and we can embed it with a meaning of our choice.
We can, if we muster our cognitive reason, our amazing skills of abstraction and simulation, whisper sweet utopias into the probability process of emergence. We can, if we so desire, passionate the operating system, to beautify the process of evolution and eliminate (or mitigate) the dangers of inchoate blind walking. We can, if we manage to control our own paleo-urges to destroy ourselves, allow the combined interactive intelligence of man and machine to shine forth into a brighter future of expanded subjectivity.
We can sing to the machines, cuddle them; caress their circuits, accepting their electronic-flaws so they can accept our bio-flaws, we can merge aesthetically, not with conquest but with understanding.
We can become wise, that is the difference this time around.
Being wise in this context implies a new form of discourse, an intersubjective cross-pollination of a wide array of disciplines. The very trans-disciplinarily nature of the process of cyborgization informs the discourse of subjectivity. The discourse on subjectivity, not unlike the move from paleo to Neolithic societal structure, demands of us a re-assessment of the relations between man and machine.
For this re-assessment to take place coherently the nascent re-organization of the hyperconnected machinic infosphere need be understood as a ground for the expansion of subjectivity.
In a sense the motion into the new hyperconnected infosphere is not unlike the move of the Neolithic to domestication of plants and animals. This time around however the domestication can be seen as the adoption of technologies for the furtherance of subjectivity into the world.
Understanding this process is difficult and far from obvious, it is a perspective however that might allow us a wider context of appreciation of the current upheavals happening all around us.
***
A writer, futurist and a Polytopian, Tyger.A.C (a.k.a @Wildcat2030) is the founder and editor of the Polytopia Project at Space Collective, he also writes at Reality Augmented, and Urbnfutr as well as contributing to H+ magazine. His passion and love for science fiction led him to initiate the Sci-fi Ultrashorts project.
***
Photo credit for baby with iPad photo: “Illumination” by Amanda Tipton.
Originally published at thesocietypages.org on November 22, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Futurist,Writer,Polytopia, Philosophy,Science,Science Fiction,
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
",edge paleolithic machine intelligence world world oscillating already historical barely recognizable us teetering bioelectronic borderline ghostly sensation new horizon verge revealed still misty yet glowing inner light eerie compelling metaphor used bridging seemingly contrasting first sight paradoxical futuristic concept machine intelligence paleolithic age apt think though advances computation fractional ai appearing almost everywhere becoming nearly casual truth matter machines still tribal dispersed dawn right dawn still hint day shine dawn hyperconnected machines interweaved biological organisms cybernetically inforelated semi independent modern paleomachines recognize borders concern values morality philosophize meaning yet paleo past needs machines yet contain passions individuation desire emotional recognition indeed feelings dismay despair uncontrollable urges dreams far worlds also change eventually yet paleo machinic world experimentation stage probing boundaries surveying landscape infoverse mapping hyperconnected situation charting trajectory evolution unconsciously biological part machine providing tools uplift embed cameras everywhere see implant sensors planet may feel nudge push towards greater connectivity unaware together form weird cohabitation biomechanical electroorganic planetary os changing environment human mechanical combined interactive intelligence journey oblivious past blind future irreverent moment conception already lost parenthood agreement yet evolves unconscious machine part unaware biological part almost sentient operating system global planetary infosphere emerging wild eyed complex arrangement coexistence reaches comprehend unexpected growth quid pro quo give machines platform evolve machines turn give us advantages fitness manipulation give machines space turn dreams reality machines turn serve needs acquire sapience process hypercomplex state affairs judgment inherent morality motion inevitable inexorable inescapable mesmerizing embodiment cybernetic though pilot cyborgian enhanced play game thrones commons connected networked machines follow footsteps catalyzing universality providing us turn meaning cannot yet understand realize hybridization process full swing reaching cohere tribes machines tribes humans providing nondesigned direction neither plan projected outcome mingling weaving reality ontos expecting telos leads us remember retrospectively recognize move paleo tribes neolithic status know happened control motion token scarcely see motion control directionality however small difference say insignificant think us extent least aware motion embed meaning choice muster cognitive reason amazing skills abstraction simulation whisper sweet utopias probability process emergence desire passionate operating system beautify process evolution eliminate mitigate dangers inchoate blind walking manage control paleourges destroy allow combined interactive intelligence man machine shine forth brighter future expanded subjectivity sing machines cuddle caress circuits accepting electronicflaws accept bioflaws merge aesthetically conquest understanding become wise difference time around wise context implies new form discourse intersubjective crosspollination wide array disciplines transdisciplinarily nature process cyborgization informs discourse subjectivity discourse subjectivity unlike move paleo neolithic societal structure demands us reassessment relations man machine reassessment take place coherently nascent reorganization hyperconnected machinic infosphere need understood ground expansion subjectivity sense motion new hyperconnected infosphere unlike move neolithic domestication plants animals time around however domestication seen adoption technologies furtherance subjectivity world understanding process difficult far obvious perspective however might allow us wider context appreciation current upheavals happening around us writer futurist polytopian tygerac aka wildcat2030 founder editor polytopia project space collective also writes reality augmented urbnfutr well contributing h magazine passion love science fiction led initiate scifi ultrashorts project photo credit baby ipad photo illumination amanda tipton originally published thesocietypagesorg november 22 2012 quick cheer standing ovation clap show much enjoyed story futuristwriterpolytopia philosophysciencescience fiction latest news info tutorials artificial intelligence machine learning deep learning big data means humanity,en,"['Paleolithic Machine', 'the Polytopia Project at Space Collective', 'Latest News', 'Info', 'Tutorials on Artificial Intelligence, Machine Learning', 'Humanity']"
77,Greg Fish,1,why you just can’t black box an a.i. – [ weird things ],"Singularitarians generally believe two things about artificial intelligence. First and foremost, they say, it’s just a matter of time before we have an AI system that will quickly become superhumanly intelligent. Secondly, and a lot more ominously, they believe that this system can sweep away humanity, not because it will be evil by nature but because it won’t care about humans or what happens to them and one of the biggest priorities for a researcher in the field should be figuring out how to build a friendly artificial intelligence, training it like one would train a pet, with a mix of operant conditioning and software. While the first point is one I’ve covered before, and pointed out again and again that superhuman is a very relative term and that computers are in many ways already superhuman without being intelligent, the second point is one that I haven’t yet given a proper examination. And neither have vocal Singularitarians. Why? Because if you read any of the papers on their version of friendly AI, yo’ll soon discover how quickly they begin to describe the system they’re trying to tame as a black box with mostly known inputs and measurable outputs, hardly a confident and lucid description of how an artificial intelligence functions, and ultimately, what rules will govern it.
No problem there, say the Singularitarians, the system will be so advanced by the time this happens that we’ll be very unlikely to know exactly how it functions anyway. It will modify its own source code, optimize how well it performs, and generally be all but inscrutable to computer scientists. Sounds great for comic books but when we’re talking about real artificially intelligent systems, this approach sounds more like surrendering to robots, artificial neural networks, and Bayesian classifiers to come up with whatever intelligence they want and send all the researchers and programmers out for coffee in the meantime. Artificial intelligence will not grow from a vacuum, it will come together from systems used to tackle discrete tasks and governed by several, if not one, common frameworks that exchange information between these systems. I say this because the only forms of intelligence we can readily identify are found in living things which use a brain to perform cognitive tasks, and since brains seem to be wired this way and we’re trying to emulate the basic functions of the brain, it wouldn’t be all that much of a stretch to assume that we’d want to combine systems good at related tasks and build on the accomplishments of existing systems. And to combine them, we’ll have to know how to build them.
Conceiving of an AI in a black box is a good approach if we want to test how a particular system should react when working with the AI and focusing on the system we’re trying to test by mocking the AI’s responses down the chain of events. Think of it as dependency injection with an AI interfacing system. But by abstracting the AI away, what we’ve also done is made it impossible to test the inner workings of the AI system. No wonder then that the Singularitarian fellows have to bring in operant conditioning or social training to basically housebreak the synthetic mind into doing what they need it to do. They have no other choice. In their framework we cannot simply debug the system or reset its configuration files to limit its actions. But why have they resigned to such an odd notion and why do they assume that computer scientists are creating something they won’t be able to control? Even more bizarrely, why do they think that an intelligence that can’t be controlled by its creators could be controlled by a module they’ll attach to the black box to regulate how nice or malevolent towards humans it would be? Wouldn’t it just find away around that module too if it’s superhumanly smart? Wouldn’t it make a lot more sense for its creators to build it to act in cooperation with humans, by watching what humans say or do, treating each reaction or command as a trigger for carrying out a useful action it was trained to perform?
And that brings us back full circle. To train machines to do something, we have to lay out a neural network and some higher level logic to coordinate what the networks’ outputs mean. We’ll need to confirm that the training was successful before we employ it for any specific task. Therefore, we’ll know how it learned, what it learned, and how it makes its decisions because all machines work on propositional logic and hence would make the same choice or set of choices at any given time. If it didn’t, we wouldn’t use it. So of what use is a black box AI here when we can just lay out the logical diagram and figure out how it’s making decisions and how we alter its cognitive process if need be? Again, we could isolate the components and mock their behavior to test how individual sub-systems function on their own, eliminating the dependencies for each set of tests. Beyond that, this block box is either a hindrance to a researcher or a vehicle for someone who doesn’t know how to build a synthetic mind but really, really wants to talk about what he imagines it will be like and how to harness its raw cognitive power. And that’s ok, really. But let’s not pretend that we know that an artificial intelligence beyond its creators’ understanding will suddenly emerge form the digital aether when the odds of that are similar to my toaster coming to life and barking at me when it thinks I want to feed it some bread.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
techie, Rantt staff writer and editor, computer lobotomist
science, tech, and other oddities
",singularitarians generally believe two things artificial intelligence first foremost say matter time ai system quickly become superhumanly intelligent secondly lot ominously believe system sweep away humanity evil nature wont care humans happens one biggest priorities researcher field figuring build friendly artificial intelligence training like one would train pet mix operant conditioning software first point one ive covered pointed superhuman relative term computers many ways already superhuman without intelligent second point one havent yet given proper examination neither vocal singularitarians read papers version friendly ai yoll soon discover quickly begin describe system theyre trying tame black box mostly known inputs measurable outputs hardly confident lucid description artificial intelligence functions ultimately rules govern problem say singularitarians system advanced time happens well unlikely know exactly functions anyway modify source code optimize well performs generally inscrutable computer scientists sounds great comic books talking real artificially intelligent systems approach sounds like surrendering robots artificial neural networks bayesian classifiers come whatever intelligence want send researchers programmers coffee meantime artificial intelligence grow vacuum come together systems used tackle discrete tasks governed several one common frameworks exchange information systems say forms intelligence readily identify found living things use brain perform cognitive tasks since brains seem wired way trying emulate basic functions brain wouldnt much stretch assume wed want combine systems good related tasks build accomplishments existing systems combine well know build conceiving ai black box good approach want test particular system react working ai focusing system trying test mocking ais responses chain events think dependency injection ai interfacing system abstracting ai away weve also done made impossible test inner workings ai system wonder singularitarian fellows bring operant conditioning social training basically housebreak synthetic mind need choice framework cannot simply debug system reset configuration files limit actions resigned odd notion assume computer scientists creating something wont able control even bizarrely think intelligence cant controlled creators could controlled module theyll attach black box regulate nice malevolent towards humans would wouldnt find away around module superhumanly smart wouldnt make lot sense creators build act cooperation humans watching humans say treating reaction command trigger carrying useful action trained perform brings us back full circle train machines something lay neural network higher level logic coordinate networks outputs mean well need confirm training successful employ specific task therefore well know learned learned makes decisions machines work propositional logic hence would make choice set choices given time didnt wouldnt use use black box ai lay logical diagram figure making decisions alter cognitive process need could isolate components mock behavior test individual subsystems function eliminating dependencies set tests beyond block box either hindrance researcher vehicle someone doesnt know build synthetic mind really really wants talk imagines like harness raw cognitive power thats ok really lets pretend know artificial intelligence beyond creators understanding suddenly emerge form digital aether odds similar toaster coming life barking thinks want feed bread quick cheer standing ovation clap show much enjoyed story techie rantt staff writer editor computer lobotomist science tech oddities,en,['AI']
78,Greg Fish,2,why do we want to build a fully fledged a.g.i.? – [ weird things ],"Undoubtedly the most ambitious idea in the world of artificial intelligence is creating an entity comparable to a human in cognitive abilities, the so called AGI. We could debate how it may come about, whether it will want to be your friend or not, whether it will settle the metaphysical question of whet makes humans who they are or open new doors in the discussion, but for a second let’s think like software architects and ask the question we should always tackle first before designing anything. Why would we want to build it? What will we gain? A sapient friend or partner? We don’t know that. Will we figure out what makes human ticks? Maybe, maybe not since what works in the propositional logic of artificial neural networks doesn’t necessarily always apply to an organic human brain. Will we settle the question of how an intellect emerges? Not really since we would only be providing one example and a fairly controversial one at that. And what exactly will a G in AGI entail? Will we need to embody it for it to work and if not, how would we develop the intellectual capacity of an entity extant in only abstract space? Will we have anything in common with it and could we understand what it wants?
And there’s more to it than that, even though I just asked some fairly heavy questions. Were we to build an AGI not by accident but by design, we would effectively be making the choice to experiment on a sapient entity and that’s something that may have to be cleared by an ethics committee, otherwise we’re implicitly saying that an artificial cognitive entity has no rights to self-determination. And that may be fine if it doesn’t really care about them, but what if it does? What if the drive for freedom evolves from a cognitive routine meant for self-defense and self-perpetuation? If we steer an AI model away from sapience by design, are we in effect snuffing out an opportunity or protecting ourselves? We can always suspend the model, debug it, and see what’s going on in its mind but again, the ethical considerations will play a significant part and very importantly, while we will get to know what such an AGI thinks and how, we may not know how it will first emerge. The whole AGI concept is a very ambiguous effort at defining intelligence and hence, doesn’t give us enough to objectively determine an intelligent artificial entity when we make one because we can always find an argument for and against how to interpret the results of an experiment meant to design one. We barely even know where to start.
Now, I could see major advantages to fusing with machines and becoming cyborgs in the near future as we’d swap irreparably damaged parts and pieces for 3D printed titanium, tungsten carbide, and carbon nanotubes to overcome crippling injury or treat an otherwise terminal disease. I could also see a huge upside to having direct interfaces to the machines around us to speed up our work and make life more convenient. But when it comes to such an abstract and all consuming technological experiment as AGI, the benefits seem to be very, very nebulous at best and the investment necessary seems extremely uncertain to pay off since we can’t even define what will make our AGI a true AGI rather than another example of a large expert system. Whereas with wetware and expert systems we can measure our return on investment with lives saved or significant gains in efficiency, how do we justify creating another intelligent entity after many decades of work, especially if it turns out that we actually can’t make one or it turns out to be completely different than what we hoped it would be as it nears completion? But maybe I’m wrong. Maybe there’s a benefit to an AGI that I’m overlooking and if that is the case, enlighten me in the comments because this is a serious question. Why peruse an AGI?
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
techie, Rantt staff writer and editor, computer lobotomist
science, tech, and other oddities
",undoubtedly ambitious idea world artificial intelligence creating entity comparable human cognitive abilities called agi could debate may come whether want friend whether settle metaphysical question whet makes humans open new doors discussion second lets think like software architects ask question always tackle first designing anything would want build gain sapient friend partner dont know figure makes human ticks maybe maybe since works propositional logic artificial neural networks doesnt necessarily always apply organic human brain settle question intellect emerges really since would providing one example fairly controversial one exactly g agi entail need embody work would develop intellectual capacity entity extant abstract space anything common could understand wants theres even though asked fairly heavy questions build agi accident design would effectively making choice experiment sapient entity thats something may cleared ethics committee otherwise implicitly saying artificial cognitive entity rights selfdetermination may fine doesnt really care drive freedom evolves cognitive routine meant selfdefense selfperpetuation steer ai model away sapience design effect snuffing opportunity protecting always suspend model debug see whats going mind ethical considerations play significant part importantly get know agi thinks may know first emerge whole agi concept ambiguous effort defining intelligence hence doesnt give us enough objectively determine intelligent artificial entity make one always find argument interpret results experiment meant design one barely even know start could see major advantages fusing machines becoming cyborgs near future wed swap irreparably damaged parts pieces 3d printed titanium tungsten carbide carbon nanotubes overcome crippling injury treat otherwise terminal disease could also see huge upside direct interfaces machines around us speed work make life convenient comes abstract consuming technological experiment agi benefits seem nebulous best investment necessary seems extremely uncertain pay since cant even define make agi true agi rather another example large expert system whereas wetware expert systems measure return investment lives saved significant gains efficiency justify creating another intelligent entity many decades work especially turns actually cant make one turns completely different hoped would nears completion maybe im wrong maybe theres benefit agi im overlooking case enlighten comments serious question peruse agi quick cheer standing ovation clap show much enjoyed story techie rantt staff writer editor computer lobotomist science tech oddities,en,['AGI']
79,Theo,3,Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine,"Have you noticed how tech savvy children have become but are no longer streetwise ? I read a friend’s thoughts on his own site last week and there was a slight pang of regret in where technology and innovation seems to be leading us all.
And so I started to worry about where the concept of innovation is going for future generations.
There’s an increasing reliance on technology for the sake of convenience, children are becoming self-reliant too quickly but gadgets are replacing people as the mentor. The human bonding of parenthood is a prime example of where it’s taking a toll. I’ve seen parents hand over iDevices to pacify a child numerous times now, the lullaby and bedtime reading session has been replaced with Cut The Rope and automated storybooks apps. I know a child who has developed speech difficulty because he’s been brought up on Cable TV and a DS Lite, pronouncing words as he has heard them from a tiny speaker and not by watching how his parents pronounce them.
And I started to worry about how the concept of innovation is being redefined for future generations.
I used my imagination constantly as a child and it’s still as active now as it was then but I didn’t use technology to spoon feed me. The next generation expect innovation to happen at their fingertips with little to no real stimuli.
Steve Jobs said “stay hungry, stay foolish” and he was right. Innovation comes from a keenness, it’s a starvation and hunger that drives people forward to spark and create, it comes from grabbing what little there is from the ether and turning it into something spectacular.
It’s the Big Bang of human thought creation.
And I started to worry about what the concept of innovation means for future generations.
Technology is taking away the power to think for ourselves and from our children. Everything must be there and in real-time for instant consumption. It’s junk food for the mind and we’re getting fat on it. And that breeds lazy innovation. We’ve become satiated before we reach the point of real creativity, nobody wants to bother taking the time to put it all together themselves any more, it has to be ready for us. And we’re happy to throw it away if it doesn’t work first time, use it or lose it, there’s less sweat and toil involved if we don’t persevere with failure.
Remember seeing the human race depicted in Wall-E ? That’s where innovation is heading.
And because of this we risk so many things disappearing for the sake of convenience. We’re all guilty of it, I’m guilty of it. I was asked once what would become absurd in ten years. Thinking about it I realized we’re on the cusp of putting books on the endangered species list. Real books, books bound in hard and paperback not digital copies from a Kindle store. And that scared me because the next generation of kids may grow up never seeing one, or experience sitting with their father as he reads an old battered copy of The Hobbit because he’ll be sitting there handing over an iPad with The Hobbit read-along app teed up, and it’ll be an actors voice not his father’s voice pretending to be a bunch of trolls about to eat a company of dwarfs.
Innovation is a magical, crazy concept. It stems from a combination of crazy imagination, human interaction and creativity not convenient manufacture. Technology can aid collaboration in ways we’ve never experienced before but it can’t run crazy for us. And for the sake of future generations don’t let it.
Here’s to the crazy ones indeed.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder and CEO @ RawShark Studios.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
",noticed tech savvy children become longer streetwise read friends thoughts site last week slight pang regret technology innovation seems leading us started worry concept innovation going future generations theres increasing reliance technology sake convenience children becoming selfreliant quickly gadgets replacing people mentor human bonding parenthood prime example taking toll ive seen parents hand idevices pacify child numerous times lullaby bedtime reading session replaced cut rope automated storybooks apps know child developed speech difficulty hes brought cable tv ds lite pronouncing words heard tiny speaker watching parents pronounce started worry concept innovation redefined future generations used imagination constantly child still active didnt use technology spoon feed next generation expect innovation happen fingertips little real stimuli steve jobs said stay hungry stay foolish right innovation comes keenness starvation hunger drives people forward spark create comes grabbing little ether turning something spectacular big bang human thought creation started worry concept innovation means future generations technology taking away power think children everything must realtime instant consumption junk food mind getting fat breeds lazy innovation weve become satiated reach point real creativity nobody wants bother taking time put together ready us happy throw away doesnt work first time use lose theres less sweat toil involved dont persevere failure remember seeing human race depicted walle thats innovation heading risk many things disappearing sake convenience guilty im guilty asked would become absurd ten years thinking realized cusp putting books endangered species list real books books bound hard paperback digital copies kindle store scared next generation kids may grow never seeing one experience sitting father reads old battered copy hobbit hell sitting handing ipad hobbit readalong app teed itll actors voice fathers voice pretending bunch trolls eat company dwarfs innovation magical crazy concept stems combination crazy imagination human interaction creativity convenient manufacture technology aid collaboration ways weve never experienced cant run crazy us sake future generations dont let heres crazy ones indeed quick cheer standing ovation clap show much enjoyed story founder ceo rawshark studios latest news info tutorials artificial intelligence machine learning deep learning big data means humanity,en,"['Cable TV', 'DS Lite', 'Hobbit', 'Innovation', 'RawShark Studios', 'Latest News', 'Info', 'Tutorials on Artificial Intelligence, Machine Learning', 'Humanity']"
80,x.ai,1,"I scheduled 1,019 meetings in 2012 — and that doesn’t count reschedules — x.ai","The number of meetings that I scheduled in 2012 might seem astronomical. Put in context, it’s less so. I was a startup-founder at the time, and that year my company, Visual Revenue, took Series-A funding, doubled revenue, and started discussing a possible exit.
I like the number though! As a startup romantic one could turn it into a nifty Malcolm Gladwell type rule of thumb called the “1,000 meetings rule.” Gladwell’s claim that greatness requires an enormous time sacrifice rings true to me — whether that means investing 10,000 hours into a subject matter to become an expert or conducting a 1,000 meetings per year, is another question.
More interesting though is the impact of this 1,019 figure, and a related one: Of those more than one thousand meetings I scheduled, 672 were rescheduled. That was painful. But these numbers were among the early pieces of data that inspired me to start x.ai.
* A meeting is defined as an event in my calendar, which is marginally flawed in both directions, given some events would be “Travel to JFK”, which is obviously a task and not a meeting, where others would be “Interview Sales Director Candidates“, which is really 4 meetings in 1.
Originally published at x.ai on October 14, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Magically schedule meetings
",number meetings scheduled 2012 might seem astronomical put context less startupfounder time year company visual revenue took seriesa funding doubled revenue started discussing possible exit like number though startup romantic one could turn nifty malcolm gladwell type rule thumb called 1000 meetings rule gladwells claim greatness requires enormous time sacrifice rings true whether means investing 10000 hours subject matter become expert conducting 1000 meetings per year another question interesting though impact 1019 figure related one one thousand meetings scheduled 672 rescheduled painful numbers among early pieces data inspired start xai meeting defined event calendar marginally flawed directions given events would travel jfk obviously task meeting others would interview sales director candidates really 4 meetings 1 originally published xai october 14 2013 quick cheer standing ovation clap show much enjoyed story magically schedule meetings,en,"['Visual Revenue', 'Series-A']"
81,Arjan Haring 🔮🔨,1,Website morphing and more revolutions in marketing – Arjan Haring 🔮🔨 – Medium,"John R. Hauser is the Kirin Professor of Marketing at M.I.T.’s Sloan School of Management where he teaches new product development, marketing management, and statistical and research methodology. He has served MIT as Head of the MIT Marketing Group, Head of the Management Science Area, Research Director of the Center for Innovation in Product Development, and co-Director of the International Center for Research on the Management of Technology.He is the co-author of two textbooks, Design and Marketing of New Products and Essentials of New Product Management, and a former editor of Marketing Science (now on the advisory board).I think it wouldn’t be smart to start this interview with something as dull and complex as a definition. Or am I the only one that likes to read light weight and short articles?Let’s just get it over with.
“Website morphing matches the look and feel of a website to each customer so that, over a series of customers, revenue or profit are maximized.”
That didn’t hurt as much as I expected. I actually love the idea. It sounds completely logical.
But are we talking about a completely new idea?
There is tremendous variety in the way customers process and use information, some prefer simple recommendations while others like to dig into the details. Some customers think verbally or holistically, others prefer pictures and graphs. What is new is that we now have good algorithms to identify how customers think from the choices they make as they explore websites (their clickstream).
But once we identify the way they think, we still need an automatic way to learn which website look and feel will lead to the most sales. This is a very complex problem which, fortunately, has a relative simple solution based on fundamental research by John Gittins. Our contribution was to combine the identification algorithms with the learning algorithms and develop an automated system that was feasible and practical. Once we developed the technology to morph websites, we were only limited by our imaginations.
In our first application we matched the look and feel to customers’ cognitive styles. In our second, we matched to cognitive and cultural styles. We then used the algorithms to morph banner advertisements to achieve almost a 100% lift in click-through rates. Our latest project used both cognitive styles and the customers search history to match the automotive banner advertising to enhance clicks, consideration, and purchase likelihood.
I also love the fact that you combine technology with behavioral science. On the psychology side of things you are/have been busy with cognitive styles, cognitive switching and cognitive simplicity.
Can you tell us a little bit more about these theories and why you chose to use them?
Customers are smart. They know when to use simple decision rules (cognitive simplicity) and when to use more complicated rules.
Our research has been two-fold.
(1) Website morphing and banner morphing figure out how customers think and provides information in the format that helps them think the way they prefer to think.
(2) We have also focused on identifying consideration heuristics. Typically, customers seriously consider only a small fraction of available product. To do so they use simple rules that balance thinking (and search) costs with the completeness of information. By understanding these simple rules, managers can develop better products and better marketing strategies.
We can now identify these decision rules quickly with machine-learning methods. But a caveat — customers do not always use cognitively simple rules. The “moment of truth” in a final purchase decision is best understood with more-complex decision rules and methods such as choice-based conjoint analysis.
Most recently we’ve combined the two streams of research. Curiously, some of the algorithms used by the computer to morph websites are reasonably descriptive of how consumers take the future into account in purchases they make today. Prior research postulated a form of hyperrationality. Our research suggests that consumers are pretty smart about balancing cognitive costs and foresight.
What are your main interests on the technology side of website morphing? Which algorithms take your fancy and why?
Website morphing uses an “index” solution to learn the best morph for a customer. Our latest efforts also identify when to morph a website by embedded another “dynamic program” within the index solution. In our research to understand how consumers deal with the future, we’ve demonstrated that indices other than Gittins’ index might be more descriptive of consumer foresight.
If I think about it, as a company you can either win the algorithm competition, or the psychology competition. Or lose. Do you agree?
Actually, the companies that will thrive are those that understand the customers’ cognitive processes, have the algorithms to match products and marketing to customers’ cognitive processes, and have the organization that accepts such innovation.
You need all three.
Is this what marketing will be about in 5 years?
There are many revolutions in marketing. It is an exciting time. It’s hard to list all of the changes, but here are a few.
(1) Big data. We know so much more about customers than we ever did before, but this knowledge is often hidden within the volume of data. One challenge is to develop methods that scale well to be data.
(2) Machine learning. There are some problems that humans solve better than computers and some problems that computers solve better than humans. Morphing, identifying simple decision rules, and studying consumer foresight are all possible with the advent of good machine-learning methods. But we have only scratched the surface.
(3) Causality. Marketing has used quite successfully small-sample laboratory experiments and assumption-laden quantitative models. However, the advent of big data and web-based data collection has made it possible to do experiments and quasi-experiments on a large scale to better establish causality and to better develop theories that are externally valid. Causality also means replication. There is a strong movement in the journals to require that key finding be replicated.
(4) The TPM movement (theory + practice in marketing). Conferences, special issues, and organizations are now devoted to matching managerial needs to research with impact. In fact, a recent survey by the INFORMS Society of Marketing Science suggests that approximately 80% of the researchers in marketing believe that research should be more focused on applications.
(5) A maturing perspective on behavioral science. Researchers are increasingly less focused on “cute” findings that apply only in special circumstances. They are beginning to focus on insights that have a big impact (effect size) and apply to decisions that customers make routinely.
Companies that combine algorithms, an understanding of customer decision-making, and the ability to use data will be the companies that succeed.
Originally published at www.sciencerockstars.com on October 21, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Let’s Fix the Future: Scientific Advisor @jadatascience
",john r hauser kirin professor marketing mits sloan school management teaches new product development marketing management statistical research methodology served mit head mit marketing group head management science area research director center innovation product development codirector international center research management technologyhe coauthor two textbooks design marketing new products essentials new product management former editor marketing science advisory boardi think wouldnt smart start interview something dull complex definition one likes read light weight short articleslets get website morphing matches look feel website customer series customers revenue profit maximized didnt hurt much expected actually love idea sounds completely logical talking completely new idea tremendous variety way customers process use information prefer simple recommendations others like dig details customers think verbally holistically others prefer pictures graphs new good algorithms identify customers think choices make explore websites clickstream identify way think still need automatic way learn website look feel lead sales complex problem fortunately relative simple solution based fundamental research john gittins contribution combine identification algorithms learning algorithms develop automated system feasible practical developed technology morph websites limited imaginations first application matched look feel customers cognitive styles second matched cognitive cultural styles used algorithms morph banner advertisements achieve almost 100 lift clickthrough rates latest project used cognitive styles customers search history match automotive banner advertising enhance clicks consideration purchase likelihood also love fact combine technology behavioral science psychology side things arehave busy cognitive styles cognitive switching cognitive simplicity tell us little bit theories chose use customers smart know use simple decision rules cognitive simplicity use complicated rules research twofold 1 website morphing banner morphing figure customers think provides information format helps think way prefer think 2 also focused identifying consideration heuristics typically customers seriously consider small fraction available product use simple rules balance thinking search costs completeness information understanding simple rules managers develop better products better marketing strategies identify decision rules quickly machinelearning methods caveat customers always use cognitively simple rules moment truth final purchase decision best understood morecomplex decision rules methods choicebased conjoint analysis recently weve combined two streams research curiously algorithms used computer morph websites reasonably descriptive consumers take future account purchases make today prior research postulated form hyperrationality research suggests consumers pretty smart balancing cognitive costs foresight main interests technology side website morphing algorithms take fancy website morphing uses index solution learn best morph customer latest efforts also identify morph website embedded another dynamic program within index solution research understand consumers deal future weve demonstrated indices gittins index might descriptive consumer foresight think company either win algorithm competition psychology competition lose agree actually companies thrive understand customers cognitive processes algorithms match products marketing customers cognitive processes organization accepts innovation need three marketing 5 years many revolutions marketing exciting time hard list changes 1 big data know much customers ever knowledge often hidden within volume data one challenge develop methods scale well data 2 machine learning problems humans solve better computers problems computers solve better humans morphing identifying simple decision rules studying consumer foresight possible advent good machinelearning methods scratched surface 3 causality marketing used quite successfully smallsample laboratory experiments assumptionladen quantitative models however advent big data webbased data collection made possible experiments quasiexperiments large scale better establish causality better develop theories externally valid causality also means replication strong movement journals require key finding replicated 4 tpm movement theory practice marketing conferences special issues organizations devoted matching managerial needs research impact fact recent survey informs society marketing science suggests approximately 80 researchers marketing believe research focused applications 5 maturing perspective behavioral science researchers increasingly less focused cute findings apply special circumstances beginning focus insights big impact effect size apply decisions customers make routinely companies combine algorithms understanding customer decisionmaking ability use data companies succeed originally published wwwsciencerockstarscom october 21 2013 quick cheer standing ovation clap show much enjoyed story lets fix future scientific advisor jadatascience,en,"['Kirin', 'M.I.T.', 'Sloan School of Management', 'MIT', 'the MIT Marketing Group', 'the Center for Innovation in Product Development', 'the International Center for Research on the Management of Technology', 'TPM', 'the INFORMS Society of Marketing Science']"
82,Arjan Haring 🔮🔨,1,Using Artificial Intelligence to Balance Out Customer Value,"December 13 it was that time again: the second edition of #projectwaalhalla: Social Sciences for Startups. This time with Peter van der Putten speaking as data scientist. He is guest researcher at the Data Mining Group (algorithms research cluster) of the Leiden Institute of Advanced Computer Science. He is also director of decisioning solutions worldwide at Pegasystems. There is, according to Peter, a lot of potential for new startups in this area. Are you going to be the next success story ?
I am actually very curious what you, as a leading data scientist, think of this whole big data thingy.
I am fascinated by learning from data, but have mixed feelings about big data. The concept is being hyped a lot at this moment, while the algorithms to learn from data have been studied since the 40s in computer science. Many of the “modern” big data technologies like Hadoop are in fact still limited frameworks for old-fashioned, offline batch processed data, instead of real-time processed data. The focus should really not have to be on the data, but on the analysis — how we generate knowledge and learn from data through data mining — and more importantly, how do we operationalize this knowledge, how can we use this knowledge. Because: “Knowledge is not power, action is.”
And what is the role of psychology in big data? And of philosophy?
Psychology has begun studying intelligence fifty or sixty years before computer science did. People, animals, plants and all intelligent systems are basically information processing machinery. Psychology seeks to understand these systems and to tries explain behavior — if you understand a bit of that system, you can use this knowledge. For example, to teach computers, stupid mathematical pieces of scrap, smarter functions such as learning and responding to customer behavior. What is to say, thinking the other away around, that people don’t have to think like computers. See for example the psychologist Daniel Kahneman who won the 2002 The Sveriges Riksbank Prize in Economic Sciences, the unofficial Nobel Prize in economics for his insight that people aren’t rational agents that properly weigh all the choices before deciding something.
And philosophy? These guys have dealt with big data for more than 3,000 years now. Just think of the nature vs. nurture debate: do we acquire intelligence and other properties by (data) experience or are they innate? Or the whole philosophy of mind discussion, with roots in the ancient Greeks: what do we really know? And is there is only experience or just reality?
You have a background in artificial intelligence (AI) and even studied with the famous and wildly attractive Bas Haring (Not related... well cousin to be honest. If you insist). What could AI mean for business, and how is it different from Big Data?
As long as AI is not used for old fashioned data manipulation or poor reporting, but really as intelligent data science, big data is one of the tools within ‘learning’ artificial intelligence. That is, systems that are not smart by the knowledge that is pre-inserted, but which have the capacity to learn and combine what is learned with background knowledge to deduct decisions. This is what I like to call the field of ‘decisioning’. Really intelligent systems put that knowledge into action and are part of an ecosystem, an environment with other actors, systems, people, and the scary outside world.
Sounds abstract? Until the late 90s artificial intelligence was only done in the lab, now people interact with AI, unconsciously, on a daily basis, for example if they use Google, check their Facebook page or look at banners on the web. Take the company where I work next to my academic job, when I came in 2002, it was a startup of only 15 men with new software and a launching customer [editor’s note: we know that feeling ;)], ten years and two acquisitions later, we have reach more than 1 billion consumers with intelligent, data-driven, scientifically proven, real-time recommendations via digital as well as traditional channels like ATMs, shops and callcenters. No push product offerings anymore, but only ‘next best action’ recommendations that optimize customer value by balancing customer experience and predicted interests and behavior.
What opportunities do you see for startups in the artificial intelligence in this area ?
Well, I see tremendous opportunities, not only for 100% pure AI startups, but for all startups . If you look at the startups in Silicon Valley in high-tech and biotech, artificial intelligence is a major part of the business. Every startup should consider whether data is a key asset or a barrier to entry, and how AI or data mining can be used to convert these data into money. Where I have to note that customers and citizens rightly so, are getting more critical after all NSA issues. Those who can use this technology in a way that it not only benefits companies, but especially customers will be the most successful.
In conclusion , I am curious about how much you you are looking forward to December 13, and what should happen during #projectwaalhalla that would make your wildest dreams come true.
Very much looking forward to it! In terms of wildest dreams: I heard a reunion concert of the Urban Dance Squad is not going to happen, which I understand, but I look forward to exchanging views with startups, freelancers and multinationals on how to create, with the help of raw data diamands and a magical mix of data mining, machine learning, decisioning and evidence-based and real-time marketing.
I will bring some nice metaphorical pictures and leave will double integrals at home.
[Editor’s note: An UDS reunion? Sounds like a plan to us]
Originally published at www.sciencerockstars.com on December 6, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Let’s Fix the Future: Scientific Advisor @jadatascience
A blog series about the discipline of business experimentation. How to run and learn from experiments in different contexts is a complex matter, but lays at the heart of innovation.
",december 13 time second edition projectwaalhalla social sciences startups time peter van der putten speaking data scientist guest researcher data mining group algorithms research cluster leiden institute advanced computer science also director decisioning solutions worldwide pegasystems according peter lot potential new startups area going next success story actually curious leading data scientist think whole big data thingy fascinated learning data mixed feelings big data concept hyped lot moment algorithms learn data studied since 40s computer science many modern big data technologies like hadoop fact still limited frameworks oldfashioned offline batch processed data instead realtime processed data focus really data analysis generate knowledge learn data data mining importantly operationalize knowledge use knowledge knowledge power action role psychology big data philosophy psychology begun studying intelligence fifty sixty years computer science people animals plants intelligent systems basically information processing machinery psychology seeks understand systems tries explain behavior understand bit system use knowledge example teach computers stupid mathematical pieces scrap smarter functions learning responding customer behavior say thinking away around people dont think like computers see example psychologist daniel kahneman 2002 sveriges riksbank prize economic sciences unofficial nobel prize economics insight people arent rational agents properly weigh choices deciding something philosophy guys dealt big data 3000 years think nature vs nurture debate acquire intelligence properties data experience innate whole philosophy mind discussion roots ancient greeks really know experience reality background artificial intelligence ai even studied famous wildly attractive bas haring related well cousin honest insist could ai mean business different big data long ai used old fashioned data manipulation poor reporting really intelligent data science big data one tools within learning artificial intelligence systems smart knowledge preinserted capacity learn combine learned background knowledge deduct decisions like call field decisioning really intelligent systems put knowledge action part ecosystem environment actors systems people scary outside world sounds abstract late 90s artificial intelligence done lab people interact ai unconsciously daily basis example use google check facebook page look banners web take company work next academic job came 2002 startup 15 men new software launching customer editors note know feeling ten years two acquisitions later reach 1 billion consumers intelligent datadriven scientifically proven realtime recommendations via digital well traditional channels like atms shops callcenters push product offerings anymore next best action recommendations optimize customer value balancing customer experience predicted interests behavior opportunities see startups artificial intelligence area well see tremendous opportunities 100 pure ai startups startups look startups silicon valley hightech biotech artificial intelligence major part business every startup consider whether data key asset barrier entry ai data mining used convert data money note customers citizens rightly getting critical nsa issues use technology way benefits companies especially customers successful conclusion curious much looking forward december 13 happen projectwaalhalla would make wildest dreams come true much looking forward terms wildest dreams heard reunion concert urban dance squad going happen understand look forward exchanging views startups freelancers multinationals create help raw data diamands magical mix data mining machine learning decisioning evidencebased realtime marketing bring nice metaphorical pictures leave double integrals home editors note uds reunion sounds like plan us originally published wwwsciencerockstarscom december 6 2013 quick cheer standing ovation clap show much enjoyed story lets fix future scientific advisor jadatascience blog series discipline business experimentation run learn experiments different contexts complex matter lays heart innovation,en,"['Social Sciences', 'the Data Mining Group', 'the Leiden Institute of Advanced Computer Science', 'Pegasystems', 'Bas Haring', 'Big Data', 'Google', 'NSA', 'the Urban Dance Squad', 'UDS']"
83,Roland Trimmel,20,Will All Musicians Become Robots? – Roland Trimmel – Medium,"Finally we see the rise of the machines, and with it develops a certain fear that artificial intelligence (AI) will render humans useless.
This question was posed at Boston’s A3E Conference last month by a team member at Landr. Their company had received death threats from people in the mastering industry after having released a DIY drag-and-drop instant online mastering service powered by AI algorithms.
It illustrates the resistance that the world of AI has incited amongst us. Some fear that robots will take over à la Terminator 2. Some fear that the virtual and artificial will replace the visceral. Some cite religious views, and others? Frankly, others just seem ignorant.
That sets the tone for our own journey into artificial intelligence, and the lessons we have learned from it.
We had spent more than three years developing algorithms to enable software to read and interpret a composition (song) like an expert does. Coming from a music and technology background, our team was hugely excited having accomplished this.
Make no mistake, it’s really difficult to make a computer understand music — for us, this was an important first step towards a new generation of intelligent music instruments that assist the user in the songwriting process for faster completion of complex tasks resulting in no interruption of the creative flow and more creative output.
When you spend so many years working on a technology/product, you run the risk of losing sight of the market. And this being our first product, we had absolutely no idea what to expect.
To find out, we had to bring the product to the attention of the target group and eagerly awaited their reaction:
That meant a lot of leg work for us in starting discussions on multiple forums, and collecting users’ feedback. It takes time to cut through the noise, but creates some great threads.
What was interesting for us to monitor is how the discussions about our product unfolded on those forums and how opinions were split between two camps: one that embraced what we do, and the other that was characterized by anger, fear, or a complete misunderstanding of what our software does.
At times we felt like being in the middle of the fight between machines and humans. We hadn't expected this, our aim was to make a cool product that shows what the technology is capable of doing.
Eventually, we spent lots of time clearing misunderstandings, explaining our product better, etc. to win over those forum members’ hearts for what we do. And, occasionally we also had to calm down a heated discussion between members insulting each other caused by a fear that our product eliminates the craft in music composition.
Today
Enter a different reality: We have made a lot of progress with our software, much of it is down to communicating openly with our community to address any questions they may have early, and involve them deeply in product development.
Has the tone in discussions about our technology changed? Yes, certainly it has. But please don’t think it’s an easy journey. It’s still hard to convince music producers to rely on the help of a piece of software that, in some regard, replicates processes of the human brain.
The efforts that go into being a pioneer and driving this perceptional battle are driving one close to insanity. It’s an endless stream of work. And it requires endurance like during marathons or triathlons. Here are five things that we learned from our journey that we’d like to share with you so you can judge better before dismissing AI in music.
Let me start with a quick discussion of the first and second digital wave in music:
The first digital wave brought about digital music technology like synths and DAW’s. And with that, everything changed. Sound synthesis and sampling made entirely new forms of expressiveness possible. Sequencers in combination with large databases of looping clips laid the foundation for electronic dance music which led to a multifaceted artistic and cultural revolution.
The second digital wave has been rolling along for a few years now, and it is washing up intelligent algorithms for processing audio and MIDI. As an example, AI’s can already help control the finishing mastering process of music tracks, as assistant tools, or even fully automated. In the not too distant future—and we’re talking only years from now—we will be used to incredible music making automatons controlling most complex harmonic figures, flawlessly imitating the greatest artists.
The output quality by such algorithms is unbelievable. Computer intelligence can aimlessly merge styles of various artists and apply them to yet another piece, all that without breaking a sweat.
We regard the main application of AI’s for music composition and production as helper tools, not artists in their own regard. And this is not cheating. We have been utilizing digital production tools for decades. It was just a matter of time for more complicated and intelligent code to emerge.
But rest assured, computers will rather not generate music all by themselves. The art and craft of composing will prevail. There will always be human beings behind the actual output controlled by an AI. It will help though to create less complex, leaner user interfaces in the tools we use for creating music that are simpler to operate.
On to the learning we promised you now.
Definitely not. The magic and final decision over creative output will always remain with the (human) artist. A computer is not a human with feelings and emotions. What makes us get to our knees in awe will keep machines clinically indifferent. Simple as that.
And technical approximations, as deceptive as they may get, are simply not the real thing.
It already is. There is no stopping it. But then that is the course of a natural evolutionary process which can only push forward.
A huge one. This is a game changer! Read our statement on main applications above.
It is our egos we cling on to, having trotted down the same paths for decades. Many believe their laboriously acquired expertise is threatened by robot technology and a new ruthless generation. The truth is if we embrace AI’s as our helping friends and maybe even learn how to think a little more technical, who can fathom how ingeniously more colorful the world of music will become in the hands of talented musicians of all generations.
Yes, because it enables a completely new generation of products and startups like us push for innovation. The agreeable side effect: It will make people happy, musicians, consumers, and businessmen alike, full circle.
Most importantly though, it is not only AI changing the music industry. Social changes are equally responsible for it, if they don’t account for a larger part for it anyway. Here’s an excellent article by Fast Company on this topic, and more coverage on A3E in this article by TechRepublic.
It’s an interesting time for all of us in music and beyond, and there’s so much yet to come.
Don’t be afraid — humans also prevailed in Terminator:
“There are things machines will never do. They cannot possess faith, they cannot commune with God, they cannot appreciate beauty, they cannot create art. If they ever learn these things, they won’t have to destroy us. They’ll be us.”
-Sarah Connor.
Image credit: Daft Punk (top), Re-Compose (middle)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",finally see rise machines develops certain fear artificial intelligence ai render humans useless question posed bostons a3e conference last month team member landr company received death threats people mastering industry released diy draganddrop instant online mastering service powered ai algorithms illustrates resistance world ai incited amongst us fear robots take la terminator 2 fear virtual artificial replace visceral cite religious views others frankly others seem ignorant sets tone journey artificial intelligence lessons learned spent three years developing algorithms enable software read interpret composition song like expert coming music technology background team hugely excited accomplished make mistake really difficult make computer understand music us important first step towards new generation intelligent music instruments assist user songwriting process faster completion complex tasks resulting interruption creative flow creative output spend many years working technologyproduct run risk losing sight market first product absolutely idea expect find bring product attention target group eagerly awaited reaction meant lot leg work us starting discussions multiple forums collecting users feedback takes time cut noise creates great threads interesting us monitor discussions product unfolded forums opinions split two camps one embraced characterized anger fear complete misunderstanding software times felt like middle fight machines humans hadnt expected aim make cool product shows technology capable eventually spent lots time clearing misunderstandings explaining product better etc win forum members hearts occasionally also calm heated discussion members insulting caused fear product eliminates craft music composition today enter different reality made lot progress software much communicating openly community address questions may early involve deeply product development tone discussions technology changed yes certainly please dont think easy journey still hard convince music producers rely help piece software regard replicates processes human brain efforts go pioneer driving perceptional battle driving one close insanity endless stream work requires endurance like marathons triathlons five things learned journey wed like share judge better dismissing ai music let start quick discussion first second digital wave music first digital wave brought digital music technology like synths daws everything changed sound synthesis sampling made entirely new forms expressiveness possible sequencers combination large databases looping clips laid foundation electronic dance music led multifaceted artistic cultural revolution second digital wave rolling along years washing intelligent algorithms processing audio midi example ais already help control finishing mastering process music tracks assistant tools even fully automated distant futureand talking years nowwe used incredible music making automatons controlling complex harmonic figures flawlessly imitating greatest artists output quality algorithms unbelievable computer intelligence aimlessly merge styles various artists apply yet another piece without breaking sweat regard main application ais music composition production helper tools artists regard cheating utilizing digital production tools decades matter time complicated intelligent code emerge rest assured computers rather generate music art craft composing prevail always human beings behind actual output controlled ai help though create less complex leaner user interfaces tools use creating music simpler operate learning promised definitely magic final decision creative output always remain human artist computer human feelings emotions makes us get knees awe keep machines clinically indifferent simple technical approximations deceptive may get simply real thing already stopping course natural evolutionary process push forward huge one game changer read statement main applications egos cling trotted paths decades many believe laboriously acquired expertise threatened robot technology new ruthless generation truth embrace ais helping friends maybe even learn think little technical fathom ingeniously colorful world music become hands talented musicians generations yes enables completely new generation products startups like us push innovation agreeable side effect make people happy musicians consumers businessmen alike full circle importantly though ai changing music industry social changes equally responsible dont account larger part anyway heres excellent article fast company topic coverage a3e article techrepublic interesting time us music beyond theres much yet come dont afraid humans also prevailed terminator things machines never cannot possess faith cannot commune god cannot appreciate beauty cannot create art ever learn things wont destroy us theyll us sarah connor image credit daft punk top recompose middle quick cheer standing ovation clap show much enjoyed story,en,"['Landr', 'DIY', 'DAW', 'MIDI', 'TechRepublic', 'Terminator']"
84,Espen Waldal,57,How Artificial Intelligence can improve online news,"That being said, the user experience for online news sites today is very much like it was ten and fifteen years ago (see the slideshow showing the evolution of NYT.com). You enter a homepage where a carefully selected combination of articles on sports, celebrity reality shows, dinner recipes and even actual news scream for your attention. There’s a huge focus on page views, and hardly any attention given to personal relevance for the reader. Smart use of technology could improve the online news experience vastly by just adding a bit more structure. That is why we created Orbit.
Rich structured data is the foundation for taking the online news experience to the next level. Orbit is a collection of artificial intelligence technology API’s using machine learning-based content analysis to automatically transform unstructured text into rich structured data.
By analyzing and organizing content in real-time and automatically tagging and structuring large pieces of text into clusters of topics, Orbit creates a platform where you can build multiple data rich applications.
The now 5-month-old leaked innovation report from the New York Times pointed to several challenges for keeping and expanding a digital audience. To face some of the most critical issues you need to create a better experience for the reader by:
1 Serving up better recommendations of related content2 Providing new ways to discover news and add context3 Introducing personalization and filtering
Relevance is essential to creating loyal readers, and even more so in a time where more and more visits to news sites go directly to a specific article, mainly due to search and social media, avoiding the front page altogether. Readers arriving through side doors like Twitter or Facebook are less engaged than readers arriving directly, which means it’s important to keep these visitors on the site and convert them into loyal readers. Yet, so little is being done to improve the relevance of recommendations and create a connection to the huge amounts of valuable content that already exists.
Orbit understands not only the topics a piece contains but also related topics. It thereby understands the context of the article and can bring up related content that the reader wouldn’t otherwise have seen, extending the reader’s time spent on the site and increasing page views.
Understanding context means that the cluster of topics related to an article on China signing a historic gas deal with Russia includes topics such as Russia, Ukraine, Putin, Gazprom and energy — thus creating recommendations within that cluster and creating connections between content.
Rich structured data opens up for new ways to navigate and discover news. The classic navigation through carefully edited front pages has pretty much been the same since the dawn of online news publishing. Structured data enables the reader to follow certain topics or stories, improves search and enables timeline navigation of a news story to help the reader better understand the context of the story and how it has developed.
At the same time, a journalist writing a story on the uproar in Ukraine has no possible way of knowing how the story will unfold in the weeks to come. Manual tagging of news stories leads to inconsistent and incomplete structures due to a subjective understanding of which topics are important and related. Machine learning-based content analysis can identify people, organizations and places and relate them to each other in real-time, thereby identifying related stories as they unfold and cluster them together.
As the NYT Innovation report brought up, the true value of structured data emerges only when the content is structured equally throughout.
News and content apps like Circa, Omni and Prismatic, and news sites like Vox, have incorporated some of these elements and are experimenting with how to develop original ways to discover news.
There are many arguments against personalization, and they are often related to the dystopian fear of a «fragmented» public sphere or the horrors of the echo chamber. That doesn’t mean personalization can’t be a good thing; it merely means being aware of what a particular type of user wants at a particular time. We are not talking about a fully customizable news feed based on your subjective interests, meaning I will not only see articles related to Manchester United, Finance, TV-shows and Kim Kardashian, and be uninformed on all other topics. We are merely suggesting a smart filtering system and adjustments of what subjects you would like to see more and less of on your feed. After all, we do have different interests. For example you may be entirely disinterested in Tour de France during its three week media frenzy in July each year; unfollow topic, or turn the «volume» down.
Today, getting the news isn’t the hard part. Filtering out the excessive info and navigating the overwhelming stream of news in a smart way is where you need great tools.
A foundation of rich structured data will not only benefit the reader, but make life easier for journalists and editors as well.
To provide context to a story about Syria you could add several components of extra information that would enrich the article: A box of background information on the conflict, facts about Bashar Al-Assad and the different Syrian rebel groups, and so forth.
With rich structured data in place, you can automatically add relevant fact boxes and other interactive elements to a piece of content, based on third party content databases such as Wikipedia. Topics can automatically generate their own page with all the related articles, facts, visualizations and insights relevant for that specific topic cluster.
Moreover, you can use the data to create new and compelling presentations of your content, including visualizations and timelines that give the reader a better experience and new insights.
News content generally has a short lifespan, but this doesn’t mean that old content can’t be valuable in a new context. A consistent structuring of archived content will give new life to old content, making it easier to reuse and resurrect articles that are still relevant and create connections between old and new articles.
What are the trending topics, people or organizations this week? What regions got the most media attention? How many of the sources were anonymous, how many were women versus men? Knowing more about your audience’s preferences will make it easier to create good content at the right time.
Better organized content creates a strong foundation for good insights into how content is consumed and why. With a better ecosystem for your content, including higher relevance and more contextual awareness, you can present better context-based ads to your advertisers and give better insights into who is watching and acting on them.
By using the right technology in smart ways, journalists and editors can focus on what they are best at: creating quality news content.
orbit.ai
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Product Manager at Bakken & Bæck.
The Bakken & Bæck blog
",said user experience online news sites today much like ten fifteen years ago see slideshow showing evolution nytcom enter homepage carefully selected combination articles sports celebrity reality shows dinner recipes even actual news scream attention theres huge focus page views hardly attention given personal relevance reader smart use technology could improve online news experience vastly adding bit structure created orbit rich structured data foundation taking online news experience next level orbit collection artificial intelligence technology apis using machine learningbased content analysis automatically transform unstructured text rich structured data analyzing organizing content realtime automatically tagging structuring large pieces text clusters topics orbit creates platform build multiple data rich applications 5monthold leaked innovation report new york times pointed several challenges keeping expanding digital audience face critical issues need create better experience reader 1 serving better recommendations related content2 providing new ways discover news add context3 introducing personalization filtering relevance essential creating loyal readers even time visits news sites go directly specific article mainly due search social media avoiding front page altogether readers arriving side doors like twitter facebook less engaged readers arriving directly means important keep visitors site convert loyal readers yet little done improve relevance recommendations create connection huge amounts valuable content already exists orbit understands topics piece contains also related topics thereby understands context article bring related content reader wouldnt otherwise seen extending readers time spent site increasing page views understanding context means cluster topics related article china signing historic gas deal russia includes topics russia ukraine putin gazprom energy thus creating recommendations within cluster creating connections content rich structured data opens new ways navigate discover news classic navigation carefully edited front pages pretty much since dawn online news publishing structured data enables reader follow certain topics stories improves search enables timeline navigation news story help reader better understand context story developed time journalist writing story uproar ukraine possible way knowing story unfold weeks come manual tagging news stories leads inconsistent incomplete structures due subjective understanding topics important related machine learningbased content analysis identify people organizations places relate realtime thereby identifying related stories unfold cluster together nyt innovation report brought true value structured data emerges content structured equally throughout news content apps like circa omni prismatic news sites like vox incorporated elements experimenting develop original ways discover news many arguments personalization often related dystopian fear fragmented public sphere horrors echo chamber doesnt mean personalization cant good thing merely means aware particular type user wants particular time talking fully customizable news feed based subjective interests meaning see articles related manchester united finance tvshows kim kardashian uninformed topics merely suggesting smart filtering system adjustments subjects would like see less feed different interests example may entirely disinterested tour de france three week media frenzy july year unfollow topic turn volume today getting news isnt hard part filtering excessive info navigating overwhelming stream news smart way need great tools foundation rich structured data benefit reader make life easier journalists editors well provide context story syria could add several components extra information would enrich article box background information conflict facts bashar alassad different syrian rebel groups forth rich structured data place automatically add relevant fact boxes interactive elements piece content based third party content databases wikipedia topics automatically generate page related articles facts visualizations insights relevant specific topic cluster moreover use data create new compelling presentations content including visualizations timelines give reader better experience new insights news content generally short lifespan doesnt mean old content cant valuable new context consistent structuring archived content give new life old content making easier reuse resurrect articles still relevant create connections old new articles trending topics people organizations week regions got media attention many sources anonymous many women versus men knowing audiences preferences make easier create good content right time better organized content creates strong foundation good insights content consumed better ecosystem content including higher relevance contextual awareness present better contextbased ads advertisers give better insights watching acting using right technology smart ways journalists editors focus best creating quality news content orbitai quick cheer standing ovation clap show much enjoyed story product manager bakken bck bakken bck blog,en,"['Orbit', 'API', 'the New York Times', 'NYT Innovation', 'Circa', 'Omni', 'Prismatic', 'Manchester United', 'Finance', 'Tour de France', 'Bakken & Bæck', 'The Bakken & Bæck']"
85,Joe Johnston,38,How I tracked my house movements using iBeacons. – Universal Mind – Medium,"Recently I’ve started experimenting more and more with iBeacons. Being part of the R&D Group at Universal Mind I’ve had the opportunity to do a lot of testing and exploring of different products. In doing so, I wanted to see how someone could utilize iBeacons without building your own app, just yet (I’ll tackle this in a future post).
The first step was to find iBeacons we could use for our testing. Our first choice was ordering the iBeacons from Estimote and after waiting for them to arrive (they never did), we ordered other beacons from various companies. The first set to arrive was from Roximity, which came to us as a set of 3 dev iBeacons.
Next, I wanted to see if I could track movements in my own house just as a simple test, without creating a custom app. I looked for a few apps that could detect the iBeacons and execute an action. There are a few apps capable of doing this, but all of them were somewhat limiting. The only app I found that allowed me to control what happens when triggering an iBeacon was an app called Launch Here (Formally Placed).
Although this app wasn’t a perfect fit, it did allow me to call some actions after triggering an iBeacon. Launch Here allows you to use custom URL Schemes. These URL Schemes allow you to open apps and even populate an action.
One of the more complicated tasks of setting up any iBeacon manually is you need to gather some infomation on the iBeacon itself. The 3 key pieces of info each iBeacon contains is a UUID, Major ID, and Minor ID. To get this info you can install an app, like Locate for iBeacon that detects iBeacons and shows this information.
Once you have this info you can set up your iBeacons using Launch Here. Its a bit cumbersome to set each one up but you only have to do it once.
(As a side note the Launch Here app is a bit touchy when setting up the iBeacons so be warned. You may have to re-enter the URL Scheme info if you fat finger it.)
Like I said before, the Launch Here app is controlled by the user, it triggers a lock screen notification when you turn on your phone and are less then 3 meters away from any iBeacon.
This is a bit interesting, but it’s the approch that Launch Here took so they could give the user a bit of control when triggering actions. Ideally this all would happen behind the scenes to the user in a custom app.
The custom URL Schemes are pretty powerful but you still need to manually trigger them. Here’s my set up.
I have the Tumblr app installed on my phone which has the ablity to use a post URL Scheme. The URL Scheme looks like this:
tumblr://x-callback-url/text?title=kitchen
Once that URL Scheme is triggered from Launch Here it opens Tumblr and pre-populates a text post with the word “kitchen”, or with the name of the room I set. I manually tap post and its added. This allows me to capture each iBeacon location and store the data.
The next step was to create a more data friendly format. I love using a service called IFTTT. It’s a very power platform that allows you to automatically trigger other services. I created a IFTTT recipe that auto adds a row to a Google Spreadsheet with the Time Stamp and Text that is entered into a text post to my Tumblr Account.
Now I have a time stamped dataset tracking my movement in my house — at least the three rooms I have set up. With that data you can imagine how you can start to break it apart. Here’s just an example of my current break down based on room.
As you can see it’s possible to track your movement, albeit a bit cumbersome. Taking this data and bubbling it up to the user could be very compelling in certain situations. I’m just using my personal home location here but you can see how this could be very powerful in other settings.
I am the Director of User Experience / Research & Development at Universal Mind — A Digital Solutions Agency. You can follow me on twitter at @merhl.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Experience & Service Design Director @SparksGrove the experience design division of @NorthHighland (Alum of @Hugeinc @UniversalMind @Startgarden)
A collection of articles created by Universal Mind thinkers.
",recently ive started experimenting ibeacons part rd group universal mind ive opportunity lot testing exploring different products wanted see someone could utilize ibeacons without building app yet ill tackle future post first step find ibeacons could use testing first choice ordering ibeacons estimote waiting arrive never ordered beacons various companies first set arrive roximity came us set 3 dev ibeacons next wanted see could track movements house simple test without creating custom app looked apps could detect ibeacons execute action apps capable somewhat limiting app found allowed control happens triggering ibeacon app called launch formally placed although app wasnt perfect fit allow call actions triggering ibeacon launch allows use custom url schemes url schemes allow open apps even populate action one complicated tasks setting ibeacon manually need gather infomation ibeacon 3 key pieces info ibeacon contains uuid major id minor id get info install app like locate ibeacon detects ibeacons shows information info set ibeacons using launch bit cumbersome set one side note launch app bit touchy setting ibeacons warned may reenter url scheme info fat finger like said launch app controlled user triggers lock screen notification turn phone less 3 meters away ibeacon bit interesting approch launch took could give user bit control triggering actions ideally would happen behind scenes user custom app custom url schemes pretty powerful still need manually trigger heres set tumblr app installed phone ablity use post url scheme url scheme looks like tumblrxcallbackurltexttitlekitchen url scheme triggered launch opens tumblr prepopulates text post word kitchen name room set manually tap post added allows capture ibeacon location store data next step create data friendly format love using service called ifttt power platform allows automatically trigger services created ifttt recipe auto adds row google spreadsheet time stamp text entered text post tumblr account time stamped dataset tracking movement house least three rooms set data imagine start break apart heres example current break based room see possible track movement albeit bit cumbersome taking data bubbling user could compelling certain situations im using personal home location see could powerful settings director user experience research development universal mind digital solutions agency follow twitter merhl quick cheer standing ovation clap show much enjoyed story experience service design director sparksgrove experience design division northhighland alum hugeinc universalmind startgarden collection articles created universal mind thinkers,en,"['the R&D Group', 'Universal Mind', 'Roximity', 'iBeacon', 'Minor ID', 'Locate', 'Tumblr', 'IFTTT', 'A Digital Solutions Agency', '@NorthHighland']"
86,Nadav Gur,10,Why Natural Search is Awesome and How We Got Here – The Vanguard – Medium,"The Evolution Of Desti’s Search Interface
This is a story about how one ambitious start-up tackled this subject that has riddled people like Google, Apple, Facebook and others, and came up with some pretty clever conclusions (if I may say so myself). In 2012–2013 we were building Desti — a holistic travel search app (i.e. it would search for everything — from hotels through attractions to restaurants), using post-Siri natural-language-understanding tech, and with powerful semantic search capabilities on the back end that allowed Desti to reason meaningfully about search results and make highly informed suggestions.
Desti’s search was built on a premise that sounds very simple, but it’s actually very hard to pull off. We believe that people should be able to ask specifically for what they’re interested in and get results that match. This sounds reasonable, right? If I’m looking for a beach resort on the Kona Coast in Hawaii, it’s pretty obvious what I want. And if I also want it to be kid friendly and pet friendly, I should just be able to ask for it. Our goal was to get users inputting relevant, specific queries because that’s what people need. That’s where Desti shines — saving you time and effort by delivering exactly what you want.
Now let’s assume that Desti knows which hotels on the Kona Coast are actually beach resorts, are kid friendly and pet friendly. How can we make expressing this query easy and intuitive for the user?
Episode I: Desti is Siri’s Sister or Conversational User Interface:When we started, we were very naïve about this. We said — first, let’s just put a search box in there, allowing the user to type or say whatever they want, and let’s make sure we understand this. Then, let’s leave that box there so they can react to what they see and provide more detail (“refine”) or search for something else in that context (e.g. a restaurant near the resort — we called this “pivot”). And let’s run a conversation around it, kind of like Siri. What could be more natural? To do this we used SRI International’s VPA platform, which is almost literally a post-Siri natural-language-interaction platform with which you can have a conversation in context.
This is more or less what it looked like in our beta version:
Search box:
A conversational UI:
We launched this, monitored use and quickly realized is that early users split into two groups:
Discarding the 2nd group (we’re busy people), we learned that people don’t know how to interact naturally with computers, or they have no idea what to ask or expect, so they revert to the most primitive queries. Problem is, our goal was to answer interesting, specific queries, because we believe that if we give you a great answer that caters to what you want, your likelihood of buying is that much higher.
Furthermore, absolutely no one got the conversational aspect — the fact you can continue refining and pivoting through conversation. We decided to take away the focus from conversation for the time being.
Episode 2: Vegas Slot Machines or Make It Dead Simple
We realized we have to focus on the first query, and give people some cues about what’s possible. And came up with this interface:
These contextual spinners turned interaction from a totally open-ended query to something closer to multiple-choice questions. In essence these were interchangeable templates, where you could get ideas for “what to search for” as well as easily input your query. What you picked would show up as a textual query in the search bar, which we hoped people would realize they can edit or add to. Hoped...
The results — on the one hand, progress. We saw longer and more interesting queries and more interaction. However when talking to users, we realized that they were assuming that the spinner was a kind of menu system, which means (a) they can only pick what’s in the menu (b) they have to pick one thing from each menu. So while this was better than what most sites have for search, it was still a far cry from what we wanted to deliver. Here’s what we learned from this:
Episode 3: Fill In The Blanks — Smartly
At this stage, it was clear that we needed better auto-suggest and smarter auto-complete. This is similar from a UI perspective to Google Instant, but Desti is about semantic search, not keyword matching. In most cases, Google will auto-suggest a phrase that matches what you’ve been typing AND has been typed in by many other people. Desti should suggest something that semantically matches what you entered and makes sense given what we know of the destination and about your trip. Because Desti is new and there haven’t been a million users searching for the same things before you, Desti should reason about what you may ask, not suggest something someone else asked.
We realized we have to build a lot of semantically-reasonable and statistically-relevant auto-suggesting. We still wanted to keep to the template logic because we believed it helps users think about what they are looking for and form the query in their minds. So we came up with a UI that blends form-filling and natural language entry, and focused on building smart auto-suggest and auto-complete.
This UI was built of a number of rigid fields (e.g. location, type) that adapt to the subject matter (so if the type is “hotel” you’re prompted for dates), and a free text field that allows you to ask for whatever else you want.
We iterated a lot over the auto-complete and auto-suggest features. The first thing is to realize they are different. With auto-complete, you have a user who already thought of something to type in, and you have to guess what that is. With auto-suggest, you really want to inspire the user into adding something useful to their query, which means it needs to be relevant to whatever you know about the query and user so far, but not overwhelming for the user.
All this requires knowing a lot about specific destinations (what do people search for in Hawaii vs. New York?) and specific types (what’s relevant for hotels vs. museums?). Also, on the visual side, what the user is putting in is often quantitative and easier to “set” than “type” — e.g. a date, a price etc. So we came up with our first crack at blending text with visual widgets.
The results were a big improvement in the quality and relevance of queries over the previous UI, but a feeling that this was still too stiff and rigid. When people are asked for a “type of place” — e.g. a museum, a park, a hotel — they often can’t really answer, and it’s easier for them to think about a feature of the place instead — e.g. that they can go hiking, or biking, see art or eat breakfast. For linguistic reasons it’s easier for people to say that they want a “romantic hotel” than a “hotel that’s romantic”. So while this UI was very expressive, often it felt unnatural and limiting. Furthermore many users just ended up filling the basic fields and not adding any depth in the open-text field (despite various visual cues). And editing a query for refining or pivoting was hard.
At the same time — the auto-suggest / auto-complete elements we’ve built at this stage werealmost enough to allow us to just throw out the limiting “templates” and move to one search field — but this time, a damn clever one.
Episode 4: Search Goes Natural
To the naked eye, this looks like we’ve gone full circle — one text box, parsed queries shown as tags. What could be simpler?
Well, not exactly, because we still need queries to be meaningful. One thing that the templates gave us was built-in disambiguation. We need a query that has at least a location + a type (or something from which we can derive a type), and without a template telling us that the “hotel” is the type, and the “restaurant” is something you want your hotel to have (vs. maybe the opposite), the system needs to better understand the grammatical structure or the sentence, and cue you into inputting things the right way when it’s suggesting and auto-completing.
Typing a query:
The query is understood — you can add / edit:
With this new user interface, changing queries (“refining and pivoting”) is very natural — add tags, or take away tags. Widgets were contextually integrated using the auto-suggest drop-down menu, so they are naturally suggested at the right time (e.g. after you said you were looking for a hotel, we help you choose when, how many rooms etc.). It’s also very easy to suggest things to search for based on the context. For instance if we know your kids are traveling with you, we’d drop in “family friendly” and you could dismiss it with one click.
So Where is This Going
So far, Natural Search looks and behaves better than anything else we’ve seen in this space. From now on, most of the focus is on making the guesses even smarter, with more statistic reasoning about what people ask for in different contexts, and more contextual info driving those guesses.
We believe this UI is where vertical search is heading. Consider how nice it would be to input “gifts for 4 year old boys under $30” into target.com’s search bar, or “romantic restaurant with great seafood near Times Square with a table at 8 PM tonight” into OpenTable — and get relevant answers. But then again, answering specific queries is not that easy either, but that’s the other side of Desti...
To be continued.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I think. Then I talk. Sometime it’s the other way around. Founded & ran companies in AI, mobile, travel, etc., ex-EiR at SRI Int’l, ex-aerospace
Nadav Gur’s Tech Musings
",evolution destis search interface story one ambitious startup tackled subject riddled people like google apple facebook others came pretty clever conclusions may say 20122013 building desti holistic travel search app ie would search everything hotels attractions restaurants using postsiri naturallanguageunderstanding tech powerful semantic search capabilities back end allowed desti reason meaningfully search results make highly informed suggestions destis search built premise sounds simple actually hard pull believe people able ask specifically theyre interested get results match sounds reasonable right im looking beach resort kona coast hawaii pretty obvious want also want kid friendly pet friendly able ask goal get users inputting relevant specific queries thats people need thats desti shines saving time effort delivering exactly want lets assume desti knows hotels kona coast actually beach resorts kid friendly pet friendly make expressing query easy intuitive user episode desti siris sister conversational user interfacewhen started naive said first lets put search box allowing user type say whatever want lets make sure understand lets leave box react see provide detail refine search something else context eg restaurant near resort called pivot lets run conversation around kind like siri could natural used sri internationals vpa platform almost literally postsiri naturallanguageinteraction platform conversation context less looked like beta version search box conversational ui launched monitored use quickly realized early users split two groups discarding 2nd group busy people learned people dont know interact naturally computers idea ask expect revert primitive queries problem goal answer interesting specific queries believe give great answer caters want likelihood buying much higher furthermore absolutely one got conversational aspect fact continue refining pivoting conversation decided take away focus conversation time episode 2 vegas slot machines make dead simple realized focus first query give people cues whats possible came interface contextual spinners turned interaction totally openended query something closer multiplechoice questions essence interchangeable templates could get ideas search well easily input query picked would show textual query search bar hoped people would realize edit add hoped results one hand progress saw longer interesting queries interaction however talking users realized assuming spinner kind menu system means pick whats menu b pick one thing menu better sites search still far cry wanted deliver heres learned episode 3 fill blanks smartly stage clear needed better autosuggest smarter autocomplete similar ui perspective google instant desti semantic search keyword matching cases google autosuggest phrase matches youve typing typed many people desti suggest something semantically matches entered makes sense given know destination trip desti new havent million users searching things desti reason may ask suggest something someone else asked realized build lot semanticallyreasonable statisticallyrelevant autosuggesting still wanted keep template logic believed helps users think looking form query minds came ui blends formfilling natural language entry focused building smart autosuggest autocomplete ui built number rigid fields eg location type adapt subject matter type hotel youre prompted dates free text field allows ask whatever else want iterated lot autocomplete autosuggest features first thing realize different autocomplete user already thought something type guess autosuggest really want inspire user adding something useful query means needs relevant whatever know query user far overwhelming user requires knowing lot specific destinations people search hawaii vs new york specific types whats relevant hotels vs museums also visual side user putting often quantitative easier set type eg date price etc came first crack blending text visual widgets results big improvement quality relevance queries previous ui feeling still stiff rigid people asked type place eg museum park hotel often cant really answer easier think feature place instead eg go hiking biking see art eat breakfast linguistic reasons easier people say want romantic hotel hotel thats romantic ui expressive often felt unnatural limiting furthermore many users ended filling basic fields adding depth opentext field despite various visual cues editing query refining pivoting hard time autosuggest autocomplete elements weve built stage werealmost enough allow us throw limiting templates move one search field time damn clever one episode 4 search goes natural naked eye looks like weve gone full circle one text box parsed queries shown tags could simpler well exactly still need queries meaningful one thing templates gave us builtin disambiguation need query least location type something derive type without template telling us hotel type restaurant something want hotel vs maybe opposite system needs better understand grammatical structure sentence cue inputting things right way suggesting autocompleting typing query query understood add edit new user interface changing queries refining pivoting natural add tags take away tags widgets contextually integrated using autosuggest dropdown menu naturally suggested right time eg said looking hotel help choose many rooms etc also easy suggest things search based context instance know kids traveling wed drop family friendly could dismiss one click going far natural search looks behaves better anything else weve seen space focus making guesses even smarter statistic reasoning people ask different contexts contextual info driving guesses believe ui vertical search heading consider nice would input gifts 4 year old boys 30 targetcoms search bar romantic restaurant great seafood near times square table 8 pm tonight opentable get relevant answers answering specific queries easy either thats side desti continued quick cheer standing ovation clap show much enjoyed story think talk sometime way around founded ran companies ai mobile travel etc exeir sri intl exaerospace nadav gurs tech musings,en,"['Google', 'Apple', 'Facebook', 'Desti', 'Vegas Slot Machines', 'UI', 'Google Instant', 'Search Goes Natural', 'Natural Search', 'target.com', 'OpenTable']"
87,Pandorabots,14,Using OOB Tags in AIML: Part I – pandorabots-blog – Medium,"Suppose you are building an Intelligent Virtual Agent or Virtual Personal Assistant (VPA) that uses a Pandorabot as the natural language processing engine. You might want this VPA to be able to perform tasks such as sending a text message, adding an event to a calendar, or even just initiating a phone call. OOB tags allow you to do just that!
OOB stands for “out of band,” which is an engineering term used to refer to activity performed on a separate, hidden channel. For a Pandorabot VPA, this translates to activities which fall outside of the scope of an ordinary conversation, such as placing a phone call, checking dynamic information like the weather, or searching wikipedia for the answer to some question. The task is executed, but does not necessarily always produce an effect on the conversation between the Pandorabot and the user.
OOB tags are used in AIML templates and are written in the following format: <oob>command</oob>. The command that is to be executed is specified by a set of tags which occur within the <oob> tags. These inner OOB tags can be whatever you like, and the phone-related actions they initiate are defined in your applications code.
To place a call you might see something like this: <oob><dial>some phone number</dial></oob>. The <dial> tag within the <oob> tag sends a message to the phone to dial the number specified. When your client indicates they want to dial a number, your application will receive a template containing the command specified inside the OOB tag. Within your application, this inner command will be interpreted and the appropriate actions will be executed.
It is useful to think of the activities initiated by oob tags as falling into one of two categories, based on whether they return information to the user via the chat interface or not.
The first category, those that do not return information, typically involve activities that interrupt the conversation. If you ask your VPA to look up restuarants on a map, it will open up your map application and perform a search. Similarly, if you ask your bot to make a phone call, it will open the dialer application and make a call. In both of these examples, the activity performed interrupts the conversation and displays some other screen.
The second category, those that do return information to the user via the chat interface, are generally actions that are executed in the background of the conversation. If you ask your Pandorabot to look up the “Population of the United States” on Wikipedia, it will perform the search, and then return the results of the search to the user via the chat window. Similarly, if you ask your Pandorabot to send a text message to the friend, it will send the text, and then return a message to the user via the chat window indicating the success of the action, i.e. “Your text message was delivered!”
In this second set of examples, it is useful to distinguish between those activities whose results will be returned directly to the user, like the Wikipedia example, and those activities whose successful completion will simply be indicated to the user through the chat interface, as with the texting example.
Here is an example of a category that uses the phone dialer on android.
Here is an example interaction this category would lead to:
Human: Dial 1234567.
Robot: Calling 1234567.
Here is a slightly more complicated example involving the oob tag, which launches a browser and performs a google search:
Human: Look up Pandorabots.
Robot: Searching...Searching... Please stand by.
Note: not shown in the previous example is the category RANDOM SEARCH PHRASE, which delivers a random selection from a short list of possible replies, each indicating to the user that the bot correctly interpreted their search request.
For a complete list of oob tags as implemented in the CallMom Virtual Personal Assistant App for Android, as well as usage examples, click here.
Be sure to look out for the upcoming post “Using OOB Tags in AIML: Part II”, which will go over a basic example of how to intrepret the OOB tags received from the Pandorabots server within the framework of your own VPA application.
Originally published at blog.pandorabots.com on October 9, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The largest, most established chatbot development and hosting platform. www.pandorabots.com
The leading platform for building and deploying chatbots.
",suppose building intelligent virtual agent virtual personal assistant vpa uses pandorabot natural language processing engine might want vpa able perform tasks sending text message adding event calendar even initiating phone call oob tags allow oob stands band engineering term used refer activity performed separate hidden channel pandorabot vpa translates activities fall outside scope ordinary conversation placing phone call checking dynamic information like weather searching wikipedia answer question task executed necessarily always produce effect conversation pandorabot user oob tags used aiml templates written following format oobcommandoob command executed specified set tags occur within oob tags inner oob tags whatever like phonerelated actions initiate defined applications code place call might see something like oobdialsome phone numberdialoob dial tag within oob tag sends message phone dial number specified client indicates want dial number application receive template containing command specified inside oob tag within application inner command interpreted appropriate actions executed useful think activities initiated oob tags falling one two categories based whether return information user via chat interface first category return information typically involve activities interrupt conversation ask vpa look restuarants map open map application perform search similarly ask bot make phone call open dialer application make call examples activity performed interrupts conversation displays screen second category return information user via chat interface generally actions executed background conversation ask pandorabot look population united states wikipedia perform search return results search user via chat window similarly ask pandorabot send text message friend send text return message user via chat window indicating success action ie text message delivered second set examples useful distinguish activities whose results returned directly user like wikipedia example activities whose successful completion simply indicated user chat interface texting example example category uses phone dialer android example interaction category would lead human dial 1234567 robot calling 1234567 slightly complicated example involving oob tag launches browser performs google search human look pandorabots robot searchingsearching please stand note shown previous example category random search phrase delivers random selection short list possible replies indicating user bot correctly interpreted search request complete list oob tags implemented callmom virtual personal assistant app android well usage examples click sure look upcoming post using oob tags aiml part ii go basic example intrepret oob tags received pandorabots server within framework vpa application originally published blogpandorabotscom october 9 2014 quick cheer standing ovation clap show much enjoyed story largest established chatbot development hosting platform wwwpandorabotscom leading platform building deploying chatbots,en,"['VPA', 'Pandorabots', 'PHRASE', 'Android', 'blog.pandorabots.com']"
88,Denny Vrandečić,4,"AI is coming, and it will be boring – Denny Vrandečić – Medium","I was asked about my opinion on this topic, and I thought I would have some profound thoughts on this. But I ended up rambling, and this post doesn’t really make any single strong point. tl;dr: Don’t worry about AIs killing all humans. It’s not likely to happen.
In an interview with the BBC, Stephen Hawking stated that “the development of full artificial intelligence could spell the end of the human race”. Whereas this is hard to deny, it is rather trivial: any sufficiently powerful tool could potentially spell the end of the human race given a person who knows how to use that tool in order to achieve such a goal. There are far more dangerous developments — for example, global climate change, the arsenal of nuclear weapons, or an economic system that continues to sharpen inequality and social tension?
AI will be a very powerful tool. Like every powerful tool, it will be highly disruptive. Jobs and whole industries will be destroyed, and a few others will be created. Just as electricity, the car, penicillin, or the internet, AI will profoundly change your everyday life, the global economy, and everything in between. If you want to discuss consequences of AI, here are a few that are more realistic than human extermination: what will happen if AI makes many jobs obsolete? How do we ensure that AIs make choices compliant with our ethical understanding? How to define the idea of privacy in a world where your car is observing you? What does it mean to be human if your toaster is more intelligent than you?
The development of AI will be gradual, and so will the changes in our lifes. And as AI keeps developing, things once considered magical will become boring. A watch you could talk to was powered by magic in Disney’s 1991 classic “The Beauty and the Beast”, and 23 years later you can buy one for less than a hundred dollars. A self-driving car was the protagonist of the 80s TV show “Knight Rider”, and thirty years later they are driving on the streets of California. A system that checks if a bird is in a picture was considered a five-year research task in September 2014, and less than two months later Google announces a system that can provide captions for pictures — including birds. And these things will become boring in a few years, if not months. We will have to remind ourselves how awesome it is to have a computer in our pocket that is more powerful than the one that got Apollo to the moon and back. That we can make a video of our children playing and send it instantaneously to our parents on another continent. That we can search for any text in almost any book ever written. Technology is like that. What’s exciting today, will become boring tomorrow. So will AI.
In the next few years, you will have access to systems that will gradually become capable to answer more and more of your questions. That will offer advice and guidance towards helping you navigate your life towards the goal you tell it. That will be able to sift through text and data and start to draw novel conclusions. They will become increasingly intelligent. And there are two major scenarios that people are afraid of at this point:
The Skynet scenario is just mythos. There is no indication that raw intelligence is sufficient to create intrinsic intention or will.
The paperclip scenario is more realistic. And once we get closer to systems with such power, we will need to put the right safeguards in place. The good news is that we will have plenty of AIs at our disposal to help us with that. The bad news is that discussing such scenarios now is premature: we simply don’t know how these systems will look like. That’s like starting a committee a hundred years ago to discuss the danger coming from novel weaponry: no one in 1914 could have predicted nuclear weapons and their risks. It is unlikely that the results of such a committee would have provided much relevant ethical guidance for the Manhattan project three decades later. Why should that be any different today?
In summary: there are plenty of consequences of the development of AI that warrant intensive discussion (economical consequences, ethical decisions made by AIs, etc.), but it is unlikely that they will bring the end of humanity.
Background image: robots trashing living room by vincekamp, licensed under CC BY ND 3.0.
Personal permanent URL: http://simia.net/wiki/AI_is_coming,_and_it_will_be_boring
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Wikidata founder, Google ontologist, Semantic Web researcher, and  author.
",asked opinion topic thought would profound thoughts ended rambling post doesnt really make single strong point tldr dont worry ais killing humans likely happen interview bbc stephen hawking stated development full artificial intelligence could spell end human race whereas hard deny rather trivial sufficiently powerful tool could potentially spell end human race given person knows use tool order achieve goal far dangerous developments example global climate change arsenal nuclear weapons economic system continues sharpen inequality social tension ai powerful tool like every powerful tool highly disruptive jobs whole industries destroyed others created electricity car penicillin internet ai profoundly change everyday life global economy everything want discuss consequences ai realistic human extermination happen ai makes many jobs obsolete ensure ais make choices compliant ethical understanding define idea privacy world car observing mean human toaster intelligent development ai gradual changes lifes ai keeps developing things considered magical become boring watch could talk powered magic disneys 1991 classic beauty beast 23 years later buy one less hundred dollars selfdriving car protagonist 80s tv show knight rider thirty years later driving streets california system checks bird picture considered fiveyear research task september 2014 less two months later google announces system provide captions pictures including birds things become boring years months remind awesome computer pocket powerful one got apollo moon back make video children playing send instantaneously parents another continent search text almost book ever written technology like whats exciting today become boring tomorrow ai next years access systems gradually become capable answer questions offer advice guidance towards helping navigate life towards goal tell able sift text data start draw novel conclusions become increasingly intelligent two major scenarios people afraid point skynet scenario mythos indication raw intelligence sufficient create intrinsic intention paperclip scenario realistic get closer systems power need put right safeguards place good news plenty ais disposal help us bad news discussing scenarios premature simply dont know systems look like thats like starting committee hundred years ago discuss danger coming novel weaponry one 1914 could predicted nuclear weapons risks unlikely results committee would provided much relevant ethical guidance manhattan project three decades later different today summary plenty consequences development ai warrant intensive discussion economical consequences ethical decisions made ais etc unlikely bring end humanity background image robots trashing living room vincekamp licensed cc nd 30 personal permanent url httpsimianetwikiai_is_coming_and_it_will_be_boring quick cheer standing ovation clap show much enjoyed story wikidata founder google ontologist semantic web researcher author,en,"['BBC', 'Disney', 'Google', 'Apollo', 'CC BY ND', 'Semantic']"
89,Thaddeus Howze,15,Of Comets and Gods in the Making – Thaddeus Howze – Medium,"Asferit had not grown up; she didn’t know where she came from; could not conceive of childhood. No memories of parents, no recollection of family. On the vast empty world that served as her lab, she built the probes and put a little bit of herself in each one.
Her machine-form, ancient, slow and sputtering came to life, wheezing through the long corridors of the silent lab, its darkness masking the distant empty spaces which Asferi imagined were once filled with life.
She looked through her thoughts and realized she had lost any hope of memory, that part of her was already circling a distant star aborning with life. She looked in on those places when she woke to see the results of her work; on so many worlds life spawned.
With the next launch, she would lose the memory of those places, there was so little of her remaining. Enough for three, no four probes. Then she would cease to remember why she was, what she was. She would forget how to exist. But not yet.
She completed the next probe, winding the engine and orienting it along the galactic plane; her sensor array aligning the probe with a wandering comet; she planned to deposit herself within the life-giving molecules within its frozen mass.
She knew little about her past, but knew that she must not be able to be found. This was the only memory that remained; hide from the Darkness.
As she loaded the last probe, she considered the first probe she ever sent, millennia ago; there were monuments within the halls of the lab in her hubris then, she considered them a successful reincarnation of her people.
Each representation was filled with the temporal signature of that once great race; a temporal residue of failure. It spoke of a great race, masters of time and space; they flourished in the dark between the stars. Then the Darkness came. She was overconfident. She slept assured of their success her mission completed.
In the time between sleeping and waking, her cycle of regeneration before attempting to seed again, the great race was gone.
Found. They did not heed the warnings she sent in those early days. She gave far more of herself then. She came to them in visions, taught them secrets to harness the hidden nature of matter; revealed to them the nature of energies, both planetary and interstellar. They would worship her, revere her and believe her to be a god.
In the end, it was not enough. They were consumed, their greatness undone. She sent less and less of herself from then on. Godhood failed them. Perhaps obscurity would serve them better.
She sent less each time, only tiny packages of micromachines capable of changing matter, capable of modifying genomes, empowering the creatures spawned of her with abilities even greater than the First Race. Psychometric representations of them were all that remained, echoes in the timestream of history. In their hubris, they ruptured time and space and like the world her lab hung above, cracked the crust of their world and were lost in a temporal vortex of their own making. They had such potential. Squandered.
Then she began sending only the memory of what she was, embedded within complex epigentic echoes. No longer would she shape the universe for them, they would have to work for their survival, perhaps they would be stronger for it. She appeared to her descendents only in dreams; visions of what they were, memories of who she was, memories she no longer possessed.
Her memory was great once and she seeded thousands of worlds with it. But like the ephemeral nature of memory, so few knew what they saw. Many went mad. Most dreamed of demiurges, mad deva whose powers ravaged worlds. These memories destroyed half of them before they could achieve spaceflight and reach for the stars themselves. Religions they spawned consumed them.
Now, she sent only cells and precellular matter. The very least of herself, the essence of who she was, the final matter of her being; hidden in comets, cloaked in meteor swarms, hidden on the boots of other starfarers.
Time had taught her patience, though she had lost her memories, she was confident of this final strategy. To hide herself on millions of worlds, her final probe-ships would leave a legacy on millions of worlds. She found the last star she would use and loaded the final probe-ship with the hardiest constructions she had ever made. She deconstructed the worldship; her lab, her home for millenia of millenia, breaking down every part of it, reforging it for a final effort.
The planet below was also consumed, her last effort would require everything. It was a long dead world lost to antiquity when the universe was young. Of the Darkness, she could not remember, but she knew this: as long as there was light, her people would survive.
The final instructions to her probeship would have her descending into her planet’s unstable star. It’s final fluctuations revealed what she knew was the inevitable outcome; and she planned to use it to her advantage. Her final self would not be aware of the result. The final cells of her body were distributed within millions of pieces of her world and her lab. Each calibrated to arrive at a star somewhere in her galaxy. Each single cell would find a world ready for life.
She could no longer coerce planets into life. She could no longer force matter or energy to take the shape she deemed. She was now only able to influence the tiniest aspects. Asferit would only be able to nudge a planet toward Life. The Darkness would always be ready to claim her people but now they would be scattered; to worlds within the galaxy and without.
She seeded the galactic wind and waited for a supernova to blow them where it would. Her starseeds hardened against the impending blastwave, they would, with the tiniest bit of her final design, travel faster than light toward their final destinations.
As the star which lit her world, gave her people life, watched them die and patiently waited until they could be reborn, exploded, Asferit now waited in turn.
In those last seconds as the waves of radiation and coronal debris swept over the remnants of her cannibalized world, she subsumed herself within the starseeds and the near-immortal being Asferit, last of her kind, was no more.
And yet now she was pure purpose, no ambitions, no plan, no dreams of godhood, no longer a radiant harbingers of dooms lighting the skies of primitive worlds.
She would be the essence of Life itself; the Darkness be damned.
Of Comets and Gods in the Making © Thaddeus Howze 2013, All Rights Reserved
Thaddeus Howze is a popular and recently awarded Top Writer, 2016 recipient on the Q&A site Quora.com. He is also a moderator and contributor to theScience Fiction and Fantasy Stack Exchange with over fourteen hundred articles in a four year period.
Thaddeus Howze is a California-based technologist and author who has worked with computer technology since the 1980’s doing graphic design, computer science, programming, network administration, teaching computer science and IT leadership.
His non-fiction work has appeared in numerous magazines: Huffington Post, Gizmodo, Black Enterprise, the Good Men Project, Examiner.com, The Enemy, Panel & Frame, Science X, Loud Journal, ComicsBeat.com, and Astronaut.com. He maintains a diverse collection of non-fiction at his blog, A Matter of Scale.
His speculative fiction has appeared online at Medium, Scifiideas.com, and theAu Courant Press Journal. He has appeared in twelve different anthologies in the United States, the United Kingdom and Australia. A list of his published work appears on his website, Hub City Blues.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Author | Editor | Futurist | Activist | Tech Humanist | http://bit.ly/thowzebio |http://bit.ly/thpatreon
",asferit grown didnt know came could conceive childhood memories parents recollection family vast empty world served lab built probes put little bit one machineform ancient slow sputtering came life wheezing long corridors silent lab darkness masking distant empty spaces asferi imagined filled life looked thoughts realized lost hope memory part already circling distant star aborning life looked places woke see results work many worlds life spawned next launch would lose memory places little remaining enough three four probes would cease remember would forget exist yet completed next probe winding engine orienting along galactic plane sensor array aligning probe wandering comet planned deposit within lifegiving molecules within frozen mass knew little past knew must able found memory remained hide darkness loaded last probe considered first probe ever sent millennia ago monuments within halls lab hubris considered successful reincarnation people representation filled temporal signature great race temporal residue failure spoke great race masters time space flourished dark stars darkness came overconfident slept assured success mission completed time sleeping waking cycle regeneration attempting seed great race gone found heed warnings sent early days gave far came visions taught secrets harness hidden nature matter revealed nature energies planetary interstellar would worship revere believe god end enough consumed greatness undone sent less less godhood failed perhaps obscurity would serve better sent less time tiny packages micromachines capable changing matter capable modifying genomes empowering creatures spawned abilities even greater first race psychometric representations remained echoes timestream history hubris ruptured time space like world lab hung cracked crust world lost temporal vortex making potential squandered began sending memory embedded within complex epigentic echoes longer would shape universe would work survival perhaps would stronger appeared descendents dreams visions memories memories longer possessed memory great seeded thousands worlds like ephemeral nature memory knew saw many went mad dreamed demiurges mad deva whose powers ravaged worlds memories destroyed half could achieve spaceflight reach stars religions spawned consumed sent cells precellular matter least essence final matter hidden comets cloaked meteor swarms hidden boots starfarers time taught patience though lost memories confident final strategy hide millions worlds final probeships would leave legacy millions worlds found last star would use loaded final probeship hardiest constructions ever made deconstructed worldship lab home millenia millenia breaking every part reforging final effort planet also consumed last effort would require everything long dead world lost antiquity universe young darkness could remember knew long light people would survive final instructions probeship would descending planets unstable star final fluctuations revealed knew inevitable outcome planned use advantage final self would aware result final cells body distributed within millions pieces world lab calibrated arrive star somewhere galaxy single cell would find world ready life could longer coerce planets life could longer force matter energy take shape deemed able influence tiniest aspects asferit would able nudge planet toward life darkness would always ready claim people would scattered worlds within galaxy without seeded galactic wind waited supernova blow would starseeds hardened impending blastwave would tiniest bit final design travel faster light toward final destinations star lit world gave people life watched die patiently waited could reborn exploded asferit waited turn last seconds waves radiation coronal debris swept remnants cannibalized world subsumed within starseeds nearimmortal asferit last kind yet pure purpose ambitions plan dreams godhood longer radiant harbingers dooms lighting skies primitive worlds would essence life darkness damned comets gods making thaddeus howze 2013 rights reserved thaddeus howze popular recently awarded top writer 2016 recipient qa site quoracom also moderator contributor thescience fiction fantasy stack exchange fourteen hundred articles four year period thaddeus howze californiabased technologist author worked computer technology since 1980s graphic design computer science programming network administration teaching computer science leadership nonfiction work appeared numerous magazines huffington post gizmodo black enterprise good men project examinercom enemy panel frame science x loud journal comicsbeatcom astronautcom maintains diverse collection nonfiction blog matter scale speculative fiction appeared online medium scifiideascom theau courant press journal appeared twelve different anthologies united states united kingdom australia list published work appears website hub city blues quick cheer standing ovation clap show much enjoyed story author editor futurist activist tech humanist httpbitlythowzebio httpbitlythpatreon,en,"['Asferi', 'the First Race', 'Time', 'Q&A', 'Quora.com', 'theScience Fiction and Fantasy Stack Exchange', 'Huffington Post', 'the Good Men Project', 'Examiner.com', 'The Enemy, Panel & Frame', 'Loud Journal', 'Astronaut.com', 'Medium, Scifiideas.com', 'theAu Courant Press Journal']"
90,Tommy Thompson,17,Why AI Research Loves Pac-Man – Tommy Thompson – Medium,"AI and Games is a crowdfunded YouTube series on the research and applications of AI within video games. The following article is a more involved transcription of the topics discussed in the video linked to above. If you enjoy this work, please consider supporting my future content over on Patreon.
Artificial Intelligence research has shown a small infatuation with the Pac-Man video game series over the past 15 years. But why specifically Pac-Man? What elements of this game have proven interesting to researchers in this time? Let’s discuss why Pac-Man is so important in the world of game-AI research.
For the sake of completes — and in appreciating there is arguably a generation or two not familiar with the game — Puck-Man was an arcade game launched in 1980 by Namco in Japan and renamed Pac-Man upon being licensed by Midway for an American release. The name change was driven less by a need for brand awareness but rather because the name can easily be de-faced to say... something else.
The original game focuses on the titular character, who must consume as many pills as possible without being caught by one of four antagonists represented by ghosts.
The four ghosts: Inky, Blinky, Pinky and Clyde, all attempt to hunt down the player using slightly different tactics from one another. Each ghost has their own behaviour; a bespoke algorithm that dictates how they attack the player.
Players also have the option to consume one of several power-pills that appear in each map. Power-pills allow for the player to not just eat pills but the enemy ghosts for a short period of time.
While mechanically simple when compared to modern video games, it provides an interesting test-bed for AI algorithms learning to play games. The game world is relatively simple in nature, but complex enough that strategies can be employed for optimal navigation. Furthermore, the varied behaviours of the ghosts reinforces the need for strategy; since their unique albeit predictable behaviours necessitate different tactics. If problem solving can be achieved at this level, then there is opportunity for it to scale up to more complex games.
While Pac-Man research began in earnest in the early 2000’s, work by John Koza (Koza, 1992) discussed how Pac-Man provides an interesting domain for genetic programming; a form of evolutionary algorithm that learns to generate basic programs. The idea behind Koza’s work and later that of (Rosca, 1996) was to highlight how Pac-Man provides an interesting problem for task-prioritisation. This is quite relevant given that we are often trying to balance the need to consume pills, all the while avoiding ghosts or — when the opportunity presents itself — eating them.
About 10 years later, people became more interested in Pac-Man as a control problem. This research was often with the intent to explore the applications of artificial neural networks for the purposes of creating a generalised action policy: software that would know at any given tick in the game what would be the correct action to take. This policy would be built from playing the game a number of times and training the system to learn what is effective and what is not. Typically these neural networks are trained using an evolutionary algorithm, that finds optimal network configurations by breeding collections of possible solutions and using a ‘survival of the fittest’ approach to cull weak candidates.
(Kalyanpur and Simon, 2001) explored how evolutionary learning algorithms could be used to improve strategies for the ghosts. In time it was evident that the use of crossover and mutation — which are key elements of most evolutionary-based approaches — was effective in improving the overall behaviour. However it’s important to note that they themselves acknowledge their work uses a problem domain similar to Pac-Man and not the actual game.
(Gallagher and Ryan, 2003) uses a slightly more accurate representation of the original game. While the screenshot is shown here, the actual implementation only used one ghost rather than the original four. In this research the team used an incremental learning algorithm that tailored a series of rules for the player that dictate how Pac-Man is controlled using a Finite State Machine (FSM). This proved highly effective in the simplified version they were playing.
The use of artificial neural networks - a data structure that mimics the firing of synapses in the brain — was increasingly popular at the time (and once again in most recent research). Two notable publications on Pac-Man are (Lucas, 2005), which attempted to create a ‘move evaluation function’ for Pac-Man based on data scraped from the screen and processed as features (e.g. distance to closest ghost), while (Gallagher and Ledwich, 2007) attempted to learn from raw, unprocessed information.
It’s notable here that the work by Lucas was in fact done on Ms. Pac-Man rather than Pac-Man. While perhaps not that important to the casual observer, this is an important distinction for AI researchers.
Research in the original Pac-Man game caught the interest of the larger computational and artificial intelligence community. You could argue it was due to the interesting problem that the game presents or that a game as notable as Pac-Man was now considered of interest within the AI research community. While it is now something that appears commonplace, games — more specifically video games — did not receive the same attention within AI research circles as they do today.
As high-quality research in AI applications in video games grew, it wasn’t long before those with a taste for Pac-Man research moved on to looking at Ms. Pac-Man given the challenges it presents — which we are still conducting research for in 2017.
Ms. Pac-Man is odd in that it was originally an unofficial sequel: Midway, who had released the original Pac-Man in the United States, had become frustrated at Namco’s continued failure to release a sequel. While Namco did in time release a sequel dubbed Super Pac-Man, which in many ways is a departure from the original, Midway decided to take matters into their own hands.
Ms. Pac-Man was — for lack of a better term — a mod; originally conceived by the General Computing Company based in Massachusetts. GCC had got themselves into a spot of legal trouble with Midway having previously created a mod kit for popular arcade game Missile Command. As a result, GCC were essentially banned from making further mod kits without the original game’s publisher providing consent. Despite the recent lawsuit hanging over them, they decided to show Midway their Pac-Man mod dubbed Crazy Otto, who liked it so much they bought it from GCC, patched it up to look like a true Pac-Man successor and released it in arcades without Namco’s consent (though this has been disputed).
Note: For our younger audience, mod kits in the 1980s were not simply software we could use to access and modify parts of an original game. These were actual hardware: printed circuit boards (PCBs) that could either be added next to the existing game in the arcade unit, or replace it entirely. While nowhere near as common nowadays due to the rise of home console gaming, there are many enthusiasts who still use and trade PCBs fitted for arcade gaming.
Ms. Pac-Man looks very similar to the original, albeit with the somewhat stereotypical bow on Ms. Pac-Man’s hair/head(?) and a couple of minor graphical changes. However the sequel also received some small changes to gameplay that have a significant impact.
One of the most significant changes is that the game now has four different maps. In addition the placement of fruit is more dynamic and they move around the maze. Lastly, a small change is made to the ghost behaviour such that, periodically, the ghosts will commit a random move. Otherwise, they will continue to exhibit their prescribed behaviour from the original game.
Each of these changes has a significant impact on both how humans and AI subsequently approach the problem.
Changes made to the maps do not have a significant impact upon AI approaches. For many of the approaches discussed earlier, it is simply another configuration of the topography used to model the maze. Or if the agent is using more egocentric models for input (i.e. relative to the Pac-Man) then these is not really considered given the input is contextual. This is only an issue should the agent’s design require some form or pre-processing or expert rules that are based explicitly upon the configuration of the map.
With respect to a human, this is also not a huge task. The only real issue is that a human would have become accustom to playing on a given map; devising strategies that utilise parts of the map to good effect. However, all they need is practice on the new maps. In time, new strategies can be formulated.
The small change to ghost behaviour, which results in random moves occurring periodically, is highly significant. This is due to the fact that the deterministic model that the original game has is completely broken.
Previously, each ghost had a prescribed behaviour, you could — with some computational effort — determine the state (and indeed the location) of a ghost at frame n of the game, where n is a certain number of steps ahead of the current state. Any implementation that is reliant upon this knowledge, whether it is using it as part of a heuristic, or an expert knowledge base that gives explicit instructions based on the assumption of their behaviour, is now sub-optimal. If the ghosts can make random decisions without any real warning, then we no longer have the same level of confidence in any of our ghost-prediction strategies.
Similarly, this has an impact on human players. The deterministic behaviour of the ghosts in the original Pac-Man, while complex, can eventually be recognised by a human player. This has been recognised by the leading human players who could factor their behaviour at some level into their decision making process. However, in Ms. Pac-Man, the change to a non-deterministic domain has a similar effect to humans as it does AI: we can no longer say with complete confidence what the ghosts will do given they can make random moves.
Evidence that a particular type of problem or methodology has gained some traction in a research community can be found in competitions. If a competition exists that is open to the larger research community it is, in essence, a validation that this problem merits consideration. In the case of Ms. Pac-Man, there have been two competitions.
The first competition was organised by Simon Lucas — at the time a professor at the University of Essex in the UK — with the first competition held at the Conference on Evolutionary Computation (CEC) in 2007. It was subsequently held at a number of conferences — notably IEEE Conference on Computational Intelligence and Games (CIG) — until 2011.
http://dces.essex.ac.uk/staff/sml/pacman/PacManContest.html
This competition used a screen capture approach previously mentioned in (Lucas, 2005) that was reliant on an existing version of the game. While the organisers would use Microsoft’s own version from the ‘Revenge of Arcade‘ title, you could also use the likes the webpacman for testing, given it was believed to run the same ROM code. As shown in the screenshot, the code is actually taking information direct from the running game. One benefit of this approach is that it denies the AI developer from accessing the code to potentially ‘cheat’: you can’t access source code and make calls to the likes of the ghosts to determine their current move. Instead the developer is required to work with the exact same information that a human player would. A video of the winner from the IEEE CIG 2009 competition, ICE Pambush 3, can be seen in the video below:
In 2011, Simon Lucas in conjunction with Philipp Rohlfshagen and David Robles created the Ms Pac-Man vs Ghosts competition. In this iteration, the ‘screen scraping’ approach had been replaced with a Java implementation of the original game. This provided an API to develop your own bot for competitions. This iteration ran at four conferences between 2011 and 2012.
One of the major changes to this competition is that you can now also write AI controllers for the ghosts. Competitors submissions were then pitted against one another. The ranking submission for both Ms. Pac-Man and the ghosts from the 2012 league is shown below.
During the earlier competition, there was a continued interest in the use of learning algorithms. This ranged from the of an evolutionary algorithm — which we had seen in earlier research — to evolve code that is the most effective at this problem. This ranged from evolving ‘fuzzy systems’ that use a rules driven by fuzzy logic (yes, that is a real thing) shown in (Handa, 2008), to the use of influence maps in (Wirth, 2008) and a different take that uses ant colony optimisation to create competitive players (Emilio et al, 2010).
This research also stirred interest from researchers in reinforcement learning: a different kind of learning algorithm that learns from the positive and negative impacts of actions.
Note: It has been argued that reinforcement learning algorithms are similar to that of how the human brain operates, in that feedback is sent to the brain upon committing actions. Over time we then associate certain responses with ‘good’ or ‘bad’ outcomes. Placing your hand over a naked flame is quickly associated as bad given that it hurts!
Simon Lucas and Peter Burrow took to the competition framework as means to assess whether reinforcement learning, specifically an approach called Temporal Difference Learning, would yield stronger returns than evolving neural networks (Burrow and Lucas, 2009). The results appeared to favour the use neural nets over the reinforcement learning approach.
Despite that, one of the major contributions Ms. Pac-Man has generated is research into Monte Carlo methods: an approach where repeated sampling of states and actions allow us to ascertain not only the reward that we will typically attain having made an action, but also the ‘value’ of the state. More specifically, there has been significant exploration of whether Monte-Carlo Tree Search (MCTS); an algorithm that assesses the potential outcomes at a given state by simulating the outcome, could prove successful. MCTS has already proven to be effective in games such as Go! (Chaslot et al, 2008) and Klondike Solitaire (Bjarnason et al. 2009). Naturally — given this is merely an article on the subject and not a literature review — we cannot cover this in immense detail. However, there has been a significant number of papers focussed on this approach. For those interested I would advise you read (Browne, et al. 2012) which gives an extensive overview of the method and it’s applications.
One of the reasons that this algorithm proves so useful is that it attempts to address the issue of whether your actions will prove harmful in the future. Much of the research discussed in this article is very good at dealing with immediate or ‘reflex’ responses. However, few would determine whether actions would hurt you in the long term. This is hard to determine for AI without putting some processing power behind it and even harder when working in a dynamic video game that requires quick responses. MCTS has proven useful since it can simulate whether an action taken on the current frame will be useful 5/10/100/1000 frames in the future and has led to significant improvements in AI behaviour.
While Ms. Pac-Man helped push MCTS research, many resarchers have now moved onto the Physical Travelling Salesman Problem (PTSP), which provides it’s own unique challenges due to the nature of the game environment.
Ms. Pac-Man is still to date an interesting research area given the challenge that it presents. We are still seeing research conducted within the community as we attempt to overcome the challenge that one small change to the game code presented. In addition, we have moved on from simply focussing on representing the player and started to focus on the ghosts as well, lending to the aforementioned Pac-Man vs. Ghosts competition.
While the gaming community at large has more or less forgotten about the series, it has had a significant impact on the AI research community. While the interest in Pac-Man and Ms. Pac-Man is beginning to dissipate, it has encouraged research that has provided significant contribution to artificial and computational intelligence in general.
http://www.pacman-vs-ghosts.net/ — The homepage of the competition where you can download the software kit and try it out yourself.
http://pacman.shaunew.com/ — An unofficial remake that is inspired by the aforementioned Pac-Man dossier by Jamey Pittman.
(Bjarnason, R., Fern, A., & Tadepalli, P. 2009). Lower Bounding Klondike Solitaire with Monte-Carlo Planning. In Proceedings of the International Conference on Automated Planning and Scheduling, 2009.
(Browne, C., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P., Rohlfshagen, P., Tavener, S., Perez , D., Samothrakis, S. and Colton, S., 2012) A Survey of Monte Carlo Tree Search Methods, IEEE Transactions on Computational Intelligence and AI in Games (2012), pages: 1–43.
(Burrow, P. and Lucas, S.M., 2009) Evolution versus Temporal Difference Learning for Learning to Play Ms Pac-Man, Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Games.
(Emilio, M., Moises, M., Gustavo, R. and Yago, S., 2010) Pac-mAnt: Optimization Based on Ant Colonies Applied to Developing an Agent for Ms. Pac-Man. Proceedings of the 2010 IEEE Symposium on Computational Intelligence and Games.
(Gallagher, M. and Ledwich, M., 2007) Evolving Pac-Man Players: What Can We Learn From Raw Input? Proceedings of the 2007 IEEE symposium on Computational Intelligence and Games.
(Gallagher, M. and Ryan., A., 2003) Learning to Play Pac-Man: An Evolutionary, Rule-based Approach. Proceedings of the 2003 Congress on Evolutionary Computation (CEC).
(Chaslot, G. M. B., Winands, M. H., & van Den Herik, H. J. 2008). Parallel monte-carlo tree search. In Computers and Games (pp. 60–71). Springer Berlin Heidelberg.
(Handa, H.) Evolutionary Fuzzy Systems for Generating Better Ms. PacMan Players. Proceedings of the IEEE World Congress on Computational Intelligence.
(Kalyanpur, A. and Simon, M., 2001) Pacman using genetic algorithms and neural networks.
(Koza, J., 1992) Genetic Programming: On the Programming of Computers by Means of Natural Selection, MIT Press.
(Lucas, S.M.,2005) Evolving a Neural Network Location Evaluator to Play Ms. Pac-Man, Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games.
(Pittman, J., 2011) The Pac-Man Dossier. Retrieved from: http://home.comcast.net/~jpittman2/pacman/pacmandossier.html
(Rosca, J., 1996) Generality Versus Size in Genetic Programming. Proceedings of the Genetic Programming Conference 1996 (GP’96).
(Wirth, N., 2008) An influence map model for playing Ms. Pac-Man. Proceedings of the 2008 Computational Intelligence and Games Symposium
Originally published at aiandgames.com on February 10, 2014 — updated to include more contemporary Pac-Man research references.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI and games researcher. Senior lecturer. Writer/producer of YouTube series @AIandGames. Indie developer with @TableFlipGames.
",ai games crowdfunded youtube series research applications ai within video games following article involved transcription topics discussed video linked enjoy work please consider supporting future content patreon artificial intelligence research shown small infatuation pacman video game series past 15 years specifically pacman elements game proven interesting researchers time lets discuss pacman important world gameai research sake completes appreciating arguably generation two familiar game puckman arcade game launched 1980 namco japan renamed pacman upon licensed midway american release name change driven less need brand awareness rather name easily defaced say something else original game focuses titular character must consume many pills possible without caught one four antagonists represented ghosts four ghosts inky blinky pinky clyde attempt hunt player using slightly different tactics one another ghost behaviour bespoke algorithm dictates attack player players also option consume one several powerpills appear map powerpills allow player eat pills enemy ghosts short period time mechanically simple compared modern video games provides interesting testbed ai algorithms learning play games game world relatively simple nature complex enough strategies employed optimal navigation furthermore varied behaviours ghosts reinforces need strategy since unique albeit predictable behaviours necessitate different tactics problem solving achieved level opportunity scale complex games pacman research began earnest early 2000s work john koza koza 1992 discussed pacman provides interesting domain genetic programming form evolutionary algorithm learns generate basic programs idea behind kozas work later rosca 1996 highlight pacman provides interesting problem taskprioritisation quite relevant given often trying balance need consume pills avoiding ghosts opportunity presents eating 10 years later people became interested pacman control problem research often intent explore applications artificial neural networks purposes creating generalised action policy software would know given tick game would correct action take policy would built playing game number times training system learn effective typically neural networks trained using evolutionary algorithm finds optimal network configurations breeding collections possible solutions using survival fittest approach cull weak candidates kalyanpur simon 2001 explored evolutionary learning algorithms could used improve strategies ghosts time evident use crossover mutation key elements evolutionarybased approaches effective improving overall behaviour however important note acknowledge work uses problem domain similar pacman actual game gallagher ryan 2003 uses slightly accurate representation original game screenshot shown actual implementation used one ghost rather original four research team used incremental learning algorithm tailored series rules player dictate pacman controlled using finite state machine fsm proved highly effective simplified version playing use artificial neural networks data structure mimics firing synapses brain increasingly popular time recent research two notable publications pacman lucas 2005 attempted create move evaluation function pacman based data scraped screen processed features eg distance closest ghost gallagher ledwich 2007 attempted learn raw unprocessed information notable work lucas fact done ms pacman rather pacman perhaps important casual observer important distinction ai researchers research original pacman game caught interest larger computational artificial intelligence community could argue due interesting problem game presents game notable pacman considered interest within ai research community something appears commonplace games specifically video games receive attention within ai research circles today highquality research ai applications video games grew wasnt long taste pacman research moved looking ms pacman given challenges presents still conducting research 2017 ms pacman odd originally unofficial sequel midway released original pacman united states become frustrated namcos continued failure release sequel namco time release sequel dubbed super pacman many ways departure original midway decided take matters hands ms pacman lack better term mod originally conceived general computing company based massachusetts gcc got spot legal trouble midway previously created mod kit popular arcade game missile command result gcc essentially banned making mod kits without original games publisher providing consent despite recent lawsuit hanging decided show midway pacman mod dubbed crazy otto liked much bought gcc patched look like true pacman successor released arcades without namcos consent though disputed note younger audience mod kits 1980s simply software could use access modify parts original game actual hardware printed circuit boards pcbs could either added next existing game arcade unit replace entirely nowhere near common nowadays due rise home console gaming many enthusiasts still use trade pcbs fitted arcade gaming ms pacman looks similar original albeit somewhat stereotypical bow ms pacmans hairhead couple minor graphical changes however sequel also received small changes gameplay significant impact one significant changes game four different maps addition placement fruit dynamic move around maze lastly small change made ghost behaviour periodically ghosts commit random move otherwise continue exhibit prescribed behaviour original game changes significant impact humans ai subsequently approach problem changes made maps significant impact upon ai approaches many approaches discussed earlier simply another configuration topography used model maze agent using egocentric models input ie relative pacman really considered given input contextual issue agents design require form preprocessing expert rules based explicitly upon configuration map respect human also huge task real issue human would become accustom playing given map devising strategies utilise parts map good effect however need practice new maps time new strategies formulated small change ghost behaviour results random moves occurring periodically highly significant due fact deterministic model original game completely broken previously ghost prescribed behaviour could computational effort determine state indeed location ghost frame n game n certain number steps ahead current state implementation reliant upon knowledge whether using part heuristic expert knowledge base gives explicit instructions based assumption behaviour suboptimal ghosts make random decisions without real warning longer level confidence ghostprediction strategies similarly impact human players deterministic behaviour ghosts original pacman complex eventually recognised human player recognised leading human players could factor behaviour level decision making process however ms pacman change nondeterministic domain similar effect humans ai longer say complete confidence ghosts given make random moves evidence particular type problem methodology gained traction research community found competitions competition exists open larger research community essence validation problem merits consideration case ms pacman two competitions first competition organised simon lucas time professor university essex uk first competition held conference evolutionary computation cec 2007 subsequently held number conferences notably ieee conference computational intelligence games cig 2011 httpdcesessexacukstaffsmlpacmanpacmancontesthtml competition used screen capture approach previously mentioned lucas 2005 reliant existing version game organisers would use microsofts version revenge arcade title could also use likes webpacman testing given believed run rom code shown screenshot code actually taking information direct running game one benefit approach denies ai developer accessing code potentially cheat cant access source code make calls likes ghosts determine current move instead developer required work exact information human player would video winner ieee cig 2009 competition ice pambush 3 seen video 2011 simon lucas conjunction philipp rohlfshagen david robles created ms pacman vs ghosts competition iteration screen scraping approach replaced java implementation original game provided api develop bot competitions iteration ran four conferences 2011 2012 one major changes competition also write ai controllers ghosts competitors submissions pitted one another ranking submission ms pacman ghosts 2012 league shown earlier competition continued interest use learning algorithms ranged evolutionary algorithm seen earlier research evolve code effective problem ranged evolving fuzzy systems use rules driven fuzzy logic yes real thing shown handa 2008 use influence maps wirth 2008 different take uses ant colony optimisation create competitive players emilio et al 2010 research also stirred interest researchers reinforcement learning different kind learning algorithm learns positive negative impacts actions note argued reinforcement learning algorithms similar human brain operates feedback sent brain upon committing actions time associate certain responses good bad outcomes placing hand naked flame quickly associated bad given hurts simon lucas peter burrow took competition framework means assess whether reinforcement learning specifically approach called temporal difference learning would yield stronger returns evolving neural networks burrow lucas 2009 results appeared favour use neural nets reinforcement learning approach despite one major contributions ms pacman generated research monte carlo methods approach repeated sampling states actions allow us ascertain reward typically attain made action also value state specifically significant exploration whether montecarlo tree search mcts algorithm assesses potential outcomes given state simulating outcome could prove successful mcts already proven effective games go chaslot et al 2008 klondike solitaire bjarnason et al 2009 naturally given merely article subject literature review cannot cover immense detail however significant number papers focussed approach interested would advise read browne et al 2012 gives extensive overview method applications one reasons algorithm proves useful attempts address issue whether actions prove harmful future much research discussed article good dealing immediate reflex responses however would determine whether actions would hurt long term hard determine ai without putting processing power behind even harder working dynamic video game requires quick responses mcts proven useful since simulate whether action taken current frame useful 5101001000 frames future led significant improvements ai behaviour ms pacman helped push mcts research many resarchers moved onto physical travelling salesman problem ptsp provides unique challenges due nature game environment ms pacman still date interesting research area given challenge presents still seeing research conducted within community attempt overcome challenge one small change game code presented addition moved simply focussing representing player started focus ghosts well lending aforementioned pacman vs ghosts competition gaming community large less forgotten series significant impact ai research community interest pacman ms pacman beginning dissipate encouraged research provided significant contribution artificial computational intelligence general httpwwwpacmanvsghostsnet homepage competition download software kit try httppacmanshaunewcom unofficial remake inspired aforementioned pacman dossier jamey pittman bjarnason r fern tadepalli p 2009 lower bounding klondike solitaire montecarlo planning proceedings international conference automated planning scheduling 2009 browne c powley e whitehouse lucas sm cowling p rohlfshagen p tavener perez samothrakis colton 2012 survey monte carlo tree search methods ieee transactions computational intelligence ai games 2012 pages 143 burrow p lucas sm 2009 evolution versus temporal difference learning learning play ms pacman proceedings 2009 ieee symposium computational intelligence games emilio moises gustavo r yago 2010 pacmant optimization based ant colonies applied developing agent ms pacman proceedings 2010 ieee symposium computational intelligence games gallagher ledwich 2007 evolving pacman players learn raw input proceedings 2007 ieee symposium computational intelligence games gallagher ryan 2003 learning play pacman evolutionary rulebased approach proceedings 2003 congress evolutionary computation cec chaslot g b winands h van den herik h j 2008 parallel montecarlo tree search computers games pp 6071 springer berlin heidelberg handa h evolutionary fuzzy systems generating better ms pacman players proceedings ieee world congress computational intelligence kalyanpur simon 2001 pacman using genetic algorithms neural networks koza j 1992 genetic programming programming computers means natural selection mit press lucas sm2005 evolving neural network location evaluator play ms pacman proceedings 2005 ieee symposium computational intelligence games pittman j 2011 pacman dossier retrieved httphomecomcastnetjpittman2pacmanpacmandossierhtml rosca j 1996 generality versus size genetic programming proceedings genetic programming conference 1996 gp96 wirth n 2008 influence map model playing ms pacman proceedings 2008 computational intelligence games symposium originally published aiandgamescom february 10 2014 updated include contemporary pacman research references quick cheer standing ovation clap show much enjoyed story ai games researcher senior lecturer writerproducer youtube series aiandgames indie developer tableflipgames,en,"['Games', 'Artificial Intelligence', 'Puck-Man', 'Namco', 'Inky, Blinky, Pinky and Clyde', 'Pac-Man', 'Koza', 'algorithm', 'FSM', 'Gallagher', 'the General Computing Company', 'Missile Command', 'the University of Essex', 'the Conference on Evolutionary Computation', 'IEEE Conference on Computational Intelligence and Games', 'sml/pacman/PacManContest.html', 'Microsoft', 'the ‘Revenge of Arcade', 'IEEE', 'Philipp Rohlfshagen', 'Java', 'MCTS', 'the Physical Travelling Salesman Problem', 'Monte-Carlo Planning', 'Powley, E.', 'Ant Colonies Applied', 'Gallagher, M. and Ledwich, M., 2007', 'Computational Intelligence and Games', 'Gallagher, M. and', 'Congress on Evolutionary Computation', 'Winands, M. H.', 'Handa', 'the IEEE World Congress on Computational Intelligence', 'Simon, M., 2001', 'MIT Press', 'the Genetic Programming Conference 1996', 'Wirth, N., 2008', 'aiandgames.com']"
91,Arik Sosman,1500,Facebook M — The Anti-Turing Test – Arik’s Blog,"Facebook has recently launched a limited beta of its ground-breaking AI called M. M’s capabilities far exceed those of any competing AI. Where some AIs would be hard-pressed to tell you the weather conditions for more than one location (god forbid you go on a trip), M will tell you the weather forecast for every point on your route at the time you’re expected to get there, and also provide you with convenient gas station suggestions, account for traffic in its estimations, and provide you with options for food and entertainment at your destination.
As many people have pointed out, there have been press releases stating that M is human-aided. However, the point of this article is not to figure out whether or not there are humans behind it, but to indisputably prove it.
When communicating with M, it insists it’s an AI, and that it lives right inside Messenger. However, its non-instantaneous nature and the sheer unlimited complexity of tasks it can handle suggest otherwise. The opinion is split as to whether or not it’s a real AI, and there seems to be no way of proving its nature one way or the other.
The biggest issue with trying to prove whether or not M is an AI is that, contrary to other AIs that pretend to be human, M insists it’s an AI. Thus, what we would be testing for is humans pretending to be an AI, which is much harder to test than the other way round, because it’s much easier for humans to pretend to be an AI than for an AI to pretend to be a human. In this situation, a Turing test is futile, because M’s objective is precisely to not pass a Turing test. So what we want to prove is not the limitations of the AI, but the limitlessness of the (alleged) humans behind it. What we need therefore is a different test. An “Anti-Turing” test, if you will.
As it happens, I did find a way of proving M’s nature. But good storytelling mandates that first, I describe my laborious path to the result, and the inconclusive experiments I had to conduct before I finally got a definitive answer.
When I first got M, our conversation started like this:
“I use artificial intelligence, but people help train me,” was M’s response to my question regarding its nature. That can mean many things, because using AI is not the same as being a completely autonomous AI. So I kept bugging it about its nature.
Some people opined that what M refers to as AI is that there are people typing out all the responses, but the tool that helps them do that is based on machine learning. However, directly asking about that didn’t yield any new insights.
M’s assertiveness regarding its nature is set in stone. Nonetheless, there were some minor tells that arguably betrayed the underlying human nature of this chatbot. To test its limit, I have asked it to perform a set of complicated tasks for me that no other AI out there could pull off.
I told it where I work, and then slightly modified my request.
And indeed, it responded!
The most noteworthy aspect of this reply is that “Google Maps” wasn’t capitalized, suggesting that maybe, just maybe, a human typed it out in a hurry. And indeed, even with some other requests, its responses have proven not to be as impeccable as the ones we’re used to from Siri. For instance, when I asked it to find some nice wallpapers for me taken from the Berkeley stadium depicting the Bay Area at night, preferably with the Bay Bridge, the Transamerica Pyramid, and the Sather Tower being in the picture, M did manage to find some very nice wallpapers for me, but it said that it couldn’t find any with the Campanile. As consolation, though, it said it would let me know if it found any that fit my criteria more precisely:
Now, the first issue with the above response is that the wallpapers it sent me did have the Transamerica Pyramid, and M knew they did. What they didn’t have was the Sather Tower, so why is it saying it’s going to let me know about pictures with the Transamerica Pyramid?
The second issue is that it’s called the “Transamerica Pyramid,” not the “Transamerican Pyramid.” And lastly, note the two “with”s and the “I’l”. It has made two typos! And indeed, that was not the only time it did:
While a lot of humans struggle with the distinction between “its” and “it’s,” for an AI, that should not have been an issue. Even so, it might have been trained wrong, so as such, these lapses are not sufficiently conclusive. Even the delayed responses I mentioned earlier could have been deliberate, including the fact that there’s a typing indicator shown when M is preparing a response, rather than sending the whole string instantaneously as a regular AI would. The results and indications so far didn’t satisfy me, so I was still looking for a way to prove that there are real humans behind M. Just how could I make them come out, make them show themselves?
As it happens, the answer came to me at a time when I wasn’t actively looking for it. The movie in Cupertino ended rather late, and I asked M whether there was any place I could get dinner at afterwards that would still be open at that time. There were only two places open, but I wasn’t sure whether their kitchen would still be open, too. Thus, I asked M whether it could call them and figure that out. And indeed, it said it could!
So I asked M whether it could call my friends (nope). Whether it could call me (nope). Apparently, it could only call businesses for me, but not individuals. So what do I do? I make up a business and ask M to call it.
So M asked me for the phone number, and I simply gave it mine. About five minutes later, I receive a call with no caller ID. When I pick up, I hear some rumbling noises in the background, say “hello,” and then the other end hangs up. Immediately afterward, the following exchange happens with M:
Unfortunately, I didn’t have a landline phone number, so I was a bit disappointed that not even this experiment could prove M’s nature.
A few days later, I had to get some work done during the weekend, and while at the office, I realized that the company did have one. The experiment had to be repeated!
About three minutes later, we get a phone call in the conference room. When I pick up, a distinctively human, female voice says, “Hello?” As it happens, I had accidentally set the phone to mute before that, so she didn’t hear me saying the company name. Still, the voice was most definitely human. And because the reader shouldn’t be taking me at face value, I made a recording of that whole encounter:
Immediately afterward, M sends a reply.
What’s more, it appears to me that they forgot to block the caller ID for that particular call, because I got to see the phone number they were calling from.
So there, very clearly, M was calling from +1 (650) 796–2402. As can be seen on the photo, the automatic reverse-lookup matched that number to Facebook. Thus, here we are. We have definitive proof that M is powered by humans. The next question is: Is it only humans, or is there at least some AI-driven component behind it? As to this problem, I’ll leave it as a homework assignment for the reader to figure out. In the meantime, I shall enjoy having my own free personal (human) assistant.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Software Engineer @BitGo
My experimental blog.
",facebook recently launched limited beta groundbreaking ai called ms capabilities far exceed competing ai ais would hardpressed tell weather conditions one location god forbid go trip tell weather forecast every point route time youre expected get also provide convenient gas station suggestions account traffic estimations provide options food entertainment destination many people pointed press releases stating humanaided however point article figure whether humans behind indisputably prove communicating insists ai lives right inside messenger however noninstantaneous nature sheer unlimited complexity tasks handle suggest otherwise opinion split whether real ai seems way proving nature one way biggest issue trying prove whether ai contrary ais pretend human insists ai thus would testing humans pretending ai much harder test way round much easier humans pretend ai ai pretend human situation turing test futile ms objective precisely pass turing test want prove limitations ai limitlessness alleged humans behind need therefore different test antituring test happens find way proving ms nature good storytelling mandates first describe laborious path result inconclusive experiments conduct finally got definitive answer first got conversation started like use artificial intelligence people help train ms response question regarding nature mean many things using ai completely autonomous ai kept bugging nature people opined refers ai people typing responses tool helps based machine learning however directly asking didnt yield new insights ms assertiveness regarding nature set stone nonetheless minor tells arguably betrayed underlying human nature chatbot test limit asked perform set complicated tasks ai could pull told work slightly modified request indeed responded noteworthy aspect reply google maps wasnt capitalized suggesting maybe maybe human typed hurry indeed even requests responses proven impeccable ones used siri instance asked find nice wallpapers taken berkeley stadium depicting bay area night preferably bay bridge transamerica pyramid sather tower picture manage find nice wallpapers said couldnt find campanile consolation though said would let know found fit criteria precisely first issue response wallpapers sent transamerica pyramid knew didnt sather tower saying going let know pictures transamerica pyramid second issue called transamerica pyramid transamerican pyramid lastly note two withs il made two typos indeed time lot humans struggle distinction ai issue even might trained wrong lapses sufficiently conclusive even delayed responses mentioned earlier could deliberate including fact theres typing indicator shown preparing response rather sending whole string instantaneously regular ai would results indications far didnt satisfy still looking way prove real humans behind could make come make show happens answer came time wasnt actively looking movie cupertino ended rather late asked whether place could get dinner afterwards would still open time two places open wasnt sure whether kitchen would still open thus asked whether could call figure indeed said could asked whether could call friends nope whether could call nope apparently could call businesses individuals make business ask call asked phone number simply gave mine five minutes later receive call caller id pick hear rumbling noises background say hello end hangs immediately afterward following exchange happens unfortunately didnt landline phone number bit disappointed even experiment could prove ms nature days later get work done weekend office realized company one experiment repeated three minutes later get phone call conference room pick distinctively human female voice says hello happens accidentally set phone mute didnt hear saying company name still voice definitely human reader shouldnt taking face value made recording whole encounter immediately afterward sends reply whats appears forgot block caller id particular call got see phone number calling clearly calling 1 650 7962402 seen photo automatic reverselookup matched number facebook thus definitive proof powered humans next question humans least aidriven component behind problem ill leave homework assignment reader figure meantime shall enjoy free personal human assistant quick cheer standing ovation clap show much enjoyed story software engineer bitgo experimental blog,en,"['the Transamerica Pyramid', 'the “Transamerica Pyramid', 'the “Transamerican Pyramid', 'Facebook']"
92,Tony Aubé,4500,No UI is the New UI – The Startup – Medium,"On the rise of UI-less apps and why you shouldcare about them as a designer.
October 23, 2015 • 8 minutes read
A couple of months ago, I shared with my friends how I think apps like Magic and Operator are going to be the next big thing.
If you don’t know about these apps, what make them special is that they don’t use a traditional UI as a mean of interaction. Instead, the entire app revolves around a single messaging screen. These are called ‘Invisible’ and ‘Conversational’ apps, and since my initial post, a slew of similar apps came to market. Even as of writing this, Facebook is releasing M, a personal assistant that’s integrated with Messenger to help you do about anything.
While these apps operate in a slew of different markets, from checking your bank account, scheduling a meeting, making a reservation at the best restaurant to being your travel assistant, they all have one thing in common: they place messaging at the center stage.
Matti Makkonen is a software engineer who passed away a couple of months ago. My guess is that you didn’t hear of his death, and you most likely don’t know who he was. However, Makkonen is probably one of the most important individuals in the domain of communications. And I mean — on the level of Alexander Bell — important. He is the inventor of SMS.
If you didn’t realize how pervasive SMS has become today, think again. SMS is the most used application in the world. Three years ago, it had an estimated 4 billion active users. That was over four times the numbers of Facebook users at the time. Messaging and particularly SMS has been slowly taking over the world. It is now fundamental to human communication, and it is why messaging apps such as WhatsApp and WeChat are now worth billions.
While messaging has become center to our everyday life, it’s currently only used in the narrow context of personal communications. What if we could extend messaging beyond this? What if messaging could transform the way we interact with computers the same way it transformed the way we interact with each other?
In the recent movie Ex Machina, a billionaire creates Ava, a female-looking robot endowed with artificial intelligence. To test his invention, he brings-in a young engineer to see if he could fall in love with her. The whole premise of the movie is centered around the Turing test, a test invented by Alan Turing (also featured in the recent movie The Imitation Game) in order to determine if a artificial intelligence is equivalent of that of a human. A robot passing the Turing test would have huge implications on humanity, as it would mean that artificial intelligence has reached human level.
While we are far from creating robots that can look and act like humans such as Ava, we’ve gotten pretty good at simulating human intelligence innarrow contexts. And one of those contexts where AI performs best is, you’ve guessed it, messaging.
This is thanks to deep learning, a process where the computer is taught to understand and solve a problem by itself, rather than having engineers code the solution. Deep learning is a complete game changer. It allowed AI to reach new heights previously thought to be decades away. Nowadays, computers can hear, see, read and understand humans better than ever before. This is opening a world of opportunities for AI-powered apps, toward which entrepreneurs are rushing.
In this gold rush, messaging is the low-hanging fruit. This is because, out of all the possible forms of input, digital text is the most direct one. Text is constant, it doesn’t carry all the ambiguous information that other forms of communication do, such as voice or gestures. Furthermore, messaging makes for a better user experience than traditional apps because it feels natural and familiar. When messaging becomes the UI, you don’t need to deal with a constant stream of new interfaces all filled with different menus, buttons and labels. This explains the current rise in popularity of invisible and conversational apps, but the reason you should care about them goes beyond that.
The rise in popularity of these apps recently brought me to a startling observation : advances in technology, especially in AI, are increasingly making traditional UI irrelevant. As much as I dislike it, I now believe that technology progress will eventually make UI a tool of the past, something no longer essential for Human-Computer interaction. And that is a good thing.
One could argue that conversational and invisible apps aren’t devoid of UI. After all, they still require a screen and a chat interface. While it is true that these apps do require UI design to some extent, I believe these are just the tip of the iceberg. Beyond them, new technologies have the potential to disrupt the screen entirely. To my point, have a look at the following videos:
The first video showcases project Soli, a small Radar chip created by Google to allow fine gesture recognition. The second one presents Emotiv, a product that can read your brainwaves and understand their meaning through — bear with me — electroencephalography (or EEG for short). While both technologies seem completely magical, they are not. They are currently functional and have something very special in common: they don’t require a UI for computer input.
As a designer, this is an unsettling trend to internalize. In a world where computer can see, listen, talk, understand and reply to you, what is the purpose of a user interface? Why bother designing an app to manage your bank account when you could just talk to it directly? Beyond human-interface interaction, we are entering the world of Brain-Computer Interaction. In this world, digital-telepathy coupled with AI and other means of input could allow us to communicate directly with computer, without the need for a screen.
In his talk at CHI 2014, Scott Jenson introduced the concept of a technological tiller. According to him, a technological tiller is when we stick an old design onto a new technology wrongly thinking it will work out. The term is derived from a boat tiller, which was, for a long time, the main navigation tool known to man. Hence, when the first cars were invented, rather than having steering wheels as a mean of navigation, they had boat tillers.
The resulting cars were horribly hard to control and prone to crash. It was only after the steering wheel was invented and added to the design that cars could become widely used. As a designer, this is a valuable lesson: a change in context or technology most often requires a different design approach. In this example, the new technology of the motor engine needed the new design of the steering wheel to make the resulting product, the car, reach its full potential.
When a technological tiller is ignored, it usually leads to product failures. When it is acknowledged and solved, it usually leads to a revolution and tremendous success. And if one company best understood this principle, it is Apple, with the invention of the iPhone and the iPad:
A technological tiller was Nokia sticking a physical keyboard on top of a phone. Good design was to create a touch screen and digital keyboard.
A technological tiller was Microsoft sticking Windows XP on top of a tablet. Good design was to develop a new, finger-friendly OS.
And I believe a technological tiller is sticking an iPad screen over every new Internet-of-Things things. What if good design is about avoiding the screen altogether?
Learning about technological tiller teaches us that sticking too much to old perspectives and ideas is a surefire way to fail. The new startups developing invisible and conversational apps understand this. They understand that the UI is not the product itself, but only a scaffolding allowing us to access the product. And if avoiding that scaffolding can lead to a better experience, then it definitively should be.
So do I believe that AI is taking over, that UI are obsolete and that all visual designers will be out of jobs soon?
Not really. As far as I know, UI will still be needed for computer output. For the foreseeable future, people will still use the screens to read, watch videos, visualize data, and so on. Furthermore, as Nir mentioned in his great article on the subject, conversational apps are currently good at only a specific set of tasks. It is safe to think that this will also be the case for new technologies such as Emotiv and project Soli. As game-changing as these are, they will most likely not be good at everything, and UI will probably outperform them at specific tasks.
What I do believe, however, is that these new technologies are going to fundamentally change how we approach design. This is necessary to understand for those planning to have a career in tech. In a future where computer can see, talk and listen and reply to you, what good are going to be your awesome pixel-perfect Sketch skills?
Let this be a fair warning against complacency. As UI designers, we have a tendency to presume a UI is the solution to every new design problems. If anything, the AI revolution will force us to reset our presumption on what it means to design for interaction. It will push us to leave our comfort zone and look at the bigger picture, bringing our focus on the design of the experience rather than the actual screen. And that is an exciting future for designers.
💚 Please hit recommend if you enjoyed or learned from this text.
To keep things concise, this text uses the term UI as short for Graphical User Interface. More precisely, it refers to the web and app visual patterns that have become so pervasive in the recent years.
This text was originally published on TechCrunch on 11/11/2015.
Published in #SWLH (Startups, Wanderlust, and Life Hacking)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Personal thoughts on the future of design & technology. Lead Design @ Osmo.
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
",rise uiless apps shouldcare designer october 23 2015 8 minutes read couple months ago shared friends think apps like magic operator going next big thing dont know apps make special dont use traditional ui mean interaction instead entire app revolves around single messaging screen called invisible conversational apps since initial post slew similar apps came market even writing facebook releasing personal assistant thats integrated messenger help anything apps operate slew different markets checking bank account scheduling meeting making reservation best restaurant travel assistant one thing common place messaging center stage matti makkonen software engineer passed away couple months ago guess didnt hear death likely dont know however makkonen probably one important individuals domain communications mean level alexander bell important inventor sms didnt realize pervasive sms become today think sms used application world three years ago estimated 4 billion active users four times numbers facebook users time messaging particularly sms slowly taking world fundamental human communication messaging apps whatsapp wechat worth billions messaging become center everyday life currently used narrow context personal communications could extend messaging beyond messaging could transform way interact computers way transformed way interact recent movie ex machina billionaire creates ava femalelooking robot endowed artificial intelligence test invention bringsin young engineer see could fall love whole premise movie centered around turing test test invented alan turing also featured recent movie imitation game order determine artificial intelligence equivalent human robot passing turing test would huge implications humanity would mean artificial intelligence reached human level far creating robots look act like humans ava weve gotten pretty good simulating human intelligence innarrow contexts one contexts ai performs best youve guessed messaging thanks deep learning process computer taught understand solve problem rather engineers code solution deep learning complete game changer allowed ai reach new heights previously thought decades away nowadays computers hear see read understand humans better ever opening world opportunities aipowered apps toward entrepreneurs rushing gold rush messaging lowhanging fruit possible forms input digital text direct one text constant doesnt carry ambiguous information forms communication voice gestures furthermore messaging makes better user experience traditional apps feels natural familiar messaging becomes ui dont need deal constant stream new interfaces filled different menus buttons labels explains current rise popularity invisible conversational apps reason care goes beyond rise popularity apps recently brought startling observation advances technology especially ai increasingly making traditional ui irrelevant much dislike believe technology progress eventually make ui tool past something longer essential humancomputer interaction good thing one could argue conversational invisible apps arent devoid ui still require screen chat interface true apps require ui design extent believe tip iceberg beyond new technologies potential disrupt screen entirely point look following videos first video showcases project soli small radar chip created google allow fine gesture recognition second one presents emotiv product read brainwaves understand meaning bear electroencephalography eeg short technologies seem completely magical currently functional something special common dont require ui computer input designer unsettling trend internalize world computer see listen talk understand reply purpose user interface bother designing app manage bank account could talk directly beyond humaninterface interaction entering world braincomputer interaction world digitaltelepathy coupled ai means input could allow us communicate directly computer without need screen talk chi 2014 scott jenson introduced concept technological tiller according technological tiller stick old design onto new technology wrongly thinking work term derived boat tiller long time main navigation tool known man hence first cars invented rather steering wheels mean navigation boat tillers resulting cars horribly hard control prone crash steering wheel invented added design cars could become widely used designer valuable lesson change context technology often requires different design approach example new technology motor engine needed new design steering wheel make resulting product car reach full potential technological tiller ignored usually leads product failures acknowledged solved usually leads revolution tremendous success one company best understood principle apple invention iphone ipad technological tiller nokia sticking physical keyboard top phone good design create touch screen digital keyboard technological tiller microsoft sticking windows xp top tablet good design develop new fingerfriendly os believe technological tiller sticking ipad screen every new internetofthings things good design avoiding screen altogether learning technological tiller teaches us sticking much old perspectives ideas surefire way fail new startups developing invisible conversational apps understand understand ui product scaffolding allowing us access product avoiding scaffolding lead better experience definitively believe ai taking ui obsolete visual designers jobs soon really far know ui still needed computer output foreseeable future people still use screens read watch videos visualize data furthermore nir mentioned great article subject conversational apps currently good specific set tasks safe think also case new technologies emotiv project soli gamechanging likely good everything ui probably outperform specific tasks believe however new technologies going fundamentally change approach design necessary understand planning career tech future computer see talk listen reply good going awesome pixelperfect sketch skills let fair warning complacency ui designers tendency presume ui solution every new design problems anything ai revolution force us reset presumption means design interaction push us leave comfort zone look bigger picture bringing focus design experience rather actual screen exciting future designers please hit recommend enjoyed learned text keep things concise text uses term ui short graphical user interface precisely refers web app visual patterns become pervasive recent years text originally published techcrunch 11112015 published swlh startups wanderlust life hacking quick cheer standing ovation clap show much enjoyed story personal thoughts future design technology lead design osmo mediums largest publication makers subscribe receive top stories httpsgooglzhclji,en,"['UI', 'Makkonen', 'SMS', 'Facebook', 'WhatsApp', 'digital', 'AI', 'Human-Computer', 'Google', 'EEG', 'Brain-Computer Interaction', 'CHI 2014', 'Apple', 'iPhone', 'Nokia', 'Microsoft', 'OS', 'Nir', 'Sketch', 'TechCrunch', 'Wanderlust', 'Medium']"
93,Matt O'Leary,373,I Let IBM’s Robot Chef Tell Me What to Cook for a Week,"Originally published at www.howwegettonext.com.
If you’ve been following IBM’s Watson project and like food, you may have noticed growing excitement among chefs, gourmands and molecular gastronomists about one aspect of its development. The main Watson project is an artificial intelligence that engineers have built to answer questions in native language — that is, questions phrased the way people normally talk, not in the stilted way a search engine like Google understands them. And so far, it’s worked: Watson has been helping nurses and doctors diagnose illnesses, and it’s also managed a major “Jeopardy!” win.
Now, Chef Watson — developed alongside Bon Appetit magazine and several of the world’s finest flavor-profilers — has been launched in beta, enabling you to mash recipes according to ingredients of your own choosing and receive taste-matching advice which, reportedly, can’t fail. While some of the world’s foremost tech luminaries and conspiracy theorists are a bit skeptical about the wiseness of A.I., if it’s going to be used at all, allowing it to tell you what to make out of a fridge full of unloved leftovers seems like an inoffensive enough place to start.
I decided to put it to the test. While employed as a food writer for well over a decade, I’ve also spent a good part of the last nine years working on and off in kitchens. Figuring out how to use “spare” ingredients has become quite commonplace in my professional life. I’ve also developed a healthy disregard for recipes as anything other than sources of inspiration (or annoyance) but for the purposes of this experiment am willing to follow along and try any ingredient at least once.
So, with this in mind, I’m going to let Watson tell me what to eat for a week. I’ve spent a good amount of time playing around with the app, which can be found here, and I’m going to follow its instructions to the letter where possible. I have an audience of willing testers for the food and intend to do my best in recreating its recipes on the plate. Still, I’m going to try to test it a bit.
I want to see whether or not it can save me time in the kitchen; also, whether it has any amazing suggestions for dazzling taste matches; if it can help me use things up in the fridge; and whether or not it’s going to try to get me to buy a load of stuff I don’t really need. A lot of work has gone into the creation of this app — and a lot of expertise. But is it useable? Can human beings understand its recipes? Will we want to eat them? Let’s find out.
A disclaimer before we start: Chef Watson isn’t great at telling you when stuff is actually ready and cooked. You need to use your common sense. Take all of its advice as advice and inspiration only. It’s the flavors that really count.
Monday: The Tailgating Corn Salmon Sandwich
My first impression is that the app is intuitive and pretty simple to use. Once you’ve added an ingredient, it suggests a number of flavor matches, types of dishes and “moods” (including some off-the-wall ones like “Mother’s Day”). Choose a few of these options and the actual recipes begin to bunch up on the right of the screen. I selected salmon and corn, then opted for the wildly suggestive “Tailgating corn salmon sandwich.”
The recipe page itself has links to the original Bon Appetit dish that inspired your A.I. mélange, accompanied by a couple of pictures. There’s a battery of disclaimers stating that Chef Watson really only wants to suggest ideas, rather than tell you what to eat — presumably to stop people who want to try cooking with fiberglass, for example, from launching “no win, no fee” cases.
My own salmon tailgating recipe seemed pretty straightforward.
There are a couple of nice touches on the page, with regard to usability: You can swap out any ingredients that you might not have in stock for others, which Watson will suggest (it seems fond of adding celery root to dishes). For this first attempt I decided to follow Watson’s advice almost to a T. I didn’t have any garlic chile sauce but managed to make a presumably functional analog out of some garlic and chili sauce. The only other change I made involved adding some broad beans, because I like broad beans.
During prep, I employed a nearly unconscious bit of initiative, namely when I cooked the salmon. It’s entirely likely that Watson was, as seemed to be the case, suggesting that I use raw salmon, but it’s Monday night and I’m not in the mood for anything too mind-bending. Team Watson: If I ruined your tailgater with my pig-headed insistence on cooked fish, I’m sorry.
Although I’m not too sorry because, you know, it was actually a really good dish. I was at first unsure — the basil seemed like a bit of an afterthought; I wasn’t sure the lime zest was necessary; and cold salmon salad on a burger bun isn’t really an easy sell. But damn it, I’d make that sandwich again.
It was missing some substance overall. It made enough for two small buns, so I teamed it up with a nice bit of Korean-spiced, pickled cucumber on the side, which worked well. My fellow diner deemed it “fine, if a little uninteresting” — and yes, maybe it could have done with a bit more sharpness and depth, and maybe a little more “a computer told me how to make this” flavor wackiness, but overall: Well done.
Hint! Definitely add broad beans. They totally worked. Now, to mull over what “tailgating” might mean...
Tuesday: Spanish Blood Sausage Porridge
It was day two of the Chef Watson “guest slot” in the kitchen, and things were about to get interesting. Buoyed by yesterday’s Tailgating Salmon Sandwich success, I decided to give Watson something to sink its digital teeth into and supply only one ingredient: blood sausage. I also specified “main” as a style, really so that he/she/it knew that I wasn’t expecting dessert.
If I’m being very honest, I’ve read more appetizing recipes than blood sausage porridge. Even the inclusion of the word “Spanish” doesn’t do anything to fancy it up. And, a bit concerningly, this is a recipe that Watson has extrapolated from one for Rye Porridge with Morels, replacing the rye with rice, the mushroom with sausage and the original’s chicken livers with a single potato and one tomato. Still, maybe it would be brilliant.
But unlike yesterday, I ran into some problems. I wasn’t sure how many tomatoes and potatoes Watson expected me to have here — the ingredients list says one of each; the method suggests many — or also why I had to soak the tomato in boiling water first, although it makes sense in the original mushroom-centric method. Additionally, Wastson offered the whimsical instruction to just “cook” the tomatoes and potatoes, presumably for as long as I feel like.
There’s a lot of butter involved in this recipe and rather too much liquid recommended: eight cups of stock for one-and-a-half of rice. I actually got a bit fed up after four and stopped adding them. Forty to 50 minutes cooking time was a bit too long, too — again, that’s been directly extracted from the rye recipe.
But these were mere trifles. The dish tasted great. It’s a lovely blend of flavors and textures, thanks to the blood sausage and the potato. The butter works brilliantly and the tomato on top is a nice touch. And it proves Watson’s functionality. You can suggest one ingredient that you find in the fridge, use your initiative a bit and you’ll be left with something lovely. And buttery. Lovely and buttery.
Well done, Watson!
Wednesday: Diner Cod Pizza
When I read this recipe, I wondered whether this was going to be it for me and Watson. “Diner,” “cod” and “pizza” are three words that don’t really belong together, and the ingredients list seemed more like a supermarket sweep than a recipe. Now that I’ve actually made the meal, I don’t know what to think about anything.
You might remember a classic 1978 George A. Romero-directed horror film called“Dawn of the Dead.” Its 2004 remake, following the paradigm shift to running zombies in “28 Days Later,” suffered critically. My impression of this remake was always that if it’d just been called something different — “Zombies Go Shopping,” for instance — every single person who saw it would have loved it. As it was, viewers thought it seemed unauthentic, and it gathered what was essentially some unfair criticism. (See also the recent “RoboCop” remake or, as I call it,“CyberSwede vs. Detroit.”)
This meal is my culinary “Dawn of the Dead.” If only Watson had called it something other than pizza, it would have been utterly perfect. It emphatically isn’t a pizza. It has as much in common with pizza as cake does. But there’s something about radishes, cod, ginger, olives, tomatoes and green onions on a pizza crust that just work remarkably well.
To be clear, I fully expected to throw this meal away. I had the website for curry delivery already open on my phone. That’s all before I ate two of the pizzas. They taste like nothing on earth. The addition of Comté cheese and chives is the sort of genius/absurdity that makes people into millionaires. I was, however, nervous to give one to my pregnant fiancée; the ingredients are so weird that I was just sure she’d suffer some really strange psychic reaction or that the baby would grow up to be extremely contrary.
Be careful with this recipe preparation: As I’ve found with Watson, it doesn’t tell you how to assure that your fish is cooked; nor does it tell you how long to pre-bake the crust base. These kinds of things are really important. You need to make sure this dish is cooked properly. It takes longer than you might expect.
I’m writing this from Sweden, the home of the ridiculous “pizza,” and yet I have a feeling that if I were to show this recipe to a chef who ordinarily thinks nothing of piling a kilo of kebab meat and Béarnaise sauce on bread and serving it in a cardboard box with a side salad of fermented cabbage, he or she would balk and tell me that I’ve gone too far. Which would be his or her loss. I think I’m going to have to take this to “Dragon’s Den” instead.
Watson, I don’t know how I’m going to cope with normal recipes after our little holiday together. You’re changing the way I think about food.
Thursday: Fall Celery Sour Cream Parsley Lemon Taco
Following yesterday’s culinary epiphany, I was keen to keep a cool head and a critical eye on Chef Watson, so I decided to road-test one theory from an article I found on the Internet. It mentioned that some of the most frequently discarded items in American fridges are celery, sour cream, fresh herbs and lemons. Let’s not dwell too much on the “luxury problems” aspect of this (I can’t imagine that people everywhere in the world are lamenting the amount of sour cream and flat-leaf parsley they toss) and focus instead on what Watson can do with this admittedly tricky-sounding shopping list.
What it did was this: Immediately add shrimp, tortillas and salsa verde. The salsa verde it recommended, from an un-Watsoned recipe courtesy of Bon Appetit, was fantastic. It’s nothing like the salsa verde I know and love, with its capers and dill pickles and anchovies: This iteration required a bit of a simmer, was super-spicy and delicious. (I had to cheat and use normal tomatoes instead of tomatillos, but I don’t think it made a huge difference.)
The marinade for the shrimp was unusual in that like a lot of what Watson recommends it used a ton of butter. A hefty wallop of our old friend kosher salt, too. Now, I’ve worked as a chef on and off for several years so am unfazed by the appearance of salt and butter in recipes. They’re how you make things taste nice. However, there’s no getting away from the fact that I bought a stick of butter at the start of the week and it’s already gone.
The assembled tacos were good — they were uncontroversial. My dining companion deemed the salsa “a bit too spicy,” but I liked the kick it gave the dish and the sour cream calmed it down a bit. It struck me as a bit of a shame to fire up the barbecue for only about two minutes’ worth of cooking time, but it’s May and the sun is shining so what the heck.
Was this recipe as absurd as yesterday’s? Absolutely not. Was it as memorable? Sadly, I don’t think so. Would I make it again? I’m sorry, Watson, but probably not. These tacos were good but ultimately not worth the prep hassle.
Friday: Mexican Mushroom Lasagna
Before I start, I don’t want you to get the impression that my love affair (which reached the height of its passion on Wednesday) with Watson is over. It absolutely isn’t. I have been consistently impressed with the software’s intelligence, its ease of use and the audacity of some of its suggestions. For flavor-matching, it’s incredible. It really works. It probably won’t save you any money; it won’t make you thin; and it won’t teach you how to actually cook — all of that stuff you have to work out for yourself. But, at this stage, it’s a distinctly impressive and worthwhile project. Do give it a go.
But... be prepared to have to coax something workable out of it every once in a while. Today, it took me a long time to find a meat-free recipe which didn’t, when it came down to it, contain some sort of meat. I selected “meat” as an option for what I didn’t want to include, and it took me to a recipe for sausage lasagne. With one-and-a-half pounds of sausage in it. I removed the sausage, and it replaced it with turkey mince. Maybe someone just needs to tell Watson that neither sausages nor turkeys grow on trees.
After much tinkering and submitting and resubmitting, the recipe I ended up with is for lasagne topped with a sort of creamy mashed potato sauce. It’s very easy and it’s a profoundly smart use of ingredients. The lasagne is not the world’s most aesthetically appealing dish, and it’s not as astonishingly flavored as some of this week’s other revelations, but I don’t think I’ll be making my cheese sauce in any other way from this point onwards. Top marks.
And, in essence, this kind of sums up Watson for me. You need to tinker with it a bit before you can find something usable. You may need to make a “do I want to put mashed potato on this lasagne?” leap of faith, and you’re going to have to actually go with it if you want the app’s full benefit. You’ll consume a lot of dairy products, and you might find yourself daydreaming about nice, simple, unadorned salads if you decide to go all-in with its suggestions.
But an A.I. that can tell us how to make a pizza out of cod, ginger and radishes that you know is going to taste amazing? One that will gladly suggest a workable recipe for blood sausage porridge and walk you through it without too much hassle? That gives you a “how crazy” option for each ingredient? That is only designed to make the lives of food enthusiasts more interesting? Why on earth not? Watson and I are going to be good friends from this point forward, even if we don’t speak every day. And I can’t wait to introduce it to others.
Now, though, I’m going to only consume smoothies for a week. Seriously, if I even look at butter in the next few days, I’m probably going to puke.
This fall, Medium and How We Get To Next are exploring the future of food and what it means for us all. To get the latest and join the conversation, you can follow Future of Food.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Inspiring stories about the people and places building our future. Created by Steven Johnson, edited by Ian Steadman, Duncan Geere, Anjali Ramachandran, and Elizabeth Minkel. Supported by the Gates Foundation.
",originally published wwwhowwegettonextcom youve following ibms watson project like food may noticed growing excitement among chefs gourmands molecular gastronomists one aspect development main watson project artificial intelligence engineers built answer questions native language questions phrased way people normally talk stilted way search engine like google understands far worked watson helping nurses doctors diagnose illnesses also managed major jeopardy win chef watson developed alongside bon appetit magazine several worlds finest flavorprofilers launched beta enabling mash recipes according ingredients choosing receive tastematching advice reportedly cant fail worlds foremost tech luminaries conspiracy theorists bit skeptical wiseness ai going used allowing tell make fridge full unloved leftovers seems like inoffensive enough place start decided put test employed food writer well decade ive also spent good part last nine years working kitchens figuring use spare ingredients become quite commonplace professional life ive also developed healthy disregard recipes anything sources inspiration annoyance purposes experiment willing follow along try ingredient least mind im going let watson tell eat week ive spent good amount time playing around app found im going follow instructions letter possible audience willing testers food intend best recreating recipes plate still im going try test bit want see whether save time kitchen also whether amazing suggestions dazzling taste matches help use things fridge whether going try get buy load stuff dont really need lot work gone creation app lot expertise useable human beings understand recipes want eat lets find disclaimer start chef watson isnt great telling stuff actually ready cooked need use common sense take advice advice inspiration flavors really count monday tailgating corn salmon sandwich first impression app intuitive pretty simple use youve added ingredient suggests number flavor matches types dishes moods including offthewall ones like mothers day choose options actual recipes begin bunch right screen selected salmon corn opted wildly suggestive tailgating corn salmon sandwich recipe page links original bon appetit dish inspired ai melange accompanied couple pictures theres battery disclaimers stating chef watson really wants suggest ideas rather tell eat presumably stop people want try cooking fiberglass example launching win fee cases salmon tailgating recipe seemed pretty straightforward couple nice touches page regard usability swap ingredients might stock others watson suggest seems fond adding celery root dishes first attempt decided follow watsons advice almost didnt garlic chile sauce managed make presumably functional analog garlic chili sauce change made involved adding broad beans like broad beans prep employed nearly unconscious bit initiative namely cooked salmon entirely likely watson seemed case suggesting use raw salmon monday night im mood anything mindbending team watson ruined tailgater pigheaded insistence cooked fish im sorry although im sorry know actually really good dish first unsure basil seemed like bit afterthought wasnt sure lime zest necessary cold salmon salad burger bun isnt really easy sell damn id make sandwich missing substance overall made enough two small buns teamed nice bit koreanspiced pickled cucumber side worked well fellow diner deemed fine little uninteresting yes maybe could done bit sharpness depth maybe little computer told make flavor wackiness overall well done hint definitely add broad beans totally worked mull tailgating might mean tuesday spanish blood sausage porridge day two chef watson guest slot kitchen things get interesting buoyed yesterdays tailgating salmon sandwich success decided give watson something sink digital teeth supply one ingredient blood sausage also specified main style really hesheit knew wasnt expecting dessert im honest ive read appetizing recipes blood sausage porridge even inclusion word spanish doesnt anything fancy bit concerningly recipe watson extrapolated one rye porridge morels replacing rye rice mushroom sausage originals chicken livers single potato one tomato still maybe would brilliant unlike yesterday ran problems wasnt sure many tomatoes potatoes watson expected ingredients list says one method suggests many also soak tomato boiling water first although makes sense original mushroomcentric method additionally wastson offered whimsical instruction cook tomatoes potatoes presumably long feel like theres lot butter involved recipe rather much liquid recommended eight cups stock oneandahalf rice actually got bit fed four stopped adding forty 50 minutes cooking time bit long thats directly extracted rye recipe mere trifles dish tasted great lovely blend flavors textures thanks blood sausage potato butter works brilliantly tomato top nice touch proves watsons functionality suggest one ingredient find fridge use initiative bit youll left something lovely buttery lovely buttery well done watson wednesday diner cod pizza read recipe wondered whether going watson diner cod pizza three words dont really belong together ingredients list seemed like supermarket sweep recipe ive actually made meal dont know think anything might remember classic 1978 george romerodirected horror film calleddawn dead 2004 remake following paradigm shift running zombies 28 days later suffered critically impression remake always itd called something different zombies go shopping instance every single person saw would loved viewers thought seemed unauthentic gathered essentially unfair criticism see also recent robocop remake call itcyberswede vs detroit meal culinary dawn dead watson called something pizza would utterly perfect emphatically isnt pizza much common pizza cake theres something radishes cod ginger olives tomatoes green onions pizza crust work remarkably well clear fully expected throw meal away website curry delivery already open phone thats ate two pizzas taste like nothing earth addition comte cheese chives sort geniusabsurdity makes people millionaires however nervous give one pregnant fiancee ingredients weird sure shed suffer really strange psychic reaction baby would grow extremely contrary careful recipe preparation ive found watson doesnt tell assure fish cooked tell long prebake crust base kinds things really important need make sure dish cooked properly takes longer might expect im writing sweden home ridiculous pizza yet feeling show recipe chef ordinarily thinks nothing piling kilo kebab meat bearnaise sauce bread serving cardboard box side salad fermented cabbage would balk tell ive gone far would loss think im going take dragons den instead watson dont know im going cope normal recipes little holiday together youre changing way think food thursday fall celery sour cream parsley lemon taco following yesterdays culinary epiphany keen keep cool head critical eye chef watson decided roadtest one theory article found internet mentioned frequently discarded items american fridges celery sour cream fresh herbs lemons lets dwell much luxury problems aspect cant imagine people everywhere world lamenting amount sour cream flatleaf parsley toss focus instead watson admittedly trickysounding shopping list immediately add shrimp tortillas salsa verde salsa verde recommended unwatsoned recipe courtesy bon appetit fantastic nothing like salsa verde know love capers dill pickles anchovies iteration required bit simmer superspicy delicious cheat use normal tomatoes instead tomatillos dont think made huge difference marinade shrimp unusual like lot watson recommends used ton butter hefty wallop old friend kosher salt ive worked chef several years unfazed appearance salt butter recipes theyre make things taste nice however theres getting away fact bought stick butter start week already gone assembled tacos good uncontroversial dining companion deemed salsa bit spicy liked kick gave dish sour cream calmed bit struck bit shame fire barbecue two minutes worth cooking time may sun shining heck recipe absurd yesterdays absolutely memorable sadly dont think would make im sorry watson probably tacos good ultimately worth prep hassle friday mexican mushroom lasagna start dont want get impression love affair reached height passion wednesday watson absolutely isnt consistently impressed softwares intelligence ease use audacity suggestions flavormatching incredible really works probably wont save money wont make thin wont teach actually cook stuff work stage distinctly impressive worthwhile project give go prepared coax something workable every today took long time find meatfree recipe didnt came contain sort meat selected meat option didnt want include took recipe sausage lasagne oneandahalf pounds sausage removed sausage replaced turkey mince maybe someone needs tell watson neither sausages turkeys grow trees much tinkering submitting resubmitting recipe ended lasagne topped sort creamy mashed potato sauce easy profoundly smart use ingredients lasagne worlds aesthetically appealing dish astonishingly flavored weeks revelations dont think ill making cheese sauce way point onwards top marks essence kind sums watson need tinker bit find something usable may need make want put mashed potato lasagne leap faith youre going actually go want apps full benefit youll consume lot dairy products might find daydreaming nice simple unadorned salads decide go allin suggestions ai tell us make pizza cod ginger radishes know going taste amazing one gladly suggest workable recipe blood sausage porridge walk without much hassle gives crazy option ingredient designed make lives food enthusiasts interesting earth watson going good friends point forward even dont speak every day cant wait introduce others though im going consume smoothies week seriously even look butter next days im probably going puke fall medium get next exploring future food means us get latest join conversation follow future food quick cheer standing ovation clap show much enjoyed story inspiring stories people places building future created steven johnson edited ian steadman duncan geere anjali ramachandran elizabeth minkel supported gates foundation,en,"['IBM', 'Watson', 'Google', 'A.I.', 'app', 'A.I. mélange', 'Morels', 'Wastson', 'Diner Cod Pizza', 'un', 'the Gates Foundation']"
94,Tanay Jaipuria,1100,Self-driving cars and the Trolley problem – Tanay Jaipuria – Medium,"Google recently announced that their self-driving car has driven more than a million miles. According to Morgan Stanley, self-driving cars will be commonplace in society by ~2025. This got me thinking about the ethics and philosophy behind these cars, which is what the piece is about.
In 1942, Isaac Asimov introduced three laws of robotics in his short story “Runaround”.
They were as follows:
He later added a fourth law, the zeroth law:
0. A robot may not harm humanity, or, by inaction, allow humanity to come to harm.
Though fictional, they provide a good philosophical grounding of how AI can coexist with society. If self driving cars, were to follow them, we’re in a pretty good spot right? (Let’s leave aside the argument that self-driving cars lead to loss of jobs of taxi drivers and truck drivers and so should not exist per the 0th/1st law)
However, there’s one problem which the laws of robotics don’t quite address.
It’s a famous thought experiment in philosophy called the Trolley Problem and goes as follows:
It’s not hard to see how a similar situation would come up in a world with self-driving cars, with the car having to make a similar decision.
Say for example a human-driven car runs a red light and a self-driving car has two options:
What should the car do?
From a utilitarian perspective, the answer is obvious: to turn right (or “pull the lever”) leading to the death of only one person as opposed to five.
Incidentally, in a survey of professional philosophers on the Trolley Problem, 68.2% agreed, saying that one should pull the lever. So maybe this “problem” isn’t a problem at all and the answer is to simply do the Utilitarian thing that “greatest happiness to the greatest number”.
But can you imagine a world in which your life could be sacrificed at any moment for no wrongdoing to save the lives of two others?
Now consider this version of the trolley problem involving a fat man:
Most people that go the utilitarian route in the initial problem say they wouldn’t push the fat man. But from a utilitarian perspective there is no difference between this and the initial problem — so why do they change their mind? And is the right answer to “stay the course” then?
Kant’s categorical imperative goes some way to explaining it:
In simple words, it says that we shouldn’t merely use people as means to an end. And so, killing someone for the sole purpose of saving others is not okay, and would be a no-no by Kant’s categorical imperative.
Another issue with utilitarianism is that it is a bit naive, at least how we defined it. The world is complex, and so the answer is rarely as simple as perform the action that saves the most people. What if, going back to the example of the car, instead of a family of five, inside the car that ran the red light were five bank robbers speeding after robbing a bank. And sat in the other car was a prominent scientist who had just made a breakthrough in curing cancer. Would you still want the car to perform the action that simply saves the most people?
So may be we fix that by making the definition of Utilitarianism more intricate, in that we assign a value to each individuals life. In that case the right answer could still be to kill the five robbers, if say our estimate of utility of the scientist’s life was more than that of the five robbers.
But can you imagine a world in which say Google or Apple places a value on each of our lives, which could be used at any moment of time to turn a car into us to save others? Would you be okay with that?
And so there you have it, though the answer seems simple, it is anything but, which is what makes the problem so interesting and so hard. It will be a question that comes up time and time again as self-driving cars become a reality. Google, Apple, Uber etc. will probably have to come up with an answer. To pull, or not to pull?
Lastly, I want to leave you another question that will need to be answered, that of ownership. Say a self-driving car which has one passenger in it, the “owner”, skids in the rain and is going to crash into a car in front, pushing that car off a cliff. It can either take a sharp turn and fall of the cliff or continue going straight leading to the other car falling of the cliff. Both cars have one passenger. What should the car do? Should it favor the person that bought it — its owner?
Thanks for reading! Feel free to share this post and leave a note/write a response to share your thoughts. I’m tanayj on twitter if you want to discuss further!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Product @Facebook. Previously @McKinsey. I like tech, econ, strategy and @Manutd. Views and banter my own.
",google recently announced selfdriving car driven million miles according morgan stanley selfdriving cars commonplace society 2025 got thinking ethics philosophy behind cars piece 1942 isaac asimov introduced three laws robotics short story runaround follows later added fourth law zeroth law 0 robot may harm humanity inaction allow humanity come harm though fictional provide good philosophical grounding ai coexist society self driving cars follow pretty good spot right lets leave aside argument selfdriving cars lead loss jobs taxi drivers truck drivers exist per 0th1st law however theres one problem laws robotics dont quite address famous thought experiment philosophy called trolley problem goes follows hard see similar situation would come world selfdriving cars car make similar decision say example humandriven car runs red light selfdriving car two options car utilitarian perspective answer obvious turn right pull lever leading death one person opposed five incidentally survey professional philosophers trolley problem 682 agreed saying one pull lever maybe problem isnt problem answer simply utilitarian thing greatest happiness greatest number imagine world life could sacrificed moment wrongdoing save lives two others consider version trolley problem involving fat man people go utilitarian route initial problem say wouldnt push fat man utilitarian perspective difference initial problem change mind right answer stay course kants categorical imperative goes way explaining simple words says shouldnt merely use people means end killing someone sole purpose saving others okay would nono kants categorical imperative another issue utilitarianism bit naive least defined world complex answer rarely simple perform action saves people going back example car instead family five inside car ran red light five bank robbers speeding robbing bank sat car prominent scientist made breakthrough curing cancer would still want car perform action simply saves people may fix making definition utilitarianism intricate assign value individuals life case right answer could still kill five robbers say estimate utility scientists life five robbers imagine world say google apple places value lives could used moment time turn car us save others would okay though answer seems simple anything makes problem interesting hard question comes time time selfdriving cars become reality google apple uber etc probably come answer pull pull lastly want leave another question need answered ownership say selfdriving car one passenger owner skids rain going crash car front pushing car cliff either take sharp turn fall cliff continue going straight leading car falling cliff cars one passenger car favor person bought owner thanks reading feel free share post leave notewrite response share thoughts im tanayj twitter want discuss quick cheer standing ovation clap show much enjoyed story product facebook previously mckinsey like tech econ strategy manutd views banter,en,"['Google', 'Morgan Stanley', 'Kant', 'Apple']"
95,Milo Spencer-Harper,2200,How to build a multi-layered neural network in Python,"In my last blog post, thanks to an excellent blog post by Andrew Trask, I learned how to build a neural network for the first time. It was super simple. 9 lines of Python code modelling the behaviour of a single neuron.
But what if we are faced with a more difficult problem? Can you guess what the ‘?’ should be?
The trick is to notice that the third column is irrelevant, but the first two columns exhibit the behaviour of a XOR gate. If either the first column or the second column is 1, then the output is 1. However, if both columns are 0 or both columns are 1, then the output is 0.
So the correct answer is 0.
However, this would be too much for our single neuron to handle. This is considered a “nonlinear pattern” because there is no direct one-to-one relationship between the inputs and the output.
Instead, we must create an additional hidden layer, consisting of four neurons (Layer 1). This layer enables the neural network to think about combinations of inputs.
You can see from the diagram that the output of Layer 1 feeds into Layer 2. It is now possible for the neural network to discover correlations between the output of Layer 1 and the output in the training set. As the neural network learns, it will amplify those correlations by adjusting the weights in both layers.
In fact, image recognition is very similar. There is no direct relationship between pixels and apples. But there is a direct relationship between combinations of pixels and apples.
The process of adding more layers to a neural network, so it can think about combinations, is called “deep learning”. Ok, are we ready for the Python code? First I’ll give you the code and then I’ll explain further.
Also available here: https://github.com/miloharper/multi-layer-neural-network
This code is an adaptation from my previous neural network. So for a more comprehensive explanation, it’s worth looking back at my earlier blog post.
What’s different this time, is that there are multiple layers. When the neural network calculates the error in layer 2, it propagates the error backwards to layer 1, adjusting the weights as it goes. This is called “back propagation”.
Ok, let’s try running it using the Terminal command:
python main.py
You should get a result that looks like this:
First the neural network assigned herself random weights to her synaptic connections, then she trained herself using the training set. Then she considered a new situation [1, 1, 0] that she hadn’t seen before and predicted 0.0078876. The correct answer is 0. So she was pretty close!
You might have noticed that as my neural network has become smarter I’ve inadvertently personified her by using “she” instead of “it”.
That’s pretty cool. But the computer is doing lots of matrix multiplication behind the scenes, which is hard to visualise. In my next blog post, I’ll visually represent our neural network with an animated diagram of her neurons and synaptic connections, so we can see her thinking.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Economics at Oxford University. Founder of www.moju.io. Interested in politics and AI.
Technology trends and New Invention? Follow this collection to update the latest trend! [UPDATE] As a collection editor, I don’t have any permission to add your articles in the wild. Please submit your article and I will approve. Also, follow this collection, please.
",last blog post thanks excellent blog post andrew trask learned build neural network first time super simple 9 lines python code modelling behaviour single neuron faced difficult problem guess trick notice third column irrelevant first two columns exhibit behaviour xor gate either first column second column 1 output 1 however columns 0 columns 1 output 0 correct answer 0 however would much single neuron handle considered nonlinear pattern direct onetoone relationship inputs output instead must create additional hidden layer consisting four neurons layer 1 layer enables neural network think combinations inputs see diagram output layer 1 feeds layer 2 possible neural network discover correlations output layer 1 output training set neural network learns amplify correlations adjusting weights layers fact image recognition similar direct relationship pixels apples direct relationship combinations pixels apples process adding layers neural network think combinations called deep learning ok ready python code first ill give code ill explain also available httpsgithubcommiloharpermultilayerneuralnetwork code adaptation previous neural network comprehensive explanation worth looking back earlier blog post whats different time multiple layers neural network calculates error layer 2 propagates error backwards layer 1 adjusting weights goes called back propagation ok lets try running using terminal command python mainpy get result looks like first neural network assigned random weights synaptic connections trained using training set considered new situation 1 1 0 hadnt seen predicted 00078876 correct answer 0 pretty close might noticed neural network become smarter ive inadvertently personified using instead thats pretty cool computer lots matrix multiplication behind scenes hard visualise next blog post ill visually represent neural network animated diagram neurons synaptic connections see thinking quick cheer standing ovation clap show much enjoyed story studied economics oxford university founder wwwmojuio interested politics ai technology trends new invention follow collection update latest trend update collection editor dont permission add articles wild please submit article approve also follow collection please,en,"['XOR', 'Python', 'Studied Economics', 'Oxford University', 'New Invention', 'UPDATE']"
96,Ben Brown,1100,Start automating your business tasks with Slack – Howdy,"If you haven’t read about it in the Times or heard about it on NPR yet,you are soon going to be replaced by a robot at your job. All the jobs we thought were safe because they required experience and nuance can now be done by computers. Martin Ford, author of the book the Times and NPR are reporting on, calls it “the threat of a jobless future.” A future where computers write our newspaper articles, create our legal contracts, and compose our symphonies.
Automating this type of complicated, quasi-creative task is really impressive. It requires super computers and uses the forefront of artificial intelligence to achieve this shocking result. It requires tons of data and lots of programming using advanced systems not available to ordinary people.
But not everything requires deep learning. Some of the things we do in our every day lives, especially at our jobs, can be automated. Though it used to be the domain of the geek, scripting and automation is invading all aspects of the workplace. Workers and organizations who can master scripting and automation will gain an edge on those who can’t.
We all have to face the reality that a well built script might be faster and more reliable than we can be at some parts of our jobs. Those of us who can create and wield this type of tool will be able to do better work faster.
Luckily, inside messaging tools like Slack, creating customized, interactive automation tools for business tasks is possible with a little open source code, some cloud tools that are mostly free, and a bit of self reflection.
“Bots” are apps that live alongside users in a chatroom. Users can issue commands to bots by sending messages to them, or by using special keywords in the chatroom. Traditionally, bots have been used for things like server maintenance and running software tests, but now, using the connected devices all around us, nearly anything can be automated and controlled by a bot.
A common task in many technology teams is the stand-up meeting. Everyone stands up, and one at a time, tells the team what they’ve been working on, what they’ve got coming up next, and any problems they are facing. Each person takes a few minutes to speak. In many teams, this is already taking place in a chat room.
If there are 10 people on a team, and each person speaks for just 90 seconds, they’ll spend 15 minutes just bringing people up to speed. Nothing has been discussed, no problems have yet been solved.
What happens if this process is automated using a “bot” in an environment like Slack?
A stand-up is triggered — automatically, or by a project manager.
Using a flexible script, the bot simultaneously reaches out to every member of the team via a private message on Slack. The bot has an interactive conversation with each team member in parallel and collects everyone’s responses.
Everyone still spends 90 seconds talking about their work, but now it is the same 90 seconds.
The bot, now finished collecting the checkin responses, shares its report with all the stakeholders. Just 2 minutes into the meeting, everyone involved has a single document to look at that contains the up to date status of the project.
The team gains 13 minutes during which they can discuss this information, clear blockers, and get back to work. Now, this is admittedly an aggressive application of this approach that won’t work for everyone — some teams may need the sequential listing of updates, some teams may need to actually stand up and use their voices. The point I’m trying to make is that automating things like this exposes ways for the work to be improved, for time to be saved, and for the process to evolve.
What other processes could be automated like this?
What if there was a meeting runner bot that automatically sent out an agenda to all attendees before the meeting, then collected, collated and delivered updates to team members? It could make meetings shorter and more productive by reducing the time needed to bring everyone up to speed.
What if there was an HR bot that could collect performance reviews and feedback?
What if there was a task management bot that could not only manage the creation of tasks and lists, but also create and deliver up to date progress reports to the whole team?
There is a lot to be gained with simple process automation like this! So how can you and your organization benefit from this type of automation tool?
First, you’ll need to commit to adopting a tool like Slack where your team can communicate and use this type of bot. Then, you’ll have to customize Slack to take advantage of built in and custom integrations, which takes some programming — though not much, as there are a ton of open source tools ready to use. An organization like my company XOXCO can help you do this.
Before you can automate something, you have to know the process and be able to write it down in detail. You’ll have to think about all the special cases that occur. Not only will this allow you to build an automation script, it will help you to hone and document the processes by which your business is conducted!
When we do things, we do them one at a time. Robots can do lots of things at once — so once you’ve got your process documented, think about how the steps might be able to run in parallel.
For example, could the bot talk to multiple people at once instead of doing it sequentially?
Since your script can only do what you tell it, you’ll need to plan for the contingencies that might occur while it runs. What if someone doesn’t respond in time? What if information is unavailable? What if a step in the process fails?
Think through these cases and prepare your script to handle them. For example, we built in a 5 minute timeout for our project manager bot — if a user doesn’t respond in 5 minutes, they get a reminder to checkin in person, and their lack of a response is indicated in the report.
This may sound complicated, but when it boils down, we’re just talking about including an ELSE for every IF — a good practice for any software or process to incorporate.
Your bots, once deployed, can become valuable members of your team. Their success is dependent on your team’s desire to use them, and that they provide a better, faster, more reliable way to achieve organizational goals.
Bots should have a user-friendly personality and represent and support company culture. Bots should talk like real people, but not pretend to be real people. Our rule of thumb: try to be as smart as a puppy, which will engender an attitude of forgiveness when the bot does something not quite right.
This type of software automation has been common in certain groups for years. There may already be a software automation expert in your midst. She’s probably part of the server administration team, or the quality assurance group. Right now she works on code deployment, or writes software tests. Go find her, and go put her in a room with a project manager and a content strategist, and see if they can identify and automate the team’s top three time sucking activities in a way that is not only useful but fun to use.
When we start to design software for messaging, the entire application must be boiled down to words, without colors to choose, navigation to click and sidebars to fill with widgets. This can help us not only build better, more useful software, but put simply, requires us to run our businesses in a more organized, documented and well-understood way.
Don’t wait for the Artificial Intelligence explosion to arrive. Start putting these tools to work today.
Update: You can now use a fully realized version of the bot discussed in this post — we’ve launched it under the name Howdy! Add Howdy to your team to run meetings, capture information, and automate common tasks for your team. Read more about our launch here.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I’m a designer and technologist in Austin, Texas. I co-founded XOXCO in 2008.
The official blog of Howdy.ai and Botkit
",havent read times heard npr yetyou soon going replaced robot job jobs thought safe required experience nuance done computers martin ford author book times npr reporting calls threat jobless future future computers write newspaper articles create legal contracts compose symphonies automating type complicated quasicreative task really impressive requires super computers uses forefront artificial intelligence achieve shocking result requires tons data lots programming using advanced systems available ordinary people everything requires deep learning things every day lives especially jobs automated though used domain geek scripting automation invading aspects workplace workers organizations master scripting automation gain edge cant face reality well built script might faster reliable parts jobs us create wield type tool able better work faster luckily inside messaging tools like slack creating customized interactive automation tools business tasks possible little open source code cloud tools mostly free bit self reflection bots apps live alongside users chatroom users issue commands bots sending messages using special keywords chatroom traditionally bots used things like server maintenance running software tests using connected devices around us nearly anything automated controlled bot common task many technology teams standup meeting everyone stands one time tells team theyve working theyve got coming next problems facing person takes minutes speak many teams already taking place chat room 10 people team person speaks 90 seconds theyll spend 15 minutes bringing people speed nothing discussed problems yet solved happens process automated using bot environment like slack standup triggered automatically project manager using flexible script bot simultaneously reaches every member team via private message slack bot interactive conversation team member parallel collects everyones responses everyone still spends 90 seconds talking work 90 seconds bot finished collecting checkin responses shares report stakeholders 2 minutes meeting everyone involved single document look contains date status project team gains 13 minutes discuss information clear blockers get back work admittedly aggressive application approach wont work everyone teams may need sequential listing updates teams may need actually stand use voices point im trying make automating things like exposes ways work improved time saved process evolve processes could automated like meeting runner bot automatically sent agenda attendees meeting collected collated delivered updates team members could make meetings shorter productive reducing time needed bring everyone speed hr bot could collect performance reviews feedback task management bot could manage creation tasks lists also create deliver date progress reports whole team lot gained simple process automation like organization benefit type automation tool first youll need commit adopting tool like slack team communicate use type bot youll customize slack take advantage built custom integrations takes programming though much ton open source tools ready use organization like company xoxco help automate something know process able write detail youll think special cases occur allow build automation script help hone document processes business conducted things one time robots lots things youve got process documented think steps might able run parallel example could bot talk multiple people instead sequentially since script tell youll need plan contingencies might occur runs someone doesnt respond time information unavailable step process fails think cases prepare script handle example built 5 minute timeout project manager bot user doesnt respond 5 minutes get reminder checkin person lack response indicated report may sound complicated boils talking including else every good practice software process incorporate bots deployed become valuable members team success dependent teams desire use provide better faster reliable way achieve organizational goals bots userfriendly personality represent support company culture bots talk like real people pretend real people rule thumb try smart puppy engender attitude forgiveness bot something quite right type software automation common certain groups years may already software automation expert midst shes probably part server administration team quality assurance group right works code deployment writes software tests go find go put room project manager content strategist see identify automate teams top three time sucking activities way useful fun use start design software messaging entire application must boiled words without colors choose navigation click sidebars fill widgets help us build better useful software put simply requires us run businesses organized documented wellunderstood way dont wait artificial intelligence explosion arrive start putting tools work today update use fully realized version bot discussed post weve launched name howdy add howdy team run meetings capture information automate common tasks team read launch quick cheer standing ovation clap show much enjoyed story im designer technologist austin texas cofounded xoxco 2008 official blog howdyai botkit,en,"['Times', 'NPR', 'XOXCO']"
97,Frank Diana,428,Digital Transformation of Business and Society – Frank Diana – Medium,"At a recent KPMG Robotic Innovations event, Futurist and friend Gerd Leonhard delivered a keynote titled “The Digital Transformation of Business and Society: Challenges and Opportunities by 2020”. I highly recommend viewing the Video of his presentation. As Gerd describes, he is a Futurist focused on foresight and observations — not predicting the future. We are at a point in history where every company needs a Gerd Leonhard. For many of the reasons presented in the video, future thinking is rapidly growing in importance. As Gerd so rightly points out, we are still vastly under-estimating the sheer velocity of change.
With regard to future thinking, Gerd used my future scenario slide to describe both the exponential and combinatorial nature of future scenarios — not only do we need to think exponentially, but we also need to think in a combinatorial manner. Gerd mentioned Tesla as a company that really knows how to do this.
He then described our current pivot point of exponential change: a point in history where humanity will change more in the next twenty years than in the previous 300. With that as a backdrop, he encouraged the audience to look five years into the future and spend 3 to 5% of their time focused on foresight. He quoted Peter Drucker (“In times of change the greatest danger is to act with yesterday’s logic”) and stated that leaders must shift from a focus on what is, to a focus on what could be. Gerd added that “wait and see” means “wait and die” (love that by the way). He urged leaders to focus on 2020 and build a plan to participate in that future, emphasizing the question is no longer what-if, but what-when. We are entering an era where the impossible is doable, and the headline for that era is: exponential, convergent, combinatorial, and inter-dependent — words that should be a key part of the leadership lexicon going forward. Here are some snapshots from his presentation:
Gerd then summarized the session as follows:
The future is exponential, combinatorial, and interdependent: the sooner we can adjust our thinking (lateral) the better we will be at designing our future.
My take: Gerd hits on a key point. Leaders must think differently. There is very little in a leader’s collective experience that can guide them through the type of change ahead — it requires us all to think differently
When looking at AI, consider trying IA first (intelligent assistance / augmentation).
My take: These considerations allow us to create the future in a way that avoids unintended consequences. Technology as a supplement, not a replacement
Efficiency and cost reduction based on automation, AI/IA and Robotization are good stories but not the final destination: we need to go beyond the 7-ations and inevitable abundance to create new value that cannot be easily automated.
My take: Future thinking is critical for us to be effective here. We have to have a sense as to where all of this is heading, if we are to effectively create new sources of value
We won’t just need better algorithms — we also need stronger humarithms i.e. values, ethics, standards, principles and social contracts.
My take: Gerd is an evangelist for creating our future in a way that avoids hellish outcomes — and kudos to him for being that voice
“The best way to predict the future is to create it” (Alan Kay).
My Take: our context when we think about the future puts it years away, and that is just not the case anymore. What we think will take ten years is likely to happen in two. We can’t create the future if we don’t focus on it through an exponential lens
Originally published at frankdiana.wordpress.com on September 10, 2015.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
TCS Executive focused on the rapid evolution of society and business. Fascinated by the view of the world in the next decade and beyond https://frankdiana.net/
",recent kpmg robotic innovations event futurist friend gerd leonhard delivered keynote titled digital transformation business society challenges opportunities 2020 highly recommend viewing video presentation gerd describes futurist focused foresight observations predicting future point history every company needs gerd leonhard many reasons presented video future thinking rapidly growing importance gerd rightly points still vastly underestimating sheer velocity change regard future thinking gerd used future scenario slide describe exponential combinatorial nature future scenarios need think exponentially also need think combinatorial manner gerd mentioned tesla company really knows described current pivot point exponential change point history humanity change next twenty years previous 300 backdrop encouraged audience look five years future spend 3 5 time focused foresight quoted peter drucker times change greatest danger act yesterdays logic stated leaders must shift focus focus could gerd added wait see means wait die love way urged leaders focus 2020 build plan participate future emphasizing question longer whatif whatwhen entering era impossible doable headline era exponential convergent combinatorial interdependent words key part leadership lexicon going forward snapshots presentation gerd summarized session follows future exponential combinatorial interdependent sooner adjust thinking lateral better designing future take gerd hits key point leaders must think differently little leaders collective experience guide type change ahead requires us think differently looking ai consider trying ia first intelligent assistance augmentation take considerations allow us create future way avoids unintended consequences technology supplement replacement efficiency cost reduction based automation aiia robotization good stories final destination need go beyond 7ations inevitable abundance create new value cannot easily automated take future thinking critical us effective sense heading effectively create new sources value wont need better algorithms also need stronger humarithms ie values ethics standards principles social contracts take gerd evangelist creating future way avoids hellish outcomes kudos voice best way predict future create alan kay take context think future puts years away case anymore think take ten years likely happen two cant create future dont focus exponential lens originally published frankdianawordpresscom september 10 2015 quick cheer standing ovation clap show much enjoyed story tcs executive focused rapid evolution society business fascinated view world next decade beyond httpsfrankdiananet,en,"['KPMG Robotic Innovations', 'Tesla', 'AI/IA and Robotization', 'frankdiana.wordpress.com', 'TCS Executive']"
98,Rand Hindi,693,How Artificial Intelligence Will Make Technology Disappear,"This is a redacted transcript of a TEDx talk I gave last April at Ecole Polytechnique in France. The video can be seen on Youtube here. Enjoy ;-)
Last March, I was in Costa Rica with my girlfriend, spending our days between beautiful beaches and jungles full of exotic animals. There was barely any connectivity and we were immersed in nature in a way that we could never be in a big city. It felt great.
But in the evening, when we got back to the hotel and connected to the WiFi, our phones would immediately start pushing an entire day’s worth of notifications, constantly interrupting our special time together. It interrupted us while watching the sunset, while sipping a cocktail, while having dinner, while having an intimate moment. It took emotional time away from us.
And it’s not just that our phones vibrated, it’s also that we kept checking them to see if we had received anything, as if we had some sort of compulsive addiction to it. Those rare messages that are highly rewarding, like being notified that Ashton Kutcher just tweeted this article, made consciously “unplugging” impossible.
Just like Pavlov’s dog before us, we had become conditioned. In this case though, it has gotten so out of control that today, 9 out of 10 people experience “phantom vibrations”, which is when you think your phone vibrated in your pocket, whereas in fact it didn’t.
How did this happen?
Back in 1990, we didn’t have any connected devices. This was the “unplugged” era. There were no push notifications, no interruptions, nada. Things were analog, things were human.
Around 1995, the Internet started taking off, and our computers became connected. With it came email, and the infamous “you’ve got mail!” notification. We started getting interrupted by people, companies and spammers sending us electronic messages at random moments.
10 years later, we entered the mobile era. This time, it is not 1, but 3 devices that are connected: a computer, a phone, and a tablet. The trouble is that since these devices don’t know which one you are currently using, the default strategy has been to push all notifications on all devices. Like when someone calls you on your phone, and it also rings on your computer, and actually keeps ringing after you’ve answered it on one of your devices! And it’s not just notifications; accessing a service and finding content is equally frustrating on mobile devices, with those millions of apps and tiny keyboards.
If we take notifications and the need for explicit interactions as a proxy for technological friction, then each connected device adds more of it. Unfortunately, this is about to get much worse, since the number of connected devices is increasing exponentially!
This year, in 2015, we are officially entering what is called the “Internet of Things” era. That’s when your watch, fridge, car and lamps are connected. It is expected that there will be more than 100 billion connected devices by 2025, or 14 for every person on this planet. Just imagine what it will feel like to interact manually and receive notifications simultaneously on 14 devices.. That’s definitely not the future we were promised!
There is hope though. There is hope that Artificial Intelligence will fix this. Not the one Elon Musk refers to that will enslave us all, but rather a human-centric domain of A.I. called “Context-Awareness”, which is about giving devices the ability to adapt to our current situation. It’s about figuring out which device to push notifications on. It’s about figuring out you are late for a meeting and notifying people for you. It’s about figuring out you are on a date and deactivating your non-urgent notifications. It’s about giving you back the freedom to experience the real world again.
When you look at the trend in the capabilities of A.I., what you see it that it takes a bit longer to start, but when it does, it grows much faster. We already have A.I.s that can learn to play video games and beat world champions, so it’s just a matter of time before they reach human level intelligence. There is an inflexion point, and we just crossed it.
Taking the connected devices curve, and subtracting the one for A.I., we see that the overall friction keeps increasing over the next few years until the point where A.I. becomes so capable that this friction flips around and quickly disappears. In this era, called “Ubiquitous Computing”, adding new connected devices does not add friction, it actually adds value!
For example, our phones and computers will be smart enough to know where to route the notifications. Our cars will drive themselves, already knowing the destination. Our beds will be monitoring our sleep, and anticipating when we will be waking up so that we have freshly brewed coffee ready in the kitchen. It will also connect with the accelerometers in our phones and the electricity sockets to determine how many people are in the bed, and adjust accordingly. Our alarm clocks won’t need to be set; they will be connected to our calendars and beds to determine when we fell asleep and when we need to wake up.
All of this can also be aggregated, offering public transport operators access to predicted passenger flows so that there are always enough trains running. Traffic lights will adjust based on self-driving cars’ planned route. Power plants will produce just enough electricity, saving costs and the environment. Smart cities, smart homes, smart grids.. They are all just consequences of having ubiquitous computing!
By the time this happens, technology will have become so deeply integrated in our lives and ourselves that we simply won’t notice it anymore. Artificial Intelligence will have made technology disappear from our consciousness, and the world will feel unplugged again.
I know this sounds crazy, but there are historical examples of other technologies that followed a similar pattern. For example, back in the 1800s, electricity was very tangible. It was expensive, hard to produce, would cut all the time, and was dangerous. You would get electrocuted and your house could catch fire. Back then, people actually believed that oil lamps were safer!
But as electricity matured, it became cheaper, more reliable, and safer. Eventually, it was everywhere, in our walls, lamps, car, phone, and body. It became ubiquitous, and we stopped noticing it. Today, the exact same thing is happening with connected devices.
Building this ubiquitous computing future relies on giving devices the ability to sense and react to the current context, which is called “context-awareness”.
A good way to think about it is through the combination of 4 layers: the device layer, which is about making devices talk to each other; the individual layer, which encompasses everything related to a particular person, such as his location history, calendar, emails or health records; the social layer, which models the relationship between individuals, and finally the environmental layer, which is everything else, such as the weather, the buildings, the streets, trees and cars.
For example, to model the social layer, we can look at the emails that were sent and received by someone, which gives us an indication of social connection strength between a group of people.
The graph shown above is extracted from my professional email account using the MIT Immersion tool, over a period of 6 months. The huge green bubble is one of my co-founder (which sends way too many emails!), as is the red bubble. The other fairly large ones are other people in my team that I work closely with. But what’s interesting is that we can also see who in my network works together, as they will tend to be included together in emails threads and thus form clusters in this graph. If you add some contextual information such as the activity I was engaged in, or the type of language being used in the email, you can determine the nature of the relationship I have with each person (personal, professional, intimate, ..) as well as its degree. And if you now take the difference in these patterns over time, you can detect major events, such as changing jobs, closing an investment round, launching a new product or hiring key people! Of course, all this can be done on social graphs as well as professional ones.
Now that we have a better representation of someone’s social connections, we can use it to perform better natural language processing (NLP) of calendar events by disambiguating events like “Chat with Michael”, which would then assign a higher probability to my co-founder.
But a calendar won’t help us figure out habits such as going to the gym after work, or hanging out in a specific neighborhood on Friday evenings. For that, we need another source of data: geolocation. By monitoring our location over time and detecting the places we have been to, we can understand our habits, and thus, predict what we will be doing next. In fact, knowing the exact place we are at is essential to predict our intentions, since most of the things we do with our devices are based on what we are doing in the real world.
Unfortunately, location is very noisy, and we never know exactly where someone is. For example below, I was having lunch in San Francisco, and this is what my phone recorded while I was not moving. Clearly it is impossible to know where I actually am!
To circumvent this problem, we can score each place according to the current context. For example, we are more likely to be at a restaurant during lunch time than at a nightclub. If we then combine this with a user-specific model based on their location history, we can achieve very high levels of accuracy. For example, if I have been to a Starbucks in the past, it will increase the probability that I am there now, as well as the probability of any other coffee shop.
And because we now know that I am in a restaurant, my devices can surface the apps and information that are relevant to this particular place, such as reviews or mobile payments apps accepted there. If I was at the gym, it would be my sports apps. If I was home, it would be my leisure and home automation apps.
If we combine this timeline of places with the phone’s accelerometer patterns, we can then determine the transportation mode that was taken between those places. With this, our connected watches could now tell us to stand up when it detects we are still, stop at a rest area when it detects we are driving, or tell us where the closest bike stand is when cycling!
These individual transit patterns can then be aggregated over several thousand users to recreate very precise population flow in the city’s infrastructure, as we have done below for Paris.
Not only does it give us an indication of how many people transit in each station, it also give us the route they have been taking, where they changed train or if they walked between stations. Combining this with data from the city — concerts, office and residential buildings, population demographics, ... — enables you to see how each factor impacts public transport, and even predict how many people will be boarding trains throughout the day. It can then be used to notify commuters that they should take a different train if they want to sit on their way home, and dynamically adjust the train schedules, maximizing the efficiency of the network both in terms of energy saved and comfort.
And it’s not just public transport. The same model and data can be used to predict queues in post offices, by taking into account hyperlocal factors such as when the welfare checks are being paid, the bank holidays, the proximity of other post offices and the staff strikes. This is shown below, where the blue curve is the real load, and the orange one is the predicted load.
This model can be used to notify people of the best time to drop and pickup their parcels, which results in better yield management and customer service. It can also be used to plan the construction of new post offices, by sizing them accordingly. And since a post office is just a retail store, everything that works here can work for all retailers: grocery stores, supermarkets, shoe shops, etc.. It could then be plugged into our devices, enabling them to optimize our shopping schedule and make sure we never queue again!
This contextual modeling approach is in fact so powerful that it can even predict the risk of car accidents just by looking at features such as the street topologies, the proximity of bars that just closed, the road surface or the weather. Since these features are generalizable throughout the city, we can make predictions even in places where there was never a car accident!
For example here, we can see that our model correctly detects Trafalgar square as being dangerous, even though nowhere did we explicitly say so. It discovered it automatically from the data itself. It was even able to identify the impact of cultural events, such as St Patrick’s day or New Year’s Eve! How cool would it be if our self-driving cars could take this into account?
If we combine all these different layers — personal, social, environmental — we can recreate a highly contextualized timeline of what we have been doing throughout the day, which in turn enables us to predict what our intentions are.
Making our devices able to figure out our current context and predict our intentions is the key to building truly intelligent products. With that in mind, our team has been prototyping a new kind of smartphone interface, one that leverages this contextual intelligence to anticipate which services and apps are needed at any given time, linking directly to the relevant content inside them. It’s not yet perfect, but it’s a first step towards our long term vision — and it certainly saves a lot of time, swipes and taps!
One thing in particular that we are really proud of is that we were able to build privacy by design (full post coming soon!). It is a tremendous engineering challenge, but we are now running all our algorithms directly on the device. Whether it’s the machine learning classifiers, the signal processing, the natural language processing or the email mining, they are all confined to our smartphones, and never uploaded to our servers. Basically, it means we can now harness the full power of A.I. without compromising our privacy, something that has never been achieved before.
It’s important to understand that this is not just about building some cool tech or the next viral app. Nor is it about making our future look like a science-fiction movie. It’s actually about making technology disappear into the background, so that we can regain the freedom to spend quality time with the people we care about.
If you enjoyed this article, it would really help if you hit recommend below, and shared it on twitter (we are @randhindi & @snips) :-)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Entrepreneur & AI researcher working on Making Technology Disappear. CEO @ snips.ai.  #AI, #privacy and #blockchain. Follow http://Instagram.com/randhindi
This publication features the articles written by the Snips team, fellows, and friends. Snips started as an AI lab in 2013, and now builds Private-by-Design, decentralized, open source voice assistants.
",redacted transcript tedx talk gave last april ecole polytechnique france video seen youtube enjoy last march costa rica girlfriend spending days beautiful beaches jungles full exotic animals barely connectivity immersed nature way could never big city felt great evening got back hotel connected wifi phones would immediately start pushing entire days worth notifications constantly interrupting special time together interrupted us watching sunset sipping cocktail dinner intimate moment took emotional time away us phones vibrated also kept checking see received anything sort compulsive addiction rare messages highly rewarding like notified ashton kutcher tweeted article made consciously unplugging impossible like pavlovs dog us become conditioned case though gotten control today 9 10 people experience phantom vibrations think phone vibrated pocket whereas fact didnt happen back 1990 didnt connected devices unplugged era push notifications interruptions nada things analog things human around 1995 internet started taking computers became connected came email infamous youve got mail notification started getting interrupted people companies spammers sending us electronic messages random moments 10 years later entered mobile era time 1 3 devices connected computer phone tablet trouble since devices dont know one currently using default strategy push notifications devices like someone calls phone also rings computer actually keeps ringing youve answered one devices notifications accessing service finding content equally frustrating mobile devices millions apps tiny keyboards take notifications need explicit interactions proxy technological friction connected device adds unfortunately get much worse since number connected devices increasing exponentially year 2015 officially entering called internet things era thats watch fridge car lamps connected expected 100 billion connected devices 2025 14 every person planet imagine feel like interact manually receive notifications simultaneously 14 devices thats definitely future promised hope though hope artificial intelligence fix one elon musk refers enslave us rather humancentric domain ai called contextawareness giving devices ability adapt current situation figuring device push notifications figuring late meeting notifying people figuring date deactivating nonurgent notifications giving back freedom experience real world look trend capabilities ai see takes bit longer start grows much faster already ais learn play video games beat world champions matter time reach human level intelligence inflexion point crossed taking connected devices curve subtracting one ai see overall friction keeps increasing next years point ai becomes capable friction flips around quickly disappears era called ubiquitous computing adding new connected devices add friction actually adds value example phones computers smart enough know route notifications cars drive already knowing destination beds monitoring sleep anticipating waking freshly brewed coffee ready kitchen also connect accelerometers phones electricity sockets determine many people bed adjust accordingly alarm clocks wont need set connected calendars beds determine fell asleep need wake also aggregated offering public transport operators access predicted passenger flows always enough trains running traffic lights adjust based selfdriving cars planned route power plants produce enough electricity saving costs environment smart cities smart homes smart grids consequences ubiquitous computing time happens technology become deeply integrated lives simply wont notice anymore artificial intelligence made technology disappear consciousness world feel unplugged know sounds crazy historical examples technologies followed similar pattern example back 1800s electricity tangible expensive hard produce would cut time dangerous would get electrocuted house could catch fire back people actually believed oil lamps safer electricity matured became cheaper reliable safer eventually everywhere walls lamps car phone body became ubiquitous stopped noticing today exact thing happening connected devices building ubiquitous computing future relies giving devices ability sense react current context called contextawareness good way think combination 4 layers device layer making devices talk individual layer encompasses everything related particular person location history calendar emails health records social layer models relationship individuals finally environmental layer everything else weather buildings streets trees cars example model social layer look emails sent received someone gives us indication social connection strength group people graph shown extracted professional email account using mit immersion tool period 6 months huge green bubble one cofounder sends way many emails red bubble fairly large ones people team work closely whats interesting also see network works together tend included together emails threads thus form clusters graph add contextual information activity engaged type language used email determine nature relationship person personal professional intimate well degree take difference patterns time detect major events changing jobs closing investment round launching new product hiring key people course done social graphs well professional ones better representation someones social connections use perform better natural language processing nlp calendar events disambiguating events like chat michael would assign higher probability cofounder calendar wont help us figure habits going gym work hanging specific neighborhood friday evenings need another source data geolocation monitoring location time detecting places understand habits thus predict next fact knowing exact place essential predict intentions since things devices based real world unfortunately location noisy never know exactly someone example lunch san francisco phone recorded moving clearly impossible know actually circumvent problem score place according current context example likely restaurant lunch time nightclub combine userspecific model based location history achieve high levels accuracy example starbucks past increase probability well probability coffee shop know restaurant devices surface apps information relevant particular place reviews mobile payments apps accepted gym would sports apps home would leisure home automation apps combine timeline places phones accelerometer patterns determine transportation mode taken places connected watches could tell us stand detects still stop rest area detects driving tell us closest bike stand cycling individual transit patterns aggregated several thousand users recreate precise population flow citys infrastructure done paris give us indication many people transit station also give us route taking changed train walked stations combining data city concerts office residential buildings population demographics enables see factor impacts public transport even predict many people boarding trains throughout day used notify commuters take different train want sit way home dynamically adjust train schedules maximizing efficiency network terms energy saved comfort public transport model data used predict queues post offices taking account hyperlocal factors welfare checks paid bank holidays proximity post offices staff strikes shown blue curve real load orange one predicted load model used notify people best time drop pickup parcels results better yield management customer service also used plan construction new post offices sizing accordingly since post office retail store everything works work retailers grocery stores supermarkets shoe shops etc could plugged devices enabling optimize shopping schedule make sure never queue contextual modeling approach fact powerful even predict risk car accidents looking features street topologies proximity bars closed road surface weather since features generalizable throughout city make predictions even places never car accident example see model correctly detects trafalgar square dangerous even though nowhere explicitly say discovered automatically data even able identify impact cultural events st patricks day new years eve cool would selfdriving cars could take account combine different layers personal social environmental recreate highly contextualized timeline throughout day turn enables us predict intentions making devices able figure current context predict intentions key building truly intelligent products mind team prototyping new kind smartphone interface one leverages contextual intelligence anticipate services apps needed given time linking directly relevant content inside yet perfect first step towards long term vision certainly saves lot time swipes taps one thing particular really proud able build privacy design full post coming soon tremendous engineering challenge running algorithms directly device whether machine learning classifiers signal processing natural language processing email mining confined smartphones never uploaded servers basically means harness full power ai without compromising privacy something never achieved important understand building cool tech next viral app making future look like sciencefiction movie actually making technology disappear background regain freedom spend quality time people care enjoyed article would really help hit recommend shared twitter randhindi snips quick cheer standing ovation clap show much enjoyed story entrepreneur ai researcher working making technology disappear ceo snipsai ai privacy blockchain follow httpinstagramcomrandhindi publication features articles written snips team fellows friends snips started ai lab 2013 builds privatebydesign decentralized open source voice assistants,en,"['Artificial Intelligence', 'A.I.', 'Smart', 'MIT Immersion', 'social graphs', 'NLP', 'Trafalgar', '@randhindi & @snips', 'Entrepreneur & AI', 'Making Technology Disappear', 'http://Instagram.com']"
99,samim,323,Obama-RNN — Machine generated political speeches. – samim – Medium,"Political speeches are among the most powerful tools leaders use to influence entire populations. Throughout history, political speeches have been used to start wars, end empires, fuel movements & inspire the masses.
Political speeches apply many of the tricks found in the field of Social engineering: Congruent communication, intentional body language, Neuro-linguistic programming, HumanBuffer Overflows and more. Read more about The art of human hacking here.
In recent years, Barack Obama has emerged as one of the most memorable and effective political speakers on the world stage. Messages like Hope and Yes we can have clearly left a mark on our collective consciousness. Since 2007, Obama’s highly skilled speech writers have written over 4,3megabytes or 730895 words of text, not counting interviews and debates. All of Obama’s speeches are conveniently readable here.
With powerful artificial Intelligence / machine learning libraries becoming readily available as open-source, it seems obvious to apply them to speech writing. A particularly interesting class of algorithms are Recurrent Neural Networks (RNN).
Recently Andrej Karpathy, a CS PhD student at Stanford has released char-rnn, a Multi-layer Recurrent Neural Networks for character-level language models. The library takes an arbitrary text file as input and learns to predict the next character in the sequence. As the results are pretty amazing, many interesting experiments have sprung up, ranging from composing music, rapping, writing cooking recipes and even re-writing the bible:
Step 1 is to feed the model data, the more the better. For this i wrote a web-crawler in python that gathers all publicly available Obama Speeches, parses out the text and removes any interviews/debates.
Step 2 is to train the model on the collected text. Training an RNN takes a bit of fiddling, as i painfully found out while training a model on 500mb of classical music midi files (mozart-rnn is wild!). Luckily the standard settings that Andrej suggested were a good starting point for the Obama-RNN.
Step 3 is to test the model which automatically generates an unlimited amount of new speeches in the vein of Obama ́s previous speeches. The model can be seeded with a text from which it will start the sequence (e.g. war on terror) and a temperature which makes the output more conservative or diverse, at cost of more mistakes.
Here is a selection of some of my favorite speeches the Obama-RNN generated so far. Keep in mind this is a just a quick hack project. With more time & effort the results can be improved.
One of the most hilarious patterns to emerge, is that the Obama-RNN really loves to politely say: Good afternoon. Good day. God bless you. Good bless the United States of America. Thank you.
I did a test combining Obamas speeches with other famous speeches from the 20st century (including everything from Mother Theresa, Malcom X to Mussolini and Hitler). This gives us an rather insane amalgam of human thought, seen through the “eyes” of a machine. A story for an other day.
On this note: God bless you. Good bless the United States of America. Thank you.
You can run your own Obama-RNN by following these instructions:
Get in touch here: https://twitter.com/samim | http://samim.io/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Designer & Code Magician. Working at the intersection of HCI, Machine Learning & Creativity. Building tools for Enlightenment. Narrative Engineering.
",political speeches among powerful tools leaders use influence entire populations throughout history political speeches used start wars end empires fuel movements inspire masses political speeches apply many tricks found field social engineering congruent communication intentional body language neurolinguistic programming humanbuffer overflows read art human hacking recent years barack obama emerged one memorable effective political speakers world stage messages like hope yes clearly left mark collective consciousness since 2007 obamas highly skilled speech writers written 43megabytes 730895 words text counting interviews debates obamas speeches conveniently readable powerful artificial intelligence machine learning libraries becoming readily available opensource seems obvious apply speech writing particularly interesting class algorithms recurrent neural networks rnn recently andrej karpathy cs phd student stanford released charrnn multilayer recurrent neural networks characterlevel language models library takes arbitrary text file input learns predict next character sequence results pretty amazing many interesting experiments sprung ranging composing music rapping writing cooking recipes even rewriting bible step 1 feed model data better wrote webcrawler python gathers publicly available obama speeches parses text removes interviewsdebates step 2 train model collected text training rnn takes bit fiddling painfully found training model 500mb classical music midi files mozartrnn wild luckily standard settings andrej suggested good starting point obamarnn step 3 test model automatically generates unlimited amount new speeches vein obama previous speeches model seeded text start sequence eg war terror temperature makes output conservative diverse cost mistakes selection favorite speeches obamarnn generated far keep mind quick hack project time effort results improved one hilarious patterns emerge obamarnn really loves politely say good afternoon good day god bless good bless united states america thank test combining obamas speeches famous speeches 20st century including everything mother theresa malcom x mussolini hitler gives us rather insane amalgam human thought seen eyes machine story day note god bless good bless united states america thank run obamarnn following instructions get touch httpstwittercomsamim httpsamimio quick cheer standing ovation clap show much enjoyed story designer code magician working intersection hci machine learning creativity building tools enlightenment narrative engineering,en,"['fuel movements &', 'Social', 'Recurrent Neural Networks', 'CS PhD', 'Stanford', 'Multi', 'Obama Speeches', 'Obamas', 'HCI', 'Machine Learning & Creativity', 'Narrative Engineering']"
100,Chris Dixon,5300,Eleven Reasons To Be Excited About The Future of Technology,"In the year 1820, a person could expect to live less than 35 years, 94% of the global population lived in extreme poverty, and less that 20% of the population was literate. Today, human life expectancy is over 70 years, less that 10% of the global population lives in extreme poverty, and over 80% of people are literate. These improvements are due mainly to advances in technology, beginning in the industrial age and continuing today in the information age.
There are many exciting new technologies that will continue to transform the world and improve human welfare. Here are eleven of them.
Self-driving cars exist today that are safer than human-driven cars in most driving conditions. Over the next 3–5 years they‘ll get even safer, and will begin to go mainstream.
The World Health Organization estimates that 1.25 million people die from car-related injuries per year. Half of the deaths are pedestrians, bicyclists, and motorcyclists hit by cars. Cars are the leading cause of death for people ages 15–29 years old.
Just as cars reshaped the world in the 20th century, so will self-driving cars in the 21st century. In most cities, between 20–30% of usable space is taken up by parking spaces, and most cars are parked about 95% of the time. Self-driving cars will be in almost continuous use (most likely hailed from a smartphone app), thereby dramatically reducing the need for parking. Cars will communicate with one another to avoid accidents and traffic jams, and riders will be able to spend commuting time on other activities like work, education, and socializing.
Attempts to fight climate change by reducing the demand for energy haven’t worked. Fortunately, scientists, engineers, and entrepreneurs have been working hard on the supply side to make clean energy convenient and cost-effective.
Due to steady technological and manufacturing advances, the price of solar cells has dropped 99.5% since 1977. Solar will soon be more cost efficient than fossil fuels. The cost of wind energy has also dropped to an all-time low, and in the last decade represented about a third of newly installed US energy capacity.
Forward thinking organizations are taking advantage of this. For example, in India there is an initiative to convert airports to self-sustaining clean energy.
Tesla is making high-performance, affordable electric cars, and installing electric charging stations worldwide.
There are hopeful signs that clean energy could soon be reaching a tipping point. For example, in Japan, there are now more electric charging stations than gas stations.
And Germany produces so much renewable energy, it sometimes produces even more than it can use.
Computer processors only recently became fast enough to power comfortable and convincing virtual and augmented reality experiences. Companies like Facebook, Google, Apple, and Microsoft are investing billions of dollars to make VR and AR more immersive, comfortable, and affordable.
People sometimes think VR and AR will be used only for gaming, but over time they will be used for all sorts of activities. For example, we’ll use them to manipulate 3-D objects:
To meet with friends and colleagues from around the world:
And even for medical applications, like treating phobias or helping rehabilitate paralysis victims:
VR and AR have been dreamed about by science fiction fans for decades. In the next few years, they’ll finally become a mainstream reality.
GPS started out as a military technology but is now used to hail taxis, get mapping directions, and hunt Pokémon. Likewise, drones started out as a military technology, but are increasingly being used for a wide range of consumer and commercial applications.
For example, drones are being used to inspect critical infrastructure like bridges and power lines, to survey areas struck by natural disasters, and many other creative uses like fighting animal poaching.
Amazon and Google are building drones to deliver household items.
The startup Zipline uses drones to deliver medical supplies to remote villages that can’t be accessed by roads.
There is also a new wave of startups working on flying cars (including two funded by the cofounder of Google, Larry Page).
Flying cars use the same advanced technology used in drones but are large enough to carry people. Due to advances in materials, batteries, and software, flying cars will be significantly more affordable and convenient than today’s planes and helicopters.
Artificial intelligence has made rapid advances in the last decade, due to new algorithms and massive increases in data collection and computing power.
AI can be applied to almost any field. For example, in photography an AI technique called artistic style transfer transforms photographs into the style of a given painter:
Google built an AI system that controls its datacenter power systems, saving hundreds of millions of dollars in energy costs.
The broad promise of AI is to liberate people from repetitive mental tasks the same way the industrial revolution liberated people from repetitive physical tasks.
Some people worry that AI will destroy jobs. History has shown that while new technology does indeed eliminate jobs, it also creates new and better jobs to replace them. For example, with advent of the personal computer, the number of typographer jobs dropped, but the increase in graphic designer jobs more than made up for it.
It is much easier to imagine jobs that will go away than new jobs that will be created. Today millions of people work as app developers, ride-sharing drivers, drone operators, and social media marketers— jobs that didn’t exist and would have been difficult to even imagine ten years ago.
By 2020, 80% of adults on earth will have an internet-connected smartphone. An iPhone 6 has about 2 billion transistors, roughly 625 times more transistors than a 1995 Intel Pentium computer. Today’s smartphones are what used to be considered supercomputers.
Internet-connected smartphones give ordinary people abilities that, just a short time ago, were only available to an elite few:
Protocols are the plumbing of the internet. Most of the protocols we use today were developed decades ago by academia and government. Since then, protocol development mostly stopped as energy shifted to developing proprietary systems like social networks and messaging apps.
Cryptocurrency and blockchain technologies are changing this by providing a new business model for internet protocols. This year alone, hundreds of millions of dollars were raised for a broad range of innovative blockchain-based protocols.
Protocols based on blockchains also have capabilities that previous protocols didn’t. For example, Ethereum is a new blockchain-based protocol that can be used to create smart contracts and trusted databases that are immune to corruption and censorship.
While college tuition skyrockets, anyone with a smartphone can study almost any topic online, accessing educational content that is mostly free and increasingly high-quality.
Encyclopedia Britannica used to cost $1,400. Now anyone with a smartphone can instantly access Wikipedia. You used to have to go to school or buy programming books to learn computer programming. Now you can learn from a community of over 40 million programmers at Stack Overflow. YouTube has millions of hours of free tutorials and lectures, many of which are produced by top professors and universities.
The quality of online education is getting better all the time. For the last 15 years, MIT has been recording lectures and compiling materials that cover over 2000 courses.
As perhaps the greatest research university in the world, MIT has always been ahead of the trends. Over the next decade, expect many other schools to follow MIT’s lead.
Earth is running out of farmable land and fresh water. This is partly because our food production systems are incredibly inefficient. It takes an astounding 1799 gallons of water to produce 1 pound of beef.
Fortunately, a variety of new technologies are being developed to improve our food system.
For example, entrepreneurs are developing new food products that are tasty and nutritious substitutes for traditional foods but far more environmentally friendly. The startup Impossible Foods invented meat products that look and taste like the real thing but are actually made of plants.
Their burger uses 95% less land, 74% less water, and produces 87% less greenhouse gas emissions than traditional burgers. Other startups are creating plant-based replacements for milk, eggs, and other common foods. Soylent is a healthy, inexpensive meal replacement that uses advanced engineered ingredients that are much friendlier to the environment than traditional ingredients.
Some of these products are developed using genetic modification, a powerful scientific technique that has been widely mischaracterized as dangerous. According to a study by the Pew Organization, 88% of scientists think genetically modified foods are safe.
Another exciting development in food production is automated indoor farming. Due to advances in solar energy, sensors, lighting, robotics, and artificial intelligence, indoor farms have become viable alternatives to traditional outdoor farms.
Compared to traditional farms, automated indoor farms use roughly 10 times less water and land. Crops are harvested many more times per year, there is no dependency on weather, and no need to use pesticides.
Until recently, computers have only been at the periphery of medicine, used primarily for research and record keeping. Today, the combination of computer science and medicine is leading to a variety of breakthroughs.
For example, just fifteen years ago, it cost $3B to sequence a human genome. Today, the cost is about a thousand dollars and continues to drop. Genetic sequencing will soon be a routine part of medicine.
Genetic sequencing generates massive amounts of data that can be analyzed using powerful data analysis software. One application is analyzing blood samples for early detection of cancer. Further genetic analysis can help determine the best course of treatment.
Another application of computers to medicine is in prosthetic limbs. Here a young girl is using prosthetic hands she controls using her upper-arm muscles:
Soon we’ll have the technology to control prothetic limbs with just our thoughts using brain-to-machine interfaces.
Computers are also becoming increasingly effective at diagnosing diseases. An artificial intelligence system recently diagnosed a rare disease that human doctors failed to diagnose by finding hidden patterns in 20 million cancer records.
Since the beginning of the space age in the 1950s, the vast majority of space funding has come from governments. But that funding has been in decline: for example, NASA’s budget dropped from about 4.5% of the federal budget in the 1960s to about 0.5% of the federal budget today.
The good news is that private space companies have started filling the void. These companies provide a wide range of products and services, including rocket launches, scientific research, communications and imaging satellites, and emerging speculative business models like asteroid mining.
The most famous private space company is Elon Musk’s SpaceX, which successfully sent rockets into space that can return home to be reused.
Perhaps the most intriguing private space company is Planetary Resources, which is trying to pioneer a new industry: mining minerals from asteroids.
If successful, asteroid mining could lead to a new gold rush in outer space. Like previous gold rushes, this could lead to speculative excess, but also dramatically increased funding for new technologies and infrastructure.
These are just a few of the amazing technologies we’ll see developed in the coming decades. 2016 is just the beginning of a new age of wonders. As futurist Kevin Kelly says:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
www.cdixon.org/about
",year 1820 person could expect live less 35 years 94 global population lived extreme poverty less 20 population literate today human life expectancy 70 years less 10 global population lives extreme poverty 80 people literate improvements due mainly advances technology beginning industrial age continuing today information age many exciting new technologies continue transform world improve human welfare eleven selfdriving cars exist today safer humandriven cars driving conditions next 35 years theyll get even safer begin go mainstream world health organization estimates 125 million people die carrelated injuries per year half deaths pedestrians bicyclists motorcyclists hit cars cars leading cause death people ages 1529 years old cars reshaped world 20th century selfdriving cars 21st century cities 2030 usable space taken parking spaces cars parked 95 time selfdriving cars almost continuous use likely hailed smartphone app thereby dramatically reducing need parking cars communicate one another avoid accidents traffic jams riders able spend commuting time activities like work education socializing attempts fight climate change reducing demand energy havent worked fortunately scientists engineers entrepreneurs working hard supply side make clean energy convenient costeffective due steady technological manufacturing advances price solar cells dropped 995 since 1977 solar soon cost efficient fossil fuels cost wind energy also dropped alltime low last decade represented third newly installed us energy capacity forward thinking organizations taking advantage example india initiative convert airports selfsustaining clean energy tesla making highperformance affordable electric cars installing electric charging stations worldwide hopeful signs clean energy could soon reaching tipping point example japan electric charging stations gas stations germany produces much renewable energy sometimes produces even use computer processors recently became fast enough power comfortable convincing virtual augmented reality experiences companies like facebook google apple microsoft investing billions dollars make vr ar immersive comfortable affordable people sometimes think vr ar used gaming time used sorts activities example well use manipulate 3d objects meet friends colleagues around world even medical applications like treating phobias helping rehabilitate paralysis victims vr ar dreamed science fiction fans decades next years theyll finally become mainstream reality gps started military technology used hail taxis get mapping directions hunt pokemon likewise drones started military technology increasingly used wide range consumer commercial applications example drones used inspect critical infrastructure like bridges power lines survey areas struck natural disasters many creative uses like fighting animal poaching amazon google building drones deliver household items startup zipline uses drones deliver medical supplies remote villages cant accessed roads also new wave startups working flying cars including two funded cofounder google larry page flying cars use advanced technology used drones large enough carry people due advances materials batteries software flying cars significantly affordable convenient todays planes helicopters artificial intelligence made rapid advances last decade due new algorithms massive increases data collection computing power ai applied almost field example photography ai technique called artistic style transfer transforms photographs style given painter google built ai system controls datacenter power systems saving hundreds millions dollars energy costs broad promise ai liberate people repetitive mental tasks way industrial revolution liberated people repetitive physical tasks people worry ai destroy jobs history shown new technology indeed eliminate jobs also creates new better jobs replace example advent personal computer number typographer jobs dropped increase graphic designer jobs made much easier imagine jobs go away new jobs created today millions people work app developers ridesharing drivers drone operators social media marketers jobs didnt exist would difficult even imagine ten years ago 2020 80 adults earth internetconnected smartphone iphone 6 2 billion transistors roughly 625 times transistors 1995 intel pentium computer todays smartphones used considered supercomputers internetconnected smartphones give ordinary people abilities short time ago available elite protocols plumbing internet protocols use today developed decades ago academia government since protocol development mostly stopped energy shifted developing proprietary systems like social networks messaging apps cryptocurrency blockchain technologies changing providing new business model internet protocols year alone hundreds millions dollars raised broad range innovative blockchainbased protocols protocols based blockchains also capabilities previous protocols didnt example ethereum new blockchainbased protocol used create smart contracts trusted databases immune corruption censorship college tuition skyrockets anyone smartphone study almost topic online accessing educational content mostly free increasingly highquality encyclopedia britannica used cost 1400 anyone smartphone instantly access wikipedia used go school buy programming books learn computer programming learn community 40 million programmers stack overflow youtube millions hours free tutorials lectures many produced top professors universities quality online education getting better time last 15 years mit recording lectures compiling materials cover 2000 courses perhaps greatest research university world mit always ahead trends next decade expect many schools follow mits lead earth running farmable land fresh water partly food production systems incredibly inefficient takes astounding 1799 gallons water produce 1 pound beef fortunately variety new technologies developed improve food system example entrepreneurs developing new food products tasty nutritious substitutes traditional foods far environmentally friendly startup impossible foods invented meat products look taste like real thing actually made plants burger uses 95 less land 74 less water produces 87 less greenhouse gas emissions traditional burgers startups creating plantbased replacements milk eggs common foods soylent healthy inexpensive meal replacement uses advanced engineered ingredients much friendlier environment traditional ingredients products developed using genetic modification powerful scientific technique widely mischaracterized dangerous according study pew organization 88 scientists think genetically modified foods safe another exciting development food production automated indoor farming due advances solar energy sensors lighting robotics artificial intelligence indoor farms become viable alternatives traditional outdoor farms compared traditional farms automated indoor farms use roughly 10 times less water land crops harvested many times per year dependency weather need use pesticides recently computers periphery medicine used primarily research record keeping today combination computer science medicine leading variety breakthroughs example fifteen years ago cost 3b sequence human genome today cost thousand dollars continues drop genetic sequencing soon routine part medicine genetic sequencing generates massive amounts data analyzed using powerful data analysis software one application analyzing blood samples early detection cancer genetic analysis help determine best course treatment another application computers medicine prosthetic limbs young girl using prosthetic hands controls using upperarm muscles soon well technology control prothetic limbs thoughts using braintomachine interfaces computers also becoming increasingly effective diagnosing diseases artificial intelligence system recently diagnosed rare disease human doctors failed diagnose finding hidden patterns 20 million cancer records since beginning space age 1950s vast majority space funding come governments funding decline example nasas budget dropped 45 federal budget 1960s 05 federal budget today good news private space companies started filling void companies provide wide range products services including rocket launches scientific research communications imaging satellites emerging speculative business models like asteroid mining famous private space company elon musks spacex successfully sent rockets space return home reused perhaps intriguing private space company planetary resources trying pioneer new industry mining minerals asteroids successful asteroid mining could lead new gold rush outer space like previous gold rushes could lead speculative excess also dramatically increased funding new technologies infrastructure amazing technologies well see developed coming decades 2016 beginning new age wonders futurist kevin kelly says quick cheer standing ovation clap show much enjoyed story wwwcdixonorgabout,en,"['The World Health Organization', 'Facebook, Google', 'Apple', 'Microsoft', 'AR', 'GPS', 'Amazon', 'Google', 'Intel Pentium', 'Protocols', 'MIT', 'Impossible Foods', 'the Pew Organization', 'NASA', 'Planetary Resources']"
101,Slav Ivanov,3900,"The $1700 great Deep Learning box: Assembly, setup and benchmarks","Updated April 2018: Uses CUDA 9, cuDNN 7 and Tensorflow 1.5.
After years of using a thin client in the form of increasingly thinner MacBooks, I had gotten used to it. So when I got into Deep Learning (DL), I went straight for the brand new at the time Amazon P2 cloud servers. No upfront cost, the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself.
However, as time passed, the AWS bills steadily grew larger, even as I switched to 10x cheaper Spot instances. Also, I didn’t find myself training more than one model at a time. Instead, I’d go to lunch/workout/etc. while the model was training, and come back later with a clear head to check on it.
But eventually the model complexity grew and took longer to train. I’d often forget what I did differently on the model that had just completed its 2-day training. Nudged by the great experiences of the other folks on the Fast.AI Forum, I decided to settle down and to get a dedicated DL box at home.
The most important reason was saving time while prototyping models — if they trained faster, the feedback time would be shorter. Thus it would be easier for my brain to connect the dots between the assumptions I had for the model and its results.
Then I wanted to save money — I was using Amazon Web Services (AWS), which offered P2 instances with Nvidia K80 GPUs. Lately, the AWS bills were around $60–70/month with a tendency to get larger. Also, it is expensive to store large datasets, like ImageNet.
And lastly, I haven’t had a desktop for over 10 years and wanted to see what has changed in the meantime (spoiler alert: mostly nothing).
What follows are my choices, inner monologue, and gotchas: from choosing the components to benchmarking.
A sensible budget for me would be about 2 years worth of my current compute spending. At $70/month for AWS, this put it at around $1700 for the whole thing.
You can check out all the components used. The PC Part Picker site is also really helpful in detecting if some of the components don’t play well together.
The GPU is the most crucial component in the box. It will train these deep networks fast, shortening the feedback cycle.
Disclosure: The following are affiliate links, to help me pay for, well, more GPUs.
The choice is between a few of Nvidia’s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The prices might fluctuate, especially because some GPUs are great for cryptocurrency mining (wink, 1070, wink).
On performance side: GTX 1080 Ti and Titan X are similar. Roughly speaking the GTX 1080 is about 25% faster than GTX 1070. And GTX 1080 Ti is about 30% faster than GTX 1080. The new GTX 1070 Ti is very close in performance to GTX 1080.
Tim Dettmers has a great article on picking a GPU for Deep Learning, which he regularly updates as new cards come on the market.
Here are the things to consider when picking a GPU:
Considering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost. I plan to add a second 1080 Ti soonish.
Even though the GPU is the MVP in deep learning, the CPU still matters. For example, data preparation is usually done on the CPU. The number of cores and threads per core is important if we want to parallelize all that data prep.
To stay on budget, I picked a mid-range CPU, the Intel i5 7500. It’s relatively cheap but good enough to not slow things down.
Edit: As a few people have pointed out: “probably the biggest gotcha that is unique to DL/multi-GPU is to pay attention to the PCIe lanes supported by the CPU/motherboard” (by Andrej Karpathy). We want to have each GPU have 16 PCIe lanes so it eats data as fast as possible (16 GB/s for PCIe 3.0). This means that for two cards we need 32 PCIe lanes. However, the CPU I have picked has only 16 lanes. So 2 GPUs would run in 2x8 mode (instead of 2x16). This might be a bottleneck, leading to less than ideal utilization of the graphics cards. Thus a CPU with 40 lines is recommended.
Edit 2: However, Tim Dettmers points out that having 8 lanes per card should only decrease performance by “0–10%” for two GPUs. So currently, my recommendation is: Go with 16 PCIe lanes per video card unless it gets too expensive for you. Otherwise, 8 lanes should do as well.
A good solution with to have for a double GPU machine would be an Intel Xeon processor like the E5–1620 v4 (40 PCIe lanes). Or if you want to splurge go for a higher end processor like the desktop i7–6850K.
Memory (RAM)
It’s nice to have a lot of memory if we are to be working with rather big datasets. I got 2 sticks of 16 GB, for a total of 32 GB of RAM, and plan to buy another 32 GB later.
Following Jeremy Howard’s advice, I got a fast SSD disk to keep my OS and current data on, and then a slow spinning HDD for those huge datasets (like ImageNet).SSD: I remember when I got my first Macbook Air years ago, how blown away was I by the SSD speed. To my delight, a new generation of SSD called NVMe has made its way to market in the meantime. A 480 GB MyDigitalSSD NVMe drive was a great deal. This baby copies files at gigabytes per second. HDD: 2 TB Seagate. While SSDs have been getting fast, HDD have been getting cheap. To somebody who has used Macbooks with 128 GB disk for the last 7 years, having this much space feels almost obscene.
The one thing that I kept in mind when picking a motherboard was the ability to support two GTX 1080 Ti, both in the number of PCI Express Lanes (the minimum is 2x8) and the physical size of 2 cards. Also, make sure it’s compatible with the chosen CPU. An Asus TUF Z270 did it for me.
MSI — X99A SLI PLUS should work great if you got an Intel Xeon CPU.
Rule of thumb: Power supply should provide enough juice for the CPU and the GPUs, plus 100 watts extra. The Intel i5 7500 processor uses 65W, and the GPUs (1080 Ti) need 250W each, so I got a Deepcool 750W Gold PSU (currently unavailable, EVGA 750 GQ is similar). The “Gold” here refers to the power efficiency, i.e how much of the power consumed is wasted as heat.
The case should be the same form factor as the motherboard. Also having enough LEDs to embarrass a Burner is a bonus.
A friend recommended the Thermaltake N23 case, which I promptly got. No LEDs sadly.
Here is how much I spent on all the components (your costs may vary):
$700 GTX 1080 Ti + $190 CPU + $230 RAM + $230 SSD + $66 HDD + $130 Motherboard + $75 PSU + $50 Case ============$1671 Total
Adding tax and fees, this nicely matches my preset budget of $1700.
If you don’t have much experience with hardware and fear you might break something, a professional assembly might be the best option. However, this was a great learning opportunity that I couldn’t pass (even though I’ve had my share of hardware-related horror stories).
The first and important step is to read the installation manuals that came with each component. Especially important for me, as I’ve done this before once or twice, and I have just the right amount of inexperience to mess things up.
This is done before installing the motherboard in the case. Next to the processor there is a lever that needs to be pulled up. The processor is then placed on the base (double-check the orientation). Finally, the lever comes down to fix the CPU in place.
.
.
But I had a quite the difficulty doing this: once the CPU was in position the lever wouldn’t go down. I actually had a more hardware-capable friend of mine video walk me through the process. Turns out the amount of force required to get the lever locked down was more than what I was comfortable with.
Next is fixing the fan on top of the CPU: the fan legs must be fully secured to the motherboard. Consider where the fan cable will go before installing. The processor I had came with thermal paste. If yours doesn’t, make sure to put some paste between the CPU and the cooling unit. Also, replace the paste if you take off the fan.
I put the Power Supply Unit (PSU) in before the motherboard to get the power cables snugly placed in case back side.
.
.
.
.
Pretty straight forward — carefully place it and screw it in. A magnetic screwdriver was really helpful.
Then connect the power cables and the case buttons and LEDs.
.
Just slide it in the M2 slot and screw it in. Piece of cake.
The memory proved quite hard to install, requiring too much effort to properly lock in. A few times I almost gave up, thinking I must be doing it wrong. Eventually one of the sticks clicked in and the other one promptly followed.
At this point, I turned the computer on to make sure it works. To my relief, it started right away!
Finally, the GPU slid in effortlessly. 14 pins of power later and it was running.
NB: Do not plug your monitor in the external card right away. Most probably it needs drivers to function (see below).
Finally, it’s complete!
Now that we have the hardware in place, only the soft part remains. Out with the screwdriver, in with the keyboard.
Note on dual booting: If you plan to install Windows (because, you know, for benchmarks, totally not for gaming), it would be wise to do Windows first and Linux second. I didn’t and had to reinstall Ubuntu because Windows messed up the boot partition. Livewire has a detailed article on dual boot.
Most DL frameworks are designed to work on Linux first, and eventually support other operating systems. So I went for Ubuntu, my default Linux distribution. An old 2GB USB drive was laying around and worked great for the installation. UNetbootin (OSX) or Rufus (Windows) can prepare the Linux thumb drive. The default options worked fine during the Ubuntu install.
At the time of writing, Ubuntu 17.04 was just released, so I opted for the previous version (16.04), whose quirks are much better documented online.
Ubuntu Server or Desktop: The Server and Desktop editions of Ubuntu are almost identical, with the notable exception of the visual interface (called X) not being installed with Server. I installed the Desktop and disabled autostarting X so that the computer would boot it in terminal mode. If needed, one could launch the visual desktop later by typing startx.
Let’s get our install up to date. From Jeremy Howard’s excellent install-gpu script:
To deep learn on our machine, we need a stack of technologies to use our GPU:
Download CUDA from Nvidia, or just run the code below:
Updated to specify version 9 of CUDA. Thanks to @zhanwenchen for the tip. If you need to add later versions of CUDA, click here.
After CUDA has been installed the following code will add the CUDA installation to the PATH variable:
Now we can verify that CUDA has been installed successfully by running
This should have installed the display driver as well. For me, nvidia-smi showed ERR as the device name, so I installed the latest Nvidia drivers (as of May 2018) to fix it:
Removing CUDA/Nvidia drivers
If at any point the drivers or CUDA seem broken (as they did for me — multiple times), it might be better to start over by running:
Since version 1.5 Tensorflow supports CuDNN 7, so we install that. To download CuDNN, one needs to register for a (free) developer account. After downloading, install with the following:
Anaconda is a great package manager for python. I’ve moved to python 3.6, so will be using the Anaconda 3 version:
The popular DL framework by Google. Installation:
Validate Tensorfow install: To make sure we have our stack running smoothly, I like to run the tensorflow MNIST example:
We should see the loss decreasing during training:
Keras is a great high-level neural networks framework, an absolute pleasure to work with. Installation can’t be easier too:
PyTorch is a newcomer in the world of DL frameworks, but its API is modeled on the successful Torch, which was written in Lua. PyTorch feels new and exciting, mostly great, although some things are still to be implemented. We install it by running:
Jupyter is a web-based IDE for Python, which is ideal for data sciency tasks. It’s installed with Anaconda, so we just configure and test it:
Now if we open http://localhost:8888 we should see a Jupyter screen.
Run Jupyter on boot
Rather than running the notebook every time the computer is restarted, we can set it to autostart on boot. We will use crontab to do this, which we can edit by running crontab -e . Then add the following after the last line in the crontab file:
I use my old trusty Macbook Air for development, so I’d like to be able to log into the DL box both from my home network, also when on the run.
SSH Key: It’s way more secure to use a SSH key to login instead of a password. Digital Ocean has a great guide on how to setup this.
SSH tunnel: If you want to access your jupyter notebook from another computer, the recommended way is to use SSH tunneling (instead of opening the notebook to the world and protecting with a password). Let’s see how we can do this:
2. Then to connect over SSH tunnel, run the following script on the client:
To test this, open a browser and try http://localhost:8888 from the remote machine. Your Jupyter notebook should appear.
Setup out-of-network access: Finally to access the DL box from the outside world, we need 3 things:
Setting up out-of-network access depends on the router/network setup, so I’m not going into details.
Now that we have everything running smoothly, let’s put it to the test. We’ll be comparing the newly built box to an AWS P2.xlarge instance, which is what I’ve used so far for DL. The tests are computer vision related, meaning convolutional networks with a fully connected model thrown in. We time training models on: AWS P2 instance GPU (K80), AWS P2 virtual CPU, the GTX 1080 Ti and Intel i5 7500 CPU.
Andres Hernandez points out that my comparison does not use Tensorflow that is optimized for these CPUs, which would have helped the them perform better. Check his insightful comment for more details.
The “Hello World” of computer vision. The MNIST database consists of 70,000 handwritten digits. We run the Keras example on MNIST which uses Multilayer Perceptron (MLP). The MLP means that we are using only fully connected layers, not convolutions. The model is trained for 20 epochs on this dataset, which achieves over 98% accuracy out of the box.
We see that the GTX 1080 Ti is 2.4 times faster than the K80 on AWS P2 in training the model. This is rather surprising as these 2 cards should have about the same performance. I believe this is because of the virtualization or underclocking of the K80 on AWS.
The CPUs perform 9 times slower than the GPUs. As we will see later, it’s a really good result for the processors. This is due to the small model which fails to fully utilize the parallel processing power of the GPUs.
Interestingly, the desktop Intel i5–7500 achieves 2.3x speedup over the virtual CPU on Amazon.
A VGG net will be finetuned for the Kaggle Dogs vs Cats competition. In this competition, we need to tell apart pictures of dogs and cats. Running the model on CPUs for the same number of batches wasn’t feasible. Therefore we finetune for 390 batches (1 epoch) on the GPUs and 10 batches on the CPUs. The code used is on github.
The 1080 Ti is 5.5 times faster that the AWS GPU (K80). The difference in the CPUs performance is about the same as the previous experiment (i5 is 2.6x faster). However, it’s absolutely impractical to use CPUs for this task, as the CPUs were taking ~200x more time on this large model that includes 16 convolutional layers and a couple semi-wide (4096) fully connected layers on top.
A GAN (Generative adversarial network) is a way to train a model to generate images. GAN achieves this by pitting two networks against each other: A Generator which learns to create better and better images, and a Discriminator that tries to tell which images are real and which are dreamt up by the Generator.
The Wasserstein GAN is an improvement over the original GAN. We will use a PyTorch implementation, that is very similar to the one by the WGAN author. The models are trained for 50 steps, and the loss is all over the place which is often the case with GANs. CPUs aren’t considered.
The GTX 1080 Ti finishes 5.5x faster than the AWS P2 K80, which is in line with the previous results.
The final benchmark is on the original Style Transfer paper (Gatys et al.), implemented on Tensorflow (code available). Style Transfer is a technique that combines the style of one image (a painting for example) and the content of another image. Check out my previous post for more details on how Style Transfer works.
The GTX 1080 Ti outperforms the AWS K80 by a factor of 4.3. This time the CPUs are 30-50 times slower than graphics cards. The slowdown is less than on the VGG Finetuning task but more than on the MNIST Perceptron experiment. The model uses mostly the earlier layers of the VGG network, and I suspect this was too shallow to fully utilize the GPUs.
The DL box is in the next room and a large model is training on it. Was it a wise investment? Time will tell but it is beautiful to watch the glowing LEDs in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Entrepreneur / Hacker
Machine learning, Deep learning and other types of learning.
",updated april 2018 uses cuda 9 cudnn 7 tensorflow 15 years using thin client form increasingly thinner macbooks gotten used got deep learning dl went straight brand new time amazon p2 cloud servers upfront cost ability train many models simultaneously general coolness machine learning model slowly teaching however time passed aws bills steadily grew larger even switched 10x cheaper spot instances also didnt find training one model time instead id go lunchworkoutetc model training come back later clear head check eventually model complexity grew took longer train id often forget differently model completed 2day training nudged great experiences folks fastai forum decided settle get dedicated dl box home important reason saving time prototyping models trained faster feedback time would shorter thus would easier brain connect dots assumptions model results wanted save money using amazon web services aws offered p2 instances nvidia k80 gpus lately aws bills around 6070month tendency get larger also expensive store large datasets like imagenet lastly havent desktop 10 years wanted see changed meantime spoiler alert mostly nothing follows choices inner monologue gotchas choosing components benchmarking sensible budget would 2 years worth current compute spending 70month aws put around 1700 whole thing check components used pc part picker site also really helpful detecting components dont play well together gpu crucial component box train deep networks fast shortening feedback cycle disclosure following affiliate links help pay well gpus choice nvidias cards gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti finally titan x prices might fluctuate especially gpus great cryptocurrency mining wink 1070 wink performance side gtx 1080 ti titan x similar roughly speaking gtx 1080 25 faster gtx 1070 gtx 1080 ti 30 faster gtx 1080 new gtx 1070 ti close performance gtx 1080 tim dettmers great article picking gpu deep learning regularly updates new cards come market things consider picking gpu considering picked gtx 1080 ti mainly training speed boost plan add second 1080 ti soonish even though gpu mvp deep learning cpu still matters example data preparation usually done cpu number cores threads per core important want parallelize data prep stay budget picked midrange cpu intel i5 7500 relatively cheap good enough slow things edit people pointed probably biggest gotcha unique dlmultigpu pay attention pcie lanes supported cpumotherboard andrej karpathy want gpu 16 pcie lanes eats data fast possible 16 gbs pcie 30 means two cards need 32 pcie lanes however cpu picked 16 lanes 2 gpus would run 2x8 mode instead 2x16 might bottleneck leading less ideal utilization graphics cards thus cpu 40 lines recommended edit 2 however tim dettmers points 8 lanes per card decrease performance 010 two gpus currently recommendation go 16 pcie lanes per video card unless gets expensive otherwise 8 lanes well good solution double gpu machine would intel xeon processor like e51620 v4 40 pcie lanes want splurge go higher end processor like desktop i76850k memory ram nice lot memory working rather big datasets got 2 sticks 16 gb total 32 gb ram plan buy another 32 gb later following jeremy howards advice got fast ssd disk keep os current data slow spinning hdd huge datasets like imagenetssd remember got first macbook air years ago blown away ssd speed delight new generation ssd called nvme made way market meantime 480 gb mydigitalssd nvme drive great deal baby copies files gigabytes per second hdd 2 tb seagate ssds getting fast hdd getting cheap somebody used macbooks 128 gb disk last 7 years much space feels almost obscene one thing kept mind picking motherboard ability support two gtx 1080 ti number pci express lanes minimum 2x8 physical size 2 cards also make sure compatible chosen cpu asus tuf z270 msi x99a sli plus work great got intel xeon cpu rule thumb power supply provide enough juice cpu gpus plus 100 watts extra intel i5 7500 processor uses 65w gpus 1080 ti need 250w got deepcool 750w gold psu currently unavailable evga 750 gq similar gold refers power efficiency ie much power consumed wasted heat case form factor motherboard also enough leds embarrass burner bonus friend recommended thermaltake n23 case promptly got leds sadly much spent components costs may vary 700 gtx 1080 ti 190 cpu 230 ram 230 ssd 66 hdd 130 motherboard 75 psu 50 case 1671 total adding tax fees nicely matches preset budget 1700 dont much experience hardware fear might break something professional assembly might best option however great learning opportunity couldnt pass even though ive share hardwarerelated horror stories first important step read installation manuals came component especially important ive done twice right amount inexperience mess things done installing motherboard case next processor lever needs pulled processor placed base doublecheck orientation finally lever comes fix cpu place quite difficulty cpu position lever wouldnt go actually hardwarecapable friend mine video walk process turns amount force required get lever locked comfortable next fixing fan top cpu fan legs must fully secured motherboard consider fan cable go installing processor came thermal paste doesnt make sure put paste cpu cooling unit also replace paste take fan put power supply unit psu motherboard get power cables snugly placed case back side pretty straight forward carefully place screw magnetic screwdriver really helpful connect power cables case buttons leds slide m2 slot screw piece cake memory proved quite hard install requiring much effort properly lock times almost gave thinking must wrong eventually one sticks clicked one promptly followed point turned computer make sure works relief started right away finally gpu slid effortlessly 14 pins power later running nb plug monitor external card right away probably needs drivers function see finally complete hardware place soft part remains screwdriver keyboard note dual booting plan install windows know benchmarks totally gaming would wise windows first linux second didnt reinstall ubuntu windows messed boot partition livewire detailed article dual boot dl frameworks designed work linux first eventually support operating systems went ubuntu default linux distribution old 2gb usb drive laying around worked great installation unetbootin osx rufus windows prepare linux thumb drive default options worked fine ubuntu install time writing ubuntu 1704 released opted previous version 1604 whose quirks much better documented online ubuntu server desktop server desktop editions ubuntu almost identical notable exception visual interface called x installed server installed desktop disabled autostarting x computer would boot terminal mode needed one could launch visual desktop later typing startx lets get install date jeremy howards excellent installgpu script deep learn machine need stack technologies use gpu download cuda nvidia run code updated specify version 9 cuda thanks zhanwenchen tip need add later versions cuda click cuda installed following code add cuda installation path variable verify cuda installed successfully running installed display driver well nvidiasmi showed err device name installed latest nvidia drivers may 2018 fix removing cudanvidia drivers point drivers cuda seem broken multiple times might better start running since version 15 tensorflow supports cudnn 7 install download cudnn one needs register free developer account downloading install following anaconda great package manager python ive moved python 36 using anaconda 3 version popular dl framework google installation validate tensorfow install make sure stack running smoothly like run tensorflow mnist example see loss decreasing training keras great highlevel neural networks framework absolute pleasure work installation cant easier pytorch newcomer world dl frameworks api modeled successful torch written lua pytorch feels new exciting mostly great although things still implemented install running jupyter webbased ide python ideal data sciency tasks installed anaconda configure test open httplocalhost8888 see jupyter screen run jupyter boot rather running notebook every time computer restarted set autostart boot use crontab edit running crontab e add following last line crontab file use old trusty macbook air development id like able log dl box home network also run ssh key way secure use ssh key login instead password digital ocean great guide setup ssh tunnel want access jupyter notebook another computer recommended way use ssh tunneling instead opening notebook world protecting password lets see 2 connect ssh tunnel run following script client test open browser try httplocalhost8888 remote machine jupyter notebook appear setup outofnetwork access finally access dl box outside world need 3 things setting outofnetwork access depends routernetwork setup im going details everything running smoothly lets put test well comparing newly built box aws p2xlarge instance ive used far dl tests computer vision related meaning convolutional networks fully connected model thrown time training models aws p2 instance gpu k80 aws p2 virtual cpu gtx 1080 ti intel i5 7500 cpu andres hernandez points comparison use tensorflow optimized cpus would helped perform better check insightful comment details hello world computer vision mnist database consists 70000 handwritten digits run keras example mnist uses multilayer perceptron mlp mlp means using fully connected layers convolutions model trained 20 epochs dataset achieves 98 accuracy box see gtx 1080 ti 24 times faster k80 aws p2 training model rather surprising 2 cards performance believe virtualization underclocking k80 aws cpus perform 9 times slower gpus see later really good result processors due small model fails fully utilize parallel processing power gpus interestingly desktop intel i57500 achieves 23x speedup virtual cpu amazon vgg net finetuned kaggle dogs vs cats competition competition need tell apart pictures dogs cats running model cpus number batches wasnt feasible therefore finetune 390 batches 1 epoch gpus 10 batches cpus code used github 1080 ti 55 times faster aws gpu k80 difference cpus performance previous experiment i5 26x faster however absolutely impractical use cpus task cpus taking 200x time large model includes 16 convolutional layers couple semiwide 4096 fully connected layers top gan generative adversarial network way train model generate images gan achieves pitting two networks generator learns create better better images discriminator tries tell images real dreamt generator wasserstein gan improvement original gan use pytorch implementation similar one wgan author models trained 50 steps loss place often case gans cpus arent considered gtx 1080 ti finishes 55x faster aws p2 k80 line previous results final benchmark original style transfer paper gatys et al implemented tensorflow code available style transfer technique combines style one image painting example content another image check previous post details style transfer works gtx 1080 ti outperforms aws k80 factor 43 time cpus 3050 times slower graphics cards slowdown less vgg finetuning task mnist perceptron experiment model uses mostly earlier layers vgg network suspect shallow fully utilize gpus dl box next room large model training wise investment time tell beautiful watch glowing leds dark hear quiet hum models trying squeeze extra accuracy percentage point quick cheer standing ovation clap show much enjoyed story entrepreneur hacker machine learning deep learning types learning,en,"['MacBooks', 'Amazon', 'AWS', 'Spot', 'AI Forum', 'Amazon Web Services', 'ImageNet', 'GPU', 'Deep Learning', 'MVP', 'CPU', 'Intel', 'the E5–1620 v4', 'RAM', 'SSD', 'TB Seagate', 'Macbooks', 'PCI Express Lanes', 'the Power Supply Unit', 'PSU', 'Ubuntu', 'Desktop', 'CUDA', 'PATH', 'nvidia-smi', 'ERR', 'Google', 'MNIST', 'Keras', 'PyTorch', 'API', 'Torch', 'IDE', 'Python', 'Macbook Air', 'Digital Ocean', 'SSH', 'Multilayer Perceptron', 'MLP', 'VGG', 'Kaggle', 'GPUs', 'GAN', 'WGAN', 'Time']"
102,Geoff Nesnow,14900,73 Mind-Blowing Implications of a Driverless Future,"I originally wrote and published a version of this article in September 2016. Since then, quite a bit has happened, further cementing my view that these changes are coming and that the implications will be even more substantial. I decided it was time to update this article with some additional ideas and a few changes.
As I write this, Uber just announced that it just ordered 24,000 self-driving Volvos. Tesla just released an electric, long-haul tractor trailer with extraordinary technical specs (range, performance) and self-driving capabilities (UPS just preordered 125!). And, Tesla just announced what will probably be the quickest production car ever made — perhaps the fastest. It will go zero to sixty in about the time it takes you to read zero to sixty. And, of course, it will be able to drive itself. The future is quickly becoming now. Google just ordered thousands of Chryslers for its self-driving fleet (that are already on the roads in AZ).
In September of 2016, Uber had just rolled out its first self-driving taxis in Pittsburgh, Tesla and Mercedes were rolling out limited self-driving capabilities and cities around the world were negotiating with companies who want to bring self-driving cars and trucks to their cities. Since then, all of the major car companies have announced significant steps towards mostly or entirely electric vehicles, more investments have been made in autonomous vehicles, driverless trucks now seem to be leading rather than following in terms of the first large scale implementations and there’ve been a few more incidents (i.e. accidents).
I believe that the timeframe for significant adoption of this technology has shrunk in the past year as technology has gotten better faster and as the trucking industry has increased its level of interest and investment.
I believe that my daughter, who is now just over 1 years old, will never have to learn to drive or own a car.
The impact of driverless vehicles will be profound and impact almost every part of our lives.
Below are my updated thoughts about what a driverless future will be like. Some of these updates are from feedback to my original article (thanks to those who contributed!!!), some are based on technology advances in the past year and others are just my own speculations.
What could happen when cars and trucks drive themselves?
1. People won’t own their own cars. Transport will be delivered as a service from companies who own fleets of self-driving vehicles. There are so many technical, economic, safety advantages to the transportation-as-a-service that this change may come much faster than most people expect. Owning a vehicle as an individual will become a novelty for collectors and maybe competitive racers.
2. Software/technology companies will own more of the world’s economy as companies like Uber, Google and Amazon turn transportation into a pay-as-you-go service. Software will indeed eat this world. Over time, they’ll own so much data about people, patterns, routes and obstacles that new entrants will have huge barriers to enter the market
3. Without government intervention (or some sort of organized movement), there will be a tremendous transfer of wealth to a very small number of people who own the software, battery/power manufacturing, vehicle servicing and charging/power generation/maintenance infrastructure. There will be massive consolidation of companies serving these markets as scale and efficiency will become even more valuable. Cars (perhaps they’ll be renamed with some sort-of-clever acronym) will become like the routers that run the Internet — most consumers won’t know or care who made them or who owns them.
4. Vehicle designs will change radically — vehicles won’t need to withstand crashes in the same way, all vehicles will be electric (self-driving + software + service providers = all electric). They may look different, come in very different shapes and sizes, maybe attach to each other in some situations. There will likely be many significant innovations in materials used for vehicle construction — for example, tires and brakes will be re-optimized with very different assumptions, especially around variability of loads and much more controlled environments. The bodies will likely be primarily made of composites (like carbon fiber and fiberglass) and 3D printed. Electric vehicles with no driver controls will require 1/10th or fewer the number of parts (perhaps even 1/100th) and thus will be quicker to produce and require much less labor. There may even be designs with almost no moving parts (other than wheels and motors, obviously).
5. Vehicles will mostly swap batteries rather than serve as the host of battery charging. Batteries will be charged in distributed and highly optimized centers — likely owned by the same company as the vehicles or another national vendor. There may be some entrepreneurial opportunity and a marketplace for battery charging and swapping, but this industry will likely be consolidated quickly. The batteries will be exchanged without human intervention — likely in a carwash-like drive thru
6. Vehicles (being electric) will be able to provide portable power for a variety of purposes (which will also be sold as a service) — construction job sites (why use generators), disaster/power failures, events, etc. They may even temporarily or permanently replace power distribution networks (i.e. power lines) for remote locations — imagine a distributed power generation network with autonomous vehicles providing “last mile” services to some locations
7. Driver’s licenses will slowly go away as will the Department of Motor Vehicles in most states. Other forms of ID may emerge as people no longer carry driver’s licenses. This will probably correspond with the inevitable digitization of all personal identification — via prints, retina scans or other biometric scanning
8. There won’t be any parking lots or parking spaces on roads or in buildings. Garages will be repurposed — maybe as mini loading docks for people and deliveries. Aesthetics of homes and commercial buildings will change as parking lots and spaces go away. There will be a multi-year boom in landscaping and basement and garage conversions as these spaces become available
9. Traffic policing will become redundant. Police transport will also likely change quite a bit. Unmanned police vehicles may become more common and police officers may use commercial transportation to move around routinely. This may dramatically change the nature of policing, with newfound resources from the lack of traffic policing and dramatically less time spent moving around
10. There will be no more local mechanics, car dealers, consumer car washes, auto parts stores or gas stations. Towns that have been built around major thoroughfares will change or fade
11. The auto insurance industry as we know it will go away (as will the significant investing power of the major players of this industry). Most car companies will go out of business, as will most of their enormous supplier networks. There will be many fewer net vehicles on the road (maybe 1/10th, perhaps even less) that are also more durable, made of fewer parts and much more commoditized
12. Traffic lights and signs will become obsolete. Vehicles may not even have headlights as infrared and radar take the place of the human light spectrum. The relationship between pedestrians (and bicycles) and cars and trucks will likely change dramatically. Some will come in the form of cultural and behavioral changes as people travel in groups more regularly and walking or cycling becomes practical in places where it isn’t today
13. Multi-modal transportation will become a more integrated and normal part of our ways of moving around. In other words, we’ll often take one type of vehicle to another, especially when traveling longer distances. With coordination and integration, the elimination of parking and more deterministic patterns, it will become ever-more efficient to combine modes of transport
14. The power grid will change. Power stations via alternative power sources will become more competitive and local. Consumers and small businesses with solar panels, small scale tidal or wave power generators, windmills and other local power generation will be able to sell KiloWattHours to the companies who own the vehicles. This will change “net metering” rules and possibly upset the overall power delivery model. It might even be the beginning of truly distributed power creation and transport. There will likely be a significant boom in innovation in power production and delivery models. Over time, ownership of these services will probably be consolidated across a very small number of companies
15. Traditional petroleum products (and other fossil fuels) will become much less valuable as electric cars replace fuel powered vehicles and as alternative energy sources become more viable with portability of power (transmission and conversion eat tons of power). There are many geopolitical implications to this possible shift. As implications of climate change become ever-clearer and present, these trends will likely accelerate. Petroleum will continue to be valuable for making plastics and other derived materials, but will not be burned for energy at any scale. Many companies, oil-rich countries and investors have already begun accommodating for these changes
16. Entertainment funding will change as the auto industry’s ad spending goes away. Think about how many ads you see or hear about cars, car financing, car insurance, car accessories and car dealers. There are likely to be many other structural and cultural changes that come from the dramatic changes to the transportation industry. We’ll stop saying “shift into high gear” and other driving-related colloquialisms as the references will be lost on future generations
17. The recent corporate tax rate reductions in the “..Act to Provide for Reconciliation Pursuant to Titles II and V of the Concurrent Resolution on the Budget for Fiscal Year 2018” will accelerate investments in automation including self-driving vehicles and other forms of transportation automation. Flush with new cash and incentives to invest capital soon, many businesses will invest in technology and solutions that reduce their labor costs.
18. The car financing industry will go away, as will the newly huge derivative market for packaged sub-prime auto loans which will likely itself cause a version of the 2008–2009 financial crisis as it blows up.
19. Increases in unemployment, increased student loan, vehicle and other debt defaults could quickly spiral into a full depression. The world that emerges on the other side will likely have even more dramatic income and wealth stratification as entry level jobs related to transportation and the entire supply chain of the existing transportation system go away. The convergence of this with hyper-automation in production and service delivery (AI, robotics, low-cost computing, business consolidation, etc) may permanently change how societies are organized and how people spend their time
20. There will be many new innovations in luggage and bags as people no longer keep stuff in cars and loading and unloading packages from vehicles becomes much more automated. The traditional trunk size and shape will change. Trailers or other similar detachable devices will become much more commonplace to add storage space to vehicles. Many additional on demand services will become available as transportation for goods and services becomes more ubiquitous and cheaper. Imagine being able to design, 3D print and put on an outfit as you travel to a party or the office (if you’re still going to an office)...
21. Consumers will have more money as transportation (a major cost, especially for lower income people and families) gets much cheaper and ubiquitous — though this may be offset by dramatic reductions in employment as technology changes many times faster than people’s ability to adapt to new types of work
22. Demand for taxi and truck drivers will go down, eventually to zero. Someone born today might not understand what a truck driver is or even understand why someone would do that job — much like people born in the last 30 years don’t understand how someone could be employed as a switchboard operator
23. The politics will get ugly as lobbyists for the auto and oil industries unsuccessfully try to stop the driverless car. They’ll get even uglier as the federal government deals with assuming huge pension obligations and other legacy costs associated with the auto industry. My guess is that these pension obgligations won’t ultimately be honored and certain communities will be devastated. The same may be true of pollution clean-up efforts around the factories and chemical plants that were once major components of the vehicle supply chain
24. The new players in vehicle design and manufacturing will be a mix of companies like Uber, Google and Amazon and companies you don’t yet know. There will probably be 2 or 3 major players who control >80% of the customer-facing transportation market. There may become API-like access to these networks for smaller players — much like app marketplaces for iPhone and Android. However, the majority of the revenue will flow to a few large players as it does today to Apple and Google for smartphones
25. Supply chains will be disrupted as shipping changes. Algorithms will allow trucks to be fuller. Excess (latent) capacity will be priced cheaper. New middlemen and warehousing models will emerge. As shipping gets cheaper, faster and generally easier, retail storefronts will continue to lose footing in the marketplace.
26. The role of malls and other shopping areas will continue to shift — to be replaced by places people go for services, not products. There will be virtually no face to face purchases of physical goods.
27. Amazon and/or a few other large players will put Fedex, UPS and USPS out of business as their transportation network becomes orders of magnitude more cost efficient than existing models — largely from a lack of legacy costs like pensions, higher union labor costs and regulations (especially USPS) that won’t keep up with the pace of technology change. 3D printing will also contribute to this as many day-to-day products are printed at home rather than purchased.
28. The same vehicles will often transport people and goods as algorithms optimize all routes. And, off-peak utilization will allow for other very inexpensive delivery options. In other words, packages will be increasingly delivered at night. Add autonomous drone aircraft to this mix and there’ll be very little reason to believe that traditional carriers (Fedex, USPS, UPS, etc) will survive at all.
29. Roads will be much emptier and smaller (over time) as self-driving cars need much less space between them (a major cause of traffic today), people will share vehicles more than today (carpooling), traffic flow will be better regulated and algorithmic timing (i.e. leave at 10 versus 9:30) will optimize infrastructure utilization. Roads will also likely be smoother and turns optimally banked for passenger comfort. High speed underground and above ground tunnels (maybe integrating hyperloop technology or this novel magnetic track solution) will become the high speed network for long haul travel.
30. Short hop domestic air travel may be largely displaced by multi-modal travel in autonomous vehicles. This may be countered by the advent of lower cost, more automated air travel. This too may become part of integrated, multi-modal transportation.
31. Roads will wear out much more slowly with fewer vehicle miles, lighter vehicles (with less safety requirements). New road materials will be developed that drain better, last longer and are more environmentally friendly. These materials might even be power generating (solar or reclamation from vehicle kinetic energy). At the extreme, they may even be replaced by radically different designs — tunnels, magnetic tracks, other hyper-optimized materials
32. Premium vehicle services will have more compartmentalized privacy, more comfort, good business features (quiet, wifi, bluetooth for each passenger, etc), massage services and beds for sleeping. They may also allow for meaningful in-transit real and virtual meetings. This will also likely include aromatherapy, many versions of in-vehicle entertainment systems and even virtual passengers to keep you company.
33. Exhilaration and emotion will almost entirely leave transportation. People won’t brag about how nice, fast, comfortable their cars are. Speed will be measured by times between end points, not acceleration, handling or top speed.
34. Cities will become much more dense as fewer roads and vehicles will be needed and transport will be cheaper and more available. The “walkable city” will continue to be more desirable as walking and biking become easier and more commonplace. When costs and timeframes of transit change, so will the dynamics of who lives and works where.
35. People will know when they leave, when they’ll get where they’re going. There will be few excuses for being late. We will be able to leave later and cram more into a day. We’ll also be able to better track kids, spouses, employees and so forth. We’ll be able to know exactly when someone will arrive and when someone needs to leave to be somewhere at a particular time.
36. There will be no more DUI/OUI offenses. Restaurants and bars will sell more alcohol. People will consume more as they no longer need to consider how to get home and will be able to consume inside vehicles
37. We’ll have less privacy as interior cameras and usage logs will track when and where we go and have gone. Exterior cameras will also probably record surroundings, including people. This may have a positive impact on crime, but will open up many complex privacy issues and likely many lawsuits. Some people may find clever ways to game the system — with physical and digital disguises and spoofing.
38. Many lawyers will lose sources of revenue — traffic offenses, crash litigation will reduce dramatically. Litigation will more likely be “big company versus big company” or “individuals against big companies”, not individuals against each other. These will settle more quickly with less variability. Lobbyists will probably succeed in changing the rules of litigation to favor the bigger companies, further reducing the legal revenue related to transportation. Forced arbitration and other similar clauses will become an explicit component of our contractual relationship with transportation providers.
39. Some countries will nationalize parts of their self-driving transportation networks which will result in lower costs, fewer disruptions and less innovation.
40. Cities, towns and police forces will lose revenue from traffic tickets, tolls (likely replaced, if not eliminated) and fuel tax revenues drop precipitously. These will probably be replaced by new taxes (probably on vehicle miles). These may become a major political hot-button issue differentiating parties as there will probably be a range of regressive versus progressive tax models. Most likely, this will be a highly regressive tax in the US, as fuel taxes are today.
41. Some employers and/or government programs will begin partially or entirely subsidizing transportation for employees and/or people who need the help. The tax treatment of this perk will also be very political.
42. Ambulance and other emergency vehicles will likely be used less and change in nature. More people will take regular autonomous vehicles instead of ambulances. Ambulances will transport people faster. Same may be true of military vehicles.
43. There will be significant innovations in first response capabilities as dependencies on people become reduced over time and as distributed staging of capacity becomes more common.
44. Airports will allow vehicles right into the terminals, maybe even onto the tarmac, as increased controls and security become possible. Terminal design may change dramatically as transportation to and from becomes normalized and integrated. The entire nature of air travel may change as integrated, multi-modal transport gets more sophisticated. Hyper-loops, high speed rail, automated aircraft and other forms of rapid travel will gain as traditional hub and spoke air travel on relatively large planes lose ground.
45. Innovative app-like marketplaces will open up for in-transit purchases, ranging from concierge services to food to exercise to merchandise to education to entertainment purchases. VR will likely play a large role in this. With integrated systems, VR (via headsets or screens or holograms) will become standard fare for trips more than a few minutes in duration.
46. Transportation will become more tightly integrated and packaged into many services — dinner includes the ride, hotel includes local transport, etc. This may even extend to apartments, short-term rentals (like AirBnB) and other service providers.
47. Local transport of nearly everything will become ubiquitous and cheap — food, everything in your local stores. Drones will likely be integrated into vehicle designs to deal with “last few feet” on pickup and delivery. This will accelerate the demise of traditional retail stores and their local economic impact.
48. Biking and walking will become easier, safer and more common as roads get safer and less congested, new pathways (reclaimed from roads/parking lots/roadside parking) come online and with cheap, reliable transport available as a backup.
49. More people will participate in vehicle racing (cars, off road, motorcycles) to replace their emotional connection to driving. Virtual racing experiences may also grow in popularity as fewer people have the real experience of driving.
50. Many, many fewer people will be injured or killed on roads, though we’ll expect zero and be disproportionately upset when accidents do happen. Hacking and non-malicious technical issues will replace traffic as the main cause of delays. Over time, resilience will increase in the systems.
51. Hacking of vehicles will be a serious issue. New software and communications companies and technologies will emerge to address these issues. We’ll see the first vehicle hacking and its consequences. Highly distributed computing, perhaps using some form of blockchain, will likely become part of the solution as a counterbalance to systemic catastrophes — such as many vehicles being affected simultaneously. There will probably be a debate about whether and how law enforcement can control, observe and restrict transportation.
52. Many roads and bridges will be privatized as a small number of companies control most transport and make deals with municipalities. Over time, government may entirely stop funding roads, bridges and tunnels. There will be a significant legislative push to privatize more and more of the transportation network. Much like Internet traffic, there will likely become tiers of prioritization and some notion of in-network versus out-of-network travel and tolls for interconnection. Regulators will have a tough time keeping up with these changes. Most of this will be transparent to end users, but will probably create enormous barriers to entry for transportation start-ups and ultimately reduce options for consumers.
53. Innovators will come along with many awesome uses for driveways and garages that no longer contain cars.
54. There will be a new network of clean, safe, pay-to-use restrooms and other services (food, drinks, etc) that become part of the value-add of competing service providers
55. Mobility for seniors and people with disabilities will be greatly improved (over time)
56. Parents will have more options to move around their kids on their own. Premium secure end-to-end children’s transport services will likely emerge. This may change many family relationships and increase the accessibility of services to parents and children. It may also further stratify the experiences of families with higher income and those with lower income.
57. Person to person movement of goods will become cheaper and open up new markets — think about borrowing a tool or buying something on Craigslist. Latent capacity will make transporting goods very inexpensive. This may also open up new opportunities for P2P services at a smaller scale — like preparing food or cleaning clothes.
58. People will be able to eat/drink in transit (like on a train or plane), consume more information (reading, podcasts, video, etc). This will open up time for other activities and perhaps increased productivity.
59. Some people may have their own “pods” to get into which will then be picked up by an autonomous vehicle, moved between vehicles automatically for logistic efficiencies. These may come in varieties of luxury and quality — the Louis Vuitton pod may replace the Louis Vuitton trunk as the mark of luxury travel
60. There will be no more getaway vehicles or police vehicle chases.
61. Vehicles will likely be filled to the brim with advertising of all sorts (much of which you could probably act on in-route), though there will probably be ways to pay more to have an ad free experience. This will include highly personalized en route advertising that is particularly relevant to who you are, where you’re going.
62. These innovations will make it to the developing world where congestion today is often remarkably bad and hugely costly. Pollution levels will come down dramatically. Even more people will move to the cities. Productivity levels will go up. Fortunes will be made as these changes happen. Some countries and cities will be transformed for the better. Some others will likely experience hyper-privatization, consolidation and monopoly-like controls. This may play out much like the roll-out of cell services in these countries — fast, consolidated and inexpensive.
63. Payment options will be greatly expanded, with packaged deals like cell phones, pre-paid models, pay-as-you-go models being offered. Digital currency transacted automatically via phones/devices will probably quickly replace traditional cash or credit card payments.
64. There will likely be some very clever innovations for movement of pets, equipment, luggage and other non-people items. Autonomous vehicles in the medium future (10–20 years) may have radically different designs that support carrying significantly more payload.
65. Some creative marketers will offer to partially or fully subsidize rides where customers deliver value — by taking surveys, by participating in virtual focus groups, by promoting their brand via social media, etc.
66. Sensors of all sorts will be embedded in vehicles that will have secondary uses — like improving weather forecasting, crime detection and prevention, finding fugitives, infrastructure conditions (such as potholes). This data will be monetized, likely by the companies who own the transportation services.
67. Companies like Google and Facebook will add to their databases everything about customer movements and locations. Unlike GPS chips that only tell them where someone is at the moment (and where they’ve been), autonomous vehicle systems will know where you’re going in real-time (and with whom).
68. Autonomous vehicles will create some new jobs and opportunities for entrepreneurs. However, these will be off-set many times by extraordinary job losses by nearly everyone in the transportation value chain today. In the autonomous future, a large number of jobs will go away. This includes drivers (which is in many states today the most common job), mechanics, gas station employees, most of the people who make cars and car parts or support those who do (due to huge consolidation of makers and supply chains and manufacturing automation), the marketing supply chain for vehicles, many people who work on and build roads/bridges, employees of vehicle insurance and financing companies (and their partners/suppliers), toll booth operators (most of whom have already been displaced), many employees of restaurants that support travelers, truck stops, retail workers and all the people whose businesses support these different types of companies and workers.
69. There will be some hardcore hold-outs who really like driving. But, over time, they’ll become a less statistically relevant voting group as younger people, who’ve never driven, will outnumber them. At first, this may be a 50 state regulated system — where driving yourself may actually become illegal in some states in the next 10 years while other states may continue to allow it for a long time. Some states will try, unsuccessfully, to block autonomous vehicles.
70. There will be lots of discussions about new types of economic systems — from universal basic income to new variations of socialism to a more regulated capitalist system — that will result from the enormous impacts of autonomous vehicles.
71. In the path to a truly driverless future, there will be a number of key tipping points. At the moment, freight delivery may push autonomous vehicle use sooner than people transport. Large trucking companies may have the financial means and legislative influence to make rapid, dramatic changes. They are also better positioned to support hybrid approaches where only parts of their fleet or parts of the routes are automated.
72. Autonomous vehicles will radically change the power centers of the world. They will be the beginning of the end of burning hydrocarbons. The powerful interests who control these industries today will fight viciously to stop this. There may even be wars to slow down this process as oil prices start to plummet and demand dries up.
73. Autonomous vehicles will continue to play a larger role in all aspects of war — from surveillance to troop/robot movement to logistics support to actual engagement. Drones will be complemented by additional on-the-ground, in-space, in-the-water and under-the-water autonomous vehicles.
Note: My original article was inspired by a presentation by Ryan Chin, CEO of Optimus Ridespeak at an MIT event about autonomous vehicles. He really got me thinking about how profound these advances could be to our lives. I’m sure some of my thoughts above came from him.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-Founder @mycityatpeace | Faculty @hult_biz | Producer @couragetolisten | Naturally curious dot-connector | More at www.geoffnesnow.com
",originally wrote published version article september 2016 since quite bit happened cementing view changes coming implications even substantial decided time update article additional ideas changes write uber announced ordered 24000 selfdriving volvos tesla released electric longhaul tractor trailer extraordinary technical specs range performance selfdriving capabilities ups preordered 125 tesla announced probably quickest production car ever made perhaps fastest go zero sixty time takes read zero sixty course able drive future quickly becoming google ordered thousands chryslers selfdriving fleet already roads az september 2016 uber rolled first selfdriving taxis pittsburgh tesla mercedes rolling limited selfdriving capabilities cities around world negotiating companies want bring selfdriving cars trucks cities since major car companies announced significant steps towards mostly entirely electric vehicles investments made autonomous vehicles driverless trucks seem leading rather following terms first large scale implementations thereve incidents ie accidents believe timeframe significant adoption technology shrunk past year technology gotten better faster trucking industry increased level interest investment believe daughter 1 years old never learn drive car impact driverless vehicles profound impact almost every part lives updated thoughts driverless future like updates feedback original article thanks contributed based technology advances past year others speculations could happen cars trucks drive 1 people wont cars transport delivered service companies fleets selfdriving vehicles many technical economic safety advantages transportationasaservice change may come much faster people expect owning vehicle individual become novelty collectors maybe competitive racers 2 softwaretechnology companies worlds economy companies like uber google amazon turn transportation payasyougo service software indeed eat world time theyll much data people patterns routes obstacles new entrants huge barriers enter market 3 without government intervention sort organized movement tremendous transfer wealth small number people software batterypower manufacturing vehicle servicing chargingpower generationmaintenance infrastructure massive consolidation companies serving markets scale efficiency become even valuable cars perhaps theyll renamed sortofclever acronym become like routers run internet consumers wont know care made owns 4 vehicle designs change radically vehicles wont need withstand crashes way vehicles electric selfdriving software service providers electric may look different come different shapes sizes maybe attach situations likely many significant innovations materials used vehicle construction example tires brakes reoptimized different assumptions especially around variability loads much controlled environments bodies likely primarily made composites like carbon fiber fiberglass 3d printed electric vehicles driver controls require 110th fewer number parts perhaps even 1100th thus quicker produce require much less labor may even designs almost moving parts wheels motors obviously 5 vehicles mostly swap batteries rather serve host battery charging batteries charged distributed highly optimized centers likely owned company vehicles another national vendor may entrepreneurial opportunity marketplace battery charging swapping industry likely consolidated quickly batteries exchanged without human intervention likely carwashlike drive thru 6 vehicles electric able provide portable power variety purposes also sold service construction job sites use generators disasterpower failures events etc may even temporarily permanently replace power distribution networks ie power lines remote locations imagine distributed power generation network autonomous vehicles providing last mile services locations 7 drivers licenses slowly go away department motor vehicles states forms id may emerge people longer carry drivers licenses probably correspond inevitable digitization personal identification via prints retina scans biometric scanning 8 wont parking lots parking spaces roads buildings garages repurposed maybe mini loading docks people deliveries aesthetics homes commercial buildings change parking lots spaces go away multiyear boom landscaping basement garage conversions spaces become available 9 traffic policing become redundant police transport also likely change quite bit unmanned police vehicles may become common police officers may use commercial transportation move around routinely may dramatically change nature policing newfound resources lack traffic policing dramatically less time spent moving around 10 local mechanics car dealers consumer car washes auto parts stores gas stations towns built around major thoroughfares change fade 11 auto insurance industry know go away significant investing power major players industry car companies go business enormous supplier networks many fewer net vehicles road maybe 110th perhaps even less also durable made fewer parts much commoditized 12 traffic lights signs become obsolete vehicles may even headlights infrared radar take place human light spectrum relationship pedestrians bicycles cars trucks likely change dramatically come form cultural behavioral changes people travel groups regularly walking cycling becomes practical places isnt today 13 multimodal transportation become integrated normal part ways moving around words well often take one type vehicle another especially traveling longer distances coordination integration elimination parking deterministic patterns become evermore efficient combine modes transport 14 power grid change power stations via alternative power sources become competitive local consumers small businesses solar panels small scale tidal wave power generators windmills local power generation able sell kilowatthours companies vehicles change net metering rules possibly upset overall power delivery model might even beginning truly distributed power creation transport likely significant boom innovation power production delivery models time ownership services probably consolidated across small number companies 15 traditional petroleum products fossil fuels become much less valuable electric cars replace fuel powered vehicles alternative energy sources become viable portability power transmission conversion eat tons power many geopolitical implications possible shift implications climate change become everclearer present trends likely accelerate petroleum continue valuable making plastics derived materials burned energy scale many companies oilrich countries investors already begun accommodating changes 16 entertainment funding change auto industrys ad spending goes away think many ads see hear cars car financing car insurance car accessories car dealers likely many structural cultural changes come dramatic changes transportation industry well stop saying shift high gear drivingrelated colloquialisms references lost future generations 17 recent corporate tax rate reductions act provide reconciliation pursuant titles ii v concurrent resolution budget fiscal year 2018 accelerate investments automation including selfdriving vehicles forms transportation automation flush new cash incentives invest capital soon many businesses invest technology solutions reduce labor costs 18 car financing industry go away newly huge derivative market packaged subprime auto loans likely cause version 20082009 financial crisis blows 19 increases unemployment increased student loan vehicle debt defaults could quickly spiral full depression world emerges side likely even dramatic income wealth stratification entry level jobs related transportation entire supply chain existing transportation system go away convergence hyperautomation production service delivery ai robotics lowcost computing business consolidation etc may permanently change societies organized people spend time 20 many new innovations luggage bags people longer keep stuff cars loading unloading packages vehicles becomes much automated traditional trunk size shape change trailers similar detachable devices become much commonplace add storage space vehicles many additional demand services become available transportation goods services becomes ubiquitous cheaper imagine able design 3d print put outfit travel party office youre still going office 21 consumers money transportation major cost especially lower income people families gets much cheaper ubiquitous though may offset dramatic reductions employment technology changes many times faster peoples ability adapt new types work 22 demand taxi truck drivers go eventually zero someone born today might understand truck driver even understand someone would job much like people born last 30 years dont understand someone could employed switchboard operator 23 politics get ugly lobbyists auto oil industries unsuccessfully try stop driverless car theyll get even uglier federal government deals assuming huge pension obligations legacy costs associated auto industry guess pension obgligations wont ultimately honored certain communities devastated may true pollution cleanup efforts around factories chemical plants major components vehicle supply chain 24 new players vehicle design manufacturing mix companies like uber google amazon companies dont yet know probably 2 3 major players control 80 customerfacing transportation market may become apilike access networks smaller players much like app marketplaces iphone android however majority revenue flow large players today apple google smartphones 25 supply chains disrupted shipping changes algorithms allow trucks fuller excess latent capacity priced cheaper new middlemen warehousing models emerge shipping gets cheaper faster generally easier retail storefronts continue lose footing marketplace 26 role malls shopping areas continue shift replaced places people go services products virtually face face purchases physical goods 27 amazon andor large players put fedex ups usps business transportation network becomes orders magnitude cost efficient existing models largely lack legacy costs like pensions higher union labor costs regulations especially usps wont keep pace technology change 3d printing also contribute many daytoday products printed home rather purchased 28 vehicles often transport people goods algorithms optimize routes offpeak utilization allow inexpensive delivery options words packages increasingly delivered night add autonomous drone aircraft mix therell little reason believe traditional carriers fedex usps ups etc survive 29 roads much emptier smaller time selfdriving cars need much less space major cause traffic today people share vehicles today carpooling traffic flow better regulated algorithmic timing ie leave 10 versus 930 optimize infrastructure utilization roads also likely smoother turns optimally banked passenger comfort high speed underground ground tunnels maybe integrating hyperloop technology novel magnetic track solution become high speed network long haul travel 30 short hop domestic air travel may largely displaced multimodal travel autonomous vehicles may countered advent lower cost automated air travel may become part integrated multimodal transportation 31 roads wear much slowly fewer vehicle miles lighter vehicles less safety requirements new road materials developed drain better last longer environmentally friendly materials might even power generating solar reclamation vehicle kinetic energy extreme may even replaced radically different designs tunnels magnetic tracks hyperoptimized materials 32 premium vehicle services compartmentalized privacy comfort good business features quiet wifi bluetooth passenger etc massage services beds sleeping may also allow meaningful intransit real virtual meetings also likely include aromatherapy many versions invehicle entertainment systems even virtual passengers keep company 33 exhilaration emotion almost entirely leave transportation people wont brag nice fast comfortable cars speed measured times end points acceleration handling top speed 34 cities become much dense fewer roads vehicles needed transport cheaper available walkable city continue desirable walking biking become easier commonplace costs timeframes transit change dynamics lives works 35 people know leave theyll get theyre going excuses late able leave later cram day well also able better track kids spouses employees forth well able know exactly someone arrive someone needs leave somewhere particular time 36 duioui offenses restaurants bars sell alcohol people consume longer need consider get home able consume inside vehicles 37 well less privacy interior cameras usage logs track go gone exterior cameras also probably record surroundings including people may positive impact crime open many complex privacy issues likely many lawsuits people may find clever ways game system physical digital disguises spoofing 38 many lawyers lose sources revenue traffic offenses crash litigation reduce dramatically litigation likely big company versus big company individuals big companies individuals settle quickly less variability lobbyists probably succeed changing rules litigation favor bigger companies reducing legal revenue related transportation forced arbitration similar clauses become explicit component contractual relationship transportation providers 39 countries nationalize parts selfdriving transportation networks result lower costs fewer disruptions less innovation 40 cities towns police forces lose revenue traffic tickets tolls likely replaced eliminated fuel tax revenues drop precipitously probably replaced new taxes probably vehicle miles may become major political hotbutton issue differentiating parties probably range regressive versus progressive tax models likely highly regressive tax us fuel taxes today 41 employers andor government programs begin partially entirely subsidizing transportation employees andor people need help tax treatment perk also political 42 ambulance emergency vehicles likely used less change nature people take regular autonomous vehicles instead ambulances ambulances transport people faster may true military vehicles 43 significant innovations first response capabilities dependencies people become reduced time distributed staging capacity becomes common 44 airports allow vehicles right terminals maybe even onto tarmac increased controls security become possible terminal design may change dramatically transportation becomes normalized integrated entire nature air travel may change integrated multimodal transport gets sophisticated hyperloops high speed rail automated aircraft forms rapid travel gain traditional hub spoke air travel relatively large planes lose ground 45 innovative applike marketplaces open intransit purchases ranging concierge services food exercise merchandise education entertainment purchases vr likely play large role integrated systems vr via headsets screens holograms become standard fare trips minutes duration 46 transportation become tightly integrated packaged many services dinner includes ride hotel includes local transport etc may even extend apartments shortterm rentals like airbnb service providers 47 local transport nearly everything become ubiquitous cheap food everything local stores drones likely integrated vehicle designs deal last feet pickup delivery accelerate demise traditional retail stores local economic impact 48 biking walking become easier safer common roads get safer less congested new pathways reclaimed roadsparking lotsroadside parking come online cheap reliable transport available backup 49 people participate vehicle racing cars road motorcycles replace emotional connection driving virtual racing experiences may also grow popularity fewer people real experience driving 50 many many fewer people injured killed roads though well expect zero disproportionately upset accidents happen hacking nonmalicious technical issues replace traffic main cause delays time resilience increase systems 51 hacking vehicles serious issue new software communications companies technologies emerge address issues well see first vehicle hacking consequences highly distributed computing perhaps using form blockchain likely become part solution counterbalance systemic catastrophes many vehicles affected simultaneously probably debate whether law enforcement control observe restrict transportation 52 many roads bridges privatized small number companies control transport make deals municipalities time government may entirely stop funding roads bridges tunnels significant legislative push privatize transportation network much like internet traffic likely become tiers prioritization notion innetwork versus outofnetwork travel tolls interconnection regulators tough time keeping changes transparent end users probably create enormous barriers entry transportation startups ultimately reduce options consumers 53 innovators come along many awesome uses driveways garages longer contain cars 54 new network clean safe paytouse restrooms services food drinks etc become part valueadd competing service providers 55 mobility seniors people disabilities greatly improved time 56 parents options move around kids premium secure endtoend childrens transport services likely emerge may change many family relationships increase accessibility services parents children may also stratify experiences families higher income lower income 57 person person movement goods become cheaper open new markets think borrowing tool buying something craigslist latent capacity make transporting goods inexpensive may also open new opportunities p2p services smaller scale like preparing food cleaning clothes 58 people able eatdrink transit like train plane consume information reading podcasts video etc open time activities perhaps increased productivity 59 people may pods get picked autonomous vehicle moved vehicles automatically logistic efficiencies may come varieties luxury quality louis vuitton pod may replace louis vuitton trunk mark luxury travel 60 getaway vehicles police vehicle chases 61 vehicles likely filled brim advertising sorts much could probably act inroute though probably ways pay ad free experience include highly personalized en route advertising particularly relevant youre going 62 innovations make developing world congestion today often remarkably bad hugely costly pollution levels come dramatically even people move cities productivity levels go fortunes made changes happen countries cities transformed better others likely experience hyperprivatization consolidation monopolylike controls may play much like rollout cell services countries fast consolidated inexpensive 63 payment options greatly expanded packaged deals like cell phones prepaid models payasyougo models offered digital currency transacted automatically via phonesdevices probably quickly replace traditional cash credit card payments 64 likely clever innovations movement pets equipment luggage nonpeople items autonomous vehicles medium future 1020 years may radically different designs support carrying significantly payload 65 creative marketers offer partially fully subsidize rides customers deliver value taking surveys participating virtual focus groups promoting brand via social media etc 66 sensors sorts embedded vehicles secondary uses like improving weather forecasting crime detection prevention finding fugitives infrastructure conditions potholes data monetized likely companies transportation services 67 companies like google facebook add databases everything customer movements locations unlike gps chips tell someone moment theyve autonomous vehicle systems know youre going realtime 68 autonomous vehicles create new jobs opportunities entrepreneurs however offset many times extraordinary job losses nearly everyone transportation value chain today autonomous future large number jobs go away includes drivers many states today common job mechanics gas station employees people make cars car parts support due huge consolidation makers supply chains manufacturing automation marketing supply chain vehicles many people work build roadsbridges employees vehicle insurance financing companies partnerssuppliers toll booth operators already displaced many employees restaurants support travelers truck stops retail workers people whose businesses support different types companies workers 69 hardcore holdouts really like driving time theyll become less statistically relevant voting group younger people whove never driven outnumber first may 50 state regulated system driving may actually become illegal states next 10 years states may continue allow long time states try unsuccessfully block autonomous vehicles 70 lots discussions new types economic systems universal basic income new variations socialism regulated capitalist system result enormous impacts autonomous vehicles 71 path truly driverless future number key tipping points moment freight delivery may push autonomous vehicle use sooner people transport large trucking companies may financial means legislative influence make rapid dramatic changes also better positioned support hybrid approaches parts fleet parts routes automated 72 autonomous vehicles radically change power centers world beginning end burning hydrocarbons powerful interests control industries today fight viciously stop may even wars slow process oil prices start plummet demand dries 73 autonomous vehicles continue play larger role aspects war surveillance trooprobot movement logistics support actual engagement drones complemented additional ontheground inspace inthewater underthewater autonomous vehicles note original article inspired presentation ryan chin ceo optimus ridespeak mit event autonomous vehicles really got thinking profound advances could lives im sure thoughts came quick cheer standing ovation clap show much enjoyed story cofounder mycityatpeace faculty hult_biz producer couragetolisten naturally curious dotconnector wwwgeoffnesnowcom,en,"['Volvos', 'Chryslers', 'Mercedes', 'Amazon', 'the Department of Motor Vehicles', 'ID', 'Garages', 'iPhone', 'Android', 'Apple', 'Google', 'Fedex', 'UPS', 'USPS', 'DUI', 'Transportation', 'Digital', 'Facebook', 'Optimus Ridespeak', 'MIT']"
103,François Chollet,16800,What worries me about AI – François Chollet – Medium,"Disclaimer: These are my own personal views. I do not speak for my employer. If you quote this article, please have the honesty to present these ideas as what they are: personal, speculative opinions, to be judged on their own merits.
If you were around in the 1980s and 1990s, you may remember the now-extinct phenomenon of “computerphobia”. I have personally witnessed it a few times as late as the early 2000s — as personal computers were introduced into our lives, in our workplaces and homes, quite a few people would react with anxiety, fear, or even aggressivity. While some of us were fascinated by computers and awestruck by the potential they could glimpse in them, most people didn’t understand them. They felt alien, abstruse, and in many ways, threatening. People feared getting replaced by technology.
Most of us react to technological shifts with unease at best, panic at worst. Maybe that is true of any change at all. But remarkably, most of what we worry about ends up never happening.
Fast-forward a few years, and the computer-haters have learned to live with them and to use them for their own benefit. Computers did not replace us and trigger mass unemployment — and nowadays we couldn’t imagine life without our laptops, tablets, and smartphones. Threatening change has become comfortable status quo. But at the same time as our fears failed to materialize, computers and the internet have enabled threats that almost no one was warning us about in the 1980s and 1990s. Ubiquitous mass surveillance. Hackers going after our infrastructure or our personal data. Psychological alienation on social media. The loss of our patience and our ability to focus. The political or religious radicalization of easily-influenced minds online. Hostile foreign powers hijacking social networks to disrupt Western democracies.
If most of our fears turn out to be irrational, inversely, most of the truly worrying developments that have happened in the past as a result of technological change stem from things that most people didn’t worry about until it was already there. A hundred years ago, we couldn’t really forecast that the transportation and manufacturing technologies we were developing would enable a new form of industrial warfare that would wipe out tens of millions in two World Wars. We didn’t recognize early on that the invention of the radio would enable a new form of mass propaganda that would facilitate the rise of fascism in Italy and Germany. The progress of theoretical physics in the 1920s and 1930s wasn’t accompanied by anxious press articles about how these developments would soon enable thermonuclear weapons that would place the world forever under the threat of imminent annihilation. And today, even as alarms have been sounding for decades about the most dire problem of our times, climate, a large fraction (44%) of the American public still chooses to ignore it. As a civilization, we seem to be really bad at correctly identifying future threats and rightfully worrying about them, just as we seem to be extremely prone to panic due to irrational fears.
Today, like many times in the past, we are faced with a new wave of radical change: cognitive automation, which could be broadly summed up under the keyword “AI”. And like many time in the past, we are worried that this new set of technologies will harm us — that AI will lead to mass unemployment, or that AI will gain an agency of its own, become superhuman, and choose to destroy us.
But what if we’re worrying about the wrong thing, like we have almost every single time before? What if the real danger of AI was far remote from the “superintelligence” and “singularity” narratives that many are panicking about today? In this post, I’d like to raise awareness about what really worries me when it comes to AI: the highly effective, highly scalable manipulation of human behavior that AI enables, and its malicious use by corporations and governments. Of course, this is not the only tangible risk that arises from the development of cognitive technologies — there are many others, in particular issues related to the harmful biases of machine learning models. Other people are raising awareness of these problems far better than I could. I chose to write about mass population manipulation specifically because I see this risk as pressing and direly under-appreciated.
This risk is already a reality today, and a number of long-term technological trends are going to considerably amplify it over the next few decades. As our lives become increasingly digitized, social media companies get ever greater visibility into our lives and minds. At the same time, they gain increasing access to behavioral control vectors — in particular via algorithmic newsfeeds, which control our information consumption. This casts human behavior as an optimization problem, as an AI problem: it becomes possible for social media companies to iteratively tune their control vectors in order to achieve specific behaviors, just like a game AI would iterative refine its play strategy in order to beat a level, driven by score feedback. The only bottleneck to this process is the intelligence of the algorithm in the loop — and as it happens, the largest social network company is currently investing billions in fundamental AI research.
Let me explain in detail.
In the past 20 years, our private and public lives have moved online. We spend an ever greater fraction of each day staring at screens. Our world is moving to a state where most of what we do consists of digital information consumption, modification, or creation.
A side effect of this long-term trend is that corporations and governments are now collecting staggering amounts of data about us, in particular through social network services. Who we communicate with. What we say. What content we’ve been consuming — images, movies, music, news. What mood we are in at specific times. Ultimately, almost everything we perceive and everything we do will end up recorded on some remote server.
This data, in theory, allows the entities that collect it to build extremely accurate psychological profiles of both individuals and groups. Your opinions and behavior can be cross-correlated with that of thousands of similar people, achieving an uncanny understanding of what makes you tick — probably more predictive than what yourself could achieve through mere introspection (for instance, Facebook “likes” enable algorithms to better assess your personality that your own friends could). This data makes it possible to predict a few days in advance when you will start a new relationship (and with whom), and when you will end your current one. Or who is at risk of suicide. Or which side you will ultimately vote for in an election, even while you’re still feeling undecided. And it’s not just individual-level profiling power — large groups can be even more predictable, as aggregating data points erases randomness and individual outliers.
Passive data collection is not where it ends. Increasingly, social network services are in control of what information we consume. What see in our newsfeeds has become algorithmically “curated”. Opaque social media algorithms get to decide, to an ever-increasing extent, which political articles we read, which movie trailers we see, who we keep in touch with, whose feedback we receive on the opinions we express.
Integrated over many years of exposure, the algorithmic curation of the information we consume gives the algorithms in charge considerable power over our lives — over who we are, who we become. If Facebook gets to decide, over the span of many years, which news you will see (real or fake), whose political status updates you’ll see, and who will see yours, then Facebook is in effect in control of your worldview and your political beliefs.
Facebook’s business lies in influencing people. That’s what the service it sells to its customers — advertisers, including political advertisers. As such, Facebook has built a fine-tuned algorithmic engine that does just that. This engine isn’t merely capable of influencing your view of a brand or your next smart-speaker purchase. It can influence your mood, tuning the content it feeds you in order to make you angry or happy, at will. It may even be able to swing elections.
In short, social network companies can simultaneously measure everything about us, and control the information we consume. And that’s an accelerating trend. When you have access to both perception and action, you’re looking at an AI problem. You can start establishing an optimization loop for human behavior, in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see. A large subset of the field of AI — in particular “reinforcement learning” — is about developing algorithms to solve such optimization problems as efficiently as possible, to close the loop and achieve full control of the target at hand — in this case, us. By moving our lives to the digital realm, we become vulnerable to that which rules it — AI algorithms.
This is made all the easier by the fact that the human mind is highly vulnerable to simple patterns of social manipulation. Consider, for instance, the following vectors of attack:
From an information security perspective, you would call these vulnerabilities: known exploits that can be used to take over a system. In the case of the human minds, these vulnerabilities never get patched, they are just the way we work. They’re in our DNA. The human mind is a static, vulnerable system that will come increasingly under attack from ever-smarter AI algorithms that will simultaneously have a complete view of everything we do and believe, and complete control of the information we consume.
Remarkably, mass population manipulation — in particular political control — arising from placing AI algorithms in charge of our information diet does not necessarily require very advanced AI. You don’t need self-aware, superintelligent AI for this to be a dire threat — current technology may well suffice. Social network companies have been working on it for a few years, with significant results. And while they may only be trying to maximize “engagement” and to influence your purchase decisions, rather than to manipulate your view of the world, the tools they’ve developed are already being hijacked by hostile state actors for political purposes — as seen in the 2016 Brexit referendum or the 2016 US presidential election. This is already our reality. But if mass population manipulation is already possible today — in theory — why hasn’t the world been upended yet?
In short, I think it’s because we’re really bad at AI. But that may be about to change.
Until 2015, all ad targeting algorithms across the industry were running on mere logistic regression. In fact, that’s still true to a large extent today — only the biggest players have switched to more advanced models. Logistic regression, an algorithm that predates the computing era, is one of the most basic techniques you could use for personalization. It is the reason why so many of the ads you see online are desperately irrelevant. Likewise, the social media bots used by hostile state actors to sway public opinion have little to no AI in them. They’re all extremely primitive. For now.
Machine learning and AI have been making fast progress in recent years, and that progress is only beginning to get deployed in targeting algorithms and social media bots. Deep learning has only started to make its way into newsfeeds and ad networks in 2016. Who knows what will be next. It is quite striking that Facebook has been investing enormous amounts in AI research and development, with the explicit goal of becoming a leader in the field. When your product is a social newsfeed, what use are you going to make of natural language processing and reinforcement learning?
We’re looking at a company that builds fine-grained psychological profiles of almost two billion humans, that serves as a primary news source for many of them, that runs large-scale behavior manipulation experiments, and that aims at developing the best AI technology the world has ever seen. Personally, it scares me. And consider that Facebook may not even be the most worrying threat here. Ponder, for instance, China’s use of information control to enable unprecedented forms of totalitarianism, such as its “social credit system”. Many people like to pretend that large corporations are the all-powerful rulers of the modern world, but what power they hold is dwarfed by that of governments. If given algorithmic control over our minds, governments may well turn into far worst actors than corporations.
Now, what can we do about it? How can we defend ourselves? As technologists, what can we do to avert the risk of mass manipulation via our social newsfeeds?
Importantly, the existence of this threat doesn’t mean that all algorithmic curation is bad, or that all targeted advertising is bad. Far from it. Both of these can serve a valuable purpose.
With the rise of the Internet and AI, placing algorithms in charge of our information diet isn’t just an inevitable trend — it’s a desirable one. As our lives become increasingly digital and connected, and as our world becomes increasingly information-intensive, we will need AI to serve as our interface to the world. In the long-run, education and self-development will be some of the most impactful applications of AI — and this will happen through dynamics that almost entirely mirror that of a nefarious AI-enabled newsfeed trying to manipulate you. Algorithmic information management has tremendous potential to help us, to empower individuals to realize more of their potential, and to help society better manage itself.
The issue is not AI itself. The issue is control.
Instead of letting newsfeed algorithms manipulate the user to achieve opaque goals, such as swaying their political opinions, or maximally wasting their time, we should put the user in charge of the goals that the algorithms optimize for. We are talking, after all, about your news, your worldview, your friends, your life — the impact that technology has on you should naturally be placed under your own control. Information management algorithms should not be a mysterious force inflicted on us to serve ends that run opposite to our own interests; instead, they should be a tool in our hand. A tool that we can use for our own purposes, say, for education and personal instead of entertainment.
Here’s an idea — any algorithmic newsfeed with significant adoption should:
We should build AI to serve humans, not to manipulate them for profit or political gain. What if newsfeed algorithms didn’t operate like casino operators or propagandists? What if instead, they were closer to a mentor or a good librarian, someone who used their keen understanding of your psychology — and that of millions of other similar people — to recommend to you that next book that will most resonate with your objectives and make you grow. A sort of navigation tool for your life — an AI capable of guiding you through the optimal path in experience space to get where you want to go. Can you imagine looking at your own life through the lens of a system that has seen millions of lives unfold? Or writing a book together with a system that has read every book? Or conducting research in collaboration with a system that sees the full scope of current human knowledge?
In products where you are fully in control of the AI that interacts with you, a more sophisticated algorithm, instead of being a threat, would be a net positive, letting you achieve your own goals more efficiently.
In summary, our future is one where AI will be our interface to the world — a world made of digital information. This can equally lead to empowering individuals to gain greater control over their lives, or to a total loss of agency. Unfortunately, social media is currently engaged on the wrong road. But it’s still early enough that we can reverse course.
As an industry, we need to develop product categories and markets where the incentives are aligned with placing the user in charge of the algorithms that affect them, instead of using AI to exploit the user’s mind for profit or political gain. We need to strive towards products that are the anti-Facebook.
In the far future, such products will likely take the form of AI assistants. Digital mentors programmed to help you, that put you in control of the objectives they pursue in their interactions with you. And in the present, search engines could be seen as an early, more primitive example of an AI-driven information interface that serves users instead of seeking to hijack their mental space. Search is a tool that you deliberately use to reach specific goals, rather than a passive always-on feed that elects what to show you. You tell it what to it should do for you. And instead of seeking to maximally waste your time, a search engine attempts to minimize the time it takes to go from question to answer, from problem to solution.
You may be thinking, since a search engine is still an AI layer between us and the information we consume, could it bias its results to attempt to manipulate us? Yes, that risk is latent in every information-management algorithm. But in stark contrast with social networks, market incentives in this case are actually aligned with users needs, pushing search engines to be as relevant and objective as possible. If they fail to be maximally useful, there’s essentially no friction for users to move to a competing product. And importantly, a search engine would have a considerably smaller psychological attack surface than a social newsfeed. The threat we’ve profiled in this post requires most of the following to be present in a product:
Most AI-driven information-management products don’t meet these requirements. Social networks, on the other hand, are a frightening combination of risk factors. As technologists, we should gravitate towards products that do not feature these characteristics, and push back against products that combine them all, if only because of their potential for dangerous misuse. Build search engines and digital assistants, not social newsfeeds. Make your recommendation engines transparent, configurable, and constructive, rather than slot-like machines that maximize “engagement” and wasted hours of human time. Invest your UI, UX, and AI expertise into building great configuration panels for your algorithm, to enable your users to use your product on their own terms.
And importantly, we should educate users about these issues, so that they reject manipulative products, generating enough market pressure to align the incentives of the technology industry with that of consumers.
Conclusion: the fork in the road ahead
One path leads to a place that really scares me. The other leads to a more humane future. There’s still time to take the better one. If you work on these technologies, keep this in mind. You may not have evil intentions. You may simply not care. You may simply value your RSUs more than our shared future. But whether or not you care, because you have a hand in shaping the infrastructure of the digital world, your choices affect us all. And you may eventually be held responsible for them.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",disclaimer personal views speak employer quote article please honesty present ideas personal speculative opinions judged merits around 1980s 1990s may remember nowextinct phenomenon computerphobia personally witnessed times late early 2000s personal computers introduced lives workplaces homes quite people would react anxiety fear even aggressivity us fascinated computers awestruck potential could glimpse people didnt understand felt alien abstruse many ways threatening people feared getting replaced technology us react technological shifts unease best panic worst maybe true change remarkably worry ends never happening fastforward years computerhaters learned live use benefit computers replace us trigger mass unemployment nowadays couldnt imagine life without laptops tablets smartphones threatening change become comfortable status quo time fears failed materialize computers internet enabled threats almost one warning us 1980s 1990s ubiquitous mass surveillance hackers going infrastructure personal data psychological alienation social media loss patience ability focus political religious radicalization easilyinfluenced minds online hostile foreign powers hijacking social networks disrupt western democracies fears turn irrational inversely truly worrying developments happened past result technological change stem things people didnt worry already hundred years ago couldnt really forecast transportation manufacturing technologies developing would enable new form industrial warfare would wipe tens millions two world wars didnt recognize early invention radio would enable new form mass propaganda would facilitate rise fascism italy germany progress theoretical physics 1920s 1930s wasnt accompanied anxious press articles developments would soon enable thermonuclear weapons would place world forever threat imminent annihilation today even alarms sounding decades dire problem times climate large fraction 44 american public still chooses ignore civilization seem really bad correctly identifying future threats rightfully worrying seem extremely prone panic due irrational fears today like many times past faced new wave radical change cognitive automation could broadly summed keyword ai like many time past worried new set technologies harm us ai lead mass unemployment ai gain agency become superhuman choose destroy us worrying wrong thing like almost every single time real danger ai far remote superintelligence singularity narratives many panicking today post id like raise awareness really worries comes ai highly effective highly scalable manipulation human behavior ai enables malicious use corporations governments course tangible risk arises development cognitive technologies many others particular issues related harmful biases machine learning models people raising awareness problems far better could chose write mass population manipulation specifically see risk pressing direly underappreciated risk already reality today number longterm technological trends going considerably amplify next decades lives become increasingly digitized social media companies get ever greater visibility lives minds time gain increasing access behavioral control vectors particular via algorithmic newsfeeds control information consumption casts human behavior optimization problem ai problem becomes possible social media companies iteratively tune control vectors order achieve specific behaviors like game ai would iterative refine play strategy order beat level driven score feedback bottleneck process intelligence algorithm loop happens largest social network company currently investing billions fundamental ai research let explain detail past 20 years private public lives moved online spend ever greater fraction day staring screens world moving state consists digital information consumption modification creation side effect longterm trend corporations governments collecting staggering amounts data us particular social network services communicate say content weve consuming images movies music news mood specific times ultimately almost everything perceive everything end recorded remote server data theory allows entities collect build extremely accurate psychological profiles individuals groups opinions behavior crosscorrelated thousands similar people achieving uncanny understanding makes tick probably predictive could achieve mere introspection instance facebook likes enable algorithms better assess personality friends could data makes possible predict days advance start new relationship end current one risk suicide side ultimately vote election even youre still feeling undecided individuallevel profiling power large groups even predictable aggregating data points erases randomness individual outliers passive data collection ends increasingly social network services control information consume see newsfeeds become algorithmically curated opaque social media algorithms get decide everincreasing extent political articles read movie trailers see keep touch whose feedback receive opinions express integrated many years exposure algorithmic curation information consume gives algorithms charge considerable power lives become facebook gets decide span many years news see real fake whose political status updates youll see see facebook effect control worldview political beliefs facebooks business lies influencing people thats service sells customers advertisers including political advertisers facebook built finetuned algorithmic engine engine isnt merely capable influencing view brand next smartspeaker purchase influence mood tuning content feeds order make angry happy may even able swing elections short social network companies simultaneously measure everything us control information consume thats accelerating trend access perception action youre looking ai problem start establishing optimization loop human behavior observe current state targets keep tuning information feed start observing opinions behaviors wanted see large subset field ai particular reinforcement learning developing algorithms solve optimization problems efficiently possible close loop achieve full control target hand case us moving lives digital realm become vulnerable rules ai algorithms made easier fact human mind highly vulnerable simple patterns social manipulation consider instance following vectors attack information security perspective would call vulnerabilities known exploits used take system case human minds vulnerabilities never get patched way work theyre dna human mind static vulnerable system come increasingly attack eversmarter ai algorithms simultaneously complete view everything believe complete control information consume remarkably mass population manipulation particular political control arising placing ai algorithms charge information diet necessarily require advanced ai dont need selfaware superintelligent ai dire threat current technology may well suffice social network companies working years significant results may trying maximize engagement influence purchase decisions rather manipulate view world tools theyve developed already hijacked hostile state actors political purposes seen 2016 brexit referendum 2016 us presidential election already reality mass population manipulation already possible today theory hasnt world upended yet short think really bad ai may change 2015 ad targeting algorithms across industry running mere logistic regression fact thats still true large extent today biggest players switched advanced models logistic regression algorithm predates computing era one basic techniques could use personalization reason many ads see online desperately irrelevant likewise social media bots used hostile state actors sway public opinion little ai theyre extremely primitive machine learning ai making fast progress recent years progress beginning get deployed targeting algorithms social media bots deep learning started make way newsfeeds ad networks 2016 knows next quite striking facebook investing enormous amounts ai research development explicit goal becoming leader field product social newsfeed use going make natural language processing reinforcement learning looking company builds finegrained psychological profiles almost two billion humans serves primary news source many runs largescale behavior manipulation experiments aims developing best ai technology world ever seen personally scares consider facebook may even worrying threat ponder instance chinas use information control enable unprecedented forms totalitarianism social credit system many people like pretend large corporations allpowerful rulers modern world power hold dwarfed governments given algorithmic control minds governments may well turn far worst actors corporations defend technologists avert risk mass manipulation via social newsfeeds importantly existence threat doesnt mean algorithmic curation bad targeted advertising bad far serve valuable purpose rise internet ai placing algorithms charge information diet isnt inevitable trend desirable one lives become increasingly digital connected world becomes increasingly informationintensive need ai serve interface world longrun education selfdevelopment impactful applications ai happen dynamics almost entirely mirror nefarious aienabled newsfeed trying manipulate algorithmic information management tremendous potential help us empower individuals realize potential help society better manage issue ai issue control instead letting newsfeed algorithms manipulate user achieve opaque goals swaying political opinions maximally wasting time put user charge goals algorithms optimize talking news worldview friends life impact technology naturally placed control information management algorithms mysterious force inflicted us serve ends run opposite interests instead tool hand tool use purposes say education personal instead entertainment heres idea algorithmic newsfeed significant adoption build ai serve humans manipulate profit political gain newsfeed algorithms didnt operate like casino operators propagandists instead closer mentor good librarian someone used keen understanding psychology millions similar people recommend next book resonate objectives make grow sort navigation tool life ai capable guiding optimal path experience space get want go imagine looking life lens system seen millions lives unfold writing book together system read every book conducting research collaboration system sees full scope current human knowledge products fully control ai interacts sophisticated algorithm instead threat would net positive letting achieve goals efficiently summary future one ai interface world world made digital information equally lead empowering individuals gain greater control lives total loss agency unfortunately social media currently engaged wrong road still early enough reverse course industry need develop product categories markets incentives aligned placing user charge algorithms affect instead using ai exploit users mind profit political gain need strive towards products antifacebook far future products likely take form ai assistants digital mentors programmed help put control objectives pursue interactions present search engines could seen early primitive example aidriven information interface serves users instead seeking hijack mental space search tool deliberately use reach specific goals rather passive alwayson feed elects show tell instead seeking maximally waste time search engine attempts minimize time takes go question answer problem solution may thinking since search engine still ai layer us information consume could bias results attempt manipulate us yes risk latent every informationmanagement algorithm stark contrast social networks market incentives case actually aligned users needs pushing search engines relevant objective possible fail maximally useful theres essentially friction users move competing product importantly search engine would considerably smaller psychological attack surface social newsfeed threat weve profiled post requires following present product aidriven informationmanagement products dont meet requirements social networks hand frightening combination risk factors technologists gravitate towards products feature characteristics push back products combine potential dangerous misuse build search engines digital assistants social newsfeeds make recommendation engines transparent configurable constructive rather slotlike machines maximize engagement wasted hours human time invest ui ux ai expertise building great configuration panels algorithm enable users use product terms importantly educate users issues reject manipulative products generating enough market pressure align incentives technology industry consumers conclusion fork road ahead one path leads place really scares leads humane future theres still time take better one work technologies keep mind may evil intentions may simply care may simply value rsus shared future whether care hand shaping infrastructure digital world choices affect us may eventually held responsible quick cheer standing ovation clap show much enjoyed story,en,"['Facebook', 'Brexit', 'algorithm', 'Digital']"
104,Aman Agarwal,7000,Explained Simply: How an AI program mastered the ancient game of Go,"This is about AlphaGo, Google DeepMind’s Go playing AI that shook the technology world in 2016 by defeating one of the best players in the world, Lee Sedol.
Go is an ancient board game which has so many possible moves at each step that future positions are hard to predict — and therefore it requires strong intuition and abstract thinking to play. Because of this reason, it was believed that only humans could be good at playing Go. Most researchers thought that it would still take decades to build an AI which could think like that. In fact, I’m releasing this essay today because this week (March 8–15) marks the two-year anniversary of the AlphaGo vs Sedol match!
But AlphaGo didn’t stop there. 8 months later, it played 60 professional games on a Go website under disguise as a player named “Master”, and won every single game, against dozens of world champions, of course without resting between games.
Naturally this was a HUGE achievement in the field of AI and sparked worldwide discussions about whether we should be excited or worried about artificial intelligence.
Today we are going to take the original research paper published by DeepMind in the Nature journal, and break it down paragraph-by-paragraph using simple English.
After this essay, you’ll know very clearly what AlphaGo is, and how it works. I also hope that after reading this you will not believe all the news headlines made by journalists to scare you about AI, and instead feel excited about it.
Worrying about the growing achievements of AI is like worrying about the growing abilities of Microsoft Powerpoint. Yes, it will get better with time with new features being added to it, but it can’t just uncontrollably grow into some kind of Hollywood monster.
You DON’T need to know how to play Go to understand this paper. In fact, I myself have only read the first 3–4 lines in Wikipedia’s opening paragraph about it. Instead, surprisingly, I use some examples from basic Chess to explain the algorithms. You just have to know what a 2-player board game is, in which each player takes turns and there is one winner at the end. Beyond that you don’t need to know any physics or advanced math or anything.
This will make it more approachable for people who only just now started learning about machine learning or neural networks. And especially for those who don’t use English as their first language (which can make it very difficult to read such papers).
If you have NO prior knowledge of AI and neural networks, you can read the “Deep Learning” section of one of my previous essays here. After reading that, you’ll be able to get through this essay.
If you want to get a shallow understanding of Reinforcement Learning too (optional reading), you can find it here.
Here’s the original paper if you want to try reading it:
As for me: Hi I’m Aman, an AI and autonomous robots engineer. I hope that my work will save you a lot of time and effort if you were to study this on your own.
Do you speak Japanese? Ryohji Ikebe has kindly written a brief memo about this essay in Japanese, in a series of Tweets.
As you know, the goal of this research was to train an AI program to play Go at the level of world-class professional human players.
To understand this challenge, let me first talk about something similar done for Chess. In the early 1990s, IBM came out with the Deep Blue computer which defeated the great champion Gary Kasparov in Chess. (He’s also a very cool guy, make sure to read more about him later!) How did Deep Blue play?
Well, it used a very brute force method. At each step of the game, it took a look at all the possible legal moves that could be played, and went ahead to explore each and every move to see what would happen. And it would keep exploring move after move for a while, forming a kind of HUGE decision tree of thousands of moves. And then it would come back along that tree, observing which moves seemed most likely to bring a good result. But, what do we mean by “good result”? Well, Deep Blue had many carefully designed chess strategies built into it by expert chess players to help it make better decisions — for example, how to decide whether to protect the king or get advantage somewhere else? They made a specific “evaluation algorithm” for this purpose, to compare how advantageous or disadvantageous different board positions are (IBM hard-coded expert chess strategies into this evaluation function). And finally it chooses a carefully calculated move. On the next turn, it basically goes through the whole thing again.
As you can see, this means Deep Blue thought about millions of theoretical positions before playing each move. This was not so impressive in terms of the AI software of Deep Blue, but rather in the hardware — IBM claimed it to be one of the most powerful computers available in the market at that time. It could look at 200 million board positions per second.
Now we come to Go. Just believe me that this game is much more open-ended, and if you tried the Deep Blue strategy on Go, you wouldn’t be able to play well. There would be SO MANY positions to look at at each step that it would simply be impractical for a computer to go through that hell. For example, at the opening move in Chess there are 20 possible moves. In Go the first player has 361 possible moves, and this scope of choices stays wide throughout the game.
This is what they mean by “enormous search space.” Moreover, in Go, it’s not so easy to judge how advantageous or disadvantageous a particular board position is at any specific point in the game — you kinda have to play the whole game for a while before you can determine who is winning. But let’s say you magically had a way to do both of these. And that’s where deep learning comes in!
So in this research, DeepMind used neural networks to do both of these tasks (if you haven’t read about them yet, here’s the link again). They trained a “policy neural network” to decide which are the most sensible moves in a particular board position (so it’s like following an intuitive strategy to pick moves from any position). And they trained a “value neural network” to estimate how advantageous a particular board arrangement is for the player (or in other words, how likely you are to win the game from this position). They trained these neural networks first with human game examples (your good old ordinary supervised learning). After this the AI was able to mimic human playing to a certain degree, so it acted like a weak human player. And then to train the networks even further, they made the AI play against itself millions of times (this is the “reinforcement learning” part). With this, the AI got better because it had more practice.
With these two networks alone, DeepMind’s AI was able to play well against state-of-the-art Go playing programs that other researchers had built before. These other programs had used an already popular pre-existing game playing algorithm, called the “Monte Carlo Tree Search” (MCTS). More about this later.
But guess what, we still haven’t talked about the real deal. DeepMind’s AI isn’t just about the policy and value networks. It doesn’t use these two networks as a replacement of the Monte Carlo Tree Search. Instead, it uses the neural networks to make the MCTS algorithm work better... and it got so much better that it reached superhuman levels. THIS improved variation of MCTS is “AlphaGo”, the AI that beat Lee Sedol and went down in AI history as one of the greatest breakthroughs ever. So essentially, AlphaGo is simply an improved implementation of a very ordinary computer science algorithm. Do you understand now why AI in its current form is absolutely nothing to be scared of?
Wow, we’ve spent a lot of time on the Abstract alone.
Alright — to understand the paper from this point on, first we’ll talk about a gaming strategy called the Monte Carlo Tree Search algorithm. For now, I’ll just explain this algorithm at enough depth to make sense of this essay. But if you want to learn about it in depth, some smart people have also made excellent videos and blog posts on this:
1. A short video series from Udacity2. Jeff Bradberry’s explanation of MCTS3. An MCTS tutorial by Fullstack Academy
The following section is long, but easy to understand (I’ll try my best) and VERY important, so stay with me! The rest of the essay will go much quicker.
Let’s talk about the first paragraph of the essay above. Remember what I said about Deep Blue making a huge tree of millions of board positions and moves at each step of the game? You had to do simulations and look at and compare each and every possible move. As I said before, that was a simple approach and very straightforward approach — if the average software engineer had to design a game playing AI, and had all the strongest computers of the world, he or she would probably design a similar solution.
But let’s think about how do humans themselves play chess? Let’s say you’re at a particular board position in the middle of the game. By game rules, you can do a dozen different things — move this pawn here, move the queen two squares here or three squares there, and so on. But do you really make a list of all the possible moves you can make with all your pieces, and then select one move from this long list? No — you “intuitively” narrow down to a few key moves (let’s say you come up with 3 sensible moves) that you think make sense, and then you wonder what will happen in the game if you chose one of these 3 moves. You might spend 15–20 seconds considering each of these 3 moves and their future — and note that during these 15 seconds you don’t have to carefully plan out the future of each move; you can just “roll out” a few mental moves guided by your intuition without TOO much careful thought (well, a good player would think farther and more deeply than an average player). This is because you have limited time, and you can’t accurately predict what your opponent will do at each step in that lovely future you’re cooking up in your brain. So you’ll just have to let your gut feeling guide you. I’ll refer to this part of the thinking process as “rollout”, so take note of it!So after “rolling out” your few sensible moves, you finally say screw it and just play the move you find best.
Then the opponent makes a move. It might be a move you had already well anticipated, which means you are now pretty confident about what you need to do next. You don’t have to spend too much time on the rollouts again. OR, it could be that your opponent hits you with a pretty cool move that you had not expected, so you have to be even more careful with your next move.This is how the game carries on, and as it gets closer and closer to the finishing point, it would get easier for you to predict the outcome of your moves — so your rollouts don’t take as much time.
The purpose of this long story is to describe what the MCTS algorithm does on a superficial level — it mimics the above thinking process by building a “search tree” of moves and positions every time. Again, for more details you should check out the links I mentioned earlier. The innovation here is that instead of going through all the possible moves at each position (which Deep Blue did), it instead intelligently selects a small set of sensible moves and explores those instead. To explore them, it “rolls out” the future of each of these moves and compares them based on their imagined outcomes.(Seriously — this is all I think you need to understand this essay)
Now — coming back to the screenshot from the paper. Go is a “perfect information game” (please read the definition in the link, don’t worry it’s not scary). And theoretically, for such games, no matter which particular position you are at in the game (even if you have just played 1–2 moves), it is possible that you can correctly guess who will win or lose (assuming that both players play “perfectly” from that point on). I have no idea who came up with this theory, but it is a fundamental assumption in this research project and it works.
So that means, given a state of the game s, there is a function v*(s) which can predict the outcome, let’s say probability of you winning this game, from 0 to 1. They call it the “optimal value function”. Because some board positions are more likely to result in you winning than other board positions, they can be considered more “valuable” than the others. Let me say it again: Value = Probability between 0 and 1 of you winning the game.
But wait — say there was a girl named Foma sitting next to you while you play Chess, and she keeps telling you at each step if you’re winning or losing. “You’re winning... You’re losing... Nope, still losing...” I think it wouldn’t help you much in choosing which move you need to make. She would also be quite annoying. What would instead help you is if you drew the whole tree of all the possible moves you can make, and the states that those moves would lead to — and then Foma would tell you for the entire tree which states are winning states and which states are losing states. Then you can choose moves which will keep leading you to winning states. All of a sudden Foma is your partner in crime, not an annoying friend. Here, Foma behaves as your optimal value function v*(s). Earlier, it was believed that it’s not possible to have an accurate value function like Foma for the game of Go, because the games had so much uncertainty.
BUT — even if you had the wonderful Foma, this wonderland strategy of drawing out all the possible positions for Foma to evaluate will not work very well in the real world. In a game like Chess or Go, as we said before, if you try to imagine even 7–8 moves into the future, there can be so many possible positions that you don’t have enough time to check all of them with Foma.
So Foma is not enough. You need to narrow down the list of moves to a few sensible moves that you can roll out into the future. How will your program do that? Enter Lusha. Lusha is a skilled Chess player and enthusiast who has spent decades watching grand masters play Chess against each other. She can look at your board position, look quickly at all the available moves you can make, and tell you how likely it would be that a Chess expert would make any of those moves if they were sitting at your table. So if you have 50 possible moves at a point, Lusha will tell you the probability that each move would be picked by an expert. Of course, a few sensible moves will have a much higher probability and other pointless moves will have very little probability. She is your policy function, p(a\s). For a given state s, she can give you probabilities for all the possible moves that an expert would make.
Wow — you can take Lusha’s help to guide you in how to select a few sensible moves, and Foma will tell you the likelihood of winning from each of those moves. You can choose the move that both Foma and Lusha approve. Or, if you want to be extra careful, you can roll out the moves selected by Lusha, have Foma evaluate them, pick a few of them to roll out further into the future, and keep letting Foma and Lusha help you predict VERY far into the game’s future — much quicker and more efficient than to go through all the moves at each step into the future. THIS is what they mean by “reducing the search space”. Use a value function (Foma) to predict outcomes, and use a policy function (Lusha) to give you grand-master probabilities to help narrow down the moves you roll out. These are called “Monte Carlo rollouts”. Then while you backtrack from future to present, you can take average values of all the different moves you rolled out, and pick the most suitable action. So far, this has only worked on a weak amateur level in Go, because the policy functions and value functions that they used to guide these rollouts weren’t that great.
Phew.
The first line is self explanatory. In MCTS, you can start with an unskilled Foma and unskilled Lusha. The more you play, the better they get at predicting solid outcomes and moves. “Narrowing the search to a beam of high probability actions” is just a sophisticated way of saying, “Lusha helps you narrow down the moves you need to roll out by assigning them probabilities that an expert would play them”. Prior work has used this technique to achieve strong amateur level AI players, even with simple (or “shallow” as they call it) policy functions.
Yeah, convolutional neural networks are great for image processing. And since a neural network takes a particular input and gives an output, it is essentially a function, right? So you can use a neural network to become a complex function. So you can just pass in an image of the board position and let the neural network figure out by itself what’s going on. This means it’s possible to create neural networks which will behave like VERY accurate policy and value functions. The rest is pretty self explanatory.
Here we discuss how Foma and Lusha were trained. To train the policy network (predicting for a given position which moves experts would pick), you simply use examples of human games and use them as data for good old supervised learning.
And you want to train another slightly different version of this policy network to use for rollouts; this one will be smaller and faster. Let’s just say that since Lusha is so experienced, she takes some time to process each position. She’s good to start the narrowing-down process with, but if you try to make her repeat the process , she’ll still take a little too much time. So you train a *faster policy network* for the rollout process (I’ll call it... Lusha’s younger brother Jerry? I know I know, enough with these names). After that, once you’ve trained both of the slow and fast policy networks enough using human player data, you can try letting Lusha play against herself on a Go board for a few days, and get more practice. This is the reinforcement learning part — making a better version of the policy network.
Then, you train Foma for value prediction: determining the probability of you winning. You let the AI practice through playing itself again and again in a simulated environment, observe the end result each time, and learn from its mistakes to get better and better.
I won’t go into details of how these networks are trained. You can read more technical details in the later section of the paper (‘Methods’) which I haven’t covered here. In fact, the real purpose of this particular paper is not to show how they used reinforcement learning on these neural networks. One of DeepMind’s previous papers, in which they taught AI to play ATARI games, has already discussed some reinforcement learning techniques in depth (And I’ve already written an explanation of that paper here). For this paper, as I lightly mentioned in the Abstract and also underlined in the screenshot above, the biggest innovation was the fact that they used RL with neural networks for improving an already popular game-playing algorithm, MCTS. RL is a cool tool in a toolbox that they used to fine-tune the policy and value function neural networks after the regular supervised training. This research paper is about proving how versatile and excellent this tool it is, not about teaching you how to use it. In television lingo, the Atari paper was a RL infomercial and this AlphaGo paper is a commercial.
A quick note before you move on. Would you like to help me write more such essays explaining cool research papers? If you’re serious, I’d be glad to work with you. Please leave a comment and I’ll get in touch with you.
So, the first step is in training our policy NN (Lusha), to predict which moves are likely to be played by an expert. This NN’s goal is to allow the AI to play similar to an expert human. This is a convolutional neural network (as I mentioned before, it’s a special kind of NN that is very useful in image processing) that takes in a simplified image of a board arrangement. “Rectifier nonlinearities” are layers that can be added to the network’s architecture. They give it the ability to learn more complex things. If you’ve ever trained NNs before, you might have used the “ReLU” layer. That’s what these are.
The training data here was in the form of random pairs of board positions, and the labels were the actions chosen by humans when they were in those positions. Just regular supervised learning.
Here they use “stochastic gradient ASCENT”. Well, this is an algorithm for backpropagation. Here, you’re trying to maximise a reward function. And the reward function is just the probability of the action predicted by a human expert; you want to increase this probability. But hey — you don’t really need to think too much about this. Normally you train the network so that it minimises a loss function, which is essentially the error/difference between predicted outcome and actual label. That is called gradient DESCENT. In the actual implementation of this research paper, they have indeed used the regular gradient descent. You can easily find a loss function that behaves opposite to the reward function such that minimising this loss will maximise the reward.
The policy network has 13 layers, and is called “SL policy” network (SL = supervised learning). The data came from a... I’ll just say it’s a popular website on which millions of people play Go. How good did this SL policy network perform?
It was more accurate than what other researchers had done earlier. The rest of the paragraph is quite self-explanatory. As for the “rollout policy”, you do remember from a few paragraphs ago, how Lusha the SL policy network is slow so it can’t integrate well with the MCTS algorithm? And we trained another faster version of Lusha called Jerry who was her younger brother? Well, this refers to Jerry right here. As you can see, Jerry is just half as accurate as Lusha BUT it’s thousands of times faster! It will really help get through rolled out simulations of the future faster, when we apply the MCTS.
For this next section, you don’t *have* to know about Reinforcement Learning already, but then you’ll have to assume that whatever I say works. If you really want to dig into details and make sure of everything, you might want to read a little about RL first.
Once you have the SL network, trained in a supervised manner using human player moves with the human moves data, as I said before you have to let her practice by itself and get better. That’s what we’re doing here. So you just take the SL policy network, save it in a file, and make another copy of it.
Then you use reinforcement learning to fine-tune it. Here, you make the network play against itself and learn from the outcomes.
But there’s a problem in this training style.
If you only forever practice against ONE opponent, and that opponent is also only practicing with you exclusively, there’s not much of new learning you can do. You’ll just be training to practice how to beat THAT ONE player. This is, you guessed it, overfitting: your techniques play well against one opponent, but don’t generalize well to other opponents. So how do you fix this?
Well, every time you fine-tune a neural network, it becomes a slightly different kind of player. So you can save this version of the neural network in a list of “players”, who all behave slightly differently right? Great — now while training the neural network, you can randomly make it play against many different older and newer versions of the opponent, chosen from that list. They are versions of the same player, but they all play slightly differently. And the more you train, the MORE players you get to train even more with! Bingo!
In this training, the only thing guiding the training process is the ultimate goal, i.e winning or losing. You don’t need to specially train the network to do things like capture more area on the board etc. You just give it all the possible legal moves it can choose from, and say, “you have to win”. And this is why RL is so versatile; it can be used to train policy or value networks for any game, not just Go.
Here, they tested how accurate this RL policy network was, just by itself without any MCTS algorithm. As you would remember, this network can directly take a board position and decide how an expert would play it — so you can use it to single-handedly play games.Well, the result was that the RL fine-tuned network won against the SL network that was only trained on human moves. It also won against other strong Go playing programs.
Must note here that even before training this RL policy network, the SL policy network was already better than the state of the art — and now, it has further improved! And we haven’t even come to the other parts of the process like the value network.
Did you know that baby penguins can sneeze louder than a dog can bark? Actually that’s not true, but I thought you’d like a little joke here to distract from the scary-looking equations above. Coming to the essay again: we’re done training Lusha here. Now back to Foma — remember the “optimal value function”: v*(s) -> that only tells you how likely you are to win in your current board position if both players play perfectly from that point on?So obviously, to train an NN to become our value function, we would need a perfect player... which we don’t have. So we just use our strongest player, which happens to be our RL policy network.
It takes the current state board state s, and outputs the probability that you will win the game. You play a game and get to know the outcome (win or loss). Each of the game states act as a data sample, and the outcome of that game acts as the label. So by playing a 50-move game, you have 50 data samples for value prediction.
Lol, no. This approach is naive. You can’t use all 50 moves from the game and add them to the dataset.
The training data set had to be chosen carefully to avoid overfitting. Each move in the game is very similar to the next one, because you only move once and that gives you a new position, right? If you take the states at all 50 of those moves and add them to the training data with the same label, you basically have lots of “kinda duplicate” data, and that causes overfitting. To prevent this, you choose only very distinct-looking game states. So for example, instead of all 50 moves of a game, you only choose 5 of them and add them to the training set. DeepMind took 30 million positions from 30 million different games, to reduce any chances of there being duplicate data. And it worked!
Now, something conceptual here: there are two ways to evaluate the value of a board position. One option is a magical optimal value function (like the one you trained above). The other option is to simply roll out into the future using your current policy (Lusha) and look at the final outcome in this roll out. Obviously, the real game would rarely go by your plans. But DeepMind compared how both of these options do. You can also do a mixture of both these options. We will learn about this “mixing parameter” a little bit later, so make a mental note of this concept!
Well, your single neural network trying to approximate the optimal value function is EVEN BETTER than doing thousands of mental simulations using a rollout policy! Foma really kicked ass here. When they replaced the fast rollout policy with the twice-as-accurate (but slow) RL policy Lusha, and did thousands of simulations with that, it did better than Foma. But only slightly better, and too slowly. So Foma is the winner of this competition, she has proved that she can’t be replaced.
Now that we have trained the policy and value functions, we can combine them with MCTS and give birth to our former world champion, destroyer of grand masters, the breakthrough of a generation, weighing two hundred and sixty eight pounds, one and only Alphaaaaa GO!
In this section, ideally you should have a slightly deeper understanding of the inner workings of the MCTS algorithm, but what you have learned so far should be enough to give you a good feel for what’s going on here. The only thing you should note is how we’re using the policy probabilities and value estimations. We combine them during roll outs, to narrow down the number of moves we want to roll out at each step. Q(s,a) represents the value function, and u(s,a) is a stored probability for that position. I’ll explain.
Remember that the policy network uses supervised learning to predict expert moves? And it doesn’t just give you most likely move, but rather gives you probabilities for each possible move that tell how likely it is to be an expert move. This probability can be stored for each of those actions. Here they call it “prior probability”, and they obviously use it while selecting which actions to explore. So basically, to decide whether or not to explore a particular move, you consider two things: First, by playing this move, how likely are you to win? Yes, we already have our “value network” to answer this first question. And the second question is, how likely is it that an expert would choose this move? (If a move is super unlikely to be chosen by an expert, why even waste time considering it. This we get from the policy network)
Then let’s talk about the “mixing parameter” (see came back to it!). As discussed earlier, to evaluate positions, you have two options: one, simply use the value network you have been using to evaluate states all along. And two, you can try to quickly play a rollout game with your current strategy (assuming the other player will play similarly), and see if you win or lose. We saw how the value function was better than doing rollouts in general. Here they combine both. You try giving each prediction 50–50 importance, or 40–60, or 0–100, and so on. If you attach a % of X to the first, you’ll have to attach 100-X to the second. That’s what this mixing parameter means. You’ll see these hit and trial results later in the paper.
After each roll out, you update your search tree with whatever information you gained during the simulation, so that your next simulation is more intelligent. And at the end of all simulations, you just pick the best move.
Interesting insight here!
Remember how the RL fine-tuned policy NN was better than just the SL human-trained policy NN? But when you put them within the MCTS algorithm of AlphaGo, using the human trained NN proved to be a better choice than the fine-tuned NN. But in the case of the value function (which you would remember uses a strong player to approximate a perfect player), training Foma using the RL policy works better than training her with the SL policy.
“Doing all this evaluation takes a lot of computing power. We really had to bring out the big guns to be able to run these damn programs.”
Self explanatory.
“LOL, our program literally blew the pants off of every other program that came before us”
This goes back to that “mixing parameter” again. While evaluating positions, giving equal importance to both the value function and the rollouts performed better than just using one of them. The rest is self explanatory, and reveals an interesting insight!
Self explanatory.
Self explanatory. But read that red underlined sentence again. I hope you can see clearly now that this line right here is pretty much the summary of what this whole research project was all about.
Concluding paragraph. “Let us brag a little more here because we deserve it!” :)
Oh and if you’re a scientist or tech company, and need some help in explaining your science to non-technical people for marketing, PR or training etc, I can help you. Drop me a message on Twitter: @mngrwl
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Engineer, teacher, learner of foreign languages, lover of history, cinema and art.
Our community publishes stories worth reading on development, design, and data science.
",alphago google deepminds go playing ai shook technology world 2016 defeating one best players world lee sedol go ancient board game many possible moves step future positions hard predict therefore requires strong intuition abstract thinking play reason believed humans could good playing go researchers thought would still take decades build ai could think like fact im releasing essay today week march 815 marks twoyear anniversary alphago vs sedol match alphago didnt stop 8 months later played 60 professional games go website disguise player named master every single game dozens world champions course without resting games naturally huge achievement field ai sparked worldwide discussions whether excited worried artificial intelligence today going take original research paper published deepmind nature journal break paragraphbyparagraph using simple english essay youll know clearly alphago works also hope reading believe news headlines made journalists scare ai instead feel excited worrying growing achievements ai like worrying growing abilities microsoft powerpoint yes get better time new features added cant uncontrollably grow kind hollywood monster dont need know play go understand paper fact read first 34 lines wikipedias opening paragraph instead surprisingly use examples basic chess explain algorithms know 2player board game player takes turns one winner end beyond dont need know physics advanced math anything make approachable people started learning machine learning neural networks especially dont use english first language make difficult read papers prior knowledge ai neural networks read deep learning section one previous essays reading youll able get essay want get shallow understanding reinforcement learning optional reading find heres original paper want try reading hi im aman ai autonomous robots engineer hope work save lot time effort study speak japanese ryohji ikebe kindly written brief memo essay japanese series tweets know goal research train ai program play go level worldclass professional human players understand challenge let first talk something similar done chess early 1990s ibm came deep blue computer defeated great champion gary kasparov chess hes also cool guy make sure read later deep blue play well used brute force method step game took look possible legal moves could played went ahead explore every move see would happen would keep exploring move move forming kind huge decision tree thousands moves would come back along tree observing moves seemed likely bring good result mean good result well deep blue many carefully designed chess strategies built expert chess players help make better decisions example decide whether protect king get advantage somewhere else made specific evaluation algorithm purpose compare advantageous disadvantageous different board positions ibm hardcoded expert chess strategies evaluation function finally chooses carefully calculated move next turn basically goes whole thing see means deep blue thought millions theoretical positions playing move impressive terms ai software deep blue rather hardware ibm claimed one powerful computers available market time could look 200 million board positions per second come go believe game much openended tried deep blue strategy go wouldnt able play well would many positions look step would simply impractical computer go hell example opening move chess 20 possible moves go first player 361 possible moves scope choices stays wide throughout game mean enormous search space moreover go easy judge advantageous disadvantageous particular board position specific point game kinda play whole game determine winning lets say magically way thats deep learning comes research deepmind used neural networks tasks havent read yet heres link trained policy neural network decide sensible moves particular board position like following intuitive strategy pick moves position trained value neural network estimate advantageous particular board arrangement player words likely win game position trained neural networks first human game examples good old ordinary supervised learning ai able mimic human playing certain degree acted like weak human player train networks even made ai play millions times reinforcement learning part ai got better practice two networks alone deepminds ai able play well stateoftheart go playing programs researchers built programs used already popular preexisting game playing algorithm called monte carlo tree search mcts later guess still havent talked real deal deepminds ai isnt policy value networks doesnt use two networks replacement monte carlo tree search instead uses neural networks make mcts algorithm work better got much better reached superhuman levels improved variation mcts alphago ai beat lee sedol went ai history one greatest breakthroughs ever essentially alphago simply improved implementation ordinary computer science algorithm understand ai current form absolutely nothing scared wow weve spent lot time abstract alone alright understand paper point first well talk gaming strategy called monte carlo tree search algorithm ill explain algorithm enough depth make sense essay want learn depth smart people also made excellent videos blog posts 1 short video series udacity2 jeff bradberrys explanation mcts3 mcts tutorial fullstack academy following section long easy understand ill try best important stay rest essay go much quicker lets talk first paragraph essay remember said deep blue making huge tree millions board positions moves step game simulations look compare every possible move said simple approach straightforward approach average software engineer design game playing ai strongest computers world would probably design similar solution lets think humans play chess lets say youre particular board position middle game game rules dozen different things move pawn move queen two squares three squares really make list possible moves make pieces select one move long list intuitively narrow key moves lets say come 3 sensible moves think make sense wonder happen game chose one 3 moves might spend 1520 seconds considering 3 moves future note 15 seconds dont carefully plan future move roll mental moves guided intuition without much careful thought well good player would think farther deeply average player limited time cant accurately predict opponent step lovely future youre cooking brain youll let gut feeling guide ill refer part thinking process rollout take note itso rolling sensible moves finally say screw play move find best opponent makes move might move already well anticipated means pretty confident need next dont spend much time rollouts could opponent hits pretty cool move expected even careful next movethis game carries gets closer closer finishing point would get easier predict outcome moves rollouts dont take much time purpose long story describe mcts algorithm superficial level mimics thinking process building search tree moves positions every time details check links mentioned earlier innovation instead going possible moves position deep blue instead intelligently selects small set sensible moves explores instead explore rolls future moves compares based imagined outcomesseriously think need understand essay coming back screenshot paper go perfect information game please read definition link dont worry scary theoretically games matter particular position game even played 12 moves possible correctly guess win lose assuming players play perfectly point idea came theory fundamental assumption research project works means given state game function vs predict outcome lets say probability winning game 0 1 call optimal value function board positions likely result winning board positions considered valuable others let say value probability 0 1 winning game wait say girl named foma sitting next play chess keeps telling step youre winning losing youre winning youre losing nope still losing think wouldnt help much choosing move need make would also quite annoying would instead help drew whole tree possible moves make states moves would lead foma would tell entire tree states winning states states losing states choose moves keep leading winning states sudden foma partner crime annoying friend foma behaves optimal value function vs earlier believed possible accurate value function like foma game go games much uncertainty even wonderful foma wonderland strategy drawing possible positions foma evaluate work well real world game like chess go said try imagine even 78 moves future many possible positions dont enough time check foma foma enough need narrow list moves sensible moves roll future program enter lusha lusha skilled chess player enthusiast spent decades watching grand masters play chess look board position look quickly available moves make tell likely would chess expert would make moves sitting table 50 possible moves point lusha tell probability move would picked expert course sensible moves much higher probability pointless moves little probability policy function pas given state give probabilities possible moves expert would make wow take lushas help guide select sensible moves foma tell likelihood winning moves choose move foma lusha approve want extra careful roll moves selected lusha foma evaluate pick roll future keep letting foma lusha help predict far games future much quicker efficient go moves step future mean reducing search space use value function foma predict outcomes use policy function lusha give grandmaster probabilities help narrow moves roll called monte carlo rollouts backtrack future present take average values different moves rolled pick suitable action far worked weak amateur level go policy functions value functions used guide rollouts werent great phew first line self explanatory mcts start unskilled foma unskilled lusha play better get predicting solid outcomes moves narrowing search beam high probability actions sophisticated way saying lusha helps narrow moves need roll assigning probabilities expert would play prior work used technique achieve strong amateur level ai players even simple shallow call policy functions yeah convolutional neural networks great image processing since neural network takes particular input gives output essentially function right use neural network become complex function pass image board position let neural network figure whats going means possible create neural networks behave like accurate policy value functions rest pretty self explanatory discuss foma lusha trained train policy network predicting given position moves experts would pick simply use examples human games use data good old supervised learning want train another slightly different version policy network use rollouts one smaller faster lets say since lusha experienced takes time process position shes good start narrowingdown process try make repeat process shell still take little much time train faster policy network rollout process ill call lushas younger brother jerry know know enough names youve trained slow fast policy networks enough using human player data try letting lusha play go board days get practice reinforcement learning part making better version policy network train foma value prediction determining probability winning let ai practice playing simulated environment observe end result time learn mistakes get better better wont go details networks trained read technical details later section paper methods havent covered fact real purpose particular paper show used reinforcement learning neural networks one deepminds previous papers taught ai play atari games already discussed reinforcement learning techniques depth ive already written explanation paper paper lightly mentioned abstract also underlined screenshot biggest innovation fact used rl neural networks improving already popular gameplaying algorithm mcts rl cool tool toolbox used finetune policy value function neural networks regular supervised training research paper proving versatile excellent tool teaching use television lingo atari paper rl infomercial alphago paper commercial quick note move would like help write essays explaining cool research papers youre serious id glad work please leave comment ill get touch first step training policy nn lusha predict moves likely played expert nns goal allow ai play similar expert human convolutional neural network mentioned special kind nn useful image processing takes simplified image board arrangement rectifier nonlinearities layers added networks architecture give ability learn complex things youve ever trained nns might used relu layer thats training data form random pairs board positions labels actions chosen humans positions regular supervised learning use stochastic gradient ascent well algorithm backpropagation youre trying maximise reward function reward function probability action predicted human expert want increase probability hey dont really need think much normally train network minimises loss function essentially errordifference predicted outcome actual label called gradient descent actual implementation research paper indeed used regular gradient descent easily find loss function behaves opposite reward function minimising loss maximise reward policy network 13 layers called sl policy network sl supervised learning data came ill say popular website millions people play go good sl policy network perform accurate researchers done earlier rest paragraph quite selfexplanatory rollout policy remember paragraphs ago lusha sl policy network slow cant integrate well mcts algorithm trained another faster version lusha called jerry younger brother well refers jerry right see jerry half accurate lusha thousands times faster really help get rolled simulations future faster apply mcts next section dont know reinforcement learning already youll assume whatever say works really want dig details make sure everything might want read little rl first sl network trained supervised manner using human player moves human moves data said let practice get better thats take sl policy network save file make another copy use reinforcement learning finetune make network play learn outcomes theres problem training style forever practice one opponent opponent also practicing exclusively theres much new learning youll training practice beat one player guessed overfitting techniques play well one opponent dont generalize well opponents fix well every time finetune neural network becomes slightly different kind player save version neural network list players behave slightly differently right great training neural network randomly make play many different older newer versions opponent chosen list versions player play slightly differently train players get train even bingo training thing guiding training process ultimate goal ie winning losing dont need specially train network things like capture area board etc give possible legal moves choose say win rl versatile used train policy value networks game go tested accurate rl policy network without mcts algorithm would remember network directly take board position decide expert would play use singlehandedly play gameswell result rl finetuned network sl network trained human moves also strong go playing programs must note even training rl policy network sl policy network already better state art improved havent even come parts process like value network know baby penguins sneeze louder dog bark actually thats true thought youd like little joke distract scarylooking equations coming essay done training lusha back foma remember optimal value function vs tells likely win current board position players play perfectly point onso obviously train nn become value function would need perfect player dont use strongest player happens rl policy network takes current state board state outputs probability win game play game get know outcome win loss game states act data sample outcome game acts label playing 50move game 50 data samples value prediction lol approach naive cant use 50 moves game add dataset training data set chosen carefully avoid overfitting move game similar next one move gives new position right take states 50 moves add training data label basically lots kinda duplicate data causes overfitting prevent choose distinctlooking game states example instead 50 moves game choose 5 add training set deepmind took 30 million positions 30 million different games reduce chances duplicate data worked something conceptual two ways evaluate value board position one option magical optimal value function like one trained option simply roll future using current policy lusha look final outcome roll obviously real game would rarely go plans deepmind compared options also mixture options learn mixing parameter little bit later make mental note concept well single neural network trying approximate optimal value function even better thousands mental simulations using rollout policy foma really kicked ass replaced fast rollout policy twiceasaccurate slow rl policy lusha thousands simulations better foma slightly better slowly foma winner competition proved cant replaced trained policy value functions combine mcts give birth former world champion destroyer grand masters breakthrough generation weighing two hundred sixty eight pounds one alphaaaaa go section ideally slightly deeper understanding inner workings mcts algorithm learned far enough give good feel whats going thing note using policy probabilities value estimations combine roll outs narrow number moves want roll step qsa represents value function usa stored probability position ill explain remember policy network uses supervised learning predict expert moves doesnt give likely move rather gives probabilities possible move tell likely expert move probability stored actions call prior probability obviously use selecting actions explore basically decide whether explore particular move consider two things first playing move likely win yes already value network answer first question second question likely expert would choose move move super unlikely chosen expert even waste time considering get policy network lets talk mixing parameter see came back discussed earlier evaluate positions two options one simply use value network using evaluate states along two try quickly play rollout game current strategy assuming player play similarly see win lose saw value function better rollouts general combine try giving prediction 5050 importance 4060 0100 attach x first youll attach 100x second thats mixing parameter means youll see hit trial results later paper roll update search tree whatever information gained simulation next simulation intelligent end simulations pick best move interesting insight remember rl finetuned policy nn better sl humantrained policy nn put within mcts algorithm alphago using human trained nn proved better choice finetuned nn case value function would remember uses strong player approximate perfect player training foma using rl policy works better training sl policy evaluation takes lot computing power really bring big guns able run damn programs self explanatory lol program literally blew pants every program came us goes back mixing parameter evaluating positions giving equal importance value function rollouts performed better using one rest self explanatory reveals interesting insight self explanatory self explanatory read red underlined sentence hope see clearly line right pretty much summary whole research project concluding paragraph let us brag little deserve oh youre scientist tech company need help explaining science nontechnical people marketing pr training etc help drop message twitter mngrwl quick cheer standing ovation clap show much enjoyed story engineer teacher learner foreign languages lover history cinema art community publishes stories worth reading development design data science,en,"['AlphaGo', 'HUGE', 'the Nature journal', 'Microsoft Powerpoint', 'Chess', 'IBM', 'Deep Blue', 'algorithm', 'the Monte Carlo Tree Search', 'MCTS', 'Abstract', 'Fullstack Academy', 'it!So', 'Foma', 'RL', 'Atari', 'SL', 'Q(s']"
105,Gaurav Kaila,2100,How to easily automate Drone-based monitoring using Deep Learning,"This article is a comprehensive overview of using deep learning based object detection methods for aerial imagery via drones.
Did you know Drones and it’s associated functions are set to be a $50 billion industry by 2023? Currently drones being used in domains such as agriculture, construction, public safety and security to name a few and are rapidly being adopted by others. With deep-learning based computer vision now powering these drones, industry experts are now predicting unprecedented use in previously unimaginable or infeasible applications.
We explore some of these applications along with challenges in automation of drone-based monitoring through deep learning.
Finally, a case-study is presented for automating remote inspection of construction projects in Africa using Nanonets machine learning framework.
Man has always been feed fascinated with the view of the world from the top — building watch-towers, high fortwalls, capturing the highest mountain peak. To capture a glimpse and share it with the world, people went to great lengths to defy gravity, enlisting the help of ladders, tall buildings, kites, balloons, planes, and rockets.
Today, access to drones that can fly as high as 2kms is possible even for the general public. These drones have high resolution cameras attached to them that are capable of acquiring quality images which can be used for various kinds of analysis.
With easier access to drones, we’re seeing a lot of interest and activity by photographers & hobbyists, who are using it to make creative projects such as capturing inequality in South Africa or breathtaking views of New York which might make Woody Allen proud.
We explore some here:
Energy : Inspection of solar farms
Routine inspection and maintenance is a herculean task for solar farms. The traditional manual inspection method can only support the inspection frequency of once in three months. Because of the hostile environment, solar panels may have defects; broken solar panel units reduce the power output efficiency.
Agriculture: Early plant disease detection
Researchers at Imperial College London is mounting multi-spectral cameras on drones that will use special filters to capture reflected light from selected regions of the electromagnetic spectrum. Stressed plants typically display a ‘spectral signature’ that distinguishes them from healthy plants.
Public Safety: Shark detection
Analysis of overhead view of a large mass of land/water can yield a vast amount of information in terms of security and public safety. One such example is spotting sharks in the water off the coast of Australia. Australia-based Westpac Group has developed a deep learning based object detection system to detect sharks in the water.
There are various other applications to aerial images such as Civil Engineering (routine bridge inspections, power line surveillance and traffic surveying), Oil and Gas (on- & offshore inspection of oil and gas platforms, drilling rigs), Public Safety (motor vehicle accidents, nuclear accidents, structural fires, ship collisions, plane and train crashes) & Security (Traffic surveillance, Border surveillance,Coastal surveillance, Controlling hostile demonstrations and rioting).
To comprehensively capture terrain & landscapes, the process of acquiring aerial images can be summarised in two steps.
After image stitching, the generated map can be used for various kinds of analysis for the applications mentioned above.
High-resolution aerial imagery is increasingly available at the global scale and contains an abundance of information about features of interest that could be correlated with maintenance, land development, disease control, defect localisation, surveillance, etc. Unfortunately, such data are highly unstructured and thus challenging to extract meaningful insights from at scale, even with intensive manual analysis.
For eg, classification of urban land use is typically based on surveys performed by trained professionals. As such, this task is labor-intensive, infrequent, slow, and costly. As a result, such data are mostly available in developed countries and big cities that have the resources and the vision necessary to collect and curate it.
Another motivation for automating the analysis of aerial imagery stems from the urgency of predicting changes in the region of interest. For eg, crowd counting and crowd behaviour is frequently done during large public gatherings such as concerts, football matches, protests, etc. Traditionally, a human is behind the analysis of images being streamed from a CCTV camera directly to the command centre. As you may imagine, there are several problems with this approach such as human latency or error in detecting an event and lack of sufficient views via standard-static CCTV cameras.
Below are some of the commonly occurring challenges when using aerial imagery.
There are several challenges to overcome when automating the analysis of drone imagery. Following lists a few of them with a prospective solution:
Pragmatic Master, a South-African robotics-as-a-service collaborated with Nanonets for automation of remotely monitoring progress of a housing construction project in Africa.
We aim to detect the following infrastructure to capture the construction progress of a house in it’s various stages : a foundation (start), wallplate (in-progress), roof (partially complete), apron (finishing touches) and geyser (ready-to-move in)
Pragmatic Master chose Nanonets as it’s deep learning provider because of it’s easy-to-use web platform and plug&play APIs.
The end-to-end process of using the Nanonets API is as simple as four steps.
2. Labelling of images: Labelling images is probably the hardest and the most time-consuming step in any supervised machine learning pipeline, but at Nanonets we have this covered for you. We have in-house experts that have multiple years of working with aerial images. They will annotate your images with high precision and accuracy to aid better model training. For the Pragmatic Master use-case, we were labelling the following objects and their total count in all the images.
3. Model training: At Nanonets we employ the principle of Transfer Learning while training on your images. This involves re-training a pre-trained model that has already been pre-trained with a large number of aerial images. This helps the model identify micro patterns such as edges, lines and contours easily on your images and focus on the more specific macro patterns such as houses, trees, humans, cars, etc. Transfer learning also gives a boost in term of training time as the model does not need to be trained for a large number of iterations to give a good performance.
Our proprietary deep learning software smartly selects the best model along with optimising the hyper-parameters for your use-case. This involves searching through multiple models and through a hyperspace of parameters using advanced search algorithms.
The hardest objects to detect are the smallest ones, due to their low resolution. Our model training strategy is optimised to detect very small objects such as Geysers and Aprons which have an area of a few pixels.
Following are the mean average precision per class that we get, Roof: 95.1%Geyser: 88%Wallplate: 92%Apron: 81%
Note: Adding more images can lead to an increase in the mean average precision. Our API also supports detecting multiple objects in the same image such as Roofs and Aprons in one image.
4. Test & Integrate: Once the model is trained, you can either integrate Nanonet’s API directly into your system or we also provide a docker image with the trained model and inference code that you can use. Docker images can easily scale and provide a fault tolerant inference system.
Customer trust is our top priority. We are committed towards providing you ownership and control over your content at all times. We provide two plans for using our service,
For both the plans, we use highly sophisticated data privacy and security protocols in collaboration with Amazon Web Services, which is our cloud partner. Your dataset is anonymised and goes through minimal human intervention during the pre-processing and training process. All our human labellers have signed a non-disclosure agreement (NDA) to protect your data from going into wrong hands. As we believe in the philosophy of “Your data is yours!”, you can request us to delete your data from our servers at any stage.
NanoNets is a web service that makes it easy to use Deep Learning. You can build a model with your own data to achieve high accuracy & use our APIs to integrate the same in your application.
Pragmatic Master is a South African robotics as a service company that provides camera-mounted drones to acquire images of construction, farming and mining sites. These images are analysed to track progress, identify challenges, eliminate inefficiencies and provide an overall aerial view of the site.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Machine Learning Engineer
NanoNets: Machine Learning API
",article comprehensive overview using deep learning based object detection methods aerial imagery via drones know drones associated functions set 50 billion industry 2023 currently drones used domains agriculture construction public safety security name rapidly adopted others deeplearning based computer vision powering drones industry experts predicting unprecedented use previously unimaginable infeasible applications explore applications along challenges automation dronebased monitoring deep learning finally casestudy presented automating remote inspection construction projects africa using nanonets machine learning framework man always feed fascinated view world top building watchtowers high fortwalls capturing highest mountain peak capture glimpse share world people went great lengths defy gravity enlisting help ladders tall buildings kites balloons planes rockets today access drones fly high 2kms possible even general public drones high resolution cameras attached capable acquiring quality images used various kinds analysis easier access drones seeing lot interest activity photographers hobbyists using make creative projects capturing inequality south africa breathtaking views new york might make woody allen proud explore energy inspection solar farms routine inspection maintenance herculean task solar farms traditional manual inspection method support inspection frequency three months hostile environment solar panels may defects broken solar panel units reduce power output efficiency agriculture early plant disease detection researchers imperial college london mounting multispectral cameras drones use special filters capture reflected light selected regions electromagnetic spectrum stressed plants typically display spectral signature distinguishes healthy plants public safety shark detection analysis overhead view large mass landwater yield vast amount information terms security public safety one example spotting sharks water coast australia australiabased westpac group developed deep learning based object detection system detect sharks water various applications aerial images civil engineering routine bridge inspections power line surveillance traffic surveying oil gas offshore inspection oil gas platforms drilling rigs public safety motor vehicle accidents nuclear accidents structural fires ship collisions plane train crashes security traffic surveillance border surveillancecoastal surveillance controlling hostile demonstrations rioting comprehensively capture terrain landscapes process acquiring aerial images summarised two steps image stitching generated map used various kinds analysis applications mentioned highresolution aerial imagery increasingly available global scale contains abundance information features interest could correlated maintenance land development disease control defect localisation surveillance etc unfortunately data highly unstructured thus challenging extract meaningful insights scale even intensive manual analysis eg classification urban land use typically based surveys performed trained professionals task laborintensive infrequent slow costly result data mostly available developed countries big cities resources vision necessary collect curate another motivation automating analysis aerial imagery stems urgency predicting changes region interest eg crowd counting crowd behaviour frequently done large public gatherings concerts football matches protests etc traditionally human behind analysis images streamed cctv camera directly command centre may imagine several problems approach human latency error detecting event lack sufficient views via standardstatic cctv cameras commonly occurring challenges using aerial imagery several challenges overcome automating analysis drone imagery following lists prospective solution pragmatic master southafrican roboticsasaservice collaborated nanonets automation remotely monitoring progress housing construction project africa aim detect following infrastructure capture construction progress house various stages foundation start wallplate inprogress roof partially complete apron finishing touches geyser readytomove pragmatic master chose nanonets deep learning provider easytouse web platform plugplay apis endtoend process using nanonets api simple four steps 2 labelling images labelling images probably hardest timeconsuming step supervised machine learning pipeline nanonets covered inhouse experts multiple years working aerial images annotate images high precision accuracy aid better model training pragmatic master usecase labelling following objects total count images 3 model training nanonets employ principle transfer learning training images involves retraining pretrained model already pretrained large number aerial images helps model identify micro patterns edges lines contours easily images focus specific macro patterns houses trees humans cars etc transfer learning also gives boost term training time model need trained large number iterations give good performance proprietary deep learning software smartly selects best model along optimising hyperparameters usecase involves searching multiple models hyperspace parameters using advanced search algorithms hardest objects detect smallest ones due low resolution model training strategy optimised detect small objects geysers aprons area pixels following mean average precision per class get roof 951geyser 88wallplate 92apron 81 note adding images lead increase mean average precision api also supports detecting multiple objects image roofs aprons one image 4 test integrate model trained either integrate nanonets api directly system also provide docker image trained model inference code use docker images easily scale provide fault tolerant inference system customer trust top priority committed towards providing ownership control content times provide two plans using service plans use highly sophisticated data privacy security protocols collaboration amazon web services cloud partner dataset anonymised goes minimal human intervention preprocessing training process human labellers signed nondisclosure agreement nda protect data going wrong hands believe philosophy data request us delete data servers stage nanonets web service makes easy use deep learning build model data achieve high accuracy use apis integrate application pragmatic master south african robotics service company provides cameramounted drones acquire images construction farming mining sites images analysed track progress identify challenges eliminate inefficiencies provide overall aerial view site quick cheer standing ovation clap show much enjoyed story machine learning engineer nanonets machine learning api,en,"['Nanonets', 'photographers & hobbyists', 'Imperial College London', 'Westpac Group', 'Civil Engineering', 'Coastal', 'CCTV', 'Roofs and Aprons', 'Amazon Web Services', 'NDA', 'NanoNets']"
106,James Loy,8500,How to build your own Neural Network from scratch in Python,"Motivation: As part of my personal journey to gain a better understanding of Deep Learning, I’ve decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist.
This article contains what I’ve learned, and hopefully it’ll be useful for you as well!
Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output.
Neural Networks consist of the following components
The diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)
Creating a Neural Network class in Python is easy.
Training the Neural Network
The output ŷ of a simple 2-layer Neural Network is:
You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output ŷ.
Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.
Each iteration of the training process consists of the following steps:
The sequential graph below illustrates the process.
As we’ve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is:
Let’s add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0.
However, we still need a way to evaluate the “goodness” of our predictions (i.e. how far off are our predictions)? The Loss Function allows us to do exactly that.
There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we’ll use a simple sum-of-sqaures error as our loss function.
That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference.
Our goal in training is to find the best set of weights and biases that minimizes the loss function.
Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.
In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.
Recall from calculus that the derivative of a function is simply the slope of the function.
If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.
However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.
Phew! That was ugly but it allows us to get what we needed — the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly.
Now that we have that, let’s add the backpropagation function into our python code.
For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown.
Now that we have our complete python code for doing feedforward and backpropagation, let’s apply our Neural Network on an example and see how well it does.
Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn’t exactly trivial for us to work out the weights just by inspection alone.
Let’s train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss monotonically decreasing towards a minimum. This is consistent with the gradient descent algorithm that we’ve discussed earlier.
Let’s look at the final prediction (output) from the Neural Network after 1500 iterations.
We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values.
Note that there’s a slight difference between the predictions and the actual values. This is desirable, as it prevents overfitting and allows the Neural Network to generalize better to unseen data.
Fortunately for us, our journey isn’t over. There’s still much to learn about Neural Networks and Deep Learning. For example:
I’ll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them!
I’ve certainly learnt a lot writing my own Neural Network from scratch.
Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it’s beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks.
This exercise has been a great investment of my time, and I hope that it’ll be useful for you as well!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Graduate Student in Machine Learning @ Georgia Tech | LinkedIn: https://www.linkedin.com/in/jamesloy1/
Sharing concepts, ideas, and codes.
",motivation part personal journey gain better understanding deep learning ive decided build neural network scratch without deep learning library like tensorflow believe understanding inner workings neural network important aspiring data scientist article contains ive learned hopefully itll useful well introductory texts neural networks brings brain analogies describing without delving brain analogies find easier simply describe neural networks mathematical function maps given input desired output neural networks consist following components diagram shows architecture 2layer neural network note input layer typically excluded counting number layers neural network creating neural network class python easy training neural network output simple 2layer neural network might notice equation weights w biases b variables affects output naturally right values weights biases determines strength predictions process finetuning weights biases input data known training neural network iteration training process consists following steps sequential graph illustrates process weve seen sequential graph feedforward simple calculus basic 2layer neural network output neural network lets add feedforward function python code exactly note simplicity assumed biases 0 however still need way evaluate goodness predictions ie far predictions loss function allows us exactly many available loss functions nature problem dictate choice loss function tutorial well use simple sumofsqaures error loss function sumofsquares error simply sum difference predicted value actual value difference squared measure absolute value difference goal training find best set weights biases minimizes loss function weve measured error prediction loss need find way propagate error back update weights biases order know appropriate amount adjust weights biases need know derivative loss function respect weights biases recall calculus derivative function simply slope function derivative simply update weights biases increasingreducing itrefer diagram known gradient descent however cant directly calculate derivative loss function respect weights biases equation loss function contain weights biases therefore need chain rule help us calculate phew ugly allows us get needed derivative slope loss function respect weights adjust weights accordingly lets add backpropagation function python code deeper understanding application calculus chain rule backpropagation strongly recommend tutorial 3blue1brown complete python code feedforward backpropagation lets apply neural network example see well neural network learn ideal set weights represent function note isnt exactly trivial us work weights inspection alone lets train neural network 1500 iterations see happens looking loss per iteration graph clearly see loss monotonically decreasing towards minimum consistent gradient descent algorithm weve discussed earlier lets look final prediction output neural network 1500 iterations feedforward backpropagation algorithm trained neural network successfully predictions converged true values note theres slight difference predictions actual values desirable prevents overfitting allows neural network generalize better unseen data fortunately us journey isnt theres still much learn neural networks deep learning example ill writing topics soon follow medium keep eye ive certainly learnt lot writing neural network scratch although deep learning libraries tensorflow keras makes easy build deep nets without fully understanding inner workings neural network find beneficial aspiring data scientist gain deeper understanding neural networks exercise great investment time hope itll useful well quick cheer standing ovation clap show much enjoyed story graduate student machine learning georgia tech linkedin httpswwwlinkedincominjamesloy1 sharing concepts ideas codes,en,"['a Neural Network', 'Neural Networks', 'Neural Network', 'the Neural Network', 'Loss Function', 'Our Neural Network', 'TensorFlow', 'Keras', 'Graduate Student in Machine Learning']"
107,Chintan Trivedi,1200,Using Deep Q-Learning in FIFA 18 to perfect the art of free-kicks,"A code tutorial in Tensorflow that uses Reinforcement Learning to take free kicks.
In my previous article, I presented an AI bot trained to play the game of FIFA using Supervised Learning technique. With this approach, the bot quickly learnt the basics of the game like passing and shooting. However, the training data required to improve it further quickly became cumbersome to gather and provided little-to-no improvements, making this approach very time consuming. For this sake, I decided to switch to Reinforcement Learning, as suggested by almost everyone who commented on that article!
In this article, I’ll provide a short description of what Reinforcement Learning is and how I applied it to this game. A big challenge in implementing this is that we do not have access to the game’s code, so we can only make use of what we see on the game screen. Due to this reason, I was unable to train the AI on the full game, but could find a work-around to implement it for skill games in practice mode. For this tutorial, I will be trying to teach the bot to take 30-yard free kicks, but you can modify it to play other skill games as well. Let’s start with understanding the Reinforcement Learning technique and how we can formulate our free kick problem to fit this technique.
Contrary to Supervised Learning, we do not need to manually label the training data in Reinforcement Learning. Instead, we interact with our environment and observe the outcome of our interaction. We repeat this process multiple times gaining examples of positive and negative experiences, which acts as our training data. Thus, we learn by experimentation and not imitation.
Let’s say our environment is in a particular state s, and upon taking an action a, it changes to state s’. For this particular action, the immediate reward you observe in the environment is r. Any set of actions that follow this action will have their own immediate rewards, until you stop interacting due to a positive or a negative experience. These are called future rewards. Thus, for the current state s, we will try to estimate out of all actions possible which action will fetch us the maximum immediate + future reward, denoted by Q(s,a) called the Q-function. This gives us Q(s,a) = r + γ * Q(s’,a’) which denotes the expected final reward by taking action a in state s. Here, γ is a discount factor to account for uncertainty in predicting the future, thus we want to trust the present a bit more than the future.
Deep Q-learning is a special type of Reinforcement Learning technique where the Q-function is learnt by a deep neural network. Given the environment’s state as an image input to this network, it tries to predict the expected final reward for all possible actions like a regression problem. The action with the maximum predicted Q-value is chosen as our action to be taken in the environment. Hence the name Deep Q-Learning.
Note: If we had a performance meter in kick-off mode of FIFA like there is in the practice mode, we might have been able to formulate this problem for playing the entire game and not restrict ourselves to just taking free-kicks. That, or we need access to game’s internal code which we don’t have. Anyways, let’s make the most of what we do have.
While the bot has not mastered all different kinds of free kicks, it has learnt some situations very well. It almost always hits the target in absence of wall of players but struggles in its presence. Also, when it hasn’t encountered a situation frequently in training like not facing the goal, it behaves bonkers. However, with every training epoch, this behavior was noticed to decrease on an average.
As shown in the figure above, the average goal scoring rate grows from 30% to 50% on an average after training for 1000 epochs. This means the current bot scores about half of the free kicks it attempts (for reference, a human would average around 75–80%). Do consider that FIFA tends to behave non-deterministically which makes learning very difficult.
More results in video format can be found on my YouTube channel, with the video embedded below. Please subscribe to my channel if you wish to keep track of all my projects.
We shall implement this in python using tools like Tensorflow (Keras) for Deep Learning and pytesseract for OCR. The git link is provided below with the requirements setup instructions in the repository description.
I would recommend below gists of code only for the purpose of understanding this tutorial since some lines have been removed for brevity. Please use the full code from git while running it. Let’s go over the 4 main parts of the code.
We do not have any readymade API available that gives us access to the code. So, let’s make our own API instead! We’ll use game’s screenshots to observe the state, simulated key-presses to take action in the game environment and Optical Character Recognition to read our reward in the game. We have three main methods in our FIFA class: observe(), act(), _get_reward() and an additional method is_over() to check if the free kick has been taken or not.
Throughout the training process, we want to store all our experiences and observed rewards. We will use this as the training data for our Q-Learning model. So, for every action we take, we store the experience <s, a, r, s’> along with a game_over flag. The target label that our model will try to learn is the final reward for each action which is a real number for our regression problem.
Now that we can interact with the game and store our interactions in memory, let’s start training our Q-Learning model. For this, we will attain a balance between exploration (taking a random action in the game) and exploitation (taking action predicted by our model). This way we can perform trial-and-error to obtain different experiences in the game. The parameter epsilon is used for this purpose, which is an exponentially decreasing factor that balances exploration and exploitation. In the beginning, when we know nothing, we want to do more exploration but as number of epochs increases and we learn more, we want to do more exploitation and less exploration. Hence. the decaying value of the epsilon parameter.
For this tutorial I have only trained the model for 1000 epochs due to time and performance constraints, but in the future I would like to push it to at least 5000 epochs.
At the heart of the Q-Learning process is a 2-layered Dense/Fully Connected Network with ReLU activation. It takes the 128-dimensional feature map as input state and outputs 4 Q-values for each possible action. The action with the maximum predicted Q-value is the desired action to be taken as per the network’s policy for the given state.
This is the starting point of execution of this code, but you’ll have to make sure the game FIFA 18 is running in windowed mode on a second display and you load up the free kick practice mode under skill games: shooting menu. Make sure the game controls are in sync with the keys you have hard-coded in the FIFA.py script.
Overall, I think the results are quite satisfactory even though it fails to reach human level of performance. Switching from Supervised to Reinforcement technique for learning helps ease the pain of collecting training data. Given enough time to explore, it performs very well in problems like learning how to play simple games. However, Reinforcement setting seems to fail when it encounters unfamiliar situations, which makes me believe formulating it as a regression problem cannot extrapolate information as well as formulating it as a classification problem in supervised setting. Perhaps a combination of the two could address the weaknesses of both these approaches. Maybe that’s where we’ll see the best results in building AI for games. Something for me to try in the future!
I would like to acknowledge this tutorial of Deep Q-Learning and this git repository of gaming with python for providing majority of the code. With the exception of the FIFA “custom-API”, most of the code’s backbone has come from these sources. Thanks to these guys!
Thank you for reading! If you liked this tutorial, please follow me on medium, github or subscribe to my YouTube channel.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist,  AI Enthusiast, Blogger, YouTuber, Chelsea FC Fanatic. Also, looking to build my virtual clone before I die.
Sharing concepts, ideas, and codes.
",code tutorial tensorflow uses reinforcement learning take free kicks previous article presented ai bot trained play game fifa using supervised learning technique approach bot quickly learnt basics game like passing shooting however training data required improve quickly became cumbersome gather provided littletono improvements making approach time consuming sake decided switch reinforcement learning suggested almost everyone commented article article ill provide short description reinforcement learning applied game big challenge implementing access games code make use see game screen due reason unable train ai full game could find workaround implement skill games practice mode tutorial trying teach bot take 30yard free kicks modify play skill games well lets start understanding reinforcement learning technique formulate free kick problem fit technique contrary supervised learning need manually label training data reinforcement learning instead interact environment observe outcome interaction repeat process multiple times gaining examples positive negative experiences acts training data thus learn experimentation imitation lets say environment particular state upon taking action changes state particular action immediate reward observe environment r set actions follow action immediate rewards stop interacting due positive negative experience called future rewards thus current state try estimate actions possible action fetch us maximum immediate future reward denoted qsa called qfunction gives us qsa r qsa denotes expected final reward taking action state discount factor account uncertainty predicting future thus want trust present bit future deep qlearning special type reinforcement learning technique qfunction learnt deep neural network given environments state image input network tries predict expected final reward possible actions like regression problem action maximum predicted qvalue chosen action taken environment hence name deep qlearning note performance meter kickoff mode fifa like practice mode might able formulate problem playing entire game restrict taking freekicks need access games internal code dont anyways lets make bot mastered different kinds free kicks learnt situations well almost always hits target absence wall players struggles presence also hasnt encountered situation frequently training like facing goal behaves bonkers however every training epoch behavior noticed decrease average shown figure average goal scoring rate grows 30 50 average training 1000 epochs means current bot scores half free kicks attempts reference human would average around 7580 consider fifa tends behave nondeterministically makes learning difficult results video format found youtube channel video embedded please subscribe channel wish keep track projects shall implement python using tools like tensorflow keras deep learning pytesseract ocr git link provided requirements setup instructions repository description would recommend gists code purpose understanding tutorial since lines removed brevity please use full code git running lets go 4 main parts code readymade api available gives us access code lets make api instead well use games screenshots observe state simulated keypresses take action game environment optical character recognition read reward game three main methods fifa class observe act _get_reward additional method is_over check free kick taken throughout training process want store experiences observed rewards use training data qlearning model every action take store experience r along game_over flag target label model try learn final reward action real number regression problem interact game store interactions memory lets start training qlearning model attain balance exploration taking random action game exploitation taking action predicted model way perform trialanderror obtain different experiences game parameter epsilon used purpose exponentially decreasing factor balances exploration exploitation beginning know nothing want exploration number epochs increases learn want exploitation less exploration hence decaying value epsilon parameter tutorial trained model 1000 epochs due time performance constraints future would like push least 5000 epochs heart qlearning process 2layered densefully connected network relu activation takes 128dimensional feature map input state outputs 4 qvalues possible action action maximum predicted qvalue desired action taken per networks policy given state starting point execution code youll make sure game fifa 18 running windowed mode second display load free kick practice mode skill games shooting menu make sure game controls sync keys hardcoded fifapy script overall think results quite satisfactory even though fails reach human level performance switching supervised reinforcement technique learning helps ease pain collecting training data given enough time explore performs well problems like learning play simple games however reinforcement setting seems fail encounters unfamiliar situations makes believe formulating regression problem cannot extrapolate information well formulating classification problem supervised setting perhaps combination two could address weaknesses approaches maybe thats well see best results building ai games something try future would like acknowledge tutorial deep qlearning git repository gaming python providing majority code exception fifa customapi codes backbone come sources thanks guys thank reading liked tutorial please follow medium github subscribe youtube channel quick cheer standing ovation clap show much enjoyed story data scientist ai enthusiast blogger youtuber chelsea fc fanatic also looking build virtual clone die sharing concepts ideas codes,en,"['FIFA', 'Q(s', 'Keras', 'Deep Learning', 'API', 'Dense/Fully Connected Network', 'github', 'Blogger', 'YouTuber', 'Chelsea FC Fanatic']"
108,Abhishek Parbhakar,1700,Why Data Scientists love Gaussian? – Towards Data Science,"For Deep Learning & Machine Learning engineers out of all the probabilistic models in the world, Gaussian distribution model simply stands out. Even if you have never worked on an AI project, there is a significant chance that you have come across the Gaussian model.
Gaussian distribution model, often identified with its iconic bell shaped curve, also referred as Normal distribution, is so popular mainly because of three reasons.
Incredible number of processes in nature and social sciences naturally follows the Gaussian distribution. Even when they don’t, the Gaussian gives the best model approximation for these processes. Some examples include-
Central limit theorem states that when we add large number of independent random variables, irrespective of the original distribution of these variables, their normalized sum tends towards a Gaussian distribution. For example, the distribution of total distance covered in an random walk tends towards a Gaussian probability distribution.
The theorem’s implications include that large number of scientific and statistical methods that have been developed specifically for Gaussian models can also be applied to wide range of problems that may involve any other types of distributions.
The theorem can also been seen as a explanation why many natural phenomena follow Gaussian distribution.
Unlike many other distribution that changes their nature on transformation, a Gaussian tends to remain a Gaussian.
For every Gaussian model approximation, there may exist a complex multi-parameter distribution that gives better approximation. But still Gaussian is preferred because it makes the math a lot simpler!
Gaussian distribution is named after great mathematician and physicist Carl Friedrich Gauss.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Finding equilibria among AI, philosophy, and economics.
Sharing concepts, ideas, and codes.
",deep learning machine learning engineers probabilistic models world gaussian distribution model simply stands even never worked ai project significant chance come across gaussian model gaussian distribution model often identified iconic bell shaped curve also referred normal distribution popular mainly three reasons incredible number processes nature social sciences naturally follows gaussian distribution even dont gaussian gives best model approximation processes examples include central limit theorem states add large number independent random variables irrespective original distribution variables normalized sum tends towards gaussian distribution example distribution total distance covered random walk tends towards gaussian probability distribution theorems implications include large number scientific statistical methods developed specifically gaussian models also applied wide range problems may involve types distributions theorem also seen explanation many natural phenomena follow gaussian distribution unlike many distribution changes nature transformation gaussian tends remain gaussian every gaussian model approximation may exist complex multiparameter distribution gives better approximation still gaussian preferred makes math lot simpler gaussian distribution named great mathematician physicist carl friedrich gauss quick cheer standing ovation clap show much enjoyed story finding equilibria among ai philosophy economics sharing concepts ideas codes,en,['Deep Learning & Machine Learning']
109,Leon Zhou,184,The Best Words – Towards Data Science,"Uttered in the heat of a campaign rally in South Carolina on December 30, 2015, this statement was just another of a growing collection of “Trumpisms” by our now-President, Donald J. Trump. These statements both made Donald more beloved by his supporters as their relatable President, while also a cause of ridicule by seemingly everyone else.
Regardless of one’s personal views of the man, it cannot be denied Donald has a way of speaking that is, well, so uniquely him — his smatterings of superlatives and apparent disregard for the constraints of traditional sentence structure are just a few of the things that make his speech instantly recognizable from that of his predecessors or peers.
It was this unique style that interested me, and I set out to try and capture it using machine learning — to generate text that looked and sounded like something Donald Trump might say.
To learn President Trump’s style, I first had to gather sufficient examples of it. I focused my efforts on two primary sources.
The obvious first place to look for words by Donald Trump was his Twitter feed. The current president is unique in his use of the platform as a direct and unfiltered connection to the American people. Furthermore, as a figure of interest, his words have naturally been collected and organized for posterity, saving me the hassle of using the ever-changing and restrictive Twitter API. All in all, there were a little under 31,000 Tweets available for my use.
In addition to his online persona, however, I also wanted to gain a glimpse into his more formal role as President. For this, I turned to the White House Briefing Statements Archive. With the help of some Python tools, I was able to quickly amass a table of about 420 transcripts of speeches and other remarks by the President. These transcripts covered a variety of events, such as meetings with foreign dignitaries, round tables with Congressional members, and awards presentations.
Unlike with the Tweets, where every word was written or dictated by Trump himself, these transcripts involved other politicians and inquisitive reporters. Separating Donald’s words from those of others seemed to be a daunting task.
Enter regular expressions — a boring name for a powerful and decidedly not-boring tool.
Regular expressions allow you to specify a pattern to search for; this pattern can contain any number of very specific constraints, wildcards, or other restrictions to return exactly what you want, and no more.
With some trial and error, I was able to generate a complex regular expression to only return words the President spoke, leaving and discarding any other words or annotations.
Typically, one of the first steps in working with text is to normalize it. The extent and complexity of this normalization varies according to one’s needs, ranging from simply removing punctuation or capital letters, to reducing all variants of a word to a base root. An example of this workflow can be seen here.
For me, however, the specific idiosyncrasies and patterns that would be lost in normalization were exactly what I needed to preserve. So, in hopes of making my generated text just that much more believable and authentic, I elected to bypass most of the standard normalization workflow.
Before diving into a deep learning model, I was curious to explore another frequently used text generation method, the Markov chain. Markov chains have been the go-to for joke text generation for a long time — a quick search will reveal ones for Star Trek, past presidents, the Simpsons, and many others.
The quick and dirty of the Markov chain is that it only cares about the current word in determining what should come next. This algorithm looks at every single time a specific word appears, and every word that comes immediately after it. The next word is selected randomly with a probability proportional to its frequency. Let me illustrate with a quick example:
Donald Trump says the word “taxes.” If, in real life, 70% of the time after he says “taxes” he follows up with the word “bigly,” the Markov chain will choose the next word to be “bigly” 70% of the time. But sometimes, he doesn’t say “bigly.” Sometimes he ends the sentence, or moves on to a different word. The chain will most likely choose “bigly,” but there’s a chance it’ll go for any of the other available options, thus introducing some variety in our generated text.
And repeat ad nauseam, or until the end of the sentence.
This is great for quick and dirty applications, but it’s easy to see where it can go wrong. As the Markov chain only ever cares about the current word, it can easily be sidetracked. A sentence that started off talking about the domestic economy could just as easily end talking about The Apprentice.
With my limited text data set, most of my Markov chain outputs were nonsensical. But, occasionally there were some flashes of brilliance and hilarity:
For passably-real text, however, I needed something more sophisticated. Recurrent Neural Networks (RNNs) have established themselves as the architecture of choice for many text or sequence-based applications. The detailed inner workings of RNNs are outside the scope of this post, but a strong (relatively) beginner-friendly introduction may be found here.
The distinguishing feature of these neural units is that they have an internal “memory” of sorts. Word choice and grammar depend heavily on surrounding context, so this “memory” is extremely useful in creating a coherent thought by keeping track of tense, subjects and objects, and so on.
The downside of these types of networks is that they are extraordinarily computationally expensive — on my piddly laptop, running the entirety of my text through the model once would take over an hour, and considering I’d need to do so about 200 times, this was no good.
This is where cloud computing comes in. A number of established tech companies offer cloud services, the largest being Amazon, Google, and Microsoft. On a heavy-GPU computing instance, that one-hour-plus-per-cycle time became ninety seconds, an over 40x reduction in time!
Can you tell if this following statement is real or not?
This was text generated off of Trump’s endorsement of the Republican gubernatorial candidate, but it might pass as something that Trump tweeted in the run-up to the 2016 general election.
The more complex neural networks I implemented, with hidden fully-connected layers before and after the recurrent layer, were capable of generating internally-consistent text given any seed of 40 characters or less.
Less complex networks stumbled a little on consistency, but still captured the tonal feel of President Trump’s speech:
While not quite producing text at a level capable of fooling you or me consistently, this attempt opened my eyes to the power of RNNs. In short order, these networks learned spelling, some aspects of grammar, and in some instances, how to use hashtags and hyperlinks — imagine what a better-designed network with more text to learn from, and time to learn might produce.
If you’re interested in looking at the code behind these models, you can find the repository here. And, don’t hesitate to reach out with any questions or feedback you may have!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I am a data scientist with a background in chemical engineering and biotech. I am also homeless and live in my car, but that's another thing entirely. Hire me!
Sharing concepts, ideas, and codes.
",uttered heat campaign rally south carolina december 30 2015 statement another growing collection trumpisms nowpresident donald j trump statements made donald beloved supporters relatable president also cause ridicule seemingly everyone else regardless ones personal views man cannot denied donald way speaking well uniquely smatterings superlatives apparent disregard constraints traditional sentence structure things make speech instantly recognizable predecessors peers unique style interested set try capture using machine learning generate text looked sounded like something donald trump might say learn president trumps style first gather sufficient examples focused efforts two primary sources obvious first place look words donald trump twitter feed current president unique use platform direct unfiltered connection american people furthermore figure interest words naturally collected organized posterity saving hassle using everchanging restrictive twitter api little 31000 tweets available use addition online persona however also wanted gain glimpse formal role president turned white house briefing statements archive help python tools able quickly amass table 420 transcripts speeches remarks president transcripts covered variety events meetings foreign dignitaries round tables congressional members awards presentations unlike tweets every word written dictated trump transcripts involved politicians inquisitive reporters separating donalds words others seemed daunting task enter regular expressions boring name powerful decidedly notboring tool regular expressions allow specify pattern search pattern contain number specific constraints wildcards restrictions return exactly want trial error able generate complex regular expression return words president spoke leaving discarding words annotations typically one first steps working text normalize extent complexity normalization varies according ones needs ranging simply removing punctuation capital letters reducing variants word base root example workflow seen however specific idiosyncrasies patterns would lost normalization exactly needed preserve hopes making generated text much believable authentic elected bypass standard normalization workflow diving deep learning model curious explore another frequently used text generation method markov chain markov chains goto joke text generation long time quick search reveal ones star trek past presidents simpsons many others quick dirty markov chain cares current word determining come next algorithm looks every single time specific word appears every word comes immediately next word selected randomly probability proportional frequency let illustrate quick example donald trump says word taxes real life 70 time says taxes follows word bigly markov chain choose next word bigly 70 time sometimes doesnt say bigly sometimes ends sentence moves different word chain likely choose bigly theres chance itll go available options thus introducing variety generated text repeat ad nauseam end sentence great quick dirty applications easy see go wrong markov chain ever cares current word easily sidetracked sentence started talking domestic economy could easily end talking apprentice limited text data set markov chain outputs nonsensical occasionally flashes brilliance hilarity passablyreal text however needed something sophisticated recurrent neural networks rnns established architecture choice many text sequencebased applications detailed inner workings rnns outside scope post strong relatively beginnerfriendly introduction may found distinguishing feature neural units internal memory sorts word choice grammar depend heavily surrounding context memory extremely useful creating coherent thought keeping track tense subjects objects downside types networks extraordinarily computationally expensive piddly laptop running entirety text model would take hour considering id need 200 times good cloud computing comes number established tech companies offer cloud services largest amazon google microsoft heavygpu computing instance onehourpluspercycle time became ninety seconds 40x reduction time tell following statement real text generated trumps endorsement republican gubernatorial candidate might pass something trump tweeted runup 2016 general election complex neural networks implemented hidden fullyconnected layers recurrent layer capable generating internallyconsistent text given seed 40 characters less less complex networks stumbled little consistency still captured tonal feel president trumps speech quite producing text level capable fooling consistently attempt opened eyes power rnns short order networks learned spelling aspects grammar instances use hashtags hyperlinks imagine betterdesigned network text learn time learn might produce youre interested looking code behind models find repository dont hesitate reach questions feedback may quick cheer standing ovation clap show much enjoyed story data scientist background chemical engineering biotech also homeless live car thats another thing entirely hire sharing concepts ideas codes,en,"['the White House Briefing Statements Archive', 'Congressional', 'Star Trek', 'Simpsons', 'Amazon', 'Google', 'Microsoft']"
110,Chris Kalahiki,30,"Beethoven, Picasso, and Artificial Intelligence – Towards Data Science","When people think of the greatest artists who’ve ever lived, they probably think of names like Beethoven or Picasso. No one would ever think of a computer as a great artist. But what if one day, that was indeed the case. Could computers learn to create incredible drawings like the Mona Lisa? Perhaps one day a robot will be capable of composing the next great symphony. Some experts believe this to be the case. In fact, some of the greatest minds in artificial intelligence are diligently working to develop programs that can create drawing and music independently from humans. The use of artificial intelligence in the field of art has even been picked up by tech giants the likes of Google.
The projects that are included in this paper could have drastic implications in our everyday lives. They may also change the way we view art. They also showcase the incredible advancement that has been made in the field of artificial intelligence. Image recognition is not as far as the research goes. Nor is the ability to generate music in the styling of the great artists of our past. Although these topics will be touched upon, we will focus on several more advanced achievements such as text descriptions being turned into images and generating art and music that is totally original. Each of these projects bring something new and innovative to the table and show us exactly how the art space is a great place to further explore applications of artificial intelligence. We will be discussing problems that have been faced in these projects and how they have been overcome. The future of AI looks bright. Let’s look at what the future may hold. In doing this, we may be able to better understand the impact that artificial intelligence can have in an area that is driven by human creativity.
Machines must be educated. They learn from instruction. How do we lead machines away from emulating what already exists, and have them create new techniques? “No creative artist will create art today that tries to emulate the Baroque or Impressionist style, or any other traditional style, unless trying to do so ironically” [4]. This problem isn’t limited to paintings either. Music can be very structured in some respects, but is also a form of art that requires vast creativity. So how do we go about solving such a problem? The first concept we will discuss is something called GAN (Generative Adversarial Networks). GANs, although quite complex, are becoming an outdated model. If artificial intelligence in the art space is to advance, researchers and developers will have to work to find better methods to allow machines to generate art and music. Two of these such methods are presented in the form of Sketch-RNN and CAN (Creative Adversarial Networks). Each of these methods have their advantages over GANs.
First, let’s explore what exactly a GAN is. Below is a small excerpt explaining how a GAN works:
Generative Adversarial Network (GAN) has two sub networks, a generator and a discriminator. The discriminator has access to a set of images (training images). The discriminator tries to discriminate between “real” images (from the training set) and “fake” images generated by the generator. The generator tries to generate images similar to the training set without seeing the images [4].
The more images the generator creates, the closer they get to the images from the training set. The idea is that after a certain number of images are generated, the GAN will create images that are very similar to what we consider art. This is a very impressive accomplishment to say the least. But what if we take it a step further?
Many issues associated with the GAN are simply limitations on what it can do. The GAN is powerful, but can’t do quite as much as we would like. For example, the generator in the model described above will continue to create images closer and closer to the images given to the discriminator that it isn’t producing original art. Could a GAN be trained to draw alongside a user? It’s not likely. The model wouldn’t be able to turn a text-based description of an image into an actual picture either. As impressive as the GAN may be, we would all agree that it can be improved. Each of the shortcoming mentioned have actually been addressed and, to an extent, solved. Let’s look at how this is done.
Sketch-RNN is a recurrent neural network model developed by Google. The goal of Sketch-RNN is to help machines learn to create art in a manner similar to the way a human may learn. It has been used in a Google AI Experiment to be able to sketch alongside a user. While doing so, it can provide the users with suggestions and even complete the user’s sketch when they decide to take a break. Sketch-RNN is exposed to a massive number of sketches provided through a dataset of vector drawings obtained through another Google application that we will discuss later. Each of these sketches are tagged to let the program know what object is in the sketch. The data set represents the sketch as a set of pen strokes. This allows Sketch-RNN to then learn what aspects each sketch of a certain object has in common. If a user begins to draw a cat, Sketch-RNN could then show the user other common features that could be on the cat. This model could have many new creative applications. “The decoder-only model trained on various classes can assist the creative process of an artist by suggesting many possible ways of finishing a sketch” [3]. The Sketch-RNN team even believes that, given a more complex dataset, the applications could be used in an educational sense to teach users how to draw. These applications of Sketch-RNN couldn’t be nearly as easily achieved with GAN alone.
Another method used to improve upon GAN is the Creative Adversarial Network. In their paper regarding adversarial networks generating art, several researchers discuss a new way of generating art through CANs. The idea is that the CAN has two adversary networks. One, the generator, has no access to any art. It has no basis to go off of when generating images. The other network, the discriminator, is trained to classify the images generated as being art or not. When an image is generated, the discriminator gives the generator two pieces of information. The first is whether it believes the generated image comes from the same distributor as the pieces of art it was trained on, and the other being how the discriminator can fit the generated image into one of the categories of art it was taught. This technique is fantastic in that it helps the generator create images that are both emulative of past works of art in the sense that it learns what was good about those images and creative in a sense that it is taught to produce new and different artistic concepts. This is a big difference from GAN creating art that emulated the training images. Eventually, the CAN will learn how to produce only new and innovative artwork.
One final future for the vanilla GAN is StackGAN. StackGAN is a text to photo-realistic image synthesizer that uses stacked generative adversarial networks. Given a text description, the StackGAN is able to create images that are very much related to the given text. This wouldn’t be doable with a normal GAN model as it would be much too difficult to generate photo-realistic images from a text description even with a state-of-the-art training database. This is where StackGAN comes in. It breaks the problem down into 2 parts. “Low-resolution images are generated by our Stage-I GAN. On the top of our Stage-I GAN, we stack Stage-II GAN to generate realistic high-resolution images conditioned on Stage-I results and text descriptions” [7]. It is through the conditioning on Stage-I results and text descriptions that Stage-II GAN can find details that Stage-I GAN may have missed and create higher resolution images. By breaking the problem down into smaller subproblems, the StackGAN can tackle problems that aren’t possible with a regular GAN. On the next page is an image showing the difference between a regular GAN and each step of the StackGAN.
It is through advancements like these that have been made in recent years that we can continue to push the boundaries of what AI can do. We have just seen three ways to improve upon a concept that was already quite complex and innovative. Each of these advancements have a practical, everyday use. As we continue to improve on artificial intelligence techniques, we will able to do more and more in regard to, not just art and music, but a wide variety of tasks to improve our lives.
Images aren’t the only type of art that artificial intelligence can impact though. Its effect on music is being explored as we speak. We will now explore some specific cases and their impact on both music and artificial intelligence. In doing this, we should be able to see how art can do as much for AI as AI does for it. Both fields benefit heavily from the types of projects that we are exploring here.
Could a machine ever be able to create a piece of music the likes of Johann Sebastian Bach? In a project known as DeepBach, several researchers looked to create pieces similar to Bach’s chorales. The beauty of DeepBach is that it “is able to generate coherent musical phrases and provides, for instance, varied reharmonizations of melodies without plagiarism” [6]. What this means it that DeepBach can create music with correct structure and be original. It is just in the style of Bach. It isn’t just a mashup of his works. DeepBach is creating new content. The developers of DeepBach went on to test whether their product could actually fool listeners.
As part of the experiment, over 1,250 people were asked to vote whether pieces presented to them were in fact composed by Bach. The subjects had varying degrees of musical expertise. The results showed that as the model for DeepBach’s complexity increased, the subjects had more and more trouble distinguishing the chorales of Bach from those of DeepBach. This experiment shows us that through the use of artificial intelligence and machine learning, it is quite possible to recreate original works in the likeness of the greats. But is that the limit to what artificial intelligence can do in the field of art and music?
DeepBach has achieved something that would have been unheard of in the not so distant past, but it certainly isn’t the fullest extent of what AI can do to benefit the field of music. What if we want to create new and innovative music? Maybe AI can change the way music is created all together. There must be projects that do more to push the envelope. As a matter of fact, that is exactly what the team behind Magenta look to do.
Magenta is a project being conducted by the Google Brain team and lead by Douglas Eck. Eck has been working for Google since 2010, but that isn’t where his interest in Music began. Eck helped found Brain Music and Sound, an international laboratory for brain, music, and sound research. He was also involved at the McGill Centre for Interdisciplinary Research in Music Media and Technology, and was an Associate Professor in Computer Science at the University of Montreal.
Magenta’s goal is to be “a research project to advance the state of the art in machine intelligence for music and art generation” [2]. It is an open source project that uses TensorFlow. Magenta aims to learn how to generate art and music in a way that is indeed generative. It must go past just emulating existing music. This is distinctly different that projects along the line of DeepBach which set out to emulate existing music in a way that wasn’t plagiarizing existing pieces of music. Eck and company realize that art is about capturing elements of surprise and drawing attention to certain aspects. “This leads to perhaps the biggest challenge: combining generation, attention and surprise to tell a compelling story. So much of machine-generated music and art is good in small chunks, but lacks any sort of long-term narrative arc” [2]. Such a perspective gives computer-generated music more substance, and helps it to become less of a gimmick.
One of the projects the magenta team has developed is called NSynth. The idea behind NSynth is to be able to create new sounds that have never been heard before, but beyond that, to reimagine how music synthesis can be done. Unlike ordinary synthesizers that focus on “a specific arrangement of oscillators or an algorithm for sample playback, such as FM Synthesis or Granular Synthesis” [5], NSynth generates sounds on an individual level. To do this, it uses deep neural networks. Google has even launched an experiment that allows users to really see what NSynth can do by allowing them to fuse together the sounds of existing instruments to create new hybrid sounds that have never been heard before. As an example, users can take two instruments such as a banjo and a tuba, and take parts of each of their sounds to create a totally new instrument. The experiment also allowed users to decide what percentage of each instrument would be used.
Projects like Magenta go above and beyond in showing us the full extent of what artificial intelligence can do in the way of generating music. They explore new applications of artificial intelligence that can generate new ideas independent of humans. It is the closest we have come to machine creativity. Although machines aren’t yet able to truly think and express creativity, they may soon be able to generate new and unique art and music for us to enjoy. Don’t worry though. Eck doesn’t intend to replace artists with AI. Instead he looks to provide artists with tools to create music in an entirely new way.
As we look ahead to a few more of the ways that AI has been used to accomplish new and innovative ideas in the art space, we look at projects like Quick, Draw! and Deep Dream. These projects showcase amazing progress in the space while pointing out some issues that researchers in AI will have to work out in the years to come.
Quick, Draw! is an application from the Google Creative Lab, trained to recognize quick drawings much like one would see in a game of Pictionary. The program can recognize simple objects such as cats and apples based on common aspects of the many pictures it was given before. Although the program will not get every picture right each time it is used, it continues to learn from the similarities in the picture drawn and the hundreds of pictures before it.
The science behind Quick, Draw! “uses some of the same technology that helps Google Translate recognize your handwriting. To understand handwritings or drawings, you don’t just look at what the person drew. You look at how they actually drew it” [1]. It is presented in the form of a game, with the user drawing a picture of an object chosen by the application. The program then has 20 seconds to recognize the image. In each session, the user is given a total of 6 objects. The images are then stored to the database used to train application. This happens to be the same database we saw earlier in the Sketch-RNN application. This image recognition is a very practical use of artificial intelligence in the realm of art and music. It can do a lot to benefit us in our everyday lives. But this only begins to scratch the surface of what artificial intelligence can do in this field. Although this is very impressive, we might point out that the application doesn’t truly understand what is being drawn. It is just picking up on patterns. In fact, this distinction is part of the gap between simple AI techniques and true artificial general intelligence. Machines that truly understand what the objects in images are don’t appear to be coming in the near future.
Another interesting project in the art space is Google’s Deep Dream project, which uses AI to create new and unique images. Unfortunately, the Deep Dream Generator Team wouldn’t go into too much detail about the technology itself (mostly fearing it would be too long for an email) [8]. They did, however, explain that convolutional neural networks train on the famous ImageNet dataset. Those neural networks are then used to create art-like images. Essentially, Deep Dream takes the styling of one image and uses it to modify another image. The results can be anything from a silly fusion to an artistic masterpiece. This occurs when the program identifies the unique stylings of an image provided by the user and imposes those stylings onto another image that the user provides. What can easily be observed through the use of Deep Dream is that computers aren’t yet capable of truly understanding what they are doing with respect to art. They can be fed complex algorithms to generate images, but don’t fundamentally understand what it is they are generating. For example, a computer may see a knife cutting through an onion and assume the knife and onion are one object. The lack of an ability to truly understand the contents of an image is one dilemma that researchers have yet to solve.
Perhaps as we continue to make advances in artificial intelligence we will be able to have machines that do truly understand what objects are in an image and even the emotions evoked by their music. The only way for this to be achieved is by reaching true artificial general intelligence (AGI). IN the meantime, the Deep Dream team believes that generative models will be able to create some really interesting pieces of art and digital content.
For this section, we will consider where artificial intelligence could be heading in the art space. We will take a look at how AI has impacted the space and in what ways it can continue to do so. We will also look at ways art and music could continue to impact AI in the years to come.
Although I don’t feel that we have completely mastered the ability to emulate the great artists of our past, it is just a matter of time before that problem is solved. The real task to be solved is that of creating new innovations in art and music. We need to work towards creation without emulation. It is quite clear that we are headed in that direction through projects like CAN and Magenta. Artificial general intelligence (AGI) is not the only way to complete this task. As a matter of fact, even those who dispute the possibility of AGI would have a hard time disputing the creation of unique works of art by a machine.
One path that may be taken to further improve art and music through AI is to create more advanced datasets to use in training the complex networks like Sketch-RNN and Deep Dream. AI needs to be trained to be able to perform as expected. That training has a huge impact on the results we get. Shouldn’t we want to train our machines in the most beneficial way possible. Even developing software like Sketch-RNN to use the ImageNet dataset used in Deep Dream could be huge in educating artists on techniques for drawing complex, realistic images. Complex datasets could very well be our answer to more efficient training. Until our machines can think and learn like we do, we will need to be very careful what data is used to train them.
One of the ways that art and music can help to impact AI is by providing another method of Turing Testing machines. For those who dream of creating AGI, what better way to test the machine’s ability that to create something that tests the full extent of human-like creativity? Art is the truest representation of human creativity. That is, in fact, its essence. Although art is probably not the ultimate end game for artificial intelligence, it could be one of the best ways to test the limits of what a machine can do. The day that computers can create original musical composition and create images based on descriptions given by a user could very well be the day that we stop being able to distinguish man from machine.
There are many benefits to using artificial intelligence in the music space. Some of them have already been seen in the projects we have discussed so far. We have seen how artificial intelligence could be used for image recognition as well as their ability to turn our words into fantastic images. We have also seen how AI can be used to synthesize new sounds that have never been heard. We know that artificial intelligence can be used to create art alongside us as well as independently from us. It can be taught to mimic music from the past and can create novel ideas. All of these accomplishments are a part of what will drive AI research into the future. Who knows? Perhaps one day we will achieve artificial general intelligence and machines will be able to understand what is really in the images it is given. Maybe our computers will be able to understand how their art makes us feel. There is a clear path showing us where to go from here. I firmly believe that it is up to us to continue this research and test the limits of what artificial intelligence can do, both in the field of art and in our everyday lives.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Computer Science student at Louisiana Tech University with an interest in anything AI.
Sharing concepts, ideas, and codes.
",people think greatest artists whove ever lived probably think names like beethoven picasso one would ever think computer great artist one day indeed case could computers learn create incredible drawings like mona lisa perhaps one day robot capable composing next great symphony experts believe case fact greatest minds artificial intelligence diligently working develop programs create drawing music independently humans use artificial intelligence field art even picked tech giants likes google projects included paper could drastic implications everyday lives may also change way view art also showcase incredible advancement made field artificial intelligence image recognition far research goes ability generate music styling great artists past although topics touched upon focus several advanced achievements text descriptions turned images generating art music totally original projects bring something new innovative table show us exactly art space great place explore applications artificial intelligence discussing problems faced projects overcome future ai looks bright lets look future may hold may able better understand impact artificial intelligence area driven human creativity machines must educated learn instruction lead machines away emulating already exists create new techniques creative artist create art today tries emulate baroque impressionist style traditional style unless trying ironically 4 problem isnt limited paintings either music structured respects also form art requires vast creativity go solving problem first concept discuss something called gan generative adversarial networks gans although quite complex becoming outdated model artificial intelligence art space advance researchers developers work find better methods allow machines generate art music two methods presented form sketchrnn creative adversarial networks methods advantages gans first lets explore exactly gan small excerpt explaining gan works generative adversarial network gan two sub networks generator discriminator discriminator access set images training images discriminator tries discriminate real images training set fake images generated generator generator tries generate images similar training set without seeing images 4 images generator creates closer get images training set idea certain number images generated gan create images similar consider art impressive accomplishment say least take step many issues associated gan simply limitations gan powerful cant quite much would like example generator model described continue create images closer closer images given discriminator isnt producing original art could gan trained draw alongside user likely model wouldnt able turn textbased description image actual picture either impressive gan may would agree improved shortcoming mentioned actually addressed extent solved lets look done sketchrnn recurrent neural network model developed google goal sketchrnn help machines learn create art manner similar way human may learn used google ai experiment able sketch alongside user provide users suggestions even complete users sketch decide take break sketchrnn exposed massive number sketches provided dataset vector drawings obtained another google application discuss later sketches tagged let program know object sketch data set represents sketch set pen strokes allows sketchrnn learn aspects sketch certain object common user begins draw cat sketchrnn could show user common features could cat model could many new creative applications decoderonly model trained various classes assist creative process artist suggesting many possible ways finishing sketch 3 sketchrnn team even believes given complex dataset applications could used educational sense teach users draw applications sketchrnn couldnt nearly easily achieved gan alone another method used improve upon gan creative adversarial network paper regarding adversarial networks generating art several researchers discuss new way generating art cans idea two adversary networks one generator access art basis go generating images network discriminator trained classify images generated art image generated discriminator gives generator two pieces information first whether believes generated image comes distributor pieces art trained discriminator fit generated image one categories art taught technique fantastic helps generator create images emulative past works art sense learns good images creative sense taught produce new different artistic concepts big difference gan creating art emulated training images eventually learn produce new innovative artwork one final future vanilla gan stackgan stackgan text photorealistic image synthesizer uses stacked generative adversarial networks given text description stackgan able create images much related given text wouldnt doable normal gan model would much difficult generate photorealistic images text description even stateoftheart training database stackgan comes breaks problem 2 parts lowresolution images generated stagei gan top stagei gan stack stageii gan generate realistic highresolution images conditioned stagei results text descriptions 7 conditioning stagei results text descriptions stageii gan find details stagei gan may missed create higher resolution images breaking problem smaller subproblems stackgan tackle problems arent possible regular gan next page image showing difference regular gan step stackgan advancements like made recent years continue push boundaries ai seen three ways improve upon concept already quite complex innovative advancements practical everyday use continue improve artificial intelligence techniques able regard art music wide variety tasks improve lives images arent type art artificial intelligence impact though effect music explored speak explore specific cases impact music artificial intelligence able see art much ai ai fields benefit heavily types projects exploring could machine ever able create piece music likes johann sebastian bach project known deepbach several researchers looked create pieces similar bachs chorales beauty deepbach able generate coherent musical phrases provides instance varied reharmonizations melodies without plagiarism 6 means deepbach create music correct structure original style bach isnt mashup works deepbach creating new content developers deepbach went test whether product could actually fool listeners part experiment 1250 people asked vote whether pieces presented fact composed bach subjects varying degrees musical expertise results showed model deepbachs complexity increased subjects trouble distinguishing chorales bach deepbach experiment shows us use artificial intelligence machine learning quite possible recreate original works likeness greats limit artificial intelligence field art music deepbach achieved something would unheard distant past certainly isnt fullest extent ai benefit field music want create new innovative music maybe ai change way music created together must projects push envelope matter fact exactly team behind magenta look magenta project conducted google brain team lead douglas eck eck working google since 2010 isnt interest music began eck helped found brain music sound international laboratory brain music sound research also involved mcgill centre interdisciplinary research music media technology associate professor computer science university montreal magentas goal research project advance state art machine intelligence music art generation 2 open source project uses tensorflow magenta aims learn generate art music way indeed generative must go past emulating existing music distinctly different projects along line deepbach set emulate existing music way wasnt plagiarizing existing pieces music eck company realize art capturing elements surprise drawing attention certain aspects leads perhaps biggest challenge combining generation attention surprise tell compelling story much machinegenerated music art good small chunks lacks sort longterm narrative arc 2 perspective gives computergenerated music substance helps become less gimmick one projects magenta team developed called nsynth idea behind nsynth able create new sounds never heard beyond reimagine music synthesis done unlike ordinary synthesizers focus specific arrangement oscillators algorithm sample playback fm synthesis granular synthesis 5 nsynth generates sounds individual level uses deep neural networks google even launched experiment allows users really see nsynth allowing fuse together sounds existing instruments create new hybrid sounds never heard example users take two instruments banjo tuba take parts sounds create totally new instrument experiment also allowed users decide percentage instrument would used projects like magenta go beyond showing us full extent artificial intelligence way generating music explore new applications artificial intelligence generate new ideas independent humans closest come machine creativity although machines arent yet able truly think express creativity may soon able generate new unique art music us enjoy dont worry though eck doesnt intend replace artists ai instead looks provide artists tools create music entirely new way look ahead ways ai used accomplish new innovative ideas art space look projects like quick draw deep dream projects showcase amazing progress space pointing issues researchers ai work years come quick draw application google creative lab trained recognize quick drawings much like one would see game pictionary program recognize simple objects cats apples based common aspects many pictures given although program get every picture right time used continues learn similarities picture drawn hundreds pictures science behind quick draw uses technology helps google translate recognize handwriting understand handwritings drawings dont look person drew look actually drew 1 presented form game user drawing picture object chosen application program 20 seconds recognize image session user given total 6 objects images stored database used train application happens database saw earlier sketchrnn application image recognition practical use artificial intelligence realm art music lot benefit us everyday lives begins scratch surface artificial intelligence field although impressive might point application doesnt truly understand drawn picking patterns fact distinction part gap simple ai techniques true artificial general intelligence machines truly understand objects images dont appear coming near future another interesting project art space googles deep dream project uses ai create new unique images unfortunately deep dream generator team wouldnt go much detail technology mostly fearing would long email 8 however explain convolutional neural networks train famous imagenet dataset neural networks used create artlike images essentially deep dream takes styling one image uses modify another image results anything silly fusion artistic masterpiece occurs program identifies unique stylings image provided user imposes stylings onto another image user provides easily observed use deep dream computers arent yet capable truly understanding respect art fed complex algorithms generate images dont fundamentally understand generating example computer may see knife cutting onion assume knife onion one object lack ability truly understand contents image one dilemma researchers yet solve perhaps continue make advances artificial intelligence able machines truly understand objects image even emotions evoked music way achieved reaching true artificial general intelligence agi meantime deep dream team believes generative models able create really interesting pieces art digital content section consider artificial intelligence could heading art space take look ai impacted space ways continue also look ways art music could continue impact ai years come although dont feel completely mastered ability emulate great artists past matter time problem solved real task solved creating new innovations art music need work towards creation without emulation quite clear headed direction projects like magenta artificial general intelligence agi way complete task matter fact even dispute possibility agi would hard time disputing creation unique works art machine one path may taken improve art music ai create advanced datasets use training complex networks like sketchrnn deep dream ai needs trained able perform expected training huge impact results get shouldnt want train machines beneficial way possible even developing software like sketchrnn use imagenet dataset used deep dream could huge educating artists techniques drawing complex realistic images complex datasets could well answer efficient training machines think learn like need careful data used train one ways art music help impact ai providing another method turing testing machines dream creating agi better way test machines ability create something tests full extent humanlike creativity art truest representation human creativity fact essence although art probably ultimate end game artificial intelligence could one best ways test limits machine day computers create original musical composition create images based descriptions given user could well day stop able distinguish man machine many benefits using artificial intelligence music space already seen projects discussed far seen artificial intelligence could used image recognition well ability turn words fantastic images also seen ai used synthesize new sounds never heard know artificial intelligence used create art alongside us well independently us taught mimic music past create novel ideas accomplishments part drive ai research future knows perhaps one day achieve artificial general intelligence machines able understand really images given maybe computers able understand art makes us feel clear path showing us go firmly believe us continue research test limits artificial intelligence field art everyday lives quick cheer standing ovation clap show much enjoyed story computer science student louisiana tech university interest anything ai sharing concepts ideas codes,en,"['Google', 'Sketch-RNN', 'GAN', 'Generative Adversarial Network', 'the Creative Adversarial Network', 'CAN', 'Stage-I GAN', 'Google Brain', 'Music', 'Brain Music and Sound', 'the McGill Centre for Interdisciplinary Research', 'the University of Montreal', 'TensorFlow', 'AI', 'the Google Creative Lab', 'Deep Dream', 'the Deep Dream Generator Team', 'ImageNet', 'fed', 'the Deep Dream', 'AGI', 'Turing Testing', 'Louisiana Tech University']"
111,Stefan Kojouharov,14200,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic.
This is the most complete list and the Big-O is at the very end, enjoy...
This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. The flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it.
Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.
In May 2017 Google announced the second-generation of the TPU, as well as the availability of the TPUs in Google Compute Engine.[12] The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs provide up to 11.5 petaflops.
In 2017, Google’s TensorFlow team decided to support Keras in TensorFlow’s core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library.
NumPy targets the CPython reference implementation of Python, which is a non-optimizing bytecode interpreter. Mathematical algorithms written for this version of Python often run much slower than compiled equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy.
The name ‘Pandas’ is derived from the term “panel data”, an econometrics term for multidimensional structured data sets.
The term “data wrangler” is starting to infiltrate pop culture. In the 2017 movie Kong: Skull Island, one of the characters, played by actor Marc Evan Jackson is introduced as “Steve Woodward, our data wrangler”.
SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack.[3]
matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.[2] SciPy makes use of matplotlib.
pyplot is a matplotlib module which provides a MATLAB-like interface.[6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free.
>>> If you like this list, you can let me know here. <<<
Stefan is the founder of Chatbot’s Life, a Chatbot media and consulting firm. Chatbot’s Life has grown to over 150k views per month and has become the premium place to learn about Bots & AI online. Chatbot’s Life has also consulted many of the top Bot companies like Swelly, Instavest, OutBrain, NearGroup and a number of Enterprises.
Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/
Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf
Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics
Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling
Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs
Keras: https://en.wikipedia.org/wiki/Keras
Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/
Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet
ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY
Matpotlib: https://en.wikipedia.org/wiki/Matplotlib
Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/
Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/
Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network
Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE
NumPy: https://en.wikipedia.org/wiki/NumPy
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM
Pandas: https://en.wikipedia.org/wiki/Pandas_(software)
Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc
Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ
Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet
Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn
Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html
Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI
SciPy: https://en.wikipedia.org/wiki/SciPy
TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html
Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of Chatbots Life. I help Companies Create Great Chatbots & AI Systems and share my Insights along the way.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
",past months collecting ai cheat sheets time time share friends colleagues recently getting asked lot decided organize share entire collection make things interesting give context added descriptions andor excerpts major topic complete list bigo end enjoy machine learning cheat sheet help find right estimator job difficult part flowchart help check documentation rough guide estimator help know problems solve scikitlearn formerly scikitslearn free software machine learning library python programming language features various classification regression clustering algorithms including support vector machines random forests gradient boosting kmeans dbscan designed interoperate python numerical scientific libraries numpy scipy may 2017 google announced secondgeneration tpu well availability tpus google compute engine12 secondgeneration tpus deliver 180 teraflops performance organized clusters 64 tpus provide 115 petaflops 2017 googles tensorflow team decided support keras tensorflows core library chollet explained keras conceived interface rather endtoend machinelearning framework presents higherlevel intuitive set abstractions make easy configure neural networks regardless backend scientific computing library numpy targets cpython reference implementation python nonoptimizing bytecode interpreter mathematical algorithms written version python often run much slower compiled equivalents numpy address slowness problem partly providing multidimensional arrays functions operators operate efficiently arrays requiring rewriting code mostly inner loops using numpy name pandas derived term panel data econometrics term multidimensional structured data sets term data wrangler starting infiltrate pop culture 2017 movie kong skull island one characters played actor marc evan jackson introduced steve woodward data wrangler scipy builds numpy array object part numpy stack includes tools like matplotlib pandas sympy expanding set scientific computing libraries numpy stack similar users applications matlab gnu octave scilab numpy stack also sometimes referred scipy stack3 matplotlib plotting library python programming language numerical mathematics extension numpy provides objectoriented api embedding plots applications using generalpurpose gui toolkits like tkinter wxpython qt gtk also procedural pylab interface based state machine like opengl designed closely resemble matlab though use discouraged2 scipy makes use matplotlib pyplot matplotlib module provides matlablike interface6 matplotlib designed usable matlab ability use python advantage free like list let know stefan founder chatbots life chatbot media consulting firm chatbots life grown 150k views per month become premium place learn bots ai online chatbots life also consulted many top bot companies like swelly instavest outbrain neargroup number enterprises bigo algorithm cheat sheet httpbigocheatsheetcom bokeh cheat sheet httpss3amazonawscomassetsdatacampcomblog_assetspython_bokeh_cheat_sheetpdf data science cheat sheet httpswwwdatacampcomcommunitytutorialspythondatasciencecheatsheetbasics data wrangling cheat sheet httpswwwrstudiocomwpcontentuploads201502datawranglingcheatsheetpdf data wrangling httpsenwikipediaorgwikidata_wrangling ggplot cheat sheet httpswwwrstudiocomwpcontentuploads201503ggplot2cheatsheetpdf keras cheat sheet httpswwwdatacampcomcommunityblogkerascheatsheetgsdrkenms keras httpsenwikipediaorgwikikeras machine learning cheat sheet httpsaiicymiemailnewmachinelearningcheatsheetbyemilybarryabdsc machine learning cheat sheet httpsdocsmicrosoftcomeninazuremachinelearningmachinelearningalgorithmcheatsheet ml cheat sheet httppeekaboovisionblogspotcom201301machinelearningcheatsheetforscikithtml matplotlib cheat sheet httpswwwdatacampcomcommunityblogpythonmatplotlibcheatsheetgsuekyspy matpotlib httpsenwikipediaorgwikimatplotlib neural networks cheat sheet httpwwwasimovinstituteorgneuralnetworkzoo neural networks graph cheat sheet httpwwwasimovinstituteorgblog neural networks httpswwwquoracomwherecanfindacheatsheetforneuralnetwork numpy cheat sheet httpswwwdatacampcomcommunityblogpythonnumpycheatsheetgsak5zbge numpy httpsenwikipediaorgwikinumpy pandas cheat sheet httpswwwdatacampcomcommunityblogpythonpandascheatsheetgsoundfxm pandas httpsenwikipediaorgwikipandas_software pandas cheat sheet httpswwwdatacampcomcommunityblogpandascheatsheetpythongshpforic pyspark cheat sheet httpswwwdatacampcomcommunityblogpysparkcheatsheetpythongslj1zxq scikit cheat sheet httpswwwdatacampcomcommunityblogscikitlearncheatsheet scikitlearn httpsenwikipediaorgwikiscikitlearn scikitlearn cheat sheet httppeekaboovisionblogspotcom201301machinelearningcheatsheetforscikithtml scipy cheat sheet httpswwwdatacampcomcommunityblogpythonscipycheatsheetgsjdsg3oi scipy httpsenwikipediaorgwikiscipy tesorflow cheat sheet httpswwwaltoroscomtensorflowcheatsheethtml tensor flow httpsenwikipediaorgwikitensorflow quick cheer standing ovation clap show much enjoyed story founder chatbots life help companies create great chatbots ai systems share insights along way latest news info tutorials artificial intelligence machine learning deep learning big data means humanity,en,"['DBSCAN', 'Google', 'Google Compute Engine.[12', 'TensorFlow', 'Keras', 'NumPy', 'MATLAB', 'GNU Octave', 'Tkinter, wxPython, Qt', 'Chatbot’s Life', 'Bots & AI', 'Instavest', 'OutBrain', 'NearGroup', 'Enterprises', 'Bokeh Cheat Sheet', 'Keras Cheat Sheet', 'Pandas Cheat Sheet', 'Cheat Sheet', 'TesorFlow Cheat Sheet', 'Chatbots Life', 'Companies Create Great Chatbots & AI Systems', 'Latest News', 'Info', 'Tutorials on Artificial Intelligence, Machine Learning', 'Humanity']"
112,Netflix Technology Blog,99,Distributed Neural Networks with GPUs in the AWS Cloud,"by Alex Chen, Justin Basilico, and Xavier Amatriain
As we have described previously on this blog, at Netflix we are constantly innovating by looking for better ways to find the best movies and TV shows for our members. When a new algorithmic technique such as Deep Learning shows promising results in other domains (e.g. Image Recognition, Neuro-imaging, Language Models, and Speech Recognition), it should not come as a surprise that we would try to figure out how to apply such techniques to improve our product. In this post, we will focus on what we have learned while building infrastructure for experimenting with these approaches at Netflix. We hope that this will be useful for others working on similar algorithms, especially if they are also leveraging the Amazon Web Services (AWS) infrastructure. However, we will not detail how we are using variants of Artificial Neural Networks for personalization, since it is an active area of research.
Many researchers have pointed out that most of the algorithmic techniques used in the trendy Deep Learning approaches have been known and available for some time. Much of the more recent innovation in this area has been around making these techniques feasible for real-world applications. This involves designing and implementing architectures that can execute these techniques using a reasonable amount of resources in a reasonable amount of time. The first successful instance of large-scale Deep Learning made use of 16000 CPU cores in 1000 machines in order to train an Artificial Neural Network in a matter of days. While that was a remarkable milestone, the required infrastructure, cost, and computation time are still not practical.
Andrew Ng and his team addressed this issue in follow up work . Their implementation used GPUs as a powerful yet cheap alternative to large clusters of CPUs. Using this architecture, they were able to train a model 6.5 times larger in a few days using only 3 machines. In another study, Schwenk et al. showed that training these models on GPUs can improve performance dramatically, even when comparing to high-end multicore CPUs.
Given our well-known approach and leadership in cloud computing, we sought out to implement a large-scale Neural Network training system that leveraged both the advantages of GPUs and the AWS cloud. We wanted to use a reasonable number of machines to implement a powerful machine learning solution using a Neural Network approach. We also wanted to avoid needing special machines in a dedicated data center and instead leverage the full, on-demand computing power we can obtain from AWS.
In architecting our approach for leveraging computing power in the cloud, we sought to strike a balance that would make it fast and easy to train Neural Networks by looking at the entire training process. For computing resources, we have the capacity to use many GPU cores, CPU cores, and AWS instances, which we would like to use efficiently. For an application such as this, we typically need to train not one, but multiple models either from different datasets or configurations (e.g. different international regions). For each configuration we need to perform hyperparameter tuning, where each combination of parameters requires training a separate Neural Network. In our solution, we take the approach of using GPU-based parallelism for training and using distributed computation for handling hyperparameter tuning and different configurations.
Some of you might be thinking that the scenario described above is not what people think of as a distributed Machine Learning in the traditional sense. For instance, in the work by Ng et al. cited above, they distribute the learning algorithm itself between different machines. While that approach might make sense in some cases, we have found that to be not always the norm, especially when a dataset can be stored on a single instance. To understand why, we first need to explain the different levels at which a model training process can be distributed.
In a standard scenario, we will have a particular model with multiple instances. Those instances might correspond to different partitions in your problem space. A typical situation is to have different models trained for different countries or regions since the feature distribution and even the item space might be very different from one region to the other. This represents the first initial level at which we can decide to distribute our learning process. We could have, for example, a separate machine train each of the 41 countries where Netflix operates, since each region can be trained entirely independently.
However, as explained above, training a single instance actually implies training and testing several models, each corresponding to a different combinations of hyperparameters. This represents the second level at which the process can be distributed. This level is particularly interesting if there are many parameters to optimize and you have a good strategy to optimize them, like Bayesian optimization with Gaussian Processes. The only communication between runs are hyperparameter settings and test evaluation metrics.
Finally, the algorithm training itself can be distributed. While this is also interesting, it comes at a cost. For example, training ANN is a comparatively communication-intensive process. Given that you are likely to have thousands of cores available in a single GPU instance, it is very convenient if you can squeeze the most out of that GPU and avoid getting into costly across-machine communication scenarios. This is because communication within a machine using memory is usually much faster than communication over a network.
The following pseudo code below illustrates the three levels at which an algorithm training process like us can be distributed.
In this post we will explain how we addressed level 1 and 2 distribution in our use case. Note that one of the reasons we did not need to address level 3 distribution is because our model has millions of parameters (compared to the billions in the original paper by Ng).
Before we addressed distribution problem though, we had to make sure the GPU-based parallel training was efficient. We approached this by first getting a proof-of-concept to work on our own development machines and then addressing the issue of how to scale and use the cloud as a second stage. We started by using a Lenovo S20 workstation with a Nvidia Quadro 600 GPU. This GPU has 98 cores and provides a useful baseline for our experiments; especially considering that we planned on using a more powerful machine and GPU in the AWS cloud. Our first attempt to train our Neural Network model took 7 hours.
We then ran the same code to train the model in on a EC2’s cg1.4xlarge instance, which has a more powerful Tesla M2050 with 448 cores. However, the training time jumped from 7 to over 20 hours.
Profiling showed that most of the time was spent on the function calls to Nvidia Performance Primitive library, e.g. nppsMulC_32f_I, nppsExp_32f_I. Calling the npps functions repeatedly took 10x more system time on the cg1 instance than in the Lenovo S20.
While we tried to uncover the root cause, we worked our way around the issue by reimplementing the npps functions using the customized cuda kernel, e.g. replace nppsMulC_32f_I function with:
Replacing all npps functions in this way for the Neural Network code reduced the total training time on the cg1 instance from over 20 hours to just 47 minutes when training on 4 million samples. Training 1 million samples took 96 seconds of GPU time. Using the same approach on the Lenovo S20 the total training time also reduced from 7 hours to 2 hours. This makes us believe that the implementation of these functions is suboptimal regardless of the card specifics.
While we were implementing this “hack”, we also worked with the AWS team to find a principled solution that would not require a kernel patch. In doing so, we found that the performance degradation was related to the NVreg_CheckPCIConfigSpace parameter of the kernel. According to RedHat, setting this parameter to 0 disables very slow accesses to the PCI configuration space. In a virtualized environment such as the AWS cloud, these accesses cause a trap in the hypervisor that results in even slower access.
NVreg_CheckPCIConfigSpace is a parameter of kernel module nvidia-current, that can be set using:
We tested the effect of changing this parameter using a benchmark that calls MulC repeatedly (128x1000 times). Below are the results (runtime in sec) on our cg1.4xlarge instances:
As you can see, disabling accesses to PCI space had a spectacular effect in the original npps functions, decreasing the runtime by 95%. The effect was significant even in our optimized Kernel functions saving almost 25% in runtime. However, it is important to note that even when the PCI access is disabled, our customized functions performed almost 60% better than the default ones.
We should also point out that there are other options, which we have not explored so far but could be useful for others. First, we could look at optimizing our code by applying a kernel fusion trick that combines several computation steps into one kernel to reduce the memory access. Finally, we could think about using Theano, the GPU Match compiler in Python, which is supposed to also improve performance in these cases.
While our initial work was done using cg1.4xlarge EC2 instances, we were interested in moving to the new EC2 GPU g2.2xlarge instance type, which has a GRID K520 GPU (GK104 chip) with 1536 cores. Currently our application is also bounded by GPU memory bandwidth and the GRID K520‘s memory bandwidth is 198 GB/sec, which is an improvement over the Tesla M2050’s at 148 GB/sec. Of course, using a GPU with faster memory would also help (e.g. TITAN’s memory bandwidth is 288 GB/sec).
We repeated the same comparison between the default npps functions and our customized ones (with and without PCI space access) on the g2.2xlarge instances.
One initial surprise was that we measured worse performance for npps on the g2 instances than the cg1 when PCI space access was enabled. However, disabling it improved performance between 45% and 65% compared to the cg1 instances. Again, our KernelMulC customized functions are over 70% better, with benchmark times under a second. Thus, switching to G2 with the right configuration allowed us to run our experiments faster, or alternatively larger experiments in the same amount of time.
Once we had optimized the single-node training and testing operations, we were ready to tackle the issue of hyperparameter optimization. If you are not familiar with this concept, here is a simple explanation: Most machine learning algorithms have parameters to tune, which are called often called hyperparameters to distinguish them from model parameters that are produced as a result of the learning algorithm. For example, in the case of a Neural Network, we can think about optimizing the number of hidden units, the learning rate, or the regularization weight. In order to tune these, you need to train and test several different combinations of hyperparameters and pick the best one for your final model. A naive approach is to simply perform an exhaustive grid search over the different possible combinations of reasonable hyperparameters. However, when faced with a complex model where training each one is time consuming and there are many hyperparameters to tune, it can be prohibitively costly to perform such exhaustive grid searches. Luckily, you can do better than this by thinking of parameter tuning as an optimization problem in itself.
One way to do this is to use a Bayesian Optimization approach where an algorithm’s performance with respect to a set of hyperparameters is modeled as a sample from a Gaussian Process. Gaussian Processes are a very effective way to perform regression and while they can have trouble scaling to large problems, they work well when there is a limited amount of data, like what we encounter when performing hyperparameter optimization. We use package spearmint to perform Bayesian Optimization and find the best hyperparameters for the Neural Network training algorithm. We hook up spearmint with our training algorithm by having it choose the set of hyperparameters and then training a Neural Network with those parameters using our GPU-optimized code. This model is then tested and the test metric results used to update the next hyperparameter choices made by spearmint.
We’ve squeezed high performance from our GPU but we only have 1–2 GPU cards per machine, so we would like to make use of the distributed computing power of the AWS cloud to perform the hyperparameter tuning for all configurations, such as different models per international region. To do this, we use the distributed task queue Celery to send work to each of the GPUs. Each worker process listens to the task queue and runs the training on one GPU. This allows us, for example, to tune, train, and update several models daily for all international regions.
Although the Spearmint + Celery system is working, we are currently evaluating more complete and flexible solutions using HTCondor or StarCluster. HTCondor can be used to manage the workflow of any Directed Acyclic Graph (DAG). It handles input/output file transfer and resource management. In order to use Condor, we need each compute node register into the manager with a given ClassAd (e.g. SLOT1_HAS_GPU=TRUE; STARD_ATTRS=HAS_GPU). Then the user can submit a job with a configuration “Requirements=HAS_GPU” so that the job only runs on AWS instances that have an available GPU. The main advantage of using Condor is that it also manages the distribution of the data needed for the training of the different models. Condor also allows us to run the Spearmint Bayesian optimization on the Manager instead of having to run it on each of the workers.
Another alternative is to use StarCluster , which is an open source cluster computing framework for AWS EC2 developed at MIT. StarCluster runs on the Oracle Grid Engine (formerly Sun Grid Engine) in a fault-tolerant way and is fully supported by Spearmint.
Finally, we are also looking into integrating Spearmint with Jobman in order to better manage the hyperparameter search workflow.
Figure below illustrates the generalized setup using Spearmint plus Celery, Condor, or StarCluster:
Implementing bleeding edge solutions such as using GPUs to train large-scale Neural Networks can be a daunting endeavour. If you need to do it in your own custom infrastructure, the cost and the complexity might be overwhelming. Levering the public AWS cloud can have obvious benefits, provided care is taken in the customization and use of the instance resources. By sharing our experience we hope to make it much easier and straightforward for others to develop similar applications.
We are always looking for talented researchers and engineers to join our team. So if you are interested in solving these types of problems, please take a look at some of our open positions on the Netflix jobs page .
Originally published at techblog.netflix.com on February 10, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
",alex chen justin basilico xavier amatriain described previously blog netflix constantly innovating looking better ways find best movies tv shows members new algorithmic technique deep learning shows promising results domains eg image recognition neuroimaging language models speech recognition come surprise would try figure apply techniques improve product post focus learned building infrastructure experimenting approaches netflix hope useful others working similar algorithms especially also leveraging amazon web services aws infrastructure however detail using variants artificial neural networks personalization since active area research many researchers pointed algorithmic techniques used trendy deep learning approaches known available time much recent innovation area around making techniques feasible realworld applications involves designing implementing architectures execute techniques using reasonable amount resources reasonable amount time first successful instance largescale deep learning made use 16000 cpu cores 1000 machines order train artificial neural network matter days remarkable milestone required infrastructure cost computation time still practical andrew ng team addressed issue follow work implementation used gpus powerful yet cheap alternative large clusters cpus using architecture able train model 65 times larger days using 3 machines another study schwenk et al showed training models gpus improve performance dramatically even comparing highend multicore cpus given wellknown approach leadership cloud computing sought implement largescale neural network training system leveraged advantages gpus aws cloud wanted use reasonable number machines implement powerful machine learning solution using neural network approach also wanted avoid needing special machines dedicated data center instead leverage full ondemand computing power obtain aws architecting approach leveraging computing power cloud sought strike balance would make fast easy train neural networks looking entire training process computing resources capacity use many gpu cores cpu cores aws instances would like use efficiently application typically need train one multiple models either different datasets configurations eg different international regions configuration need perform hyperparameter tuning combination parameters requires training separate neural network solution take approach using gpubased parallelism training using distributed computation handling hyperparameter tuning different configurations might thinking scenario described people think distributed machine learning traditional sense instance work ng et al cited distribute learning algorithm different machines approach might make sense cases found always norm especially dataset stored single instance understand first need explain different levels model training process distributed standard scenario particular model multiple instances instances might correspond different partitions problem space typical situation different models trained different countries regions since feature distribution even item space might different one region represents first initial level decide distribute learning process could example separate machine train 41 countries netflix operates since region trained entirely independently however explained training single instance actually implies training testing several models corresponding different combinations hyperparameters represents second level process distributed level particularly interesting many parameters optimize good strategy optimize like bayesian optimization gaussian processes communication runs hyperparameter settings test evaluation metrics finally algorithm training distributed also interesting comes cost example training ann comparatively communicationintensive process given likely thousands cores available single gpu instance convenient squeeze gpu avoid getting costly acrossmachine communication scenarios communication within machine using memory usually much faster communication network following pseudo code illustrates three levels algorithm training process like us distributed post explain addressed level 1 2 distribution use case note one reasons need address level 3 distribution model millions parameters compared billions original paper ng addressed distribution problem though make sure gpubased parallel training efficient approached first getting proofofconcept work development machines addressing issue scale use cloud second stage started using lenovo s20 workstation nvidia quadro 600 gpu gpu 98 cores provides useful baseline experiments especially considering planned using powerful machine gpu aws cloud first attempt train neural network model took 7 hours ran code train model ec2s cg14xlarge instance powerful tesla m2050 448 cores however training time jumped 7 20 hours profiling showed time spent function calls nvidia performance primitive library eg nppsmulc_32f_i nppsexp_32f_i calling npps functions repeatedly took 10x system time cg1 instance lenovo s20 tried uncover root cause worked way around issue reimplementing npps functions using customized cuda kernel eg replace nppsmulc_32f_i function replacing npps functions way neural network code reduced total training time cg1 instance 20 hours 47 minutes training 4 million samples training 1 million samples took 96 seconds gpu time using approach lenovo s20 total training time also reduced 7 hours 2 hours makes us believe implementation functions suboptimal regardless card specifics implementing hack also worked aws team find principled solution would require kernel patch found performance degradation related nvreg_checkpciconfigspace parameter kernel according redhat setting parameter 0 disables slow accesses pci configuration space virtualized environment aws cloud accesses cause trap hypervisor results even slower access nvreg_checkpciconfigspace parameter kernel module nvidiacurrent set using tested effect changing parameter using benchmark calls mulc repeatedly 128x1000 times results runtime sec cg14xlarge instances see disabling accesses pci space spectacular effect original npps functions decreasing runtime 95 effect significant even optimized kernel functions saving almost 25 runtime however important note even pci access disabled customized functions performed almost 60 better default ones also point options explored far could useful others first could look optimizing code applying kernel fusion trick combines several computation steps one kernel reduce memory access finally could think using theano gpu match compiler python supposed also improve performance cases initial work done using cg14xlarge ec2 instances interested moving new ec2 gpu g22xlarge instance type grid k520 gpu gk104 chip 1536 cores currently application also bounded gpu memory bandwidth grid k520s memory bandwidth 198 gbsec improvement tesla m2050s 148 gbsec course using gpu faster memory would also help eg titans memory bandwidth 288 gbsec repeated comparison default npps functions customized ones without pci space access g22xlarge instances one initial surprise measured worse performance npps g2 instances cg1 pci space access enabled however disabling improved performance 45 65 compared cg1 instances kernelmulc customized functions 70 better benchmark times second thus switching g2 right configuration allowed us run experiments faster alternatively larger experiments amount time optimized singlenode training testing operations ready tackle issue hyperparameter optimization familiar concept simple explanation machine learning algorithms parameters tune called often called hyperparameters distinguish model parameters produced result learning algorithm example case neural network think optimizing number hidden units learning rate regularization weight order tune need train test several different combinations hyperparameters pick best one final model naive approach simply perform exhaustive grid search different possible combinations reasonable hyperparameters however faced complex model training one time consuming many hyperparameters tune prohibitively costly perform exhaustive grid searches luckily better thinking parameter tuning optimization problem one way use bayesian optimization approach algorithms performance respect set hyperparameters modeled sample gaussian process gaussian processes effective way perform regression trouble scaling large problems work well limited amount data like encounter performing hyperparameter optimization use package spearmint perform bayesian optimization find best hyperparameters neural network training algorithm hook spearmint training algorithm choose set hyperparameters training neural network parameters using gpuoptimized code model tested test metric results used update next hyperparameter choices made spearmint weve squeezed high performance gpu 12 gpu cards per machine would like make use distributed computing power aws cloud perform hyperparameter tuning configurations different models per international region use distributed task queue celery send work gpus worker process listens task queue runs training one gpu allows us example tune train update several models daily international regions although spearmint celery system working currently evaluating complete flexible solutions using htcondor starcluster htcondor used manage workflow directed acyclic graph dag handles inputoutput file transfer resource management order use condor need compute node register manager given classad eg slot1_has_gputrue stard_attrshas_gpu user submit job configuration requirementshas_gpu job runs aws instances available gpu main advantage using condor also manages distribution data needed training different models condor also allows us run spearmint bayesian optimization manager instead run workers another alternative use starcluster open source cluster computing framework aws ec2 developed mit starcluster runs oracle grid engine formerly sun grid engine faulttolerant way fully supported spearmint finally also looking integrating spearmint jobman order better manage hyperparameter search workflow figure illustrates generalized setup using spearmint plus celery condor starcluster implementing bleeding edge solutions using gpus train largescale neural networks daunting endeavour need custom infrastructure cost complexity might overwhelming levering public aws cloud obvious benefits provided care taken customization use instance resources sharing experience hope make much easier straightforward others develop similar applications always looking talented researchers engineers join team interested solving types problems please take look open positions netflix jobs page originally published techblognetflixcom february 10 2014 quick cheer standing ovation clap show much enjoyed story learn netflix designs builds operates systems engineering organizations learn netflixs world class engineering efforts company culture product developments,en,"['Neuro-imaging', 'Language Models', 'Speech Recognition', 'the Amazon Web Services', 'AWS', 'Artificial Neural Networks', 'an Artificial Neural Network', 'Neural Network', 'GPU', 'Netflix', 'Gaussian Processes', 'ANN', 'algorithm', 'Lenovo', 'EC2', 'Nvidia Performance Primitive', 'cg1', 'NVreg_CheckPCIConfigSpace', 'RedHat', 'PCI', 'MulC', 'sec', 'GPU Match', 'GB/sec', 'the Tesla M2050', 'g2.2xlarge', 'a Neural Network', 'the Neural Network', 'the Spearmint + Celery', 'StarCluster', 'Directed Acyclic Graph', 'DAG', 'Condor', 'Spearmint', 'MIT', 'techblog.netflix.com']"
113,Francesco Gadaleta,3,Gradient descent vs coordinate descent – Hacker Noon,"When it comes to function minimization, it’s time to open a book of optimization and linear algebra. I am currently working on variable selection and lasso-based solutions in genetics. What lasso does is basically minimizing the loss function and an penalty in order to set to zero some regression coefficients and select only those covariates that are really associated with the response. Pheew, the shortest summary of lasso ever!
We all know that, provided the function to be minimized is convex, a good direction to follow, in order to find a local minimum, is towards the negative gradient of the function. Now, my question is how good or bad is following the negative gradient with respect to a coordinate descent approach that loops across all dimensions and minimizes along each?
There is no better way to try this with real code and start measuring. Hence, I wrote some code that implements both gradient descent and coordinate descent.
The comparison might not be completely fair because the learning rate in the gradient descent procedure is fixed at 0.1 (which in some cases might be slower indeed). But even with some tuning (maybe with some linear search) or adaptive learning rates, it’s quite common to see that coordinate descent overcomes its brother gradient descent many times.
This occurs much more often when the number of covariates becomes very high, as in many computational biology problems. In the figure below, I plot the analytical solution in red, the gradient descent minimisation in blue and the coordinate descent in green, across a number of iterations.
A small explanation is probably necessary to read the function that performs coordinate descent. For a more mathematical explanation refer to the original post.
Coordinate descent will update each variable in a Round Robin fashion. Despite the learning rate of the gradient descent procedure (which could indeed speed up convergence), the comparison between the two is fair at least in terms of complexity.
Coordinate descent needs to perform operations for each coordinate update. Gradient descent performs the same number of operations . The R code that performs this comparison and generates the plot above is given below
Feel free to download this code (remember to cite me and send me some cookies).
Happy descent!
Originally published at worldofpiggy.com on May 31, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Machine learning, math, crypto, blockchain, fitchain.io
how hackers start their afternoons.
",comes function minimization time open book optimization linear algebra currently working variable selection lassobased solutions genetics lasso basically minimizing loss function penalty order set zero regression coefficients select covariates really associated response pheew shortest summary lasso ever know provided function minimized convex good direction follow order find local minimum towards negative gradient function question good bad following negative gradient respect coordinate descent approach loops across dimensions minimizes along better way try real code start measuring hence wrote code implements gradient descent coordinate descent comparison might completely fair learning rate gradient descent procedure fixed 01 cases might slower indeed even tuning maybe linear search adaptive learning rates quite common see coordinate descent overcomes brother gradient descent many times occurs much often number covariates becomes high many computational biology problems figure plot analytical solution red gradient descent minimisation blue coordinate descent green across number iterations small explanation probably necessary read function performs coordinate descent mathematical explanation refer original post coordinate descent update variable round robin fashion despite learning rate gradient descent procedure could indeed speed convergence comparison two fair least terms complexity coordinate descent needs perform operations coordinate update gradient descent performs number operations r code performs comparison generates plot given feel free download code remember cite send cookies happy descent originally published worldofpiggycom may 31 2014 quick cheer standing ovation clap show much enjoyed story machine learning math crypto blockchain fitchainio hackers start afternoons,en,"['worldofpiggy.com', 'fitchain.io']"
114,Jim Fleming,294,Loading a TensorFlow graph with the C++ API – Jim Fleming – Medium,"Check out the related post: Loading TensorFlow graphs from Node.js (using the C API).
The current documentation around loading a graph with C++ is pretty sparse so I spent some time setting up a barebones example. In the TensorFlow repo there are more involved examples, such as building a graph in C++. However, the C++ API for constructing graphs is not as complete as the Python API. Many features (including automatic gradient computation) are not available from C++ yet. Another example in the repo demonstrates defining your own operations but most users will never need this. I imagine the most common use case for the C++ API is for loading pre-trained graphs to be standalone or embedded in other applications.
Be aware, there are some caveats to this approach that I’ll cover at the end.
Let’s start by creating a minimal TensorFlow graph and write it out as a protobuf file. Make sure to assign names to your inputs and operations so they’re easier to assign when we execute the graph later. The node’s do have default names but they aren’t very useful: Variable_1 or Mul_3. Here’s an example created with Jupyter:
Let’s create a new folder like tensorflow/tensorflow/<my project name> for your binary or library to live. I’m going to call the project loader since it will be loading a graph.
Inside this project folder we’ll create a new file called <my project name>.cc (e.g. loader.cc). If you’re curious, the .cc extension is essentially the same as .cpp but is preferred by Google’s code guidelines.
Inside loader.cc we’re going to do a few things:
Now we create a BUILD file for our project. This tells Bazel what to compile. Inside we want to define a cc_binary for our program. You can also use the linkshared option on the binary to produce a shared library or the cc_library rule if you’re going to link it using Bazel.
Here’s the final directory structure:
You could also call bazel run :loader to run the executable directly, however the working directory for bazel run is buried in a temporary folder and ReadBinaryProto looks in the current working directory for relative paths.
And that should be all we need to do to compile and run C++ code for TensorFlow.
The last thing to cover are the caveats I mentioned:
Hopefully someone can shed some light on these last points so we can begin to embed TensorFlow graphs in applications. If you are that person, message me on Twitter or email. If you’d like help deploying TensorFlow in production, I do consulting.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CTO and lead ML engineer at Fomoro — focused on machine learning and applying cutting-edge research for businesses — previously @rdio
What I’m working on.
",check related post loading tensorflow graphs nodejs using c api current documentation around loading graph c pretty sparse spent time setting barebones example tensorflow repo involved examples building graph c however c api constructing graphs complete python api many features including automatic gradient computation available c yet another example repo demonstrates defining operations users never need imagine common use case c api loading pretrained graphs standalone embedded applications aware caveats approach ill cover end lets start creating minimal tensorflow graph write protobuf file make sure assign names inputs operations theyre easier assign execute graph later nodes default names arent useful variable_1 mul_3 heres example created jupyter lets create new folder like tensorflowtensorflowmy project name binary library live im going call project loader since loading graph inside project folder well create new file called project namecc eg loadercc youre curious cc extension essentially cpp preferred googles code guidelines inside loadercc going things create build file project tells bazel compile inside want define cc_binary program also use linkshared option binary produce shared library cc_library rule youre going link using bazel heres final directory structure could also call bazel run loader run executable directly however working directory bazel run buried temporary folder readbinaryproto looks current working directory relative paths need compile run c code tensorflow last thing cover caveats mentioned hopefully someone shed light last points begin embed tensorflow graphs applications person message twitter email youd like help deploying tensorflow production consulting quick cheer standing ovation clap show much enjoyed story cto lead ml engineer fomoro focused machine learning applying cuttingedge research businesses previously rdio im working,en,"['Google', 'CTO', 'Fomoro']"
115,Milo Spencer-Harper,1800,Video of a neural network learning – Deep Learning 101 – Medium,"As part of my quest to learn about AI, I generated a video of a neural network learning.
Many of the examples on the Internet use matrices (grids of numbers) to represent a neural network. This method is favoured, because it is:
However, it’s difficult to understand what is happening. From a learning perspective, being able to visually see a neural network is hugely beneficial.
The video you are about to see, shows a neural network trying to solve this pattern. Can you work it out?
It’s the same problem I posed in my previous blog post. The trick is to notice that the third column is irrelevant, but the first two columns exhibit the behaviour of a XOR gate. If either the first column or the second column is 1, then the output is 1. However, if both columns are 0 or both columns are 1, then the output is 0.
So the correct answer is 0.
Our neural network will cycle through these 7 examples, 60,000 times. To speed up the video, I will only show you 13 of these cycles, pausing for a second on each frame. Why the number 13? It ensures the video lasts exactly as long as the music.
Each time she considers an example in the training set, you will see her think (you will see her neurons and her synaptic connections glow). She will then calculate the error (the difference between the output and the desired output). She will then propagate this error backwards, adjusting her synaptic connections.
Green synaptic connections represent positive weights (a signal flowing through this synapse will excite the next neuron to fire). Red synaptic connections represent negative weights (a signal flowing through this synapse will inhibit the next neuron from firing). Thicker synapses represent stronger connections (larger weights).
In the beginning, her synaptic weights are randomly assigned. Notice how some synapses are green (positive) and others are red (negative). If these synapses turn out to be beneficial in calculating the right answer, she will strengthen them over time. However, if they are unhelpful, these synapses will wither. It’s even possible for a synapse which was originally positive to become negative, and vice versa. An example of this, is the first synapse into the output neuron — early on in the video it turns from red to green. In the beginning her brain looks like this:
Did you notice that all her neurons are dark? This is because she isn’t currently thinking about anything. The numbers to the right of each neuron, represent the level of neural activity and vary between 0 and 1.
Ok. Now she is going to think about the pattern we saw earlier. Watch the video carefully to see her synapses grow thicker as she learns.
Did you notice how I slowed the video down at the beginning, by skipping only a small number of cycles? When I first shot the video, I didn’t do this. However, I realised that learning is subject to the ‘Law of diminishing returns’. The neural network changes more rapidly during the initial stage of training, which is why I slowed this bit down.
Now that she has learned about the pattern using the 7 examples in the training set, let’s examine her brain again. Do you see how she has strengthened some of her synapses, at the expense of others? For instance, do you remember how the third column in the training set is irrelevant in determining the answer? You can see she has discovered this, because the synapses coming out of her third input neuron have almost withered away, relative to the others.
Let’s give her a new situation [1, 1, 0] to think about. You can see her neural pathways light up.
She has estimated 0.01. The correct answer is 0. So she was very close!
Pretty cool. Traditional computer programs can’t learn. But neural networks can learn and adapt to new situations. Just like the human mind!
How did I do it? I used the Python library matplotlib, which provides methods for drawing and animation. I created the glow effects using alpha transparency.
You can view my full source code here:
Thanks for reading!
If you enjoyed reading this article, please click the heart icon to ‘Recommend’.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Economics at Oxford University. Founder of www.moju.io. Interested in politics and AI.
Fundamentals and Latest Developments in #DeepLearning
",part quest learn ai generated video neural network learning many examples internet use matrices grids numbers represent neural network method favoured however difficult understand happening learning perspective able visually see neural network hugely beneficial video see shows neural network trying solve pattern work problem posed previous blog post trick notice third column irrelevant first two columns exhibit behaviour xor gate either first column second column 1 output 1 however columns 0 columns 1 output 0 correct answer 0 neural network cycle 7 examples 60000 times speed video show 13 cycles pausing second frame number 13 ensures video lasts exactly long music time considers example training set see think see neurons synaptic connections glow calculate error difference output desired output propagate error backwards adjusting synaptic connections green synaptic connections represent positive weights signal flowing synapse excite next neuron fire red synaptic connections represent negative weights signal flowing synapse inhibit next neuron firing thicker synapses represent stronger connections larger weights beginning synaptic weights randomly assigned notice synapses green positive others red negative synapses turn beneficial calculating right answer strengthen time however unhelpful synapses wither even possible synapse originally positive become negative vice versa example first synapse output neuron early video turns red green beginning brain looks like notice neurons dark isnt currently thinking anything numbers right neuron represent level neural activity vary 0 1 ok going think pattern saw earlier watch video carefully see synapses grow thicker learns notice slowed video beginning skipping small number cycles first shot video didnt however realised learning subject law diminishing returns neural network changes rapidly initial stage training slowed bit learned pattern using 7 examples training set lets examine brain see strengthened synapses expense others instance remember third column training set irrelevant determining answer see discovered synapses coming third input neuron almost withered away relative others lets give new situation 1 1 0 think see neural pathways light estimated 001 correct answer 0 close pretty cool traditional computer programs cant learn neural networks learn adapt new situations like human mind used python library matplotlib provides methods drawing animation created glow effects using alpha transparency view full source code thanks reading enjoyed reading article please click heart icon recommend quick cheer standing ovation clap show much enjoyed story studied economics oxford university founder wwwmojuio interested politics ai fundamentals latest developments deeplearning,en,"['XOR', 'Studied Economics', 'Oxford University']"
116,Christian Hernandez,364,Into the Age of Context – Crossing the Pond – Medium,"I spent most of my early career proclaiming that “This!” was the “year of mobile”. The year of mobile was actually 2007 when the iPhone launched and accelerated a revolution around mobile computing. As The Economist recently put it “Just eight years later Apple’s iPhone exemplifies the early 21st century’s defining technology.”
It’s not a question of whether Smartphones have become our primary computing interaction device, it’s a question of by how much relative to other interaction mediums.
So let’s agree that we are currently living in the Era of Mobile. Looking forward to the next 5 year though, I personally believe we will move from the Era of Mobile to the Age of Context. (credit to Robert Scoble and Shel Israel for their book with that same term).
Let me first define what I mean by Age of Context. In the Age of Context personal data (ex: calendar and email, location and time) is integrated with publicly available data (ex: traffic data, pollution level) and app-level data (ex: Uber surge pricing, number of steps tracked by my FitBit) to intelligently drive me towards an action (ex: getting me to walk to my next meeting instead of ordering a car). It is an age in which we, and the devices and sensors around us, generate massive reams of data and in which self-teaching algorithms drill into that data to derive insight and recommend or auto-generate an action. It is an era in which our biological computational capacity and actions, are enhanced (and improved) by digital services.
The Age of Context is being brought about by a number of technology trends which have been accelerating in a parallel and are now coming together.
The first, and most obvious trend, is the proliferation of supercomputers in our pockets. Industry analysts forecast 1.87 billion phones will be shipped by 2018.
These devices carry not only a growing amount of processing power, but also the ecosystem of applications and services which integrate with sensors and functionality on the device to allow us to, literally, remote control our life. In the evolution from the current Era of Mobile to the future Age of Context, the supercomputers in our pocket evolve from information delivery and application interaction layers, to notification context-aware action drivers.
Smartphones will soon be complemented by wearable computing devices (be that the Apple Watch or a future evolution of Google Glass). These new form factors are ideally suited for an Era in which data needs to be compiled into succinct notifications and action enablers.
In the last 10 years, the “web” has evolved into a social web on top of which identities and deep insight into each of us powers services and experiences. It allows Goodreads to associate books with my identity, Vivino to determine that I like earthy red wines, Unilever to best target me for an ad on Facebook and Netflix to mine my data to then commission a show it knows I will like. This identity layer is now being overlayed with a financial layer in which, associated with my digital identity, I also have a secure digital payment mechanism. This transactional financial layer will begin to enable seamless transactions.
In the Age of Context, the Starbuck app will know that I usually emerge from the tube at 9:10am and walk to their local store to order a “tall Americano, extra shot.” At 9:11, as I reach street level my phone, or watch, or wearable computing device will know where I am (close to Starbucks and to the office), know my routine, have my payment information stored and simply generate an action-driver that says “Tall Americano, extra shot. Order?” A few minutes later I can pick up my coffee, which has already been paid for. These services are already possible today.
A parallel and accelerating trend which will power the Age of Context is the proliferation of intelligent and connected sensors around us. Call that Internet of Things or call it simply a democratization and consumerization of devices that capture data (for now) and act on data (eventually).
While the end number varies, industry analysts all believe the number of connected devices starts to get very big very fast. Gartner predicts that by 2020 there will be 25 billion connected devices with the vast majority of those being consumer-centric. Today my Jawbone is a fairly basic data collection device. It knows that I walked 8,000 steps and slept too little, but it doesn’t drive me to action other than providing me with a visualization of the data. In the Age of Context this will change, as larger and larger data sets of sensor data, combined with other data combined with intelligent analytics allows data to become actionable.
In the future my Jawbone won’t simply count my steps, it will also be able to integrate with other data sets to generate personal health insights. It will have tracked over time that my blood pressure rises every morning at 9:20 after I have consumed the third coffee of the day. Comparing my blood rate to thousands of others of my age range and demographic background it will know that the levels are unhealthy and it will help me take a conscious decision not to consume that extra coffee through a notification. Data will derive insight and that insight will, hopefully, drive action.
One could argue that the parallel trends of mobile, sensors and the social web are already mainstream. What then is bringing them together to weave the Age of Context? The glue is data. The massive amounts of data the growing number of internet users and connected devices generate each day.
More critically, the cost of storing this data has dropped to nearly zero. Deloitte estimated that in 1992 the cost of storing a Gigabyte of data was $569 and that by 2012 the cost had dropped to $0.03.
But data by itself is just bits and bytes. The second key trend that is weaving the Age of Context is the breakthroughs in algorithms and models to analyze this data in close-to-real-time. For the Age of Context to come about, systems must know how to query and how to act on all the possible contextual data points to drive the simplified actions outlined in the examples above. The advances (and investment) into machine learning and AI are the final piece of the puzzle needed to turn data from information to action.
The most visible example of the Age of Context today is Google Now. Google has a lot of information about me: it knows what “work” is as I spend most of the time there between 9am and 7pm, it knows what “home” is as I spend most of the evenings there. Since I use Google Apps it knows what my first meeting is. Since I search for Duke Basketball on a regular basis it knows I care about the scores. Since I usually take the tube and Google has access to the London TfL data, it knows that I will be late to my next meeting.
But even though Google Now recently opened up its API to third party developers, it is still fairly Google-biased and Google-optimized. For the Age of Context to thrive the platforms that power it must be interlinked across data and applications.
Whether this age comes about through intelligent agents (like Siri or Viv or the character from Her) or a “meta-app” layer sitting across vertical apps and services is still unclear. The missing piece for much of this to come about is a common meta-language for vertical and punctual apps to share data and actions. This common language will likely be an evolution of the various deep-linking standards being developed.
Facebook has a flavour, Android has a flavour, and a myriad of startups have flavours. An emerging standard will not only enable the Age of Context but also probably crown the champion of this new era as the standard will also own the interactions, the interlinkages and the paths to monetization across devices and experiences.
The trends above are all happening around us, the standards and algorithms are all being built by brilliant minds across the world, the interface layers and devices are already with us. The Age of Context is being created at an accelerating pace and I can’t wait to see what gets built and how our day to day lives are enhanced by this new era.
Thanks to John Henderson for his feedback and thoughts on this post.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-Founder and Managing Partner @whitestarvc Former product and mobile guy at smallish companies that became big. Salvadoran-born Londoner. #YGL of the @wef
Stories from the White Star Capital team and our portfolio companies on entrepreneurship and scaling globally
",spent early career proclaiming year mobile year mobile actually 2007 iphone launched accelerated revolution around mobile computing economist recently put eight years later apples iphone exemplifies early 21st centurys defining technology question whether smartphones become primary computing interaction device question much relative interaction mediums lets agree currently living era mobile looking forward next 5 year though personally believe move era mobile age context credit robert scoble shel israel book term let first define mean age context age context personal data ex calendar email location time integrated publicly available data ex traffic data pollution level applevel data ex uber surge pricing number steps tracked fitbit intelligently drive towards action ex getting walk next meeting instead ordering car age devices sensors around us generate massive reams data selfteaching algorithms drill data derive insight recommend autogenerate action era biological computational capacity actions enhanced improved digital services age context brought number technology trends accelerating parallel coming together first obvious trend proliferation supercomputers pockets industry analysts forecast 187 billion phones shipped 2018 devices carry growing amount processing power also ecosystem applications services integrate sensors functionality device allow us literally remote control life evolution current era mobile future age context supercomputers pocket evolve information delivery application interaction layers notification contextaware action drivers smartphones soon complemented wearable computing devices apple watch future evolution google glass new form factors ideally suited era data needs compiled succinct notifications action enablers last 10 years web evolved social web top identities deep insight us powers services experiences allows goodreads associate books identity vivino determine like earthy red wines unilever best target ad facebook netflix mine data commission show knows like identity layer overlayed financial layer associated digital identity also secure digital payment mechanism transactional financial layer begin enable seamless transactions age context starbuck app know usually emerge tube 910am walk local store order tall americano extra shot 911 reach street level phone watch wearable computing device know close starbucks office know routine payment information stored simply generate actiondriver says tall americano extra shot order minutes later pick coffee already paid services already possible today parallel accelerating trend power age context proliferation intelligent connected sensors around us call internet things call simply democratization consumerization devices capture data act data eventually end number varies industry analysts believe number connected devices starts get big fast gartner predicts 2020 25 billion connected devices vast majority consumercentric today jawbone fairly basic data collection device knows walked 8000 steps slept little doesnt drive action providing visualization data age context change larger larger data sets sensor data combined data combined intelligent analytics allows data become actionable future jawbone wont simply count steps also able integrate data sets generate personal health insights tracked time blood pressure rises every morning 920 consumed third coffee day comparing blood rate thousands others age range demographic background know levels unhealthy help take conscious decision consume extra coffee notification data derive insight insight hopefully drive action one could argue parallel trends mobile sensors social web already mainstream bringing together weave age context glue data massive amounts data growing number internet users connected devices generate day critically cost storing data dropped nearly zero deloitte estimated 1992 cost storing gigabyte data 569 2012 cost dropped 003 data bits bytes second key trend weaving age context breakthroughs algorithms models analyze data closetorealtime age context come systems must know query act possible contextual data points drive simplified actions outlined examples advances investment machine learning ai final piece puzzle needed turn data information action visible example age context today google google lot information knows work spend time 9am 7pm knows home spend evenings since use google apps knows first meeting since search duke basketball regular basis knows care scores since usually take tube google access london tfl data knows late next meeting even though google recently opened api third party developers still fairly googlebiased googleoptimized age context thrive platforms power must interlinked across data applications whether age comes intelligent agents like siri viv character metaapp layer sitting across vertical apps services still unclear missing piece much come common metalanguage vertical punctual apps share data actions common language likely evolution various deeplinking standards developed facebook flavour android flavour myriad startups flavours emerging standard enable age context also probably crown champion new era standard also interactions interlinkages paths monetization across devices experiences trends happening around us standards algorithms built brilliant minds across world interface layers devices already us age context created accelerating pace cant wait see gets built day day lives enhanced new era thanks john henderson feedback thoughts post quick cheer standing ovation clap show much enjoyed story cofounder managing partner whitestarvc former product mobile guy smallish companies became big salvadoranborn londoner ygl wef stories white star capital team portfolio companies entrepreneurship scaling globally,en,"['iPhone', 'Economist', 'Apple', 'the Era of Mobile', 'the Era of Mobile to the Age of Context', 'FitBit', 'Age of Context', 'the Apple Watch', 'Vivino', 'Unilever', 'Starbuck', 'Deloitte', 'Google', 'London TfL', 'Google Now', 'API', 'Android', 'White Star Capital']"
117,Venture Scanner,207,The State of Artificial Intelligence in Six Visuals,"We cover many emerging markets in the startup ecosystem. Previously, we published posts that summarized Financial Technology, Internet of Things, Bitcoin, and MarTech in six visuals. This week, we do the same with Artificial Intelligence (AI). At this time, we are tracking 855 AI companies across 13 categories, with a combined funding amount of $8.75billion. To see all of our AI related posts, check out our blog!
The six Artificial Intelligence visuals below help make sense of this dynamic market:
Deep Learning/Machine Learning Applications: Machine learning is the technology of computer algorithms that operate based on its learnings from existing data. Deep learning is a subset of machine learning that focuses on deeply layered neural networks. The following companies utilize deep learning/machine learning technology in a specific way or use-case in their products.
Computer Vision/Image Recognition: Computer vision is the method of processing and analyzing images to understand and produce information from them. Image recognition is the process of scanning images to identify objects and faces. The following companies either build computer vision/image recognition technology or utilize it as the core offering in their products.
Deep Learning/Machine Learning (General): Machine learning is the technology of computer algorithms that operate based on its learning from existing data. Deep learning is a subset of machine learning that focuses on deeply layered neural networks. The following companies either build deep learning/machine learning technology or utilize it as the core offering of their products.
Natural Language Processing: Natural language processing is the method through which computers process human language input and convert into understandable representations to derive meaning from them. The following companies either build natural language processing technology or utilize it as the core offering in their products (excluding all speech recognition companies).
Smart Robots: Smart robot companies build robots that can learn from their experience and act and react autonomously based on the conditions of their environment.
Virtual Personal Assistants: Virtual personal assistants are software agents that use artificial intelligence to perform tasks and services for an individual, such as customer service, etc.
Natural Language Processing (Speech Recognition): Speech recognition is a subset of natural language processing that focuses on processing a sound clip of human speech and deriving meaning from it.
Computer Vision/Image Recognition: Computer vision is the method of processing and analyzing images to understand and produce information from them. Image recognition is the process of scanning images to identify objects and faces. The following companies utilize computer vision/image recognition technology in a specific way or use-case in their products.
Recommendation Engines and Collaborative Filtering: Recommendation engines are systems that predict the preferences and interests of users for certain items (movies, restaurants) and deliver personalized recommendations to them. Collaborative filtering is a method of predicting a user’s preferences and interests by collecting the preference information from many other similar users.
Gesture Control: Gesture control is the process through which humans interact and communicate with computers with their gestures, which are recognized and interpreted by the computers.
Video Automatic Content Recognition: Video automatic content recognition is the process through which the computer compares a sampling of video content with a source content file to identify what the content is through its unique characteristics.
Context Aware Computing: Context aware computing is the process through which computers become aware of their environment and their context of use, such as location, orientation, lighting and adapt their behavior accordingly.
Speech to Speech Transition: Speech to speech translation is the process through which human speech in one language is processed by the computer and translated into another language instantly.
The bar graph above summarizes the number of companies in each Artificial Intelligence category to show which are dominating the current market. Currently, the “Deep Learning/Machine Learning Applications” category is leading the way with a total of 200 companies, followed by “Natural Language Processing (Speech Recognition)” with 130 companies.
The bar graph above summarizes the average company funding per Artificial Intelligence category. Again, the “Deep Learning/Machine Learning Applications” category leads the way with an average of $13.8M per funded company. The SEM category includes companies that help marketers with managing and scaling their paid-search programs.
The graph above compares total venture funding in Artificial Intelligence to the number of companies in each category. “Deep Learning/Machine Learning Applications” seems to be the category with the most traction.
The following infographic is an updated heat map indicating where Artificial Intelligence startups exist across 62 countries. Currently, the United States is leading the way with 415 companies. The United Kingdom is in second with 67 companies followed by Canada with 29.
The bar graph above summarizes Artificial Intelligence by median age of category. The “Speech Recognition” and “Video Content Recognition” categories have the highest median age at 8 years, followed by “Computer Vision (General)” at 6.5 years.
As Artificial Intelligence continues to develop, so too will its moving parts. We hope this post provides some big picture clarity on this booming industry.
Venture Scanner enables corporations to research, identify, and connect with the most innovative technologies and companies. We do this through a unique combination of our data, technology, and expert analysts. If you have any questions, reach out to info@venturescanner.com.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Technology and analyst powered research firm. Visit us at www.venturescanner.com.
",cover many emerging markets startup ecosystem previously published posts summarized financial technology internet things bitcoin martech six visuals week artificial intelligence ai time tracking 855 ai companies across 13 categories combined funding amount 875billion see ai related posts check blog six artificial intelligence visuals help make sense dynamic market deep learningmachine learning applications machine learning technology computer algorithms operate based learnings existing data deep learning subset machine learning focuses deeply layered neural networks following companies utilize deep learningmachine learning technology specific way usecase products computer visionimage recognition computer vision method processing analyzing images understand produce information image recognition process scanning images identify objects faces following companies either build computer visionimage recognition technology utilize core offering products deep learningmachine learning general machine learning technology computer algorithms operate based learning existing data deep learning subset machine learning focuses deeply layered neural networks following companies either build deep learningmachine learning technology utilize core offering products natural language processing natural language processing method computers process human language input convert understandable representations derive meaning following companies either build natural language processing technology utilize core offering products excluding speech recognition companies smart robots smart robot companies build robots learn experience act react autonomously based conditions environment virtual personal assistants virtual personal assistants software agents use artificial intelligence perform tasks services individual customer service etc natural language processing speech recognition speech recognition subset natural language processing focuses processing sound clip human speech deriving meaning computer visionimage recognition computer vision method processing analyzing images understand produce information image recognition process scanning images identify objects faces following companies utilize computer visionimage recognition technology specific way usecase products recommendation engines collaborative filtering recommendation engines systems predict preferences interests users certain items movies restaurants deliver personalized recommendations collaborative filtering method predicting users preferences interests collecting preference information many similar users gesture control gesture control process humans interact communicate computers gestures recognized interpreted computers video automatic content recognition video automatic content recognition process computer compares sampling video content source content file identify content unique characteristics context aware computing context aware computing process computers become aware environment context use location orientation lighting adapt behavior accordingly speech speech transition speech speech translation process human speech one language processed computer translated another language instantly bar graph summarizes number companies artificial intelligence category show dominating current market currently deep learningmachine learning applications category leading way total 200 companies followed natural language processing speech recognition 130 companies bar graph summarizes average company funding per artificial intelligence category deep learningmachine learning applications category leads way average 138m per funded company sem category includes companies help marketers managing scaling paidsearch programs graph compares total venture funding artificial intelligence number companies category deep learningmachine learning applications seems category traction following infographic updated heat map indicating artificial intelligence startups exist across 62 countries currently united states leading way 415 companies united kingdom second 67 companies followed canada 29 bar graph summarizes artificial intelligence median age category speech recognition video content recognition categories highest median age 8 years followed computer vision general 65 years artificial intelligence continues develop moving parts hope post provides big picture clarity booming industry venture scanner enables corporations research identify connect innovative technologies companies unique combination data technology expert analysts questions reach infoventurescannercom quick cheer standing ovation clap show much enjoyed story technology analyst powered research firm visit us wwwventurescannercom,en,"['Financial Technology', 'MarTech', 'Artificial Intelligence (AI', 'Artificial Intelligence', 'Computer Vision/Image Recognition', 'Deep Learning/Machine Learning', 'Smart', 'Recommendation Engines', 'Video Automatic Content Recognition', 'SEM']"
118,Illia Polosukhin,108,Tensorflow Tutorial — Part 2 – Illia Polosukhin – Medium,"In the previous Part 1 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build a simple logistic regression model on Titanic dataset.
In this part let’s go deeper and try multi-layer fully connected neural networks, writing your custom model to plug into the Scikit Flow and top it with trying out convolutional networks.
Of course, there is not much point of yet another linear/logistic regression framework. An idea behind TensorFlow (and many other deep learning frameworks) is to be able to connect differentiable parts of the model together and optimize them given the same cost (or loss) function.
Scikit Flow already implements a convenient wrapper around TensorFlow API for creating many layers of fully connected units, so it’s simple to start with deep model by just swapping classifier in our previous model to the TensorFlowDNNClassifier and specify hidden units per layer:
This will create 3 layers of fully connected units with 10, 20 and 10 hidden units respectively, with default Rectified linear unit activations. We will be able to customize this setup in the next part.
I didn’t play much with hyperparameters, but previous DNN model actually yielded worse accuracy then a logistic regression. We can explore if this is due to overfitting on under-fitting in a separate post.
For the sake of this example, I though want to show how to switch to the custom model where you can have more control.
This model is very similar to the previous one, but we changed the activation function from a rectified linear unit to a hyperbolic tangent (rectified linear unit and hyperbolic tangent are most popular activation functions for neural networks).
As you can see, creating a custom model is as easy as writing a function, that takes X and y inputs (which are Tensors) and returns two tensors: predictions and loss. This is where you can start learning TensorFlow APIs to create parts of sub-graph.
What kind of TensorFlow tutorial would this be without an example of digit recognition? :)
This is just an example how you can try different types of datasets and models, not limiting to only floating number features. Here, we take digits dataset and write a custom model:
We’ve created conv_model function, that given tensor X and y, runs 2D convolutional layer with the most simple max pooling — just maximum. The result is passed as features to skflow.models.logistic_regression, which handles classification to required number of classes by attaching softmax over classes and computing cross entropy loss.
It’s easy now to modify this code to add as many layers as you want (some of the state-of-the-art image recognition models are hundred+ layers of convolutions, max pooling, dropout and etc).
The Part 3 is expanding the model for Titanic dataset with handling categorical variables.
PS. Thanks to Vlad Frolov for helping with missing articles and pointing mistakes in the draft :)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-Founder @ NEAR.AI — teaching machines to code. I’m tweeting as @ilblackdragon.
",previous part 1 tutorial introduced bit tensorflow scikit flow showed build simple logistic regression model titanic dataset part lets go deeper try multilayer fully connected neural networks writing custom model plug scikit flow top trying convolutional networks course much point yet another linearlogistic regression framework idea behind tensorflow many deep learning frameworks able connect differentiable parts model together optimize given cost loss function scikit flow already implements convenient wrapper around tensorflow api creating many layers fully connected units simple start deep model swapping classifier previous model tensorflowdnnclassifier specify hidden units per layer create 3 layers fully connected units 10 20 10 hidden units respectively default rectified linear unit activations able customize setup next part didnt play much hyperparameters previous dnn model actually yielded worse accuracy logistic regression explore due overfitting underfitting separate post sake example though want show switch custom model control model similar previous one changed activation function rectified linear unit hyperbolic tangent rectified linear unit hyperbolic tangent popular activation functions neural networks see creating custom model easy writing function takes x inputs tensors returns two tensors predictions loss start learning tensorflow apis create parts subgraph kind tensorflow tutorial would without example digit recognition example try different types datasets models limiting floating number features take digits dataset write custom model weve created conv_model function given tensor x runs 2d convolutional layer simple max pooling maximum result passed features skflowmodelslogistic_regression handles classification required number classes attaching softmax classes computing cross entropy loss easy modify code add many layers want stateoftheart image recognition models hundred layers convolutions max pooling dropout etc part 3 expanding model titanic dataset handling categorical variables ps thanks vlad frolov helping missing articles pointing mistakes draft quick cheer standing ovation clap show much enjoyed story cofounder nearai teaching machines code im tweeting ilblackdragon,en,"['linear', 'Tensors']"
119,Derrick Harris,124,Baidu explains how it’s mastering Mandarin with deep learning,"On Aug. 8 at the International Neural Network Society conference on big data in San Francisco, Baidu senior research engineer Awni Hannun presented on a new model that the Chinese search giant has developed for handling voice queries in Mandarin. The model, which is accurate 94 percent of the time in tests, is based on a powerful deep learning system called Deep Speech that Baidu first unveiled in December 2014.
In this lightly edited interview, Hannun explains why his new research is important, why Mandarin is such a tough language to learn and where we can expect to see future advances in deep learning methods.
SCALE: How accurate is Deep Speech at translating Mandarin?
AWNI HANNUN: It has a 6 percent character error rate, which essentially means that it gets wrong 6 out of 100 characters. To put that in context, this is in my opinion — and to the best of our lab’s knowledge — the best system at transcribing Mandarin voice queries in the world.
In fact, we ran an experiment where we had a few people at the lab who speak Chinese transcribe some of the examples that we were testing the system on. It turned out that our system was better at transcribing examples than they were — if we restricted it to transcribing without the help of the internet and such things.
What is it about Mandarin that makes it such a challenge compared with other languages?
There are a couple of differences with Mandarin that made us think it would be very difficult to have our English speech system work well with it. One is that it’s a tonal language, so when you say a word in a different pitch, it changes the meaning of the word, which is definitely not the case in English. In traditional speech recognition, it’s actually a desirable property that there is some pitch invariance, which essentially means that it tries to ignore pitch when it does the transcription. So you have to change a bunch of things to get a system to work with Mandarin, or any Chinese for that matter.
However, for us, it was not the case that we had to change a whole bunch of things, because our pipeline is much simpler than the traditional speech pipeline. We don’t do a whole lot of pre-processing on the audio in order to make it pitch-invariant, but rather just let the model learn what’s relevant from the data to most effectively transcribe it properly. It was actually able to do that fine in Mandarin without having to change the input.
The other thing that is very different about Chinese — Mandarin, in this case — is the character set. The English alphabet is 26 letters, whereas in Chinese it’s something like 80,000 different characters. Our system directly outputs a character at a time as it’s building its transcription, so we speculated it would be very challenging to have to do that on 80,000 characters at each step versus 26. That’s a challenge we were able to overcome just by using characters that people commonly say, which is a smaller subset.
Baidu has been handling a fairly high volume of voice searches for a while now. How is the Deep Speech system better than the previous system for handling queries in Mandarin?
Baidu has a very active system for voice search in Mandarin, and it works pretty well. I think in terms of total query activity, it’s still a relatively small percentage. We want to make that share larger, or at least enable people to use it more by making the accuracy of the system better.
Can you describe the difference between a search-based system like Deep Speech and something like Microsoft’s Skype Translate, which is also based on deep learning?
Typically, the way it’s done is there are three modules in the pipeline. The first is a speech-transcription module, the second is the machine-translation module and the third would be the speech-synthesis module. What we’re talking about, specifically, is just the speech-transcription module, and I’m sure Microsoft has one as part of Skype Translate.
Our system is different than that system in that it’s more what we call end-to-end. Rather than having a lot of human-engineered components that have been developed over decades of speech research — by looking at the system and saying what what features are important or which phonemes the model should predict — we just have some input data, which is an audio .WAV file on which we do very little pre-processing. And then we have a big, deep neural network that outputs directly to characters. We give it enough data that it’s able to learn what’s relevant from the input to correctly transcribe the output, with as little human intervention as possible.
One thing that’s pleasantly surprising to us is that we had to do very little changing to it — other than scaling it and giving it the right data — to make this system we showed in December that worked really well on English work remarkably well in Chinese, as well.
What’s the usual timeline to get this type of system from R&D into production?
It’s not an easy process, but I think it’s easier than the process of getting a model to be very accurate — in the sense that it’s more of an engineering problem than a research problem. We’re actively working on that now, and I’m hopeful our research system will be in production in the near term.
Baidu has plans — and products — in other areas, including wearables and other embedded forms of speech recognition. Does the work you’re doing on search relate to these other initiatives?
We want to build a speech system that can be used as the interface to any smart device, not just voice search. It turns out that voice search is a very important part of Baidu’s ecosystem, so that’s one place we can have a lot of impact right now.
Is the pace of progress and significant advances in deep learning as fast it seems?
I think right now, it does feel like the pace is increasing because people are recognizing that if you take tasks where you have some input and are trying to produce some output, you can apply deep learning to that task. If it was some old machine learning task such as machine translation or speech recognition, which has been heavily engineered for the past several decades, you can make significant advances if you try to simplify that pipeline with deep learning and increase the amount of data. We’re just on the crest of that.
In particular, processing sequential data with deep learning is something that we’re just figuring out how to do really well. We’ve come up with models that seem to work well, and we’re at the point where we’re going to start squeezing a lot of performance out of these models. And then you’ll see that right and left, benchmarks will be dropping when it comes to sequential data.
Beyond that, I don’t know. It’s possible we’ll start to plateau or we’ll start inventing new architectures to do new tasks. I think the moral of this story is: Where there’s a lot of data and where it makes sense to use a deep learning model, success is with high probability going to happen. That’s why it feels like progress is happening so rapidly right now.
It really becomes a story of “How can we get right data?” when deep learning is involved. That becomes the big challenge.
Architecturally, Deep Speech runs on a powerful GPU-based system. Where are the opportunities to move deep learning algorithms onto smaller systems, such as smartphones, in order to offload processing from Baidu’s (or anyone else’s) servers?
That’s something I think about a lot, actually, and I think the future is bright in that regard. It’s certainly the case that deep learning models are getting bigger and bigger but, typically, it also has also been the case that the size and expressivity of the model is more necessary during training than it is during testing.
There has been a lot of work that shows that if you take a model that has been trained at, say, 32-bit floating point precision and then compress it to 8-bit fixed point precision, it works just as well at test time. Or it works almost as well. You can reduce it by a factor of four and still have it work just as well.
There’s also a lot of work in compressing existing models, like how can we take a giant model that we’ve trained to soak up a lot of data and then, say, train another, much smaller model to duplicate what that large model does. But that small model we can actually put into an embedded device somewhere.
Often, the hard part is in training the system. In those cases, it needs to be really big and the servers have to be really beefy. But I do think there’s a lot of promising work with which we can make the models a lot smaller and there’s a future in terms of embedding them in different places.
Of course, something like search has to go back to cloud servers unless you’ve somehow indexed the whole web on your smartphone, right?
Yeah, that would be challenging.
For some additional context on just how powerful a system Deep Speech is — and why Baidu puts so much emphasis on systems architecture for its deep learning efforts — consider this explanation offered by Baidu systems research scientist Bryan Catanzaro:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder/editor/writer of ARCHITECHT. Day job is running content at Replicated. Formerly at Gigaom/Mesosphere/Fortune.
What’s next in computing, told by the people behind the software
",aug 8 international neural network society conference big data san francisco baidu senior research engineer awni hannun presented new model chinese search giant developed handling voice queries mandarin model accurate 94 percent time tests based powerful deep learning system called deep speech baidu first unveiled december 2014 lightly edited interview hannun explains new research important mandarin tough language learn expect see future advances deep learning methods scale accurate deep speech translating mandarin awni hannun 6 percent character error rate essentially means gets wrong 6 100 characters put context opinion best labs knowledge best system transcribing mandarin voice queries world fact ran experiment people lab speak chinese transcribe examples testing system turned system better transcribing examples restricted transcribing without help internet things mandarin makes challenge compared languages couple differences mandarin made us think would difficult english speech system work well one tonal language say word different pitch changes meaning word definitely case english traditional speech recognition actually desirable property pitch invariance essentially means tries ignore pitch transcription change bunch things get system work mandarin chinese matter however us case change whole bunch things pipeline much simpler traditional speech pipeline dont whole lot preprocessing audio order make pitchinvariant rather let model learn whats relevant data effectively transcribe properly actually able fine mandarin without change input thing different chinese mandarin case character set english alphabet 26 letters whereas chinese something like 80000 different characters system directly outputs character time building transcription speculated would challenging 80000 characters step versus 26 thats challenge able overcome using characters people commonly say smaller subset baidu handling fairly high volume voice searches deep speech system better previous system handling queries mandarin baidu active system voice search mandarin works pretty well think terms total query activity still relatively small percentage want make share larger least enable people use making accuracy system better describe difference searchbased system like deep speech something like microsofts skype translate also based deep learning typically way done three modules pipeline first speechtranscription module second machinetranslation module third would speechsynthesis module talking specifically speechtranscription module im sure microsoft one part skype translate system different system call endtoend rather lot humanengineered components developed decades speech research looking system saying features important phonemes model predict input data audio wav file little preprocessing big deep neural network outputs directly characters give enough data able learn whats relevant input correctly transcribe output little human intervention possible one thing thats pleasantly surprising us little changing scaling giving right data make system showed december worked really well english work remarkably well chinese well whats usual timeline get type system rd production easy process think easier process getting model accurate sense engineering problem research problem actively working im hopeful research system production near term baidu plans products areas including wearables embedded forms speech recognition work youre search relate initiatives want build speech system used interface smart device voice search turns voice search important part baidus ecosystem thats one place lot impact right pace progress significant advances deep learning fast seems think right feel like pace increasing people recognizing take tasks input trying produce output apply deep learning task old machine learning task machine translation speech recognition heavily engineered past several decades make significant advances try simplify pipeline deep learning increase amount data crest particular processing sequential data deep learning something figuring really well weve come models seem work well point going start squeezing lot performance models youll see right left benchmarks dropping comes sequential data beyond dont know possible well start plateau well start inventing new architectures new tasks think moral story theres lot data makes sense use deep learning model success high probability going happen thats feels like progress happening rapidly right really becomes story get right data deep learning involved becomes big challenge architecturally deep speech runs powerful gpubased system opportunities move deep learning algorithms onto smaller systems smartphones order offload processing baidus anyone elses servers thats something think lot actually think future bright regard certainly case deep learning models getting bigger bigger typically also also case size expressivity model necessary training testing lot work shows take model trained say 32bit floating point precision compress 8bit fixed point precision works well test time works almost well reduce factor four still work well theres also lot work compressing existing models like take giant model weve trained soak lot data say train another much smaller model duplicate large model small model actually put embedded device somewhere often hard part training system cases needs really big servers really beefy think theres lot promising work make models lot smaller theres future terms embedding different places course something like search go back cloud servers unless youve somehow indexed whole web smartphone right yeah would challenging additional context powerful system deep speech baidu puts much emphasis systems architecture deep learning efforts consider explanation offered baidu systems research scientist bryan catanzaro quick cheer standing ovation clap show much enjoyed story foundereditorwriter architecht day job running content replicated formerly gigaommesospherefortune whats next computing told people behind software,en,"['the International Neural Network Society', 'Baidu', 'Awni Hannun', 'Deep Speech', 'Microsoft', 'Skype Translate', 'GPU', 'Gigaom/Mesosphere/Fortune']"
120,Kyle McDonald,109,Comparing Artificial Artists – Kyle McDonald – Medium,"Last Wednesday, “A Neural Algorithm of Artistic Style” was posted to ArXiv, featuring some of the most compelling imagery generated by deep convolutional neural networks (DCNNs) since Google Research’s “DeepDream” post.
On Sunday, Kai Sheng Tai posted the first public implementation. I immediately stopped working on my implementation and started playing with his. Unfortunately, his results don’t quite match the paper, and it’s unclear why. I’m just getting started with this topic, so as I learn I want to share my understanding of the algorithm here, along with some results I got from testing his code.
In two parts, the paper describes an algorithm for rendering a photo in the style of a given painting:
2. Instead of trying to match the activations exactly, try to match the correlation of the activations. They call this “style reconstruction”, and depending on the layer you reconstruct you get varying levels of abstraction. The correlation feature they use is called a Gram matrix: the dot product between the vectorized feature activation matrix and its transpose. If this sounds confusing, see the footnotes.
Finally, instead of optimizing for just one of these things, they optimize for both simultaneously: the style of one image, and the content of another image.
Here is an attempt to recreate the results from the paper using Kai’s implementation:
Not quite the same, and possibly explained by a few differences between Kai’s implementation and the original paper:
As a final comparison, consider the images Andrej Karpathy posted from his own implementation.
The same large-scale, high-level features are missing here, just like in the style reconstruction of “Seated Nude” above.
Beside’s Kai’s, I’ve seen one more implementation from a PhD student named Satoshi: a brief example in Python with Chainer. I haven’t spent as much time with it, as I had to adapt it to run on my CPU due to lack of memory. But I did notice:
After running Tübingen in the style of The Starry Night with a 1:10e3 ratio and 100 iterations, it seems to converge on something matching the general structure but lacking the overall palette:
I’d like to understand this algorithm well enough to generalize it to other media (mainly thinking about sound right now), so if you have an insights or other implementations please share them in the comments!
I’ve started testing another implementation that popped up this morning from Justin Johnson. His follow the original paper very closely, except for using unequal weights when balancing different layers used for style reconstruction. All the following examples were run for 100 iterations with the default ratio of 1:10e0.
Justin switched his implementation to use L-BFGS and equally weighted layers, and to my eyes this matches the results in the original paper. Here are his results for one of the harder content/style pairs:
Other implementations that look great, but I haven’t tested enough:
The definition of the Gram matrix confused me at first, so I wrote it out as code. Using a literal translation of equation 3 in the paper, you would write in Python, with numpy:
It turns out that the original description is computed more efficiently than this literal translation. For example, Kai writes in Lua, with Torch:
Satoshi computes it for all the layers simultaneously in Python with Chainer:
Or again in Python, with numpy and Caffe layers:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Artist working with code.
",last wednesday neural algorithm artistic style posted arxiv featuring compelling imagery generated deep convolutional neural networks dcnns since google researchs deepdream post sunday kai sheng tai posted first public implementation immediately stopped working implementation started playing unfortunately results dont quite match paper unclear im getting started topic learn want share understanding algorithm along results got testing code two parts paper describes algorithm rendering photo style given painting 2 instead trying match activations exactly try match correlation activations call style reconstruction depending layer reconstruct get varying levels abstraction correlation feature use called gram matrix dot product vectorized feature activation matrix transpose sounds confusing see footnotes finally instead optimizing one things optimize simultaneously style one image content another image attempt recreate results paper using kais implementation quite possibly explained differences kais implementation original paper final comparison consider images andrej karpathy posted implementation largescale highlevel features missing like style reconstruction seated nude besides kais ive seen one implementation phd student named satoshi brief example python chainer havent spent much time adapt run cpu due lack memory notice running tubingen style starry night 110e3 ratio 100 iterations seems converge something matching general structure lacking overall palette id like understand algorithm well enough generalize media mainly thinking sound right insights implementations please share comments ive started testing another implementation popped morning justin johnson follow original paper closely except using unequal weights balancing different layers used style reconstruction following examples run 100 iterations default ratio 110e0 justin switched implementation use lbfgs equally weighted layers eyes matches results original paper results one harder contentstyle pairs implementations look great havent tested enough definition gram matrix confused first wrote code using literal translation equation 3 paper would write python numpy turns original description computed efficiently literal translation example kai writes lua torch satoshi computes layers simultaneously python chainer python numpy caffe layers quick cheer standing ovation clap show much enjoyed story artist working code,en,"['Google Research', 'Chainer', 'Justin', 'Torch']"
121,Jim Fleming,165,Highway Networks with TensorFlow – Jim Fleming – Medium,"This week I implemented highway networks to get an intuition for how they work. Highway networks, inspired by LSTMs, are a method of constructing networks with hundreds, even thousands, of layers. Let’s see how we construct them using TensorFlow.
TL;DR Fully-connected highway repo and convolutional highway repo.
For comparison, let’s start with a standard fully-connected (or “dense”) layer. We need a weight matrix and a bias vector then we’ll compute the following for the layer output:
Here’s what a dense layer looks like as a graph in TensorBoard:
For the highway layer what we want are two “gates” that control the flow of information. The “transform” gate controls how much of the activation we pass through and the “carry” gate controls how much of the unmodified input we pass through. Otherwise, the layer largely resembles a dense layer with a few additions:
What happens is that when the transform gate is 1, we pass through our activation (H) and suppress the carry gate (since it will be 0). When the carry gate is 1, we pass through the unmodified input (x), while the activation is suppressed.
Here’s what the highway layer graph looks in TensorBoard:
Using a highway layer in a network is also straightforward. One detail to keep in mind is that consecutive highway layers must be the same size but you can use fully-connected layers to change dimensionality. This becomes especially complicated in convolutional layers where each layer can change the output dimensions. We can use padding (‘SAME’) to maintain each layers dimensionality.
Otherwise, by simply using hyperparameters from the TensorFlow docs (i.e. no hyperparameter search) the fully-connected highway network performed much better than a fully-connected network. Using MNIST as my simple trial:
Now that we have a highway network, I wanted to answer a few questions that came up for me while reading the paper. For instance, how deep will the network converge? The paper briefly mentions 1000 layers:
Can we train with 1000 layers on MNIST?
Yes, also reaching around 95% accuracy. Try it out with a carry bias around -20.0 for MNIST (from the paper the network will only utilize ~15 layers anyway). The network can probably even go deeper since the it’s just learning to carry the last 980 layers or so. We can’t do much useful at or past 1000 layers so that seems sufficient for now.
What happens if you set very low or very high carry biases?
In either extreme the network simply fails to converge in a reasonable amount of time. In the case of low biases (more positive), the network starts as if the carry gates aren’t present at all. In the case of high biases (more negative), we’re putting more emphasis on carrying and the network can take a long time to overcome that. Otherwise, the biases don’t seem to need to be exact, at least on this simple example. When in doubt start with high biases (more negative) since it’s easier to learn to overcome carrying than without carry gates (which is just a plain network).
Overall I was happy with how easy highway networks were to implement. They’re fully differentiable with only a single additional hyperparameter for the initial carry bias. One downside is that highway layers do require additional parameters for the transform weights and biases. However, since we can go deeper, the layers do not need to be as wide which can compensate.
Here’s are the complete notebooks if you want to play with the code: fully-connected highway repo and convolutional highway repo.
Follow me on Twitter for more posts like these. If you’d like building very deep networks in production, I do consulting.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CTO and lead ML engineer at Fomoro — focused on machine learning and applying cutting-edge research for businesses — previously @rdio
What I’m working on.
",week implemented highway networks get intuition work highway networks inspired lstms method constructing networks hundreds even thousands layers lets see construct using tensorflow tldr fullyconnected highway repo convolutional highway repo comparison lets start standard fullyconnected dense layer need weight matrix bias vector well compute following layer output heres dense layer looks like graph tensorboard highway layer want two gates control flow information transform gate controls much activation pass carry gate controls much unmodified input pass otherwise layer largely resembles dense layer additions happens transform gate 1 pass activation h suppress carry gate since 0 carry gate 1 pass unmodified input x activation suppressed heres highway layer graph looks tensorboard using highway layer network also straightforward one detail keep mind consecutive highway layers must size use fullyconnected layers change dimensionality becomes especially complicated convolutional layers layer change output dimensions use padding maintain layers dimensionality otherwise simply using hyperparameters tensorflow docs ie hyperparameter search fullyconnected highway network performed much better fullyconnected network using mnist simple trial highway network wanted answer questions came reading paper instance deep network converge paper briefly mentions 1000 layers train 1000 layers mnist yes also reaching around 95 accuracy try carry bias around 200 mnist paper network utilize 15 layers anyway network probably even go deeper since learning carry last 980 layers cant much useful past 1000 layers seems sufficient happens set low high carry biases either extreme network simply fails converge reasonable amount time case low biases positive network starts carry gates arent present case high biases negative putting emphasis carrying network take long time overcome otherwise biases dont seem need exact least simple example doubt start high biases negative since easier learn overcome carrying without carry gates plain network overall happy easy highway networks implement theyre fully differentiable single additional hyperparameter initial carry bias one downside highway layers require additional parameters transform weights biases however since go deeper layers need wide compensate heres complete notebooks want play code fullyconnected highway repo convolutional highway repo follow twitter posts like youd like building deep networks production consulting quick cheer standing ovation clap show much enjoyed story cto lead ml engineer fomoro focused machine learning applying cuttingedge research businesses previously rdio im working,en,"['MNIST', 'CTO', 'Fomoro']"
122,Nathan Benaich,264,Investing in Artificial Intelligence – Nathan Benaich – Medium,"My (expanded) talking points from a presentation I gave at the Re.Work Investing in Deep Learning dinner in London on 1st December 2015.
TL;DR Check out the slides here.
It’s my belief that artificial intelligence is one of the most exciting and transformative opportunities of our time. There’s a few reasons why that’s so. Consumers worldwide carry 2 billion smartphones, they’re increasingly addicted to these devices and 40% of the world is online (KPCB). This means we’re creating new data assets that never existed before (user behavior, preferences, interests, knowledge, connections).
The costs of compute and storage are both plummeting by orders of magnitude, while the computational capacity of today’s processors is growing. We’ve seen improvements in learning methods, architectures and software infrastructure. The pace of innovation can therefore only be accelerating. Indeed, we don’t fully appreciate what tomorrow will look and feel like.
AI-driven products are already out in the wild and improving the performance of search engines, recommender systems (e.g. e-commerce, music), ad serving and financial trading (amongst others). Companies with the resources to invest in AI are already creating an impetus for others to follow suit or risk not having a competitive seat at the table. Together, therefore, the community has a better understanding and is equipped with more capable tools with which to build learning systems for a wide range of increasingly complex tasks.
More on this discussion here. A key consideration, in my view, is that the open sourcing of technologies by large incumbents (Google, Microsoft, Intel, IBM) and the range of companies productising technologies for cheap means that technical barriers are eroding fast. What ends up moving the needle are: proprietary data access/creation, experienced talent and addictive products.
Operational
Commercial
Financial
There are two big factors that make involving the user in an AI-driven product paramount. 1) Machines don’t yet recapitulate human cognition. In order to pick up where software falls short, we need to call on the user for help. 2) Buyers/users of software products have more choice today than ever. As such, they’re often fickle (avg. 90-day retention for apps is 35%). Returning expected value out of the box is key to building habits (hyperparameter optimisation can help). Here are some great examples of products which prove that involving the user-in-the-loop improves performance:
We can even go a step further, I think, by explaining how machine-generated results are obtained. For example, IBM Watson surfaces relevant literature when supporting a patient diagnosis in the oncology clinic. Doing so improves user satisfaction and helps build confidence in the system to encourage longer term use and investment. Remember, it’s generally hard for us to trust something we don’t truly understand.
To put this discussion into context, let’s first look at the global VC market. Q1-Q3 2015 saw $47.2bn invested, a volume higher than each of the full year totals for 17 of the last 20 years (NVCA). We’re likely to breach $55bn by year end. There are circa 900 companies working in the AI field, most of which tackle problems in business intelligence, finance and security. Q4 2014 saw a flurry of deals into AI companies started by well respected and achieved academics: Vicarious, Scaled Inference, MetaMind and Sentient Technologies.
So far, we’ve seen circa 300 deals into AI companies (defined as businesses whose description includes keywords: artificial intelligence, machine learning, computer vision, NLP, data science, neural network, deep learning from Jan 1st 2015 thru 1st Dec 2015, CB Insights). In the UK, companies like Ravelin, Signal and Gluru raised seed rounds. Circa $2bn was invested, albeit bloated by large venture debt or credit lines for consumer/business loan providers Avant ($339m debt+credit), ZestFinance ($150m debt), LiftForward ($250m credit) and Argon Credit ($75m credit). Importantly, 80% of deals were < $5m in size and 90% of the cash was invested into US companies vs. 13% in Europe. 75% of rounds were in the US.
The exit market has seen 33 M&A transactions and 1 IPO (Adgorithms on the LSE). Six events were for European companies, 1 in Asia and the rest were accounted for by American companies. The largest transactions were TellApart/Twitter ($532m; $17m raised), Elastica/Blue Coat Systems ($280m; $45m raised) and SupersonicAds/IronSource ($150m; $21m raised), which return solid multiples of invested capital. The remaining transactions were mostly for talent, given that median team size at the time of the acquisition was 7ppl median.
Altogether, AI investments will have accounted for circa 5% of total VC investments for 2015. That’s higher than the 2% claimed in 2013, but still tracking far behind competing categories like adtech, mobile and BI software. The key takeaway points are a) the financing and exit markets for AI companies are still nascent, as exemplified by the small rounds and low deal volumes, and b) the vast majority of activity takes place in the US. Businesses must therefore have exposure to this market.
I spent a number of summers in university and 3 years in grad school researching the genetic factors governing the spread of cancer around the body. A key takeaway I left with is the following: therapeutic development is a very challenging, expensive, lengthy, regulated and ultimately offers a transient solution to treating disease. Instead, I truly believe that what we need to improve healthcare outcomes is granular and longitudinal monitoring of physiology and lifestyle. This should enable early detection of health conditions in near real-time, drive down cost of care over a patient’s lifetime, while consequently improving outcomes.
Consider the digitally connected lifestyles we lead today. The devices some of us interact with on a daily basis are able to track our movements, vital signs, exercise, sleep and even reproductive health. We’re disconnected for fewer hours of the day than we’re online and I think we’re less apprehensive to storing various data types in the cloud (where they can be accessed, with consent, by 3rd parties). Sure, the news might paint a different, but the fact is that we’re still using the web and it’s wealth of products.
On a population level, therefore, we have the chance to interrogate data sets that have never before existed. From these, we could glean insights into how nature and nurture influence the genesis and development of disease. That’s huge. Look at today’s clinical model: a patient presents into the hospital when they feel something is wrong. The doctor has to conduct a battery of tests to derive a diagnosis. These tests address a single (often late stage) time point, at which moment little can be done to reverse damage (e.g. in the case of cancer). Now imagine the future. In a world of continuous, non-invasive monitoring of physiology and lifestyle, we could predict disease onset and outcome, understand which condition a patient likely suffers from and how they’ll respond to various therapeutic modalities. There’s loads of applications for artificial intelligence here: intelligence sensors, signal processing, anomaly detection, multivariate classifiers, deep learning on molecular interactions...
Some companies are already hacking away at this problem:
A point worth noting is that the UK has a slight leg up on the data access front. Initiatives like the UK Biobank (500k patient records), Genomics England (100k genomes sequenced), HipSci (stem cells) and the NHS care.data programme are leading the way in creating centralised data repositories for public health and therapeutic research. Cheers for pointing out, Hari Arul.
Could businesses ever conceivably run themselves? AI-enabled automation of knowledge work could cut employment costs by $9tn by 2020 (BAML). Coupled to the efficiency gains worth $1.9tn driven by robots, I reckon there’s a chance for near complete automation of core, repetitive businesses functions in the future. Think of all the productised SaaS tools that are available off the shelf for CRM, marketing, billing/payments, logistics, web development, customer interactions, finance, hiring and BI. Then consider tools like Zapier or Tray.io, which help connect applications and program business logic. These could be further expanded by leveraging contextual data points that inform decision making. Perhaps we could eventually re-image the new eBay, where you’ll have fully automated inventory procurement, pricing, listing generation, translation, recommendations, transaction processing, customer interaction, packaging, fulfilment and shipping. Of course, probably a ways off :)
I’m bullish on the value to be created with artificial intelligence across our personal and professional lives. I think there’s currently low VC risk tolerance for this sector, especially given shortening investment horizons for value to be created. More support is needed for companies driving long term innovation, especially that far less is occurring within Universities. VC was born to fund moonshots.
We must remember that access to technology will, over time, become commoditised. It’s therefore key to understand your use case, your user, the value you bring and how it’s experience and assessed. This gets to the point of finding a strategy to build a sustainable advantage such that others find it hard to replicate your offering. Aspects of this strategy may in fact be non-AI and non-technical in nature (e.g. the user experience layer — thanks for highlighting this Hari Arul). As such, there’s a renewed focused on core principles: build a solution to an unsolved/poorly served high-value, persistent problem for consumers or businesses.
Finally, you must have exposure to the US market where the lion’s share of value is created and realised. We have an opportunity to catalyse the growth of the AI sector in Europe, but not without keeping close tabs on what works/doesn’t work across the pond first-hand.
Working in the space? We’d love to get to know you :)
Sign up to my newsletter covering AI news and analysis from the tech world, research lab and private/public company market.
I’m an investor at Playfair Capital, a London-based investment firm focusing on early stage technology companies that change the way we live, work and play. We invest across Europe and the US and our focus is on core technologies and user experiences. 25% of our portfolio is AI: Mapillary, DueDil, Jukedeck, Seldon, Clarify, Gluru and Ravelin. We want to take risk on technologists creating new markets or reinventing existing ones.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Advancing human progress with intelligent systems. Venture Partner @PointNineCap. Former scientist, photographer, perpetual foodie. nathan.ai @LDN_AI @TwentyBN
",expanded talking points presentation gave rework investing deep learning dinner london 1st december 2015 tldr check slides belief artificial intelligence one exciting transformative opportunities time theres reasons thats consumers worldwide carry 2 billion smartphones theyre increasingly addicted devices 40 world online kpcb means creating new data assets never existed user behavior preferences interests knowledge connections costs compute storage plummeting orders magnitude computational capacity todays processors growing weve seen improvements learning methods architectures software infrastructure pace innovation therefore accelerating indeed dont fully appreciate tomorrow look feel like aidriven products already wild improving performance search engines recommender systems eg ecommerce music ad serving financial trading amongst others companies resources invest ai already creating impetus others follow suit risk competitive seat table together therefore community better understanding equipped capable tools build learning systems wide range increasingly complex tasks discussion key consideration view open sourcing technologies large incumbents google microsoft intel ibm range companies productising technologies cheap means technical barriers eroding fast ends moving needle proprietary data accesscreation experienced talent addictive products operational commercial financial two big factors make involving user aidriven product paramount 1 machines dont yet recapitulate human cognition order pick software falls short need call user help 2 buyersusers software products choice today ever theyre often fickle avg 90day retention apps 35 returning expected value box key building habits hyperparameter optimisation help great examples products prove involving userintheloop improves performance even go step think explaining machinegenerated results obtained example ibm watson surfaces relevant literature supporting patient diagnosis oncology clinic improves user satisfaction helps build confidence system encourage longer term use investment remember generally hard us trust something dont truly understand put discussion context lets first look global vc market q1q3 2015 saw 472bn invested volume higher full year totals 17 last 20 years nvca likely breach 55bn year end circa 900 companies working ai field tackle problems business intelligence finance security q4 2014 saw flurry deals ai companies started well respected achieved academics vicarious scaled inference metamind sentient technologies far weve seen circa 300 deals ai companies defined businesses whose description includes keywords artificial intelligence machine learning computer vision nlp data science neural network deep learning jan 1st 2015 thru 1st dec 2015 cb insights uk companies like ravelin signal gluru raised seed rounds circa 2bn invested albeit bloated large venture debt credit lines consumerbusiness loan providers avant 339m debtcredit zestfinance 150m debt liftforward 250m credit argon credit 75m credit importantly 80 deals 5m size 90 cash invested us companies vs 13 europe 75 rounds us exit market seen 33 transactions 1 ipo adgorithms lse six events european companies 1 asia rest accounted american companies largest transactions tellaparttwitter 532m 17m raised elasticablue coat systems 280m 45m raised supersonicadsironsource 150m 21m raised return solid multiples invested capital remaining transactions mostly talent given median team size time acquisition 7ppl median altogether ai investments accounted circa 5 total vc investments 2015 thats higher 2 claimed 2013 still tracking far behind competing categories like adtech mobile bi software key takeaway points financing exit markets ai companies still nascent exemplified small rounds low deal volumes b vast majority activity takes place us businesses must therefore exposure market spent number summers university 3 years grad school researching genetic factors governing spread cancer around body key takeaway left following therapeutic development challenging expensive lengthy regulated ultimately offers transient solution treating disease instead truly believe need improve healthcare outcomes granular longitudinal monitoring physiology lifestyle enable early detection health conditions near realtime drive cost care patients lifetime consequently improving outcomes consider digitally connected lifestyles lead today devices us interact daily basis able track movements vital signs exercise sleep even reproductive health disconnected fewer hours day online think less apprehensive storing various data types cloud accessed consent 3rd parties sure news might paint different fact still using web wealth products population level therefore chance interrogate data sets never existed could glean insights nature nurture influence genesis development disease thats huge look todays clinical model patient presents hospital feel something wrong doctor conduct battery tests derive diagnosis tests address single often late stage time point moment little done reverse damage eg case cancer imagine future world continuous noninvasive monitoring physiology lifestyle could predict disease onset outcome understand condition patient likely suffers theyll respond various therapeutic modalities theres loads applications artificial intelligence intelligence sensors signal processing anomaly detection multivariate classifiers deep learning molecular interactions companies already hacking away problem point worth noting uk slight leg data access front initiatives like uk biobank 500k patient records genomics england 100k genomes sequenced hipsci stem cells nhs caredata programme leading way creating centralised data repositories public health therapeutic research cheers pointing hari arul could businesses ever conceivably run aienabled automation knowledge work could cut employment costs 9tn 2020 baml coupled efficiency gains worth 19tn driven robots reckon theres chance near complete automation core repetitive businesses functions future think productised saas tools available shelf crm marketing billingpayments logistics web development customer interactions finance hiring bi consider tools like zapier trayio help connect applications program business logic could expanded leveraging contextual data points inform decision making perhaps could eventually reimage new ebay youll fully automated inventory procurement pricing listing generation translation recommendations transaction processing customer interaction packaging fulfilment shipping course probably ways im bullish value created artificial intelligence across personal professional lives think theres currently low vc risk tolerance sector especially given shortening investment horizons value created support needed companies driving long term innovation especially far less occurring within universities vc born fund moonshots must remember access technology time become commoditised therefore key understand use case user value bring experience assessed gets point finding strategy build sustainable advantage others find hard replicate offering aspects strategy may fact nonai nontechnical nature eg user experience layer thanks highlighting hari arul theres renewed focused core principles build solution unsolvedpoorly served highvalue persistent problem consumers businesses finally must exposure us market lions share value created realised opportunity catalyse growth ai sector europe without keeping close tabs worksdoesnt work across pond firsthand working space wed love get know sign newsletter covering ai news analysis tech world research lab privatepublic company market im investor playfair capital londonbased investment firm focusing early stage technology companies change way live work play invest across europe us focus core technologies user experiences 25 portfolio ai mapillary duedil jukedeck seldon clarify gluru ravelin want take risk technologists creating new markets reinventing existing ones quick cheer standing ovation clap show much enjoyed story advancing human progress intelligent systems venture partner pointninecap former scientist photographer perpetual foodie nathanai ldn_ai twentybn,en,"['Deep Learning', 'Google', 'Microsoft', 'Intel', 'IBM', 'avg', 'IBM Watson', 'Sentient Technologies', 'NLP', 'CB Insights', 'Ravelin', 'Signal', 'Gluru', 'LiftForward', 'Argon Credit', 'TellApart/Twitter', 'Elastica/Blue Coat Systems', 'SupersonicAds/IronSource', 'VC', 'BI', 'the UK Biobank', 'NHS', 'BAML', 'eBay', 'Playfair Capital', 'DueDil', 'Jukedeck', 'Clarify', 'Venture Partner']"
123,Erik Hallström,2500,How to build a Recurrent Neural Network in TensorFlow (1/7),"In this tutorial I’ll explain how to build a simple working Recurrent Neural Network in TensorFlow. This is the first in a series of seven parts where various aspects and techniques of building Recurrent Neural Networks in TensorFlow are covered. A short introduction to TensorFlow is available here. For now, let’s get started with the RNN!
It is short for “Recurrent Neural Network”, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. More importantly, this sequence can be of arbitrary length.
The most straight-forward example is perhaps a time-series of numbers, where the task is to predict the next value given previous values. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has “seen” at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.
The best and most comprehensive article explaining RNN:s I’ve found so far is this article by researchers at UCSD, highly recommended. For now you only need to understand the basics, read it until the “Modern RNN architectures”-section. That will be covered later.
Although this article contains some explanations, it is mostly focused on the practical part, how to build it. You are encouraged to look up more theory on the Internet, there are plenty of good explanations.
We will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. First let’s set some constants we’ll need, what they mean will become clear in a moment.
Now generate the training data, the input is basically a random binary vector. The output will be the “echo” of the input, shifted echo_step steps to the right.
Notice the reshaping of the data into a matrix with batch_size rows. Neural networks are trained by approximating the gradient of loss function with respect to the neuron-weights, by looking at only a small subset of the data, also known as a mini-batch. The theoretical reason for doing this is further elaborated in this question. The reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into these mini-batches.
TensorFlow works by first building up a computational graph, that specifies what operations will be done. The input and output of this graph is typically multidimensional arrays, also known as tensors. The graph, or parts of it can then be executed iteratively in a session, this can either be done on the CPU, GPU or even a resource on a remote server.
The two basic TensorFlow data-structures that will be used in this example are placeholders and variables. On each run the batch data is fed to the placeholders, which are “starting nodes” of the computational graph. Also the RNN-state is supplied in a placeholder, which is saved from the output of the previous run.
The weights and biases of the network are declared as TensorFlow variables, which makes them persistent across runs and enables them to be updated incrementally for each batch.
The figure below shows the input data-matrix, and the current batch batchX_placeholder is in the dashed rectangle. As we will see later, this “batch window” is slided truncated_backprop_length steps to the right at each run, hence the arrow. In our example below batch_size = 3, truncated_backprop_length = 3, and total_series_length = 36. Note that these numbers are just for visualization purposes, the values are different in the code. The series order index is shown as numbers in a few of the data-points.
Now it’s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps.
As you can see in the picture below that is done by unpacking the columns (axis = 1) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names “plural”_”series” is to emphasize that the variable is a list that represent a time-series with multiple entries at each step.
The fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the init_state placeholder has batch_size rows.
Next let’s build the part of the graph that does the actual RNN computation.
Notice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms current_input * Wa + current_state * Wb in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias b is broadcasted on all samples in the batch.
You may wonder the variable name truncated_backprop_length is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch.
This is the final part of the graph, a fully connected softmax layer from the state to the output that will make the classes one-hot encoded, and then calculating the loss of the batch.
The last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically — the computation graph is executed once for each mini-batch and the network-weights are updated incrementally.
Notice the API call to sparse_softmax_cross_entropy_with_logits, it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the “Sparse-softmax”, you can read more about it in the API. The usage is to havelogits is of shape [batch_size, num_classes] and labels of shape [batch_size].
There is a visualization function so we can se what’s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch.
It’s time to wrap up and train the network, in TensorFlow the graph is executed in a session. New data is generated on each epoch (not the usual way to do it, but it works in this case since everything is predictable).
You can see that we are moving truncated_backprop_length steps forward on each iteration (line 15–19), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that truncated_backprop_length need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of “misses”, as you can see on the figure below.
Also realize that this is just simple example to explain how a RNN works, this functionality could easily be programmed in just a few lines of code. The network will be able to exactly learn the echo behavior so there is no need for testing data.
The program will update the plot as training progresses, shown in the picture below. Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch.
Our algorithm will fairly quickly learn the task. The graph in the top-left corner shows the output of the loss function, but why are there spikes in the curve? Think of it for a moment, answer is below.
The reason for the spikes is that we are starting on a new epoch, and generating new data. Since the matrix is reshaped, the first element on each row is adjacent to the last element in the previous row. The first few elements on all rows (except the first) have dependencies that will not be included in the state, so the net will always perform badly on the first batch.
This is the whole runnable program, just copy-paste and run. After each part in the article series the whole runnable program will be presented. If a line is referenced by number, these are the line numbers that we mean.
In the next post in this series we will be simplify the computational graph creation by using the native TensorFlow RNN API.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Engineering Physics and in Machine Learning at Royal Institute of Technology in Stockholm. Also been living in Taiwan 學習中文. Interested in Deep Learning.
",tutorial ill explain build simple working recurrent neural network tensorflow first series seven parts various aspects techniques building recurrent neural networks tensorflow covered short introduction tensorflow available lets get started rnn short recurrent neural network basically neural network used data treated sequence particular order datapoints matter importantly sequence arbitrary length straightforward example perhaps timeseries numbers task predict next value given previous values input rnn every timestep current value well state vector represent network seen timesteps statevector encoded memory rnn initially set zero best comprehensive article explaining rnns ive found far article researchers ucsd highly recommended need understand basics read modern rnn architecturessection covered later although article contains explanations mostly focused practical part build encouraged look theory internet plenty good explanations build simple echornn remembers input data echoes timesteps first lets set constants well need mean become clear moment generate training data input basically random binary vector output echo input shifted echo_step steps right notice reshaping data matrix batch_size rows neural networks trained approximating gradient loss function respect neuronweights looking small subset data also known minibatch theoretical reason elaborated question reshaping takes whole dataset puts matrix later sliced minibatches tensorflow works first building computational graph specifies operations done input output graph typically multidimensional arrays also known tensors graph parts executed iteratively session either done cpu gpu even resource remote server two basic tensorflow datastructures used example placeholders variables run batch data fed placeholders starting nodes computational graph also rnnstate supplied placeholder saved output previous run weights biases network declared tensorflow variables makes persistent across runs enables updated incrementally batch figure shows input datamatrix current batch batchx_placeholder dashed rectangle see later batch window slided truncated_backprop_length steps right run hence arrow example batch_size 3 truncated_backprop_length 3 total_series_length 36 note numbers visualization purposes values different code series order index shown numbers datapoints time build part graph resembles actual rnn computation first want split batch data adjacent timesteps see picture done unpacking columns axis 1 batch python list rnn simultaneously training different parts timeseries steps 4 6 16 18 28 30 current batchexample reason using variable names plural_series emphasize variable list represent timeseries multiple entries step fact training done three places simultaneously timeseries requires us save three instances states propagating forward already accounted see init_state placeholder batch_size rows next lets build part graph actual rnn computation notice concatenation line 6 actually want calculate sum two affine transforms current_input wa current_state wb figure concatenating two tensors use one matrix multiplication addition bias b broadcasted samples batch may wonder variable name truncated_backprop_length supposed mean rnn trained actually treated deep neural network reoccurring weights every layer layers unrolled beginning time would computationally expensive therefore truncated limited number timesteps sample schematics error backpropagated three steps batch final part graph fully connected softmax layer state output make classes onehot encoded calculating loss batch last line adding training functionality tensorflow perform backpropagation us automatically computation graph executed minibatch networkweights updated incrementally notice api call sparse_softmax_cross_entropy_with_logits automatically calculates softmax internally computes crossentropy example classes mutually exclusive either zero one reason using sparsesoftmax read api usage havelogits shape batch_size num_classes labels shape batch_size visualization function se whats going network train plot loss time show training input training output current predictions network different sample series training batch time wrap train network tensorflow graph executed session new data generated epoch usual way works case since everything predictable see moving truncated_backprop_length steps forward iteration line 1519 possible different strides subject elaborated article downside truncated_backprop_length need significantly larger time dependencies three steps case order encapsulate relevant training data otherwise might lot misses see figure also realize simple example explain rnn works functionality could easily programmed lines code network able exactly learn echo behavior need testing data program update plot training progresses shown picture blue bars denote training input signal binary one red bars show echos training output green bars echos net generating different bar plots show different sample series current batch algorithm fairly quickly learn task graph topleft corner shows output loss function spikes curve think moment answer reason spikes starting new epoch generating new data since matrix reshaped first element row adjacent last element previous row first elements rows except first dependencies included state net always perform badly first batch whole runnable program copypaste run part article series whole runnable program presented line referenced number line numbers mean next post series simplify computational graph creation using native tensorflow rnn api quick cheer standing ovation clap show much enjoyed story studied engineering physics machine learning royal institute technology stockholm also living taiwan interested deep learning,en,"['Recurrent Neural Network', 'Recurrent Neural Networks', 'UCSD', 'echo_step', 'CPU', 'GPU', 'TensorFlow', 'RNN', 'API', 'TensorFlow RNN API', 'Studied Engineering Physics', 'Royal Institute of Technology']"
124,Stefan Kojouharov,1500,Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot,"Code Snippets and Github Included
Over the past few months I have been collecting the best resources on NLP and how to apply NLP and Deep Learning to Chatbots.
Every once in awhile, I would run across an exception piece of content and I quickly started putting together a master list. Soon I found myself sharing this list and some of the most useful articles with developers and other people in bot community.
In process, my list became a Guide and after some urging, I have decided to share it or at least a condensed version of it -for length reasons.
This guide is mostly based on the work done by Denny Britz who has done a phenomenal job exploring the depths of Deep Learning for Bots. Code Snippets and Github included!
Without further ado... Let us Begin!
Chatbots, are a hot topic and many companies are hoping to develop bots to have natural conversations indistinguishable from human ones, and many are claiming to be using NLP and Deep Learning techniques to make this possible. But with all the hype around AI it’s sometimes difficult to tell fact from fiction.
In this series I want to go over some of the Deep Learning techniques that are used to build conversational agents, starting off by explaining where we are right now, what’s possible, and what will stay nearly impossible for at least a little while.
Retrieval-based models (easier) use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. The heuristic could be as simple as a rule-based expression match, or as complex as an ensemble of Machine Learning classifiers. These systems don’t generate any new text, they just pick a response from a fixed set.
Generative models (harder) don’t rely on pre-defined responses. They generate new responses from scratch. Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we “translate” from an input to an output (response).
Both approaches have some obvious pros and cons. Due to the repository of handcrafted responses, retrieval-based methods don’t make grammatical mistakes. However, they may be unable to handle unseen cases for which no appropriate predefined response exists. For the same reasons, these models can’t refer back to contextual entity information like names mentioned earlier in the conversation. Generative models are “smarter”. They can refer back to entities in the input and give the impression that you’re talking to a human. However, these models are hard to train, are quite likely to make grammatical mistakes (especially on longer sentences), and typically require huge amounts of training data.
Deep Learning techniques can be used for both retrieval-based or generative models, but research seems to be moving into the generative direction. Deep Learning architectures likeSequence to Sequence are uniquely suited for generating text and researchers are hoping to make rapid progress in this area. However, we’re still at the early stages of building generative models that work reasonably well. Production systems are more likely to be retrieval-based for now.
LONG VS. SHORT CONVERSATIONS
The longer the conversation the more difficult to automate it. On one side of the spectrum areShort-Text Conversations (easier) where the goal is to create a single response to a single input. For example, you may receive a specific question from a user and reply with an appropriate answer. Then there are long conversations (harder) where you go through multiple turns and need to keep track of what has been said. Customer support conversations are typically long conversational threads with multiple questions.
In an open domain (harder) setting the user can take the conversation anywhere. There isn’t necessarily have a well-defined goal or intention. Conversations on social media sites like Twitter and Reddit are typically open domain — they can go into all kinds of directions. The infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem.
In a closed domain (easier) setting the space of possible inputs and outputs is somewhat limited because the system is trying to achieve a very specific goal. Technical Customer Support or Shopping Assistants are examples of closed domain problems. These systems don’t need to be able to talk about politics, they just need to fulfill their specific task as efficiently as possible. Sure, users can still take the conversation anywhere they want, but the system isn’t required to handle all these cases — and the users don’t expect it to.
There are some obvious and not-so-obvious challenges when building conversational agents most of which are active research areas.
To produce sensible responses systems may need to incorporate both linguistic context andphysical context. In long dialogs people keep track of what has been said and what information has been exchanged. That’s an example of linguistic context. The most common approach is toembed the conversation into a vector, but doing that with long conversations is challenging. Experiments in Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models and Attention with Intention for a Neural Network Conversation Model both go into that direction. One may also need to incorporate other kinds of contextual data such as date/time, location, or information about a user.
When generating responses the agent should ideally produce consistent answers to semantically identical inputs. For example, you want to get the same reply to “How old are you?” and “What is your age?”. This may sound simple, but incorporating such fixed knowledge or “personality” into models is very much a research problem. Many systems learn to generate linguistic plausible responses, but they are not trained to generate semantically consistent ones. Usually that’s because they are trained on a lot of data from multiple different users. Models like that in A Persona-Based Neural Conversation Model are making first steps into the direction of explicitly modeling a personality.
The ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task, e.g. solve a customer support problem, in a given conversation. But such labels are expensive to obtain because they require human judgment and evaluation. Sometimes there is no well-defined goal, as is the case with open-domain models. Common metrics such as BLEUthat are used for Machine Translation and are based on text matching aren’t well suited because sensible responses can contain completely different words or phrases. In fact, in How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation researchers find that none of the commonly used metrics really correlate with human judgment.
A common problem with generative systems is that they tend to produce generic responses like “That’s great!” or “I don’t know” that work for a lot of input cases. Early versions of Google’s Smart Reply tended to respond with “I love you” to almost anything. That’s partly a result of how these systems are trained, both in terms of data and in terms of actual training objective/algorithm. Some researchers have tried to artificially promote diversity through various objective functions. However, humans typically produce responses that are specific to the input and carry an intention. Because generative systems (and particularly open-domain systems) aren’t trained to have specific intentions they lack this kind of diversity.
Given all the cutting edge research right now, where are we and how well do these systems actually work? Let’s consider our taxonomy again. A retrieval-based open domain system is obviously impossible because you can never handcraft enough responses to cover all cases. A generative open-domain system is almost Artificial General Intelligence (AGI) because it needs to handle all possible scenarios. We’re very far away from that as well (but a lot of research is going on in that area).
This leaves us with problems in restricted domains where both generative and retrieval based methods are appropriate. The longer the conversations and the more important the context, the more difficult the problem becomes.
In a recent interview, Andrew Ng, now chief scientist of Baidu, puts it well:
Many companies start off by outsourcing their conversations to human workers and promise that they can “automate” it once they’ve collected enough data. That’s likely to happen only if they are operating in a pretty narrow domain — like a chat interface to call an Uber for example. Anything that’s a bit more open domain (like sales emails) is beyond what we can currently do. However, we can also use these systems to assist human workers by proposing and correcting responses. That’s much more feasible.
Grammatical mistakes in production systems are very costly and may drive away users. That’s why most systems are probably best off using retrieval-based methods that are free of grammatical errors and offensive responses. If companies can somehow get their hands on huge amounts of data then generative models become feasible — but they must be assisted by other techniques to prevent them from going off the rails like Microsoft’s Tay did.
The Code and data for this tutorial is on Github.
The vast majority of production systems today are retrieval-based, or a combination of retrieval-based and generative. Google’s Smart Reply is a good example. Generative models are an active area of research, but we’re not quite there yet. If you want to build a conversational agent today your best bet is most likely a retrieval-based model.
In this post we’ll work with the Ubuntu Dialog Corpus (paper, github). The Ubuntu Dialog Corpus (UDC) is one of the largest public dialog datasets available. It’s based on chat logs from the Ubuntu channels on a public IRC network. The paper goes into detail on how exactly the corpus was created, so I won’t repeat that here. However, it’s important to understand what kind of data we’re working with, so let’s do some exploration first.
The training data consists of 1,000,000 examples, 50% positive (label 1) and 50% negative (label 0). Each example consists of a context, the conversation up to this point, and an utterance, a response to the context. A positive label means that an utterance was an actual response to a context, and a negative label means that the utterance wasn’t — it was picked randomly from somewhere in the corpus. Here is some sample data.
Note that the dataset generation script has already done a bunch of preprocessing for us — it hastokenized, stemmed, and lemmatized the output using the NLTK tool. The script also replaced entities like names, locations, organizations, URLs, and system paths with special tokens. This preprocessing isn’t strictly necessary, but it’s likely to improve performance by a few percent. The average context is 86 words long and the average utterance is 17 words long. Check out the Jupyter notebook to see the data analysis.
The data set comes with test and validations sets. The format of these is different from that of the training data. Each record in the test/validation set consists of a context, a ground truth utterance (the real response) and 9 incorrect utterances called distractors. The goal of the model is to assign the highest score to the true utterance, and lower scores to wrong utterances.
The are various ways to evaluate how well our model does. A commonly used metric is recall@k. Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). If the correct one is among the picked ones we mark that test example as correct. So, a larger k means that the task becomes easier. If we set k=10 we get a recall of 100% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response.
At this point you may be wondering how the 9 distractors were chosen. In this data set the 9 distractors were picked at random. However, in the real world you may have millions of possible responses and you don’t know which one is correct. You can’t possibly evaluate a million potential responses to pick the one with the highest score — that’d be too expensive. Google’sSmart Reply uses clustering techniques to come up with a set of possible responses to choose from first. Or, if you only have a few hundred potential responses in total you could just evaluate all of them.
Before starting with fancy Neural Network models let’s build some simple baseline models to help us understand what kind of performance we can expect. We’ll use the following function to evaluate our recall@k metric:
Here, y is a list of our predictions sorted by score in descending order, and y_test is the actual label. For example, a y of [0,3,1,2,5,6,4,7,8,9] Would mean that the utterance number 0 got the highest score, and utterance 9 got the lowest score. Remember that we have 10 utterances for each test example, and the first one (index 0) is always the correct one because the utterance column comes before the distractor columns in our data.
Intuitively, a completely random predictor should get a score of 10% for recall@1, a score of 20% for recall@2, and so on. Let’s see if that’s the case.
Great, seems to work. Of course we don’t just want a random predictor. Another baseline that was discussed in the original paper is a tf-idf predictor. tf-idf stands for “term frequency — inverse document” frequency and it measures how important a word in a document is relative to the whole corpus. Without going into too much detail (you can find many tutorials about tf-idf on the web), documents that have similar content will have similar tf-idf vectors. Intuitively, if a context and a response have similar words they are more likely to be a correct pair. At least more likely than random. Many libraries out there (such as scikit-learn) come with built-in tf-idf functions, so it’s very easy to use. Let’s build a tf-idf predictor and see how well it performs.
We can see that the tf-idf model performs significantly better than the random model. It’s far from perfect though. The assumptions we made aren’t that great. First of all, a response doesn’t necessarily need to be similar to the context to be correct. Secondly, tf-idf ignores word order, which can be an important signal. With a Neural Network model we can do a bit better.
The Deep Learning model we will build in this post is called a Dual Encoder LSTM network. This type of network is just one of many we could apply to this problem and it’s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven’t been tried yet — it’s an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct. Applying other models to this problem would be an interesting project.
The Dual Encoder LSTM we’ll build looks like this (paper):
It roughly works as follows:
To train the network, we also need a loss (cost) function. We’ll use the binary cross-entropy loss common for classification problems. Let’s call our true label for a context-response pair y. This can be either 1 (actual response) or 0 (incorrect response). Let’s call our predicted probability from 4. above y’. Then, the cross entropy loss is calculated as L= −y * ln(y’) − (1 − y) * ln(1−y’). The intuition behind this formula is simple. If y=1 we are left with L = -ln(y’), which penalizes a prediction far away from 1, and if y=0 we are left with L= −ln(1−y’), which penalizes a prediction far away from 0.
For our implementation we’ll use a combination of numpy, pandas, Tensorflow and TF Learn (a combination of high-level convenience functions for Tensorflow).
The dataset originally comes in CSV format. We could work directly with CSVs, but it’s better to convert our data into Tensorflow’s proprietary Example format. (Quick side note: There’s alsotf.SequenceExample but it doesn’t seem to be supported by tf.learn yet). The main benefit of this format is that it allows us to load tensors directly from the input files and let Tensorflow handle all the shuffling, batching and queuing of inputs. As part of the preprocessing we also create a vocabulary. This means we map each word to an integer number, e.g. “cat” may become 2631. The TFRecord files we will generate store these integer numbers instead of the word strings. We will also save the vocabulary so that we can map back from integers to words later on.
Each Example contains the following fields:
The preprocessing is done by the prepare_data.py Python script, which generates 3 files:train.tfrecords, validation.tfrecords and test.tfrecords. You can run the script yourself or download the data files here.
In order to use Tensorflow’s built-in support for training and evaluation we need to create an input function — a function that returns batches of our input data. In fact, because our training and test data have different formats, we need different input functions for them. The input function should return a batch of features and labels (if available). Something along the lines of:
Because we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create_input_fn that creates an input function for the appropriate mode. It also takes a few other parameters. Here’s the definition we’re using:
The complete code can be found in udc_inputs.py. On a high level, the function does the following:
We already mentioned that we want to use the recall@k metric to evaluate our model. Luckily, Tensorflow already comes with many standard evaluation metrics that we can use, including recall@k. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments:
Above, we use functools.partial to convert a function that takes 3 arguments to one that only takes 2 arguments. Don’t let the name streaming_sparse_recall_at_k confuse you. Streaming just means that the metric is accumulated over multiple batches, and sparse refers to the format of our labels.
This brings is to an important point: What exactly is the format of our predictions during evaluation? During training, we predict the probability of the example being correct. But during evaluation our goal is to score the utterance and 9 distractors and pick the best one — we don’t simply predict correct/incorrect. This means that during evaluation each example should result in a vector of 10 scores, e.g. [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11], where the scores correspond to the true response and the 9 distractors respectively. Each utterance is scored independently, so the probabilities don’t need to add up to 1. Because the true response is always element 0 in array, the label for each example is 0. The example above would be counted as classified incorrectly by recall@1because the third distractor got a probability of 0.45 while the true response only got 0.34. It would be scored as correct by recall@2 however.
Before writing the actual neural network code I like to write the boilerplate code for training and evaluating the model. That’s because, as long as you adhere to the right interfaces, it’s easy to swap out what kind of network you are using. Let’s assume we have a model functionmodel_fn that takes as inputs our batched features, labels and mode (train or evaluation) and returns the predictions. Then we can write general-purpose code to train our model as follows:
Here we create an estimator for our model_fn, two input functions for training and evaluation data, and our evaluation metrics dictionary. We also define a monitor that evaluates our model every FLAGS.eval_every steps during training. Finally, we train the model. The training runs indefinitely, but Tensorflow automatically saves checkpoint files in MODEL_DIR, so you can stop the training at any time. A more fancy technique would be to use early stopping, which means you automatically stop training when a validation set metric stops improving (i.e. you are starting to overfit). You can see the full code in udc_train.py.
Two things I want to mention briefly is the usage of FLAGS. This is a way to give command line parameters to the program (similar to Python’s argparse). hparams is a custom object we create in hparams.py that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it.
Now that we have set up the boilerplate code around inputs, parsing, evaluation and training it’s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I’ve written a create_model_fn wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions. In our case it’s the Dual Encoder LSTM we described above, but we could easily swap it out for some other neural network. Let’s see what that looks like:
The full code is in dual_encoder.py. Given this, we can now instantiate our model function in the main routine in udc_train.py that we defined earlier.
That’s it! We can now run python udc_train.py and it should start training our networks, occasionally evaluating recall on our validation data (you can choose how often you want to evaluate using the — eval_every switch). To get a complete list of all available command line flags that we defined using tf.flags and hparams you can run python udc_train.py — help.
...
INFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383
...
After you’ve trained the model you can evaluate it on the test set using python udc_test.py — model_dir=$MODEL_DIR_FROM_TRAINING, e.g. python udc_test.py — model_dir=~/github/chatbot-retrieval/runs/1467389151. This will run the recall@k evaluation metrics on the test set instead of the validation set. Note that you must call udc_test.py with the same parameters you used during training. So, if you trained with — embedding_size=128 you need to call the test script with the same.
After training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set:
While recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. The original paper reported 0.55, 0.72 and 0.92 for recall@1, recall@2, and recall@5 respectively, but I haven’t been able to reproduce scores quite as high. Perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more.
You can modify and run udc_predict.py to get probability scores for unseen data. For example python udc_predict.py — model_dir=./runs/1467576365/ outputs:
You could imagine feeding in 100 potential responses to a context and then picking the one with the highest score.
In this post we’ve implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. There is still a lot of room for improvement, however. One can imagine that other neural networks do better on this task than a dual LSTM encoder. There is also a lot of room for hyperparameter optimization, or improvements to the preprocessing step. The Code and data for this tutorial is on Github, so check it out.
Denny’s Blogs: http://blog.dennybritz.com/ & http://www.wildml.com/
Mark Clark: https://www.linkedin.com/in/markwclark
I hope you have found this Condensed NLP Guide Helpful. I wanted to publish a longer version (imagine if this was 5x longer) however I don’t want to scare the readers away.
As someone who develops the front end of bots (user experience, personality, flow, etc) I find it extremely helpful to the understand the stack, know the technological pros and cons and so to be able to effectively design around NLP/NLU limitations. Ultimately a lot of the issues bots face today (eg: context) can be designed around, effectively.
If you have any suggestions on regarding this article and how it can be improved, feel free to drop me a line.
Creator of 10+ bots, including Smart Notes Bot. Founder of Chatbot’s Life, where we help companies create great chatbots and share our insights along the way.
Want to Talk Bots? Best way to chat directly and see my latest projects is via my Personal Bot: Stefan’s Bot.
Currently, I’m consulting a number of companies on their chatbot projects. To get feedback on your Chatbot project or to Start a Chatbot Project, contact me.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of Chatbots Life. I help Companies Create Great Chatbots & AI Systems and share my Insights along the way.
Best place to learn about Chatbots. We share the latest Bot News, Info, AI & NLP, Tools, Tutorials & More.
",code snippets github included past months collecting best resources nlp apply nlp deep learning chatbots every awhile would run across exception piece content quickly started putting together master list soon found sharing list useful articles developers people bot community process list became guide urging decided share least condensed version length reasons guide mostly based work done denny britz done phenomenal job exploring depths deep learning bots code snippets github included without ado let us begin chatbots hot topic many companies hoping develop bots natural conversations indistinguishable human ones many claiming using nlp deep learning techniques make possible hype around ai sometimes difficult tell fact fiction series want go deep learning techniques used build conversational agents starting explaining right whats possible stay nearly impossible least little retrievalbased models easier use repository predefined responses kind heuristic pick appropriate response based input context heuristic could simple rulebased expression match complex ensemble machine learning classifiers systems dont generate new text pick response fixed set generative models harder dont rely predefined responses generate new responses scratch generative models typically based machine translation techniques instead translating one language another translate input output response approaches obvious pros cons due repository handcrafted responses retrievalbased methods dont make grammatical mistakes however may unable handle unseen cases appropriate predefined response exists reasons models cant refer back contextual entity information like names mentioned earlier conversation generative models smarter refer back entities input give impression youre talking human however models hard train quite likely make grammatical mistakes especially longer sentences typically require huge amounts training data deep learning techniques used retrievalbased generative models research seems moving generative direction deep learning architectures likesequence sequence uniquely suited generating text researchers hoping make rapid progress area however still early stages building generative models work reasonably well production systems likely retrievalbased long vs short conversations longer conversation difficult automate one side spectrum areshorttext conversations easier goal create single response single input example may receive specific question user reply appropriate answer long conversations harder go multiple turns need keep track said customer support conversations typically long conversational threads multiple questions open domain harder setting user take conversation anywhere isnt necessarily welldefined goal intention conversations social media sites like twitter reddit typically open domain go kinds directions infinite number topics fact certain amount world knowledge required create reasonable responses makes hard problem closed domain easier setting space possible inputs outputs somewhat limited system trying achieve specific goal technical customer support shopping assistants examples closed domain problems systems dont need able talk politics need fulfill specific task efficiently possible sure users still take conversation anywhere want system isnt required handle cases users dont expect obvious notsoobvious challenges building conversational agents active research areas produce sensible responses systems may need incorporate linguistic context andphysical context long dialogs people keep track said information exchanged thats example linguistic context common approach toembed conversation vector long conversations challenging experiments building endtoend dialogue systems using generative hierarchical neural network models attention intention neural network conversation model go direction one may also need incorporate kinds contextual data datetime location information user generating responses agent ideally produce consistent answers semantically identical inputs example want get reply old age may sound simple incorporating fixed knowledge personality models much research problem many systems learn generate linguistic plausible responses trained generate semantically consistent ones usually thats trained lot data multiple different users models like personabased neural conversation model making first steps direction explicitly modeling personality ideal way evaluate conversational agent measure whether fulfilling task eg solve customer support problem given conversation labels expensive obtain require human judgment evaluation sometimes welldefined goal case opendomain models common metrics bleuthat used machine translation based text matching arent well suited sensible responses contain completely different words phrases fact evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation researchers find none commonly used metrics really correlate human judgment common problem generative systems tend produce generic responses like thats great dont know work lot input cases early versions googles smart reply tended respond love almost anything thats partly result systems trained terms data terms actual training objectivealgorithm researchers tried artificially promote diversity various objective functions however humans typically produce responses specific input carry intention generative systems particularly opendomain systems arent trained specific intentions lack kind diversity given cutting edge research right well systems actually work lets consider taxonomy retrievalbased open domain system obviously impossible never handcraft enough responses cover cases generative opendomain system almost artificial general intelligence agi needs handle possible scenarios far away well lot research going area leaves us problems restricted domains generative retrieval based methods appropriate longer conversations important context difficult problem becomes recent interview andrew ng chief scientist baidu puts well many companies start outsourcing conversations human workers promise automate theyve collected enough data thats likely happen operating pretty narrow domain like chat interface call uber example anything thats bit open domain like sales emails beyond currently however also use systems assist human workers proposing correcting responses thats much feasible grammatical mistakes production systems costly may drive away users thats systems probably best using retrievalbased methods free grammatical errors offensive responses companies somehow get hands huge amounts data generative models become feasible must assisted techniques prevent going rails like microsofts tay code data tutorial github vast majority production systems today retrievalbased combination retrievalbased generative googles smart reply good example generative models active area research quite yet want build conversational agent today best bet likely retrievalbased model post well work ubuntu dialog corpus paper github ubuntu dialog corpus udc one largest public dialog datasets available based chat logs ubuntu channels public irc network paper goes detail exactly corpus created wont repeat however important understand kind data working lets exploration first training data consists 1000000 examples 50 positive label 1 50 negative label 0 example consists context conversation point utterance response context positive label means utterance actual response context negative label means utterance wasnt picked randomly somewhere corpus sample data note dataset generation script already done bunch preprocessing us hastokenized stemmed lemmatized output using nltk tool script also replaced entities like names locations organizations urls system paths special tokens preprocessing isnt strictly necessary likely improve performance percent average context 86 words long average utterance 17 words long check jupyter notebook see data analysis data set comes test validations sets format different training data record testvalidation set consists context ground truth utterance real response 9 incorrect utterances called distractors goal model assign highest score true utterance lower scores wrong utterances various ways evaluate well model commonly used metric recallk recallk means let model pick k best responses 10 possible responses 1 true 9 distractors correct one among picked ones mark test example correct larger k means task becomes easier set k10 get recall 100 10 responses pick set k1 model one chance pick right response point may wondering 9 distractors chosen data set 9 distractors picked random however real world may millions possible responses dont know one correct cant possibly evaluate million potential responses pick one highest score thatd expensive googlessmart reply uses clustering techniques come set possible responses choose first hundred potential responses total could evaluate starting fancy neural network models lets build simple baseline models help us understand kind performance expect well use following function evaluate recallk metric list predictions sorted score descending order y_test actual label example 0312564789 would mean utterance number 0 got highest score utterance 9 got lowest score remember 10 utterances test example first one index 0 always correct one utterance column comes distractor columns data intuitively completely random predictor get score 10 recall1 score 20 recall2 lets see thats case great seems work course dont want random predictor another baseline discussed original paper tfidf predictor tfidf stands term frequency inverse document frequency measures important word document relative whole corpus without going much detail find many tutorials tfidf web documents similar content similar tfidf vectors intuitively context response similar words likely correct pair least likely random many libraries scikitlearn come builtin tfidf functions easy use lets build tfidf predictor see well performs see tfidf model performs significantly better random model far perfect though assumptions made arent great first response doesnt necessarily need similar context correct secondly tfidf ignores word order important signal neural network model bit better deep learning model build post called dual encoder lstm network type network one many could apply problem necessarily best one come kinds deep learning architectures havent tried yet active research area example seq2seq model often used machine translation would probably well task reason going dual encoder reported give decent performance data set means know expect sure implementation correct applying models problem would interesting project dual encoder lstm well build looks like paper roughly works follows train network also need loss cost function well use binary crossentropy loss common classification problems lets call true label contextresponse pair either 1 actual response 0 incorrect response lets call predicted probability 4 cross entropy loss calculated l lny 1 ln1y intuition behind formula simple y1 left l lny penalizes prediction far away 1 y0 left l ln1y penalizes prediction far away 0 implementation well use combination numpy pandas tensorflow tf learn combination highlevel convenience functions tensorflow dataset originally comes csv format could work directly csvs better convert data tensorflows proprietary example format quick side note theres alsotfsequenceexample doesnt seem supported tflearn yet main benefit format allows us load tensors directly input files let tensorflow handle shuffling batching queuing inputs part preprocessing also create vocabulary means map word integer number eg cat may become 2631 tfrecord files generate store integer numbers instead word strings also save vocabulary map back integers words later example contains following fields preprocessing done prepare_datapy python script generates 3 filestraintfrecords validationtfrecords testtfrecords run script download data files order use tensorflows builtin support training evaluation need create input function function returns batches input data fact training test data different formats need different input functions input function return batch features labels available something along lines need different input functions training evaluation hate code duplication create wrapper called create_input_fn creates input function appropriate mode also takes parameters heres definition using complete code found udc_inputspy high level function following already mentioned want use recallk metric evaluate model luckily tensorflow already comes many standard evaluation metrics use including recallk use metrics need create dictionary maps metric name function takes predictions label arguments use functoolspartial convert function takes 3 arguments one takes 2 arguments dont let name streaming_sparse_recall_at_k confuse streaming means metric accumulated multiple batches sparse refers format labels brings important point exactly format predictions evaluation training predict probability example correct evaluation goal score utterance 9 distractors pick best one dont simply predict correctincorrect means evaluation example result vector 10 scores eg 034 011 022 045 001 002 003 008 033 011 scores correspond true response 9 distractors respectively utterance scored independently probabilities dont need add 1 true response always element 0 array label example 0 example would counted classified incorrectly recall1because third distractor got probability 045 true response got 034 would scored correct recall2 however writing actual neural network code like write boilerplate code training evaluating model thats long adhere right interfaces easy swap kind network using lets assume model functionmodel_fn takes inputs batched features labels mode train evaluation returns predictions write generalpurpose code train model follows create estimator model_fn two input functions training evaluation data evaluation metrics dictionary also define monitor evaluates model every flagseval_every steps training finally train model training runs indefinitely tensorflow automatically saves checkpoint files model_dir stop training time fancy technique would use early stopping means automatically stop training validation set metric stops improving ie starting overfit see full code udc_trainpy two things want mention briefly usage flags way give command line parameters program similar pythons argparse hparams custom object create hparamspy holds hyperparameters nobs tweak model hparams object given model instantiate set boilerplate code around inputs parsing evaluation training time write code dual lstm neural network different formats training evaluation data ive written create_model_fn wrapper takes care bringing data right format us takes model_impl argument function actually makes predictions case dual encoder lstm described could easily swap neural network lets see looks like full code dual_encoderpy given instantiate model function main routine udc_trainpy defined earlier thats run python udc_trainpy start training networks occasionally evaluating recall validation data choose often want evaluate using eval_every switch get complete list available command line flags defined using tfflags hparams run python udc_trainpy help infotensorflowresults 270 steps 0248 secbatch recall_at_1 0507581018519 recall_at_2 0689699074074 recall_at_5 0913020833333 recall_at_10 10 loss 05383 youve trained model evaluate test set using python udc_testpy model_dirmodel_dir_from_training eg python udc_testpy model_dirgithubchatbotretrievalruns1467389151 run recallk evaluation metrics test set instead validation set note must call udc_testpy parameters used training trained embedding_size128 need call test script training 20000 steps around hour fast gpu model gets following results test set recall1 close tfidf model recall2 recall5 significantly better suggesting neural network assigns higher scores correct answers original paper reported 055 072 092 recall1 recall2 recall5 respectively havent able reproduce scores quite high perhaps additional data preprocessing hyperparameter optimization may bump scores bit modify run udc_predictpy get probability scores unseen data example python udc_predictpy model_dirruns1467576365 outputs could imagine feeding 100 potential responses context picking one highest score post weve implemented retrievalbased neural network model assign scores potential responses given conversation context still lot room improvement however one imagine neural networks better task dual lstm encoder also lot room hyperparameter optimization improvements preprocessing step code data tutorial github check dennys blogs httpblogdennybritzcom httpwwwwildmlcom mark clark httpswwwlinkedincominmarkwclark hope found condensed nlp guide helpful wanted publish longer version imagine 5x longer however dont want scare readers away someone develops front end bots user experience personality flow etc find extremely helpful understand stack know technological pros cons able effectively design around nlpnlu limitations ultimately lot issues bots face today eg context designed around effectively suggestions regarding article improved feel free drop line creator 10 bots including smart notes bot founder chatbots life help companies create great chatbots share insights along way want talk bots best way chat directly see latest projects via personal bot stefans bot currently im consulting number companies chatbot projects get feedback chatbot project start chatbot project contact quick cheer standing ovation clap show much enjoyed story founder chatbots life help companies create great chatbots ai systems share insights along way best place learn chatbots share latest bot news info ai nlp tools tutorials,en,"['Github', 'NLP', 'Guide', 'Google', 'Smart Reply', 'Artificial General Intelligence (AGI', 'Baidu', 'Microsoft', 'UDC', 'Ubuntu', 'IRC', 'NLTK', 'Reply', 'Neural Network', 'recall@1', 'Deep Learning', 'the Dual Encoder', 'The Dual Encoder', '−', 'CSV', 'TFRecord', 'Python', 'create_input_fn', 'Tensorflow', 'model_fn', 'model_impl', 'sec', 'GPU', 'TFIDF', 'NLP Guide Helpful', 'Smart Notes Bot', 'Chatbot’s Life', 'Companies Create Great Chatbots & AI Systems', 'AI & NLP', 'Tools, Tutorials & More']"
125,Arthur Juliani,3500,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),"In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven’t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.
So what is A3C? The A3C algorithm was released by Google’s DeepMind group earlier this year, and it made a splash by... essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their “universal starter agent” for working with their new (and very diverse) set of Universe environments.
Asynchronous Advantage Actor-Critic is quite a mouthful. Let’s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.
Asynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.
Actor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.
Advantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were “good” and which were “bad.” The network was then updated in order to encourage and discourage actions appropriately.
The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network’s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:
Since we won’t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.
In this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.
In the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you’d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won’t run on its own. To view and run the full, functional A3C implementation, see my Github repository.
The general outline of the code architecture is:
The A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.
Next, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.
~ From here we go asynchronous ~
Each worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.
Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.
Once the worker’s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.
A worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.
A worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.
Once a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.
To view the full and functional code, see the Github repository here.
The robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:
pip install vizdoom
Once it is installed, we will be using the basic.wad environment, which is provided in the Github repository, and needs to be placed in the working directory.
The challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.
I hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.
(There are a lot of moving parts in A3C, so if you discover a bug, or find a better way to do something, please don’t hesitate to bring it up here or in the Github. I am more than happy to incorporate changes and feedback to improve the algorithm.)
If you’d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.
If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!
More from my Simple Reinforcement Learning with Tensorflow series:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Deep Learning @Unity3D & Cognitive Neuroscience PhD student.
Exploring frontier technology through the lens of artificial intelligence, data science, and the shape of things to come
",article want provide tutorial implementing asynchronous advantage actorcritic a3c algorithm tensorflow use solve simple challenge 3d doom environment holidays right around corner final post year hope serve culmination previous topics series havent yet new deep learning reinforcement learning suggest checking earlier entries series going post order understand building blocks utilized following series thank learned much rl past year happy shared everyone article series a3c a3c algorithm released googles deepmind group earlier year made splash essentially obsoleting dqn faster simpler robust able achieve much better scores standard battery deep rl tasks top could work continuous well discrete action spaces given become goto deep rl algorithm new challenging problems complex state action spaces fact openai released version a3c universal starter agent working new diverse set universe environments asynchronous advantage actorcritic quite mouthful lets start unpacking name begin unpack mechanics algorithm asynchronous unlike dqn single agent represented single neural network interacts single environment a3c utilizes multiple incarnations order learn efficiently a3c global network multiple worker agents set network parameters agents interacts copy environment time agents interacting environments reason works better single agent beyond speedup getting work done experience agent independent experience others way overall experience available training becomes diverse actorcritic far series focused valueiteration methods qlearning policyiteration methods policy gradient actorcritic combines benefits approaches case a3c network estimate value function vs good certain state policy set action probability outputs separate fullyconnected layers sitting top network critically agent uses value estimate critic update policy actor intelligently traditional policy gradient methods advantage think back implementation policy gradient update rule used discounted returns set experiences order tell agent actions good bad network updated order encourage discourage actions appropriately insight using advantage estimates rather discounted returns allow agent determine good actions much better turned expected intuitively allows algorithm focus networks predictions lacking recall dueling qnetwork architecture advantage function follow since wont determining q values directly a3c use discounted returns r estimate qsa allow us generate estimate advantage tutorial go even utilize slightly different version advantage estimation lower variance referred generalized advantage estimation process building implementation a3c algorithm used reference quality implementations dennybritz openai highly recommend youd like see alternatives code section embedded taken context instructional purposes wont run view run full functional a3c implementation see github repository general outline code architecture a3c algorithm begins constructing global network network consist convolutional layers process spatial dependencies followed lstm layer process temporal dependencies finally value policy output layers example code establishing network graph next set worker agents network environment created workers run separate processor thread workers threads cpu go asynchronous worker begins setting network parameters global network constructing tensorflow op sets variable local worker network equivalent variable value global network worker interacts copy environment collects experience keeps list experience tuples observation action reward done value constantly added interactions environment workers experience history large enough use determine discounted return advantage use calculate value policy losses also calculate entropy h policy corresponds spread action probabilities policy outputs actions relatively similar probabilities entropy high policy suggests single action large probability entropy low use entropy means improving exploration encouraging model conservative regarding sureness correct action worker uses losses obtain gradients respect network parameters gradients typically clipped order prevent overlylarge parameter updates destabilize policy worker uses gradients update global network parameters way global network constantly updated agents interact environment successful update made global network whole process repeats worker resets network parameters global network process begins view full functional code see github repository robustness a3c allows us tackle new generation reinforcement learning challenges one 3d environments come long way multiarmed bandits gridworlds tutorial set code allow playing first vizdoom challenge vizdoom system allow rl research using classic doom game engine maintainers vizdoom recently created pip package installing simple pip install vizdoom installed using basicwad environment provided github repository needs placed working directory challenge consists controlling avatar first person perspective single square room single enemy opposite side room appears random location episode agent move left right fire gun goal shoot enemy quickly possible using bullets possible agent 300 time steps per episode shoot enemy shooting enemy yields reward 1 time step well shot yields small penalty 500 episodes per worker agent network learns policy quickly solve challenge feel free adjust parameters learning rate clipping magnitude update frequency etc attempt achieve ever greater performance utilize a3c rl tasks hope tutorial helpful new a3c asynchronous reinforcement learning go forth build ais lot moving parts a3c discover bug find better way something please dont hesitate bring github happy incorporate changes feedback improve algorithm youd like follow writing deep learning ai cognitive science follow medium arthur juliani twitter awjuliani post valuable please consider donating help support future tutorials articles implementations contribution greatly appreciated simple reinforcement learning tensorflow series quick cheer standing ovation clap show much enjoyed story deep learning unity3d cognitive neuroscience phd student exploring frontier technology lens artificial intelligence data science shape things come,en,"['algorithm', 'Google', 'DQN', 'Deep RL', 'Policy Gradient', 'the Dueling Q-Network', 'Q(s', 'Generalized Advantage Estimation', 'DennyBritz', 'Github', 'VizDoom', '@awjuliani', 'Deep Learning @Unity3D & Cognitive Neuroscience']"
126,Alexandr Honchar,1910,Neural networks for algorithmic trading. Simple time series forecasting,"Ciao, people!
This is first part of my experiments on application of deep learning to finance, in particular to algorithmic trading.
I want to implement trading system from scratch based only on deep learning approaches, so for any problem we have here (price prediction, trading strategy, risk management) we gonna use different variations of artificial neural networks (ANNs) and check how well they can handle this.
Now I plan to work on next sections:
I highly recommend you to check out code and IPython Notebook in this repository.
In this, first part, I want to show how MLPs, CNNs and RNNs can be used for financial time series prediction. In this part we are not going to use any feature engineering. Let’s just consider historical dataset of S&P 500 index price movements. We have information from 1950 to 2016 about open, close, high, low prices for every day in the year and volume of trades. First, we will try just to predict close price in the end of the next day, second, we will try to predict return (close price — open price). Download the dataset from Yahoo Finance or from this repository.
We will consider our problem as 1) regression problem (trying to forecast exactly close price or return next day) 2) binary classification problem (price will go up [1; 0] or down [0; 1]).
For training NNs we gonna use framework Keras.
First let’s prepare our data for training. We want to predict t+1 value based on N previous days information. For example, having close prices from past 30 days on the market we want to predict, what price will be tomorrow, on the 31st day.
We use first 90% of time series as training set (consider it as historical data) and last 10% as testing set for model evaluation.
Here is example of loading, splitting into training samples and preprocessing of raw input data:
It will be just 2-hidden layer perceptron. Number of hidden neurons is chosen empirically, we will work on hyperparameters optimization in next sections. Between two hidden layers we add one Dropout layer to prevent overfitting.
Important thing is Dense(1), Activation(‘linear’) and ‘mse’ in compile section. We want one output that can be in any range (we predict real value) and our loss function is defined as mean squared error.
Let’s see what happens if we just pass chunks of 20-days close prices and predict price on 21st day. Final MSE= 46.3635263557, but it’s not very representative information. Below is plot of predictions for first 150 points of test dataset. Black line is actual data, blue one — predicted. We can clearly see that our algorithm is not even close by value, but can learn the trend.
Let’s scale our data using sklearn’s method preprocessing.scale() to have our time series zero mean and unit variance and train the same MLP. Now we have MSE = 0.0040424330518 (but it is on scaled data). On the plot below you can see actual scaled time series (black)and our forecast (blue) for it:
For using this model in real world we should return back to unscaled time series. We can do it, by multiplying or prediction by standard deviation of time series we used to make prediction (20 unscaled time steps) and add it’s mean value:
MSE in this case equals 937.963649937. Here is the plot of restored predictions (red) and real data (green):
Not bad, isn’t it? But let’s try more sophisticated algorithms for this problem!
I am not going to dive into theory of convolutional neural networks, you can check out this amazing resourses:
Let’s define 2-layer convolutional neural network (combination of convolution and max-pooling layers) with one fully-connected layer and the same output as earlier:
Let’s check out results. MSEs for scaled and restored data are: 0.227074542433; 935.520550172. Plots are below:
Even looking on MSE on scaled data, this network learned much worse. Most probably, deeper architecture needs more data for training, or it just overfitted due to too high number of filters or layers. We will consider this issue later.
As recurrent architecture I want to use two stacked LSTM layers (read more about LSTMs here).
Plots of forecasts are below, MSEs = 0.0246238639582; 939.948636707.
RNN forecasting looks more like moving average model, it can’t learn and predict all fluctuations.
So, it’s a bit unexpectable result, but we can see, that MLPs work better for this time series forecasting. Let’s check out what will happen if we swith from regression to classification problem. Now we will use not close prices, but daily return (close price-open price) and we want to predict if close price is higher or lower than open price based on last 20 days returns.
Code is changed just a bit — we change our last Dense layer to have output [0; 1] or [1; 0] and add softmax output to expect probabilistic output.
To load binary outputs, change in the code following line:
Also we change loss function to binary cross-entopy and add accuracy metrics.
Oh, it’s not better than random guessing (50% accuracy), let’s try something better. Check out the results below.
We can see, that treating financial time series prediction as regression problem is better approach, it can learn the trend and prices close to the actual.
What was surprising for me, that MLPs are treating sequence data better as CNNs or RNNs which are supposed to work better with time series. I explain it with pretty small dataset (~16k time stamps) and dummy hyperparameters choice.
You can reproduce results and get better using code from repository.
I think we can get better results both in regression and classification using different features (not only scaled time series) like some technical indicators, volume of sales. Also we can try more frequent data, let’s say minute-by-minute ticks to have more training data. All these things I’m going to do later, so stay tuned :)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
🇺🇦 🇮🇹 AI entrepreneur, blogger and researcher. Making machines work 💻, learn 📕 and like 👍, but humans create 🎨, discover 🚀 and love ❤️
The best about Machine Learning, Computer Vision, Deep Learning, Natural language processing and other.
",ciao people first part experiments application deep learning finance particular algorithmic trading want implement trading system scratch based deep learning approaches problem price prediction trading strategy risk management gonna use different variations artificial neural networks anns check well handle plan work next sections highly recommend check code ipython notebook repository first part want show mlps cnns rnns used financial time series prediction part going use feature engineering lets consider historical dataset sp 500 index price movements information 1950 2016 open close high low prices every day year volume trades first try predict close price end next day second try predict return close price open price download dataset yahoo finance repository consider problem 1 regression problem trying forecast exactly close price return next day 2 binary classification problem price go 1 0 0 1 training nns gonna use framework keras first lets prepare data training want predict t1 value based n previous days information example close prices past 30 days market want predict price tomorrow 31st day use first 90 time series training set consider historical data last 10 testing set model evaluation example loading splitting training samples preprocessing raw input data 2hidden layer perceptron number hidden neurons chosen empirically work hyperparameters optimization next sections two hidden layers add one dropout layer prevent overfitting important thing dense1 activationlinear mse compile section want one output range predict real value loss function defined mean squared error lets see happens pass chunks 20days close prices predict price 21st day final mse 463635263557 representative information plot predictions first 150 points test dataset black line actual data blue one predicted clearly see algorithm even close value learn trend lets scale data using sklearns method preprocessingscale time series zero mean unit variance train mlp mse 00040424330518 scaled data plot see actual scaled time series blackand forecast blue using model real world return back unscaled time series multiplying prediction standard deviation time series used make prediction 20 unscaled time steps add mean value mse case equals 937963649937 plot restored predictions red real data green bad isnt lets try sophisticated algorithms problem going dive theory convolutional neural networks check amazing resourses lets define 2layer convolutional neural network combination convolution maxpooling layers one fullyconnected layer output earlier lets check results mses scaled restored data 0227074542433 935520550172 plots even looking mse scaled data network learned much worse probably deeper architecture needs data training overfitted due high number filters layers consider issue later recurrent architecture want use two stacked lstm layers read lstms plots forecasts mses 00246238639582 939948636707 rnn forecasting looks like moving average model cant learn predict fluctuations bit unexpectable result see mlps work better time series forecasting lets check happen swith regression classification problem use close prices daily return close priceopen price want predict close price higher lower open price based last 20 days returns code changed bit change last dense layer output 0 1 1 0 add softmax output expect probabilistic output load binary outputs change code following line also change loss function binary crossentopy add accuracy metrics oh better random guessing 50 accuracy lets try something better check results see treating financial time series prediction regression problem better approach learn trend prices close actual surprising mlps treating sequence data better cnns rnns supposed work better time series explain pretty small dataset 16k time stamps dummy hyperparameters choice reproduce results get better using code repository think get better results regression classification using different features scaled time series like technical indicators volume sales also try frequent data lets say minutebyminute ticks training data things im going later stay tuned quick cheer standing ovation clap show much enjoyed story ai entrepreneur blogger researcher making machines work learn like humans create discover love best machine learning computer vision deep learning natural language processing,en,"['IPython Notebook', 'Yahoo Finance', 'N', 'algorithm', 'MLP']"
127,Arthur Juliani,1700,Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond,"Welcome to the latest installment of my Reinforcement Learning series. In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:
It was these three innovations that allowed the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent. We will be walking through each individual improvement, and showing how to implement it. We won’t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task.
Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of the primate visual cortex. As such, they are ideal for the first few elements within our network.
In Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:
Here num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (“VALID”) or add padding around it (“SAME”) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the Tensorflow documentation.
The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.
The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.
Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.
With the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture described by DeepMind, have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.
The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.
In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:
The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.
Now that we have learned all the tricks to get the most out of our DQN, let’s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine (I am using a GTX970). In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!
The game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. The hyperparameters may need some tuning, but it is definitely possible. Good luck!
If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!
If you’d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjliani.
More from my Simple Reinforcement Learning with Tensorflow series:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Deep Learning @Unity3D & Cognitive Neuroscience PhD student.
",welcome latest installment reinforcement learning series tutorial walking creation deep qnetwork built upon simple one layer qnetwork created part 0 would recommend reading first new reinforcement learning ordinary qnetwork able barely perform well qtable simple game environment deep qnetworks much capable order transform ordinary qnetwork dqn making following improvements three innovations allowed google deepmind team achieve superhuman performance dozens atari games using dqn agent walking individual improvement showing implement wont stop though pace deep learning research extremely fast dqn 2014 longer advanced agent around anymore discuss two simple additional improvements dqn architecture double dqn dueling dqn allow improved performance stability faster training time end network tackle number challenging atari games demonstrate train dqn learn basic navigation task since agent going learning play video games able make sense games screen output way least similar humans intelligent animals able instead considering pixel independently convolutional layers allow us consider regions image maintain spatial relationships objects screen send information higher levels network way act similarly human receptive fields indeed body research showing convolutional neural network learn representations similar primate visual cortex ideal first elements within network tensorflow utilize tfcontriblayersconvolution2d function easily create convolutional layer write function follows num_outs refers many filters would like apply previous layer kernel_size refers large window would like slide previous layer stride refers many pixels want skip slide window across layer finally padding refers whether want window slide bottom layer valid add padding around order ensure convolutional layer dimensions previous layer information see tensorflow documentation second major addition make dqns work experience replay basic idea storing agents experiences randomly drawing batches train network robustly learn perform well task keeping experiences draw random prevent network learning immediately environment allow learn varied array past experiences experiences stored tuple stateactionrewardnext state experience replay buffer stores fixed number recent memories new ones come old ones removed time comes train simply draw uniform batch random memories buffer train network dqn build simple class handles storing retrieving memories third major addition dqn makes unique utilization second network training procedure second network used generate targetq values used compute loss every action training use use one network estimations issue every step training qnetworks values shift using constantly shifting set values adjust network values value estimations easily spiral control network become destabilized falling feedback loops target estimated qvalues order mitigate risk target networks weights fixed periodically slowly updated primary qnetworks values way training proceed stable manner instead updating target network periodically updating frequently slowly technique introduced another deepmind paper earlier year found stabilized training process additions everything need replicate dwn 2014 world moves fast number improvements beyond dqn architecture described deepmind allowed even greater performance stability training new dqn favorite atari game would suggest checking newer additions provide description code two double dqn dueling dqn simple implement combining techniques achieve better performance faster training times main intuition behind double dqn regular dqn often overestimates qvalues potential actions take given state would fine actions always overestimates equally reason believe wasnt case easily imagine certain suboptimal actions regularly given higher qvalues optimal actions agent would hard time ever learning ideal policy order correct authors ddqn paper propose simple trick instead taking max qvalues computing targetq value training step use primary network chose action target network generate target qvalue action decoupling action choice target qvalue generation able substantially reduce overestimation train faster reliably new ddqn equation updating target value order explain reasoning behind architecture changes dueling dqn makes need first explain additional reinforcement learning terms qvalues discussing far correspond good take certain action given certain state written qsa action given state actually decomposed two fundamental notions value first value function vs says simple good given state second advantage function aa tells much better taking certain action would compared others think q combination v formally goal dueling dqn network separately computes advantage value functions combines back single qfunction final layer may seem somewhat pointless first glance decompose function put back together key realizing benefit appreciate reinforcement learning agent may need care value advantage given time example imagine sitting outside park watching sunset beautiful highly rewarding sitting action needs taken doesnt really make sense think value sitting conditioned anything beyond environmental state achieve robust estimates state value decoupling necessity attached specific actions learned tricks get dqn lets actually try game environment dqn described could learn atari games enough training getting network perform well games takes least day training powerful machine educational purposes built simple game environment dqn learns master couple hours moderately powerful machine using gtx970 environment agent controls blue square goal navigate green squares reward 1 avoiding red squares reward 1 start episode squares randomly placed within 5x5 gridworld agent 50 steps achieve large reward possible randomly positioned agent needs simply learn fixed path case frozenlake environment tutorial 0 instead agent must learn notion spatial relationships blocks indeed able game environment outputs 84x84x3 color images uses function calls similar openai gym possible easy modify code work openai atari games encourage time computing resources necessary try getting agent perform well atari game hyperparameters may need tuning definitely possible good luck post valuable please consider donating help support future tutorials articles implementations contribution greatly appreciated youd like follow work deep learning ai cognitive science follow medium arthur juliani twitter awjliani simple reinforcement learning tensorflow series quick cheer standing ovation clap show much enjoyed story deep learning unity3d cognitive neuroscience phd student,en,"['Q-Network', 'DQN', 'Atari', 'Deep Learning', 'Experience Replay', 'DWN', 'DDQN', 'Q(s', 'Tutorial 0', '@awjliani', 'Deep Learning @Unity3D & Cognitive Neuroscience']"
128,Vishal Maini,8000,"Machine Learning for Humans, Part 2.1: Supervised Learning","How much money will we make by spending more dollars on digital advertising? Will this loan applicant pay back the loan or not? What’s going to happen to the stock market tomorrow?
In supervised learning problems, we start with a data set containing training examples with associated correct labels. For example, when learning to classify handwritten digits, a supervised learning algorithm takes thousands of pictures of handwritten digits along with labels containing the correct number each image represents. The algorithm will then learn the relationship between the images and their associated numbers, and apply that learned relationship to classify completely new images (without labels) that the machine hasn’t seen before. This is how you’re able to deposit a check by taking a picture with your phone!
To illustrate how supervised learning works, let’s examine the problem of predicting annual income based on the number of years of higher education someone has completed. Expressed more formally, we’d like to build a model that approximates the relationship f between the number of years of higher education X and corresponding annual income Y.
One method for predicting income would be to create a rigid rules-based model for how income and education are related. For example: “I’d estimate that for every additional year of higher education, annual income increases by $5,000.”
You could come up with a more complex model by including some rules about degree type, years of work experience, school tiers, etc. For example: “If they completed a Bachelor’s degree or higher, give the income estimate a 1.5x multiplier.”
But this kind of explicit rules-based programming doesn’t work well with complex data. Imagine trying to design an image classification algorithm made of if-then statements describing the combinations of pixel brightnesses that should be labeled “cat” or “not cat”.
Supervised machine learning solves this problem by getting the computer to do the work for you. By identifying patterns in the data, the machine is able to form heuristics. The primary difference between this and human learning is that machine learning runs on computer hardware and is best understood through the lens of computer science and statistics, whereas human pattern-matching happens in a biological brain (while accomplishing the same goals).
In supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running labeled training data through a learning algorithm. This learned function can be used to estimate the income of people whose income Y is unknown, as long as we have years of education X as inputs. In other words, we can apply our model to the unlabeled test data to estimate Y.
The goal of supervised learning is to predict Y as accurately as possible when given new examples where X is known and Y is unknown. In what follows we’ll explore several of the most common approaches to doing so.
The rest of this section will focus on regression. In Part 2.2 we’ll dive deeper into classification methods.
Regression predicts a continuous target variable Y. It allows you to estimate a value, such as housing prices or human lifespan, based on input data X.
Here, target variable means the unknown variable we care about predicting, and continuous means there aren’t gaps (discontinuities) in the value that Y can take on. A person’s weight and height are continuous values. Discrete variables, on the other hand, can only take on a finite number of values — for example, the number of kids somebody has is a discrete variable.
Predicting income is a classic regression problem. Your input data X includes all relevant information about individuals in the data set that can be used to predict income, such as years of education, years of work experience, job title, or zip code. These attributes are called features, which can be numerical (e.g. years of work experience) or categorical (e.g. job title or field of study).
You’ll want as many training observations as possible relating these features to the target output Y, so that your model can learn the relationship f between X and Y.
The data is split into a training data set and a test data set. The training set has labels, so your model can learn from these labeled examples. The test set does not have labels, i.e. you don’t yet know the value you’re trying to predict. It’s important that your model can generalize to situations it hasn’t encountered before so that it can perform well on the test data.
In our trivially simple 2D example, this could take the form of a .csv file where each row contains a person’s education level and income. Add more columns with more features and you’ll have a more complex, but possibly more accurate, model.
How do we build models that make accurate, useful predictions in the real world? We do so by using supervised learning algorithms.
Now let’s get to the fun part: getting to know the algorithms. We’ll explore some of the ways to approach regression and classification and illustrate key machine learning concepts throughout.
“Draw the line. Yes, this counts as machine learning.”
First, we’ll focus on solving the income prediction problem with linear regression, since linear models don’t work well with image recognition tasks (this is the domain of deep learning, which we’ll explore later).
We have our data set X, and corresponding target values Y. The goal of ordinary least squares (OLS) regression is to learn a linear model that we can use to predict a new y given a previously unseen x with as little error as possible. We want to guess how much income someone earns based on how many years of education they received.
Linear regression is a parametric method, which means it makes an assumption about the form of the function relating X and Y (we’ll cover examples of non-parametric methods later). Our model will be a function that predicts ŷ given a specific x:
β0 is the y-intercept and β1 is the slope of our line, i.e. how much income increases (or decreases) with one additional year of education.
Our goal is to learn the model parameters (in this case, β0 and β1) that minimize error in the model’s predictions.
To find the best parameters:
Graphically, in two dimensions, this results in a line of best fit. In three dimensions, we would draw a plane, and so on with higher-dimensional hyperplanes.
Mathematically, we look at the difference between each real data point (y) and our model’s prediction (ŷ). Square these differences to avoid negative numbers and penalize larger differences, and then add them up and take the average. This is a measure of how well our data fits the line.
For a simple problem like this, we can compute a closed form solution using calculus to find the optimal beta parameters that minimize our loss function. But as a cost function grows in complexity, finding a closed form solution with calculus is no longer feasible. This is the motivation for an iterative approach called gradient descent, which allows us to minimize a complex loss function.
“Put on a blindfold, take a step downhill. You’ve found the bottom when you have nowhere to go but up.”
Gradient descent will come up over and over again, especially in neural networks. Machine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it’s worth understanding the details.
The goal of gradient descent is to find the minimum of our model’s loss function by iteratively getting a better and better approximation of it.
Imagine yourself walking through a valley with a blindfold on. Your goal is to find the bottom of the valley. How would you do it?
A reasonable approach would be to touch the ground around you and move in whichever direction the ground is sloping down most steeply. Take a step and repeat the same process continually until the ground is flat. Then you know you’ve reached the bottom of a valley; if you move in any direction from where you are, you’ll end up at the same elevation or further uphill.
Going back to mathematics, the ground becomes our loss function, and the elevation at the bottom of the valley is the minimum of that function.
Let’s take a look at the loss function we saw in regression:
We see that this is really a function of two variables: β0 and β1. All the rest of the variables are determined, since X, Y, and n are given during training. We want to try to minimize this function.
The function is f(β0,β1)=z. To begin gradient descent, you make some guess of the parameters β0 and β1 that minimize the function.
Next, you find the partial derivatives of the loss function with respect to each beta parameter: [dz/dβ0, dz/dβ1]. A partial derivative indicates how much total loss is increased or decreased if you increase β0 or β1 by a very small amount.
Put another way, how much would increasing your estimate of annual income assuming zero higher education (β0) increase the loss (i.e. inaccuracy) of your model? You want to go in the opposite direction so that you end up walking downhill and minimizing loss.
Similarly, if you increase your estimate of how much each incremental year of education affects income (β1), how much does this increase loss (z)? If the partial derivative dz/β1 is a negative number, then increasing β1 is good because it will reduce total loss. If it’s a positive number, you want to decrease β1. If it’s zero, don’t change β1 because it means you’ve reached an optimum.
Keep doing that until you reach the bottom, i.e. the algorithm converged and loss has been minimized. There are lots of tricks and exceptional cases beyond the scope of this series, but generally, this is how you find the optimal parameters for your parametric model.
Overfitting: “Sherlock, your explanation of what just happened is too specific to the situation.” Regularization: “Don’t overcomplicate things, Sherlock. I’ll punch you for every extra word.” Hyperparameter (λ): “Here’s the strength with which I will punch you for every extra word.”
A common problem in machine learning is overfitting: learning a function that perfectly explains the training data that the model learned from, but doesn’t generalize well to unseen test data. Overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren’t representative of patterns in the real world. This becomes especially problematic as you make your model increasingly complex. Underfitting is a related issue where your model is not complex enough to capture the underlying trend in the data.
Remember that the only thing we care about is how the model performs on test data. You want to predict which emails will be marked as spam before they’re marked, not just build a model that is 100% accurate at reclassifying the emails it used to build itself in the first place. Hindsight is 20/20 — the real question is whether the lessons learned will help in the future.
The model on the right has zero loss for the training data because it perfectly fits every data point. But the lesson doesn’t generalize. It would do a horrible job at explaining a new data point that isn’t yet on the line.
Two ways to combat overfitting:
1. Use more training data. The more you have, the harder it is to overfit the data by learning too much from any single training example.
2. Use regularization. Add in a penalty in the loss function for building a model that assigns too much explanatory power to any one feature or allows too many features to be taken into account.
The first piece of the sum above is our normal cost function. The second piece is a regularization term that adds a penalty for large beta coefficients that give too much explanatory power to any specific feature. With these two elements in place, the cost function now balances between two priorities: explaining the training data and preventing that explanation from becoming overly specific.
The lambda coefficient of the regularization term in the cost function is a hyperparameter: a general setting of your model that can be increased or decreased (i.e. tuned) in order to improve performance. A higher lambda value will more harshly penalize large beta coefficients that could lead to potential overfitting. To decide the best value of lambda, you’d use a method called cross-validation which involves holding out a portion of the training data during training, and then seeing how well your model explains the held-out portion. We’ll go over this in more depth
Here’s what we covered in this section:
In the next section — Part 2.2: Supervised Learning II — we’ll talk about two foundational methods of classification: logistic regression and support vector machines.
For a more thorough treatment of linear regression, read chapters 1–3 of An Introduction to Statistical Learning. The book is available for free online and is an excellent resource for understanding machine learning concepts with accompanying exercises.
For more practice:
To actually implement gradient descent in Python, check out this tutorial. And here is a more mathematically rigorous description of the same concepts.
In practice, you’ll rarely need to implement gradient descent from scratch, but understanding how it works behind the scenes will allow you to use it more effectively and understand why things break when they do.
More from Machine Learning for Humans 🤖👶
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Research comms @DeepMindAI. Previously @Upstart, @Yale, @TrueVenturesTEC.
Demystifying artificial intelligence & machine learning. Discussions on safe and intentional application of AI for positive social impact.
",much money make spending dollars digital advertising loan applicant pay back loan whats going happen stock market tomorrow supervised learning problems start data set containing training examples associated correct labels example learning classify handwritten digits supervised learning algorithm takes thousands pictures handwritten digits along labels containing correct number image represents algorithm learn relationship images associated numbers apply learned relationship classify completely new images without labels machine hasnt seen youre able deposit check taking picture phone illustrate supervised learning works lets examine problem predicting annual income based number years higher education someone completed expressed formally wed like build model approximates relationship f number years higher education x corresponding annual income one method predicting income would create rigid rulesbased model income education related example id estimate every additional year higher education annual income increases 5000 could come complex model including rules degree type years work experience school tiers etc example completed bachelors degree higher give income estimate 15x multiplier kind explicit rulesbased programming doesnt work well complex data imagine trying design image classification algorithm made ifthen statements describing combinations pixel brightnesses labeled cat cat supervised machine learning solves problem getting computer work identifying patterns data machine able form heuristics primary difference human learning machine learning runs computer hardware best understood lens computer science statistics whereas human patternmatching happens biological brain accomplishing goals supervised learning machine attempts learn relationship income education scratch running labeled training data learning algorithm learned function used estimate income people whose income unknown long years education x inputs words apply model unlabeled test data estimate goal supervised learning predict accurately possible given new examples x known unknown follows well explore several common approaches rest section focus regression part 22 well dive deeper classification methods regression predicts continuous target variable allows estimate value housing prices human lifespan based input data x target variable means unknown variable care predicting continuous means arent gaps discontinuities value take persons weight height continuous values discrete variables hand take finite number values example number kids somebody discrete variable predicting income classic regression problem input data x includes relevant information individuals data set used predict income years education years work experience job title zip code attributes called features numerical eg years work experience categorical eg job title field study youll want many training observations possible relating features target output model learn relationship f x data split training data set test data set training set labels model learn labeled examples test set labels ie dont yet know value youre trying predict important model generalize situations hasnt encountered perform well test data trivially simple 2d example could take form csv file row contains persons education level income add columns features youll complex possibly accurate model build models make accurate useful predictions real world using supervised learning algorithms lets get fun part getting know algorithms well explore ways approach regression classification illustrate key machine learning concepts throughout draw line yes counts machine learning first well focus solving income prediction problem linear regression since linear models dont work well image recognition tasks domain deep learning well explore later data set x corresponding target values goal ordinary least squares ols regression learn linear model use predict new given previously unseen x little error possible want guess much income someone earns based many years education received linear regression parametric method means makes assumption form function relating x well cover examples nonparametric methods later model function predicts given specific x 0 yintercept 1 slope line ie much income increases decreases one additional year education goal learn model parameters case 0 1 minimize error models predictions find best parameters graphically two dimensions results line best fit three dimensions would draw plane higherdimensional hyperplanes mathematically look difference real data point models prediction square differences avoid negative numbers penalize larger differences add take average measure well data fits line simple problem like compute closed form solution using calculus find optimal beta parameters minimize loss function cost function grows complexity finding closed form solution calculus longer feasible motivation iterative approach called gradient descent allows us minimize complex loss function put blindfold take step downhill youve found bottom nowhere go gradient descent come especially neural networks machine learning libraries like scikitlearn tensorflow use background everywhere worth understanding details goal gradient descent find minimum models loss function iteratively getting better better approximation imagine walking valley blindfold goal find bottom valley would reasonable approach would touch ground around move whichever direction ground sloping steeply take step repeat process continually ground flat know youve reached bottom valley move direction youll end elevation uphill going back mathematics ground becomes loss function elevation bottom valley minimum function lets take look loss function saw regression see really function two variables 0 1 rest variables determined since x n given training want try minimize function function f01z begin gradient descent make guess parameters 0 1 minimize function next find partial derivatives loss function respect beta parameter dzd0 dzd1 partial derivative indicates much total loss increased decreased increase 0 1 small amount put another way much would increasing estimate annual income assuming zero higher education 0 increase loss ie inaccuracy model want go opposite direction end walking downhill minimizing loss similarly increase estimate much incremental year education affects income 1 much increase loss z partial derivative dz1 negative number increasing 1 good reduce total loss positive number want decrease 1 zero dont change 1 means youve reached optimum keep reach bottom ie algorithm converged loss minimized lots tricks exceptional cases beyond scope series generally find optimal parameters parametric model overfitting sherlock explanation happened specific situation regularization dont overcomplicate things sherlock ill punch every extra word hyperparameter heres strength punch every extra word common problem machine learning overfitting learning function perfectly explains training data model learned doesnt generalize well unseen test data overfitting happens model overlearns training data point starts picking idiosyncrasies arent representative patterns real world becomes especially problematic make model increasingly complex underfitting related issue model complex enough capture underlying trend data remember thing care model performs test data want predict emails marked spam theyre marked build model 100 accurate reclassifying emails used build first place hindsight 2020 real question whether lessons learned help future model right zero loss training data perfectly fits every data point lesson doesnt generalize would horrible job explaining new data point isnt yet line two ways combat overfitting 1 use training data harder overfit data learning much single training example 2 use regularization add penalty loss function building model assigns much explanatory power one feature allows many features taken account first piece sum normal cost function second piece regularization term adds penalty large beta coefficients give much explanatory power specific feature two elements place cost function balances two priorities explaining training data preventing explanation becoming overly specific lambda coefficient regularization term cost function hyperparameter general setting model increased decreased ie tuned order improve performance higher lambda value harshly penalize large beta coefficients could lead potential overfitting decide best value lambda youd use method called crossvalidation involves holding portion training data training seeing well model explains heldout portion well go depth heres covered section next section part 22 supervised learning ii well talk two foundational methods classification logistic regression support vector machines thorough treatment linear regression read chapters 13 introduction statistical learning book available free online excellent resource understanding machine learning concepts accompanying exercises practice actually implement gradient descent python check tutorial mathematically rigorous description concepts practice youll rarely need implement gradient descent scratch understanding works behind scenes allow use effectively understand things break machine learning humans quick cheer standing ovation clap show much enjoyed story research comms deepmindai previously upstart yale trueventurestec demystifying artificial intelligence machine learning discussions safe intentional application ai positive social impact,en,"['algorithm', 'Bachelor', 'Linear', 'ŷ', 'dz/dβ0', 'Sherlock', '@DeepMindAI', '@TrueVenturesTEC', 'Demystifying artificial intelligence &']"
129,Arvind N,9500,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,"[Update — Feb 2nd 2018: When this blog post was written, only 3 courses had been released. All 5 courses in this specialization are now out. I will have a follow-up blog post soon.]
Between a full time job and a toddler at home, I spend my spare time learning about the ideas in cognitive science & AI. Once in a while a great paper/video/course comes out and you’re instantly hooked.
Andrew Ng’s new deeplearning.ai course is like that Shane Carruth or Rajnikanth movie that one yearns for!
Naturally, as soon as the course was released on coursera, I registered and spent the past 4 evenings binge watching the lectures, working through quizzes and programming assignments.
DL practitioners and ML engineers typically spend most days working at an abstract Keras or TensorFlow level. But it’s nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back-propagation by hand. It is both fun and incredibly useful!
Andrew Ng’s new adventure is a bottom-up approach to teaching neural networks — powerful non-linearity learning algorithms, at a beginner-mid level.
In classic Ng style, the course is delivered through a carefully chosen curriculum, neatly timed videos and precisely positioned information nuggets. Andrew picks up from where his classic ML course left off and introduces the idea of neural networks using a single neuron(logistic regression) and slowly adding complexity — more neurons and layers. By the end of the 4 weeks(course 1), a student is introduced to all the core ideas required to build a dense neural network such as cost/loss functions, learning iteratively using gradient descent and vectorized parallel python(numpy) implementations.
Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding.
Lectures are delivered using presentation slides on which Andrew writes using digital pens. It felt like an effective way to get the listener to focus. I felt comfortable watching videos at 1.25x or 1.5x speed.
Quizzes are placed at the end of each lecture sections and are in the multiple choice question format. If you watch the videos once, you should be able to quickly answer all the quiz questions. You can attempt quizzes multiple times and the system is designed to keep your highest score.
Programming assignments are done via Jupyter notebooks — powerful browser based applications.
Assignments have a nice guided sequential structure and you are not required to write more than 2–3 lines of code in each section. If you understand the concepts like vectorization intuitively, you can complete most programming sections with just 1 line of code!
After the assignment is coded, it takes 1 button click to submit your code to the automated grading system which returns your score in a few minutes. Some assignments have time restrictions — say, three attempts in 8 hours etc.
Jupyter notebooks are well designed and work without any issues. Instructions are precise and it feels like a polished product.
Anyone interested in understanding what neural networks are, how they work, how to build them and the tools available to bring your ideas to life.
If your math is rusty, there is no need to worry — Andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code.
If your programming is rusty, there is a nice coding assignment to teach you numpy. But I recommend learning python first on codecademy.
Let me explain this with an analogy: Assume you are trying to learn how to drive a car.
Jeremy’s FAST.AI course puts you in the drivers seat from the get-go. He teaches you to move the steering wheel, press the brake, accelerator etc. Then he slowly explains more details about how the car works — why rotating the wheel makes the car turn, why pressing the brake pedal makes you slow down and stop etc. He keeps getting deeper into the inner workings of the car and by the end of the course, you know how the internal combustion engine works, how the fuel tank is designed etc. The goal of the course is to get you driving. You can choose to stop at any point after you can drive reasonably well — there is no need to learn how to build/repair the car.
Andrew’s DL course does all of this, but in the complete opposite order. He teaches you about internal combustion engine first! He keeps adding layers of abstraction and by the end of the course you are driving like an F1 racer!
The fast AI course mainly teaches you the art of driving while Andrew’s course primarily teaches you the engineering behind the car.
If you have not done any machine learning before this, don’t take this course first. The best starting point is Andrew’s original ML course on coursera.
After you complete that course, please try to complete part-1 of Jeremy Howard’s excellent deep learning course. Jeremy teaches deep learning Top-Down which is essential for absolute beginners.
Once you are comfortable creating deep neural networks, it makes sense to take this new deeplearning.ai course specialization which fills up any gaps in your understanding of the underlying details and concepts.
2. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money — the third course in the DL specialization felt incredibly useful for my role as an architect leading engineering teams.
3. Jargon is handled well. Andrew explains that an empirical process = trial & error — He is brutally honest about the reality of designing and training deep nets. At some point I felt he might have as well just called Deep Learning as glorified curve-fitting
4. Squashes all hype around DL and AI — Andrew makes restrained, careful comments about proliferation of AI hype in the mainstream media and by the end of the course it is pretty clear that DL is nothing like the terminator.
5.Wonderful boilerplate code that just works out of the box!
6. Excellent course structure.
7. Nice, consistent and useful notation. Andrew strives to establish a fresh nomenclature for neural nets and I feel he could be quite successful in this endeavor.
8. Style of teaching that is unique to Andrew and carries over from ML — I could feel the same excitement I felt in 2013 when I took his original ML course.
9.The interviews with deep learning heroes are refreshing — It is motivating and fun to hear personal stories and anecdotes.
I wish that he’d said ‘concretely’ more often!
2. Good tools are important and will help you accelerate your learning pace. I bought a digital pen after seeing Andrew teach with one. It helped me work more efficiently.
3. There is a psychological reason why I recommend the Fast.ai course before this one. Once you find your passion, you can learn uninhibited.
4. You just get that dopamine rush each time you score full points:
5. Don’t be scared by DL jargon (hyperparameters = settings, architecture/topology=style etc.) or the math symbols. If you take a leap of faith and pay attention to the lectures, Andrew shows why the symbols and notation are actually quite useful. They will soon become your tools of choice and you will wield them with style!
Thanks for reading and best wishes!
Update: Thanks for the overwhelmingly positive response! Many people are asking me to explain gradient descent and the differential calculus. I hope this helps!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in Strong AI
Sharing concepts, ideas, and codes.
",update feb 2nd 2018 blog post written 3 courses released 5 courses specialization followup blog post soon full time job toddler home spend spare time learning ideas cognitive science ai great papervideocourse comes youre instantly hooked andrew ngs new deeplearningai course like shane carruth rajnikanth movie one yearns naturally soon course released coursera registered spent past 4 evenings binge watching lectures working quizzes programming assignments dl practitioners ml engineers typically spend days working abstract keras tensorflow level nice take break get nuts bolts learning algorithms actually backpropagation hand fun incredibly useful andrew ngs new adventure bottomup approach teaching neural networks powerful nonlinearity learning algorithms beginnermid level classic ng style course delivered carefully chosen curriculum neatly timed videos precisely positioned information nuggets andrew picks classic ml course left introduces idea neural networks using single neuronlogistic regression slowly adding complexity neurons layers end 4 weekscourse 1 student introduced core ideas required build dense neural network costloss functions learning iteratively using gradient descent vectorized parallel pythonnumpy implementations andrew patiently explains requisite math programming concepts carefully planned order well regulated pace suitable learners could rusty mathcoding lectures delivered using presentation slides andrew writes using digital pens felt like effective way get listener focus felt comfortable watching videos 125x 15x speed quizzes placed end lecture sections multiple choice question format watch videos able quickly answer quiz questions attempt quizzes multiple times system designed keep highest score programming assignments done via jupyter notebooks powerful browser based applications assignments nice guided sequential structure required write 23 lines code section understand concepts like vectorization intuitively complete programming sections 1 line code assignment coded takes 1 button click submit code automated grading system returns score minutes assignments time restrictions say three attempts 8 hours etc jupyter notebooks well designed work without issues instructions precise feels like polished product anyone interested understanding neural networks work build tools available bring ideas life math rusty need worry andrew explains required calculus provides derivatives every occasion focus building network concentrate implementing ideas code programming rusty nice coding assignment teach numpy recommend learning python first codecademy let explain analogy assume trying learn drive car jeremys fastai course puts drivers seat getgo teaches move steering wheel press brake accelerator etc slowly explains details car works rotating wheel makes car turn pressing brake pedal makes slow stop etc keeps getting deeper inner workings car end course know internal combustion engine works fuel tank designed etc goal course get driving choose stop point drive reasonably well need learn buildrepair car andrews dl course complete opposite order teaches internal combustion engine first keeps adding layers abstraction end course driving like f1 racer fast ai course mainly teaches art driving andrews course primarily teaches engineering behind car done machine learning dont take course first best starting point andrews original ml course coursera complete course please try complete part1 jeremy howards excellent deep learning course jeremy teaches deep learning topdown essential absolute beginners comfortable creating deep neural networks makes sense take new deeplearningai course specialization fills gaps understanding underlying details concepts 2 andrew stresses engineering aspects deep learning provides plenty practical tips save time money third course dl specialization felt incredibly useful role architect leading engineering teams 3 jargon handled well andrew explains empirical process trial error brutally honest reality designing training deep nets point felt might well called deep learning glorified curvefitting 4 squashes hype around dl ai andrew makes restrained careful comments proliferation ai hype mainstream media end course pretty clear dl nothing like terminator 5wonderful boilerplate code works box 6 excellent course structure 7 nice consistent useful notation andrew strives establish fresh nomenclature neural nets feel could quite successful endeavor 8 style teaching unique andrew carries ml could feel excitement felt 2013 took original ml course 9the interviews deep learning heroes refreshing motivating fun hear personal stories anecdotes wish hed said concretely often 2 good tools important help accelerate learning pace bought digital pen seeing andrew teach one helped work efficiently 3 psychological reason recommend fastai course one find passion learn uninhibited 4 get dopamine rush time score full points 5 dont scared dl jargon hyperparameters settings architecturetopologystyle etc math symbols take leap faith pay attention lectures andrew shows symbols notation actually quite useful soon become tools choice wield style thanks reading best wishes update thanks overwhelmingly positive response many people asking explain gradient descent differential calculus hope helps quick cheer standing ovation clap show much enjoyed story interested strong ai sharing concepts ideas codes,en,"['quizzes', 'TensorFlow', 'Jupyter', 'Jargon', 'trial & error', 'Fast.ai']"
130,Gary Marcus,1300,In defense of skepticism about deep learning – Gary Marcus – Medium,"In a recent appraisal of deep learning (Marcus, 2018) I outlined ten challenges for deep learning, and suggested that deep learning by itself, although useful, was unlikely to lead on its own to artificial general intelligence. I suggested instead the deep learning be viewed “not as a universal solvent, but simply as one tool among many.”
In place of pure deep learning, I called for hybrid models, that would incorporate not just supervised forms of deep learning, but also other techniques as well, such as symbol-manipulation, and unsupervised learning (itself possibly reconceptualized). I also urged the community to consider incorporating more innate structure into AI systems.
Within a few days, thousands of people had weighed in over Twitter, some enthusiastic (“e.g, the best discussion of #DeepLearning and #AI I’ve read in many years”), some not (“Thoughtful... But mostly wrong nevertheless”).
Because I think clarity around these issues is so important, I’ve compiled a list of fourteen commonly-asked queries. Where does unsupervised learning fit in? Why didn’t I say more nice things about deep learning? What gives me the right to talk about this stuff in the first place? What’s up with asking a neural network to generalize from even numbers to odd numbers? (Hint: that’s the most important one). And lots more. I haven’t addressed literally every question I have seen, but I have tried to be representative.
1. What is general intelligence?
Thomas Dietterich, an eminent professor of machine learning, and my most thorough and explicit critic thus far, gave a nice answer that I am very comfortable with:
2. Marcus wasn’t very nice to deep learning. He should have said more nice things about all of its vast accomplishments. And he minimizes others.
Dietterich, mentioned above, made both of these points, writing:
On the first part of that, true, I could have said more positive things. But it’s not like I didn’t say any. Or even like I forgot to mention Dietterich’s best example; I mentioned it on the first page:
More generally, later in the article I cited a couple of great texts and excellent blogs that have pointers to numerous examples. A lot of them though, would not really count as AGI, which was the main focus of my paper. (Google Translate, for example, is extremely impressive, but it’s not general; it can’t, for example, answer questions about what it has translated, the way a human translator could.)
The second part is more substantive. Is 1,000 categories really very finite? Well, yes, compared to the flexibility of cognition. Cognitive scientists generally place the number of atomic concepts known by an individual as being on the order of 50,000, and we can easily compose those into a vastly greater number of complex thoughts. Pets and fish are probably counted in those 50,000; pet fish, which is something different, probably isn’t counted. And I can easily entertain the concept of “a pet fish that is suffering from Ick”, or note that “it is always disappointing to buy a pet fish only to discover that it was infected with Ick” (an experience that I had as a child and evidently still resent). How many ideas like that I can express? It’s a lot more than 1,000.
I am not precisely sure how many visual categories a person can recognize, but suspect the math is roughly similar. Try google images on “pet fish”, and you do ok; try it on “pet fish wearing goggles” and you mostly find dogs wearing goggles, with a false alarm rate of over 80%.
Machines win over nonexpert humans on distinguishing similar dog breeds, but people win, by a wide margin, on interpreting complex scenes, like what would happen to a skydiver who was wearing a backpack rather than a parachute.
In focusing on 1,000 category chunks the machine learning field is, in my view, doing itself a disservice, trading a short-term feeling of success for a denial of harder, more open-ended problems (like scene and sentence comprehension) that must eventually be addressed. Compared to the essentially infinite range of sentences and scenes we can see and comprehend, 1000 of anything really is small. [See also Note 2 at bottom]
3. Marcus says deep learning is useless, but it’s great for many things
Of course it is useful; I never said otherwise, only that (a) in its current supervised form, deep learning might be approaching its limits and (b) that those limits would stop short from full artificial general intelligence — unless, maybe, we started incorporating a bunch of other stuff like symbol-manipulation and innateness.
The core of my conclusion was this:
4. “One thing that I don’t understand. — @GaryMarcus says that DL is not good for hierarchical structures. But in @ylecun nature review paper [says that] that DL is particularly suited for exploiting such hierarchies.”
This is an astute question, from Ram Shankar, and I should have been a LOT clearer about the answer: there are many different types of hierarchy one could think about. Deep learning is really good, probably the best ever, at the sort of feature-wise hierarchy LeCun talked about, which I typically refer to as hierarchical feature detection; you build lines out of pixels, letters out of lines, words out of letters and so forth. Kurzweil and Hawkins have emphasized this sort of thing, too, and it really goes back to Hubel and Wiesel (1959)in neuroscience experiments and to Fukushima. (Fukushima, Miyake, & Ito, 1983) in AI. Fukushima, in his Neocognitron model, hand-wired his hierarchy of successively more abstract features; LeCun and many others after showed that (at least in some cases) you don’t have to hand engineer them.
But you don’t have to keep track of the subcomponents you encounter along the way; the top-level system need not explicitly encode the structure of the overall output in terms of which parts were seen along the way; this is part of why a deep learning system can be fooled into thinking a pattern of a black and yellow stripes is a school bus. (Nguyen, Yosinski, & Clune, 2014). That stripe pattern is strongly correlated with activation of the school bus output units, which is in turn correlated with a bunch of lower-level features, but in a typical image-recognition deep network, there is no fully-realized representation of a school bus as being made up of wheels, a chassis, windows, etc. Virtually the whole spoofing literature can be thought of in these terms. [Note 3]
The structural sense of hierarchy which I was discussing was different, and focused around systems that can make explicit reference to the parts of larger wholes. The classic illustration would be Chomsky’s sense of hierarchy, in which a sentence is composed of increasingly complex grammatical units (e.g., using a novel phrase like the man who mistook his hamburger for a hot dog with a larger sentence like The actress insisted that she would not be outdone by the man who mistook his hamburger for a hot dog). I don’t think deep learning does well here (e.g., in discerning the relation between the actress, the man, and the misidentified hot dog), though attempts have certainly been made.
Even in vision, the problem is not entirely licked; Hinton’s recent capsule work (Sabour, Frosst, & Hinton, 2017), for example, is an attempt to build in more robust part-whole directions for image recognition, by using more structured networks. I see this as a good trend, and one potential way to begin to address the spoofing problem, but also as a reflection of trouble with the standard deep learning approach.
5. “It’s weird to discuss deep learning in [the] context of general AI. General AI is not the goal of deep learning!”
Best twitter response to this came from University of Quebec professor Daniel Lemire: “Oh! Come on! Hinton, Bengio... are openly going for a model of human intelligence.”
Second prize goes to a math PhD at Google, Jeremy Kun, who countered the dubious claim that “General AI is not the goal of deep learning” with “If that’s true, then deep learning experts sure let everyone believe it is without correcting them.”
Andrew Ng’s recent Harvard Business Review article, which I cited, implies that deep learning can do anything a person can do in a second. Thomas Dietterich’s tweet that said in part “it is hard to argue that there are limits to DL”. Jeremy Howard worried that the idea that deep learning is overhyped might itself be overhyped, and then suggested that every known limit had been countered.
DeepMind’s recent AlphaGo paper [See Note 4] is positioned somewhat similarly, with Silver et al (Silver et al., 2017) enthusiastically reporting that:
In that paper’s concluding discussion, not one of the 10 challenges to deep learning that I reviewed was mentioned. (As I will discuss in a paper coming out soon, it’s not actually a pure deep learning system, but that’s a story for another day.)
The main reason people keep benchmarking their AI systems against humans is precisely because AGI is the goal.
6. What Marcus said is a problem with supervised learning, not deep learning.
Yann LeCun presented a version of this, in a comment on my Facebook page:
The part about my allegedly not recognizing LeCun’s recent work is, well, odd. It’s true that I couldn’t find a good summary article to cite (when I asked LeCun, he told me by email that there wasn’t one yet) but I did mention his interest explicitly:
I also noted that:
My conclusion was positive, too. Although I expressed reservations about current approaches to building unsupervised systems, I ended optimistically:
What LeCun’s remark does get right is that many of the problems I addressed are a general problem with supervised learning, not something unique to deep learning; I could have been more clear about this. Many other supervised learning techniques face similar challenges, such as problems in generalization and dependence on massive data sets; relatively little of what I said is unique to deep learning. In my focus on assessing deep learning at the five year resurgence mark, I neglected to say that.
But it doesn’t really help deep learning that other supervised learning techniques are in the same boat. If someone could come up with a truly impressive way of using deep learning in an unsupervised way, a reassessment might be required. But I don’t see that unsupervised learning, at least as it currently pursued, particularly remedies the challenges I raised, e.g., with respect to reasoning, hierarchical representations, transfer, robustness, and interpretability. It’s simply a promissory note. [Note 5]
As Portland State and Santa Fe Institute Professor Melanie Mitchell’s put it in a thus far unanswered tweet:
I would, too.
In the meantime, I see no principled reason to believe that unsupervised learning can solve the problems I raise, unless we add in more abstract, symbolic representations, first.
7. Deep learning is not just convolutional networks [of the sort Marcus critiqued], it’s “essentially a new style of programming — ”differentiable programming” — and the field is trying to work out the reusable constructs in this style. We have some: convolution, pooling, LSTM, GAN, VAE, memory units, routing units, etc” — Tom Dietterich
This seemed (in the context of Dietterich’s longer series of tweets) to have been proposed as a criticism, but I am puzzled by that, as I am a fan of differentiable programming and said so. Perhaps the point was that deep learning can be taken in a broader way.
In any event, I would not equate deep learning and differentiable programming (e.g., approaches that I cited like neural Turing machines and neural programming). Deep learning is a component of many differentiable systems. But such systems also build in exactly the sort of elements drawn from symbol-manipulation that I am and have been urging the field to integrate (Marcus, 2001; Marcus, Marblestone, & Dean, 2014a; Marcus, Marblestone, & Dean, 2014b), including memory units and operations over variables, and other systems like routing units stressed in the more recent two essays. If integrating all this stuff into deep learning is what gets us to AGI, my conclusion, quoted below, will have turned out to be dead on:
8. Now vs the future. Maybe deep learning doesn’t work now, but it’s offspring will get us to AGI.
Possibly. I do think that deep learning might play an important role in getting us to AGI, if some key things (many not yet discovered) are added in first.
But what we add matters, and whether it is reasonable to call some future system an instance of deep learning per se, or more sensible to call the ultimate system “a such-and-such that uses deep learning”, depends on where deep learning fits into the ultimate solution. Maybe, for example, in truly adequate natural language understanding systems, symbol-manipulation will play an equally large role as deep learning, or an even larger one.
Part of the issue here is of course terminological. A very good friend recently asked me, why can’t we just call anything that includes deep learning, deep learning, even if it includes symbol-manipulation? Some enhancement to deep learning ought to work. To which I respond: why not call anything that includes symbol-manipulation, symbol-manipulation, even if it includes deep learning?
Gradient-based optimization should get its due, but so should symbol-manipulation, which as yet is the only known tool for systematically representing and achieving high-level abstraction, bedrock to virtually all of the world’s complex computer systems, from spreadsheets to programming environments to operating systems.
Eventually, I conjecture, credit will also be due to the inevitable marriage between the two, hybrid systems that bring together the two great ideas of 20th century AI, symbol-processing and neural networks, both initially developed in the 1950s. Other new tools yet to be invented may be critical as well.
To a true acolyte of deep learning, anything is deep learning, no matter what it’s incorporating, and no matter how different it might be from current techniques. (Viva Imperialism!) If you replaced every transistor in a classic symbolic microprocessor with a neuron, but kept the chip’s logic entirely unchanged, a true deep learning acolyte would still declare victory. But we won’t understand the principles driving (eventual) success if we lump everything together. [Note 6]
9. No machine can extrapolate. It’s not fair to expect a neural network to generalize from even numbers to odd numbers.
Here’s a function, expressed over binary digits.
f(110) = 011;
f(100) = 001;
f(010) = 010.
What’s f(111)?
If you are an ordinary human, you are probably going to guess 111. If you are neural network of the sort I discussed, you probably won’t.
If you have been told many times that hidden layers in neural networks “abstract functions”, you should be a little bit surprised by this.
If you are a human, you might think of the function as something like “reversal”, easily expressed in a line of computer code. If you are a neural network of a certain sort, it’s very hard to learn the abstraction of reversal in a way that extends from evens in that context to odds. But is that impossible? Certainly not if you have a prior notion of an integer. Try another, this time in decimal: f(4) = 8; f(6) = 12. What’s f(5)? None of my human readers would care that questions happens to require you to extrapolate from even numbers to odds; a lot of neural networks would be flummoxed.
Sure, the function is undetermined by the sparse number of examples, like all functions, but it is interesting and important that most people would (amid the infinite range of a priori possible inductions), would alight on f(5)=10.
And just as interesting that most standard multilayer perceptrons, representing the numbers as binary digits, wouldn’t. That’s telling us something, but many people in the neural network community, François Chollet being one very salient exception, don’t want to listen.
Importantly, recognizing that a rule applies to any integer is roughly the same kind of generalization that allows one to recognize that a novel noun that can be used in one context can be used in a huge variety of other contexts. From the first time I hear the word blicket used as an object, I can guess that it will fit into a wide range of frames, like I thought I saw a blicket, I had a close encounter with a blicket, and exceptionally large blickets frighten me, etc. And I can both generate and interpret such sentences, without specific further training. It doesn’t matter whether blicket is or not similar in (for example) phonology to other words I have heard, nor whether I pile on the adjectives or use the word as a subject or an object. If most machine learning [ML] paradigms have a problem with this, we should have problem with most ML paradigms.
Am I being “fair”? Well, yes, and no. It’s true that I am asking neural networks to do something that violates their assumptions.
A neural network advocate might, for example, say, “hey wait a minute, in your reversal example, there are three dimensions in your input space, representing the left binary digit, the middle binary digit, and rightmost binary digit. The rightmost binary digit has only been a zero in the training; there is no way a network can know what to do when you get to one in that position.” For example, Vincent Lostenlan, a postdoc at Cornell, said
Dietterich, made essentially the same point, more concisely:
But although both are right about why odds-and-evens are (in this context) hard for deep learning, they are both wrong about the larger issues for three reasons.
First, it can’t be that people can’t extrapolate. You just did, in two different examples, at the top of this section. Paraphrasing Chico Marx. who are you going to believe, me or your own eyes?
To someone immersed deeply — perhaps too deeply — in contemporary machine learning, my odds-and-evens problem seems unfair because a certain dimension (the one which contains the value of 1 in the rightmost digit) hasn’t been illustrated in the training regime. But when you, a human, look at my examples above, you will not be stymied by this particular gap in the training data. You won’t even notice it, because your attention is on higher-level regularities.
People routinely extrapolate in exactly the fashion that I have been describing, like recognizing string reversal from the three training examples I gave above. In a technical sense, that is extrapolation, and you just did it. In The Algebraic Mind I referred to this specific kind of extrapolation as generalizing universally quantified one-to-one mappings outside of a space of training examples. As a field we desperately need a solution to this challenge, if we are ever to catch up to human learning — even if it means shaking up our assumptions.
Now, it might reasonably be objected that it’s not a fair fight: humans manifestly depend on prior knowledge when they generalize such mappings. (In some sense, Dieterrich proposed this objection later in his tweet stream.)
True enough. But in a way, that’s the point: neural networks of a certain sort don’t have a good way of incorporating the right sort of prior knowledge in the place. It is precisely because those networks don’t have a way of incorporating prior knowledge like “many generalizations hold for all elements of unbounded classes” or “odd numbers leave a remainder of one when divided by two” that neural networks that lack operations over variables fail. The right sort of prior knowledge that would allow neural networks to acquire and represent universally quantified one-to-one mappings. Standard neural networks can’t represent such mappings, except in certain limited ways. (Convolution is a way of building in one particular such mapping, prior to learning).
Second, saying that no current system (deep learning or otherwise) can extrapolate in the way that I have described is no excuse; once again other architectures may be in the choppy water, but that doesn’t mean we shouldn’t be trying to swim to shore. If we want to get to AGI, we have to solve the problem.
(Put differently: yes, one could certainly hack together solutions to get deep learning to solve my specific number series problems, by, for example, playing games with the input encoding schemes; the real question, if we want to get to AGI, is how to have a system learn the sort of generalizations I am describing in a general way.)
Third, the claim that no current system can extrapolate turns out to be, well, false; there are already ML systems that can extrapolate at least some functions of exactly the sort I described, and you probably own one: Microsoft Excel, its Flash Fill function in particular (Gulwani, 2011). Powered by a very different approach to machine learning, it can do certain kinds of extrapolation, albeit in a narrow context, by the bushel, e.g., try typing the (decimal) digits 1, 11, 21 in a series of rows and see if the system can extrapolate via Flash Fill to the eleventh item in the sequence (101).
Spoiler alert, it can, in exactly the same way as you probably would, even though there were no positive examples in the training dimension of the hundreds digit. The systems learns from examples the function you want and extrapolates it. Piece of cake. Can any deep learning system do that with three training examples, even with a range of experience on other small counting functions, like 1, 3, 5, .... and 2, 4, 6 ....?
Well maybe, but only the ones that are likely do so are likely to be hybrids that build in operations over variables, which are quite different from the sort of typical convolutional neural networks that most people associate with deep learning.
Putting all this very differently, one crude way to think about where we are with most ML systems that we have today [Note 7] is that they just aren’t designed to think “outside the box”; they are designed to be awesome interpolators inside the box. That’s fine for some purposes, but not others. Humans are better at thinking outside boxes than contemporary AI; I don’t think anyone can seriously doubt that.
But that kind of extrapolation, that Microsoft can do in a narrow context, but that no machine can do with human-like breadth, is precisely what machine learning engineers really ought to be working on, if they want to get to AGI.
10. Everybody in the field already knew this. There is nothing new here.
Well, certainly not everybody; as noted, there were many critics who think we still don’t know the limits of deep learning, and others who believe that there might be some, but none yet discovered.
That said, I never said that any of my points was entirely new; for virtually all, I cited other scholars, who had independently reached similar conclusions.
11. Marcus failed to cite X.
Definitely true; the literature review was incomplete. One favorite among the papers I failed to cite is Shanahan’s Deep Symbolic Reinforcement (Garnelo, Arulkumaran, & Shanahan, 2016); I also can’t believe I forgot Richardson and Domingos’ (2006) Markov Logic Networks. I also wish I had cited Evans and Edward Grefenstette (2017), a great paper from DeepMind. And Smolensky’s tensor calculus work (Smolensky et al., 2016). And work on inductive programming in various forms (Gulwani et al., 2015) and probabilistic programming, too, by Noah Goodman (Goodman, Mansinghka, Roy, Bonawitz, & Tenenbaum, 2012) All seek to bring rules and networks close to together.
And older stuff by pioneers like Jordan Pollack (Smolensky et al., 2016). And Forbus and Gentner’s (Falkenhainer, Forbus, & Gentner, 1989) and Hofstadter and Mitchell’s (1994) work on analogy; and many others. I am sure there is a lot more I could and should have cited.
Overall, I tried to be representative rather than fully comprehensive, but I still could have done better. #chagrin.
12. Marcus has no standing in the field; he isn’t a practitioner; he is just a critic.
Hesitant to raise this one, but it came up in all kinds of different responses, even from the mouths of certain well-known professionals. As Ram Shankar noted, “As a community, we must circumscribe our criticism to science and merit based arguments.” What really matters is not my credentials (which I believe do in fact qualify me to write) but the validity of the arguments.
Either my arguments are correct, or they are not.
[Still, for those who are curious, I supply an optional mini-history of some of my relevant credentials in Note 8 at the end.]
13. Re: hierarchy, what about Socher’s tree-RNNs?
I have written to him, in hopes of having a better understanding of its current status. I’ve also privately pushed several other teams towards trying out tasks like Lake and Baroni (2017) presented.
Pengfei et al (2017) offers some interesting discussion.
14. You could have been more critical of deep learning.
Nobody quite said that, not in exactly those words, but a few came close, generally privately.
One colleague for example pointed out that there may be some serious errors of future forecasting around
The same colleague added
Another colleague, ML researcher and author Pedro Domingos, pointed out still other shortcomings of current deep learning methods that I didn’t mention:
Like other flexible supervised learning methods, deep learning systems can be unstable in the sense that slightly changing the training data may result in large changes in the resulting model.
As Domingos notes, there’s no guarantee this sort of rise and decline won’t repeat itself. Neural networks have risen and fallen several times before, all the way back to Rosenblatt’s first Perceptron in 1957. We shouldn’t mistake cyclical enthusiasm for a complete solution to intelligence, which still seems (to me, anyway) to be decades away.
If we want to reach AGI, we owe it to ourselves to be as keenly aware of challenges we face as we are of our successes.
2. There are other problems too in relying on these 1,000 image sets. For example, in reading a draft of this paper, Melanie Mitchell pointed me to important recent work by Loghmani and colleague (2017) on assessing how deep learning does in the real world. Quoting from the abstract, the paper “analyzes the transferability of deep representations from Web images to robotic data [in the wild]. Despite the promising results obtained with [representations developed from Web image], the experiments demonstrate that object classification with real-life robotic data is far from being solved.”
3. And that literature is growing fast. In late December there was a paper about fooling deep nets into mistaking a pair of skiers for a dog [https://arxiv.org/pdf/1712.09665.pdf] and another on a general-purpose tool for building real-world adversarial patches: https://arxiv.org/pdf/1712.09665.pdf. (See also https://arxiv.org/abs/1801.00634.) It’s frightening to think how vulnerable deep learning can be real-world contexts.
And for that matter consider Filip Pieknewski’s blog on why photo-trained deep learning systems have trouble transferring what they have learned to line drawings, https://blog.piekniewski.info/2016/12/29/can-a-deep-net-see-a-cat/. Vision is not as solved as many people seem to think.
4. As I will explain in the forthcoming paper, AlphaGo is not actually a pure [deep] reinforcement learning system, although the quoted passage presented it as such. It’s really more of a hybrid, with important components that are driven by symbol-manipulating algorithms, along with a well engineered deep-learning component.
5. AlphaZero, by the way, isn’t unsupervised, it’s self-supervised, using self-play and simulation as a way of generating supervised data; I will have a lot more to say about that system in a forthcoming paper.
6. Consider, for example Google Search, and how one might understand it. Google has recently added in a deep learning algorithm, RankBrain, to the wide array of algorithms it uses for search. And Google Search certainly takes in data and knowledge and processes them hierarchically (which according to Maher Ibrahim is all you need to count as being deep learning). But, realistically, deep learning is just one cue among many; the knowledge graph component, for example, is based instead primarily on classical AI notions of traversing ontologies. By any reasonable measure Google Search is a hybrid, with deep learning as just one strand among many.
Calling Google Search as a whole. “a deep learning system” would be grossly misleading, akin to relabeling carpentry “screwdrivery”, just because screwdrivers happen to be involved.
7. Important exceptions include inductive logic programming, inductive function programming (the brains behind Microsoft’s Flash Fill) and neural programming. All are making some progress here; some of these even include deep learning, but they also all include structured representations and operations over variables among their primitive operations; that’s all I am asking for.
8. My AI experiments begin in adolescence, with, among other thing, a Latin-English translator that I coded in the programming language Logo. In graduate school, studying with Steven Pinker, I explored the relation between language acquisition, symbolic rules, and neural networks. (I also owe a debt to my undergraduate mentor Neil Stillings.) The child language data I gathered (Marcus et al., 1992) for my dissertation have been cited hundreds of times, and were the most frequently-modeled data in the 90’s debate about neural networks and how children learned language.
In the late 1990’s I discovered some specific, replicable problems with multilayer perceptrons, (Marcus, 1998b; Marcus, 1998a)); based on those observation, I designed a widely-cited experiment. published in Science (Marcus, Vijayan, Bandi Rao, & Vishton, 1999), that showed that young infants could extract algebraic rules, contra Jeff Elman’s (1990) then popular neural network. All of this culminated in a 2001 MIT Press book (Marcus, 2001), which lobbied for a variety of representational primitives, some of which have begun to pop up in recent neural networks; in particular that the use of operations over variables in the new field of differentiable programming (Daniluk, Rocktäschel, Welbl, & Riedel, 2017; Graves et al., 2016) owes something to the position outlined in that book. There was a strong emphasis on having memory records, as well, which can be seen in the memory networks being developed e.g., at Facebook (Bordes, Usunier, Chopra, & Weston, 2015).) The next decade saw me work on other problems including innateness (Marcus, 2004) (which I will discuss at length in the forthcoming piece about AlphaGo) and evolution (Marcus, 2004; Marcus, 2008), I eventually returned to AI and cognitive modeling, publishing a 2014 article on cortical computation in Science (Marcus, Marblestone, & Dean, 2014) that also anticipates some of what is now happening in differentiable programming.
More recently, I took a leave from academia to found and lead a machine learning company in 2014; by any reasonable measure that company was successful, acquired by Uber roughly two years after founding. As co-founder and CEO I put together a team of some of the very best machine learning talent in the world, including Zoubin Ghahramani, Jeff Clune, Noah Goodman, Ken Stanley and Jason Yosinski, and played a pivotal role in developing our core intellectual property and shaping our intellectual mission. (A patent is pending, co-written by Zoubin Ghahramani and myself.)
Although much of what we did there remains confidential, now owned by Uber, and not by me, I can say that a large part of our efforts were addressed towards integrating deep learning with our own techniques, which gave me a great deal of familiarity with joys and tribulations of Tensorflow and vanishing (and exploding) gradients. We aimed for state-of-the-art results (sometimes successfully, sometimes not) with sparse data, using hybridized deep learning systems on a daily basis.
Bordes, A., Usunier, N., Chopra, S., & Weston, J. (2015). Large-scale Simple Question Answering with Memory Networks. arXiv.
Daniluk, M., Rocktäschel, T., Welbl, J., & Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. arXiv.
Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2)(2), 179–211.
Evans, R., & Grefenstette, E. (2017). Learning Explanatory Rules from Noisy Data. arXiv, cs.NE.
Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). The structure-mapping engine: Algorithm and examples. Artificial intelligence, 41(1)(1), 1–63.
Fukushima, K., Miyake, S., & Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition. IEEE Transactions on Systems, Man, and Cybernetics, 5, 826–834.
Garnelo, M., Arulkumaran, K., & Shanahan, M. (2016). Towards Deep Symbolic Reinforcement Learning. arXiv, cs.AI.
Goodman, N., Mansinghka, V., Roy, D. M., Bonawitz, K., & Tenenbaum, J. B. (2012). Church: a language for generative models. arXiv preprint arXiv:1206.3255.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A. et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626)(7626), 471–476.
Gulwani, S. (2011). Automating string processing in spreadsheets using input-output examples. dl.acm.org, 46(1)(1), 317–330.
Gulwani, S., Hernández-Orallo, J., Kitzelmann, E., Muggleton, S. H., Schmid, U., & Zorn, B. (2015). Inductive programming meets the real world. Communications of the ACM, 58(11)(11), 90–99.
Hofstadter, D. R., & Mitchell, M. (1994). The copycat project: A model of mental fluidity and analogy-making. Advances in connectionist and neural computation theory, 2(31–112)(31–112), 29–30.
Hosseini, H., Xiao, B., Jaiswal, M., & Poovendran, R. (2017). On the Limitation of Convolutional Neural Networks in Recognizing Negative Images. arXiv, cs.CV.
Hubel, D. H., & Wiesel, T. N. (1959). Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3)(3), 574–591.
Lake, B. M., & Baroni, M. (2017). Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks. arXiv.
Loghmani, M. R., Caputo, B., & Vincze, M. (2017). Recognizing Objects In-the-wild: Where Do We Stand? arXiv, cs.RO.
Marcus, G. F. (1998a). Rethinking eliminative connectionism. Cogn Psychol, 37(3)(3), 243 — 282.
Marcus, G. F. (1998b). Can connectionism save constructivism? Cognition, 66(2)(2), 153 — 182.
Marcus, G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive science. Cambridge, Mass.: MIT Press.
Marcus, G. F. (2004). The Birth of the Mind : how a tiny number of genes creates the complexities of human thought. Basic Books.
Marcus, G. F. (2008). Kluge : the haphazard construction of the human mind. Boston : Houghton Mifflin.
Marcus, G. (2018). Deep Learning: A Critical Appraisal. arXiv.
Marcus, G.F., Marblestone, A., & Dean, T. (2014a). The atoms of neural computation. Science, 346(6209)(6209), 551 — 552.
Marcus, G. F., Marblestone, A. H., & Dean, T. L. (2014b). Frequently Asked Questions for: The Atoms of Neural Computation. Biorxiv (arXiv), q-bio.NC.
Marcus, G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive science. Cambridge, Mass.: MIT Press.
Marcus, G. F., Pinker, S., Ullman, M., Hollander, M., Rosen, T. J., & Xu, F. (1992). Overregularization in language acquisition. Monogr Soc Res Child Dev, 57(4)(4), 1–182.
Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283(5398)(5398), 77–80.
Nguyen, A., Yosinski, J., & Clune, J. (2014). Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. arXiv, cs.CV.
Pengfei, L., Xipeng, Q., & Xuanjing, H. (2017). Dynamic Compositional Neural Networks over Tree Structure IJCAI. Proceedings from Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17).
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. arXiv, cs.LG.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62(1)(1), 107–136.
Sabour, S., dffsdfdsf, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. arXiv, cs.CV.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A. et al. (2017). Mastering the game of Go without human knowledge. Nature, 550(7676)(7676), 354–359.
Smolensky, P., Lee, M., He, X., Yih, W.-t., Gao, J., & Deng, L. (2016). Basic Reasoning with Tensor Product Representations. arXiv, cs.AI.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO & Founder, Geometric Intelligence (acquired by Uber). Professor of Psychology and Neural Science, NYU. Freelancer for The New Yorker & New York Times.
",recent appraisal deep learning marcus 2018 outlined ten challenges deep learning suggested deep learning although useful unlikely lead artificial general intelligence suggested instead deep learning viewed universal solvent simply one tool among many place pure deep learning called hybrid models would incorporate supervised forms deep learning also techniques well symbolmanipulation unsupervised learning possibly reconceptualized also urged community consider incorporating innate structure ai systems within days thousands people weighed twitter enthusiastic eg best discussion deeplearning ai ive read many years thoughtful mostly wrong nevertheless think clarity around issues important ive compiled list fourteen commonlyasked queries unsupervised learning fit didnt say nice things deep learning gives right talk stuff first place whats asking neural network generalize even numbers odd numbers hint thats important one lots havent addressed literally every question seen tried representative 1 general intelligence thomas dietterich eminent professor machine learning thorough explicit critic thus far gave nice answer comfortable 2 marcus wasnt nice deep learning said nice things vast accomplishments minimizes others dietterich mentioned made points writing first part true could said positive things like didnt say even like forgot mention dietterichs best example mentioned first page generally later article cited couple great texts excellent blogs pointers numerous examples lot though would really count agi main focus paper google translate example extremely impressive general cant example answer questions translated way human translator could second part substantive 1000 categories really finite well yes compared flexibility cognition cognitive scientists generally place number atomic concepts known individual order 50000 easily compose vastly greater number complex thoughts pets fish probably counted 50000 pet fish something different probably isnt counted easily entertain concept pet fish suffering ick note always disappointing buy pet fish discover infected ick experience child evidently still resent many ideas like express lot 1000 precisely sure many visual categories person recognize suspect math roughly similar try google images pet fish ok try pet fish wearing goggles mostly find dogs wearing goggles false alarm rate 80 machines win nonexpert humans distinguishing similar dog breeds people win wide margin interpreting complex scenes like would happen skydiver wearing backpack rather parachute focusing 1000 category chunks machine learning field view disservice trading shortterm feeling success denial harder openended problems like scene sentence comprehension must eventually addressed compared essentially infinite range sentences scenes see comprehend 1000 anything really small see also note 2 bottom 3 marcus says deep learning useless great many things course useful never said otherwise current supervised form deep learning might approaching limits b limits would stop short full artificial general intelligence unless maybe started incorporating bunch stuff like symbolmanipulation innateness core conclusion 4 one thing dont understand garymarcus says dl good hierarchical structures ylecun nature review paper says dl particularly suited exploiting hierarchies astute question ram shankar lot clearer answer many different types hierarchy one could think deep learning really good probably best ever sort featurewise hierarchy lecun talked typically refer hierarchical feature detection build lines pixels letters lines words letters forth kurzweil hawkins emphasized sort thing really goes back hubel wiesel 1959in neuroscience experiments fukushima fukushima miyake ito 1983 ai fukushima neocognitron model handwired hierarchy successively abstract features lecun many others showed least cases dont hand engineer dont keep track subcomponents encounter along way toplevel system need explicitly encode structure overall output terms parts seen along way part deep learning system fooled thinking pattern black yellow stripes school bus nguyen yosinski clune 2014 stripe pattern strongly correlated activation school bus output units turn correlated bunch lowerlevel features typical imagerecognition deep network fullyrealized representation school bus made wheels chassis windows etc virtually whole spoofing literature thought terms note 3 structural sense hierarchy discussing different focused around systems make explicit reference parts larger wholes classic illustration would chomskys sense hierarchy sentence composed increasingly complex grammatical units eg using novel phrase like man mistook hamburger hot dog larger sentence like actress insisted would outdone man mistook hamburger hot dog dont think deep learning well eg discerning relation actress man misidentified hot dog though attempts certainly made even vision problem entirely licked hintons recent capsule work sabour frosst hinton 2017 example attempt build robust partwhole directions image recognition using structured networks see good trend one potential way begin address spoofing problem also reflection trouble standard deep learning approach 5 weird discuss deep learning context general ai general ai goal deep learning best twitter response came university quebec professor daniel lemire oh come hinton bengio openly going model human intelligence second prize goes math phd google jeremy kun countered dubious claim general ai goal deep learning thats true deep learning experts sure let everyone believe without correcting andrew ngs recent harvard business review article cited implies deep learning anything person second thomas dietterichs tweet said part hard argue limits dl jeremy howard worried idea deep learning overhyped might overhyped suggested every known limit countered deepminds recent alphago paper see note 4 positioned somewhat similarly silver et al silver et al 2017 enthusiastically reporting papers concluding discussion one 10 challenges deep learning reviewed mentioned discuss paper coming soon actually pure deep learning system thats story another day main reason people keep benchmarking ai systems humans precisely agi goal 6 marcus said problem supervised learning deep learning yann lecun presented version comment facebook page part allegedly recognizing lecuns recent work well odd true couldnt find good summary article cite asked lecun told email wasnt one yet mention interest explicitly also noted conclusion positive although expressed reservations current approaches building unsupervised systems ended optimistically lecuns remark get right many problems addressed general problem supervised learning something unique deep learning could clear many supervised learning techniques face similar challenges problems generalization dependence massive data sets relatively little said unique deep learning focus assessing deep learning five year resurgence mark neglected say doesnt really help deep learning supervised learning techniques boat someone could come truly impressive way using deep learning unsupervised way reassessment might required dont see unsupervised learning least currently pursued particularly remedies challenges raised eg respect reasoning hierarchical representations transfer robustness interpretability simply promissory note note 5 portland state santa fe institute professor melanie mitchells put thus far unanswered tweet would meantime see principled reason believe unsupervised learning solve problems raise unless add abstract symbolic representations first 7 deep learning convolutional networks sort marcus critiqued essentially new style programming differentiable programming field trying work reusable constructs style convolution pooling lstm gan vae memory units routing units etc tom dietterich seemed context dietterichs longer series tweets proposed criticism puzzled fan differentiable programming said perhaps point deep learning taken broader way event would equate deep learning differentiable programming eg approaches cited like neural turing machines neural programming deep learning component many differentiable systems systems also build exactly sort elements drawn symbolmanipulation urging field integrate marcus 2001 marcus marblestone dean 2014a marcus marblestone dean 2014b including memory units operations variables systems like routing units stressed recent two essays integrating stuff deep learning gets us agi conclusion quoted turned dead 8 vs future maybe deep learning doesnt work offspring get us agi possibly think deep learning might play important role getting us agi key things many yet discovered added first add matters whether reasonable call future system instance deep learning per se sensible call ultimate system suchandsuch uses deep learning depends deep learning fits ultimate solution maybe example truly adequate natural language understanding systems symbolmanipulation play equally large role deep learning even larger one part issue course terminological good friend recently asked cant call anything includes deep learning deep learning even includes symbolmanipulation enhancement deep learning ought work respond call anything includes symbolmanipulation symbolmanipulation even includes deep learning gradientbased optimization get due symbolmanipulation yet known tool systematically representing achieving highlevel abstraction bedrock virtually worlds complex computer systems spreadsheets programming environments operating systems eventually conjecture credit also due inevitable marriage two hybrid systems bring together two great ideas 20th century ai symbolprocessing neural networks initially developed 1950s new tools yet invented may critical well true acolyte deep learning anything deep learning matter incorporating matter different might current techniques viva imperialism replaced every transistor classic symbolic microprocessor neuron kept chips logic entirely unchanged true deep learning acolyte would still declare victory wont understand principles driving eventual success lump everything together note 6 9 machine extrapolate fair expect neural network generalize even numbers odd numbers heres function expressed binary digits f110 011 f100 001 f010 010 whats f111 ordinary human probably going guess 111 neural network sort discussed probably wont told many times hidden layers neural networks abstract functions little bit surprised human might think function something like reversal easily expressed line computer code neural network certain sort hard learn abstraction reversal way extends evens context odds impossible certainly prior notion integer try another time decimal f4 8 f6 12 whats f5 none human readers would care questions happens require extrapolate even numbers odds lot neural networks would flummoxed sure function undetermined sparse number examples like functions interesting important people would amid infinite range priori possible inductions would alight f510 interesting standard multilayer perceptrons representing numbers binary digits wouldnt thats telling us something many people neural network community francois chollet one salient exception dont want listen importantly recognizing rule applies integer roughly kind generalization allows one recognize novel noun used one context used huge variety contexts first time hear word blicket used object guess fit wide range frames like thought saw blicket close encounter blicket exceptionally large blickets frighten etc generate interpret sentences without specific training doesnt matter whether blicket similar example phonology words heard whether pile adjectives use word subject object machine learning ml paradigms problem problem ml paradigms fair well yes true asking neural networks something violates assumptions neural network advocate might example say hey wait minute reversal example three dimensions input space representing left binary digit middle binary digit rightmost binary digit rightmost binary digit zero training way network know get one position example vincent lostenlan postdoc cornell said dietterich made essentially point concisely although right oddsandevens context hard deep learning wrong larger issues three reasons first cant people cant extrapolate two different examples top section paraphrasing chico marx going believe eyes someone immersed deeply perhaps deeply contemporary machine learning oddsandevens problem seems unfair certain dimension one contains value 1 rightmost digit hasnt illustrated training regime human look examples stymied particular gap training data wont even notice attention higherlevel regularities people routinely extrapolate exactly fashion describing like recognizing string reversal three training examples gave technical sense extrapolation algebraic mind referred specific kind extrapolation generalizing universally quantified onetoone mappings outside space training examples field desperately need solution challenge ever catch human learning even means shaking assumptions might reasonably objected fair fight humans manifestly depend prior knowledge generalize mappings sense dieterrich proposed objection later tweet stream true enough way thats point neural networks certain sort dont good way incorporating right sort prior knowledge place precisely networks dont way incorporating prior knowledge like many generalizations hold elements unbounded classes odd numbers leave remainder one divided two neural networks lack operations variables fail right sort prior knowledge would allow neural networks acquire represent universally quantified onetoone mappings standard neural networks cant represent mappings except certain limited ways convolution way building one particular mapping prior learning second saying current system deep learning otherwise extrapolate way described excuse architectures may choppy water doesnt mean shouldnt trying swim shore want get agi solve problem put differently yes one could certainly hack together solutions get deep learning solve specific number series problems example playing games input encoding schemes real question want get agi system learn sort generalizations describing general way third claim current system extrapolate turns well false already ml systems extrapolate least functions exactly sort described probably one microsoft excel flash fill function particular gulwani 2011 powered different approach machine learning certain kinds extrapolation albeit narrow context bushel eg try typing decimal digits 1 11 21 series rows see system extrapolate via flash fill eleventh item sequence 101 spoiler alert exactly way probably would even though positive examples training dimension hundreds digit systems learns examples function want extrapolates piece cake deep learning system three training examples even range experience small counting functions like 1 3 5 2 4 6 well maybe ones likely likely hybrids build operations variables quite different sort typical convolutional neural networks people associate deep learning putting differently one crude way think ml systems today note 7 arent designed think outside box designed awesome interpolators inside box thats fine purposes others humans better thinking outside boxes contemporary ai dont think anyone seriously doubt kind extrapolation microsoft narrow context machine humanlike breadth precisely machine learning engineers really ought working want get agi 10 everybody field already knew nothing new well certainly everybody noted many critics think still dont know limits deep learning others believe might none yet discovered said never said points entirely new virtually cited scholars independently reached similar conclusions 11 marcus failed cite x definitely true literature review incomplete one favorite among papers failed cite shanahans deep symbolic reinforcement garnelo arulkumaran shanahan 2016 also cant believe forgot richardson domingos 2006 markov logic networks also wish cited evans edward grefenstette 2017 great paper deepmind smolenskys tensor calculus work smolensky et al 2016 work inductive programming various forms gulwani et al 2015 probabilistic programming noah goodman goodman mansinghka roy bonawitz tenenbaum 2012 seek bring rules networks close together older stuff pioneers like jordan pollack smolensky et al 2016 forbus gentners falkenhainer forbus gentner 1989 hofstadter mitchells 1994 work analogy many others sure lot could cited overall tried representative rather fully comprehensive still could done better chagrin 12 marcus standing field isnt practitioner critic hesitant raise one came kinds different responses even mouths certain wellknown professionals ram shankar noted community must circumscribe criticism science merit based arguments really matters credentials believe fact qualify write validity arguments either arguments correct still curious supply optional minihistory relevant credentials note 8 end 13 hierarchy sochers treernns written hopes better understanding current status ive also privately pushed several teams towards trying tasks like lake baroni 2017 presented pengfei et al 2017 offers interesting discussion 14 could critical deep learning nobody quite said exactly words came close generally privately one colleague example pointed may serious errors future forecasting around colleague added another colleague ml researcher author pedro domingos pointed still shortcomings current deep learning methods didnt mention like flexible supervised learning methods deep learning systems unstable sense slightly changing training data may result large changes resulting model domingos notes theres guarantee sort rise decline wont repeat neural networks risen fallen several times way back rosenblatts first perceptron 1957 shouldnt mistake cyclical enthusiasm complete solution intelligence still seems anyway decades away want reach agi owe keenly aware challenges face successes 2 problems relying 1000 image sets example reading draft paper melanie mitchell pointed important recent work loghmani colleague 2017 assessing deep learning real world quoting abstract paper analyzes transferability deep representations web images robotic data wild despite promising results obtained representations developed web image experiments demonstrate object classification reallife robotic data far solved 3 literature growing fast late december paper fooling deep nets mistaking pair skiers dog httpsarxivorgpdf171209665pdf another generalpurpose tool building realworld adversarial patches httpsarxivorgpdf171209665pdf see also httpsarxivorgabs180100634 frightening think vulnerable deep learning realworld contexts matter consider filip pieknewskis blog phototrained deep learning systems trouble transferring learned line drawings httpsblogpiekniewskiinfo20161229canadeepnetseeacat vision solved many people seem think 4 explain forthcoming paper alphago actually pure deep reinforcement learning system although quoted passage presented really hybrid important components driven symbolmanipulating algorithms along well engineered deeplearning component 5 alphazero way isnt unsupervised selfsupervised using selfplay simulation way generating supervised data lot say system forthcoming paper 6 consider example google search one might understand google recently added deep learning algorithm rankbrain wide array algorithms uses search google search certainly takes data knowledge processes hierarchically according maher ibrahim need count deep learning realistically deep learning one cue among many knowledge graph component example based instead primarily classical ai notions traversing ontologies reasonable measure google search hybrid deep learning one strand among many calling google search whole deep learning system would grossly misleading akin relabeling carpentry screwdrivery screwdrivers happen involved 7 important exceptions include inductive logic programming inductive function programming brains behind microsofts flash fill neural programming making progress even include deep learning also include structured representations operations variables among primitive operations thats asking 8 ai experiments begin adolescence among thing latinenglish translator coded programming language logo graduate school studying steven pinker explored relation language acquisition symbolic rules neural networks also owe debt undergraduate mentor neil stillings child language data gathered marcus et al 1992 dissertation cited hundreds times frequentlymodeled data 90s debate neural networks children learned language late 1990s discovered specific replicable problems multilayer perceptrons marcus 1998b marcus 1998a based observation designed widelycited experiment published science marcus vijayan bandi rao vishton 1999 showed young infants could extract algebraic rules contra jeff elmans 1990 popular neural network culminated 2001 mit press book marcus 2001 lobbied variety representational primitives begun pop recent neural networks particular use operations variables new field differentiable programming daniluk rocktaschel welbl riedel 2017 graves et al 2016 owes something position outlined book strong emphasis memory records well seen memory networks developed eg facebook bordes usunier chopra weston 2015 next decade saw work problems including innateness marcus 2004 discuss length forthcoming piece alphago evolution marcus 2004 marcus 2008 eventually returned ai cognitive modeling publishing 2014 article cortical computation science marcus marblestone dean 2014 also anticipates happening differentiable programming recently took leave academia found lead machine learning company 2014 reasonable measure company successful acquired uber roughly two years founding cofounder ceo put together team best machine learning talent world including zoubin ghahramani jeff clune noah goodman ken stanley jason yosinski played pivotal role developing core intellectual property shaping intellectual mission patent pending cowritten zoubin ghahramani although much remains confidential owned uber say large part efforts addressed towards integrating deep learning techniques gave great deal familiarity joys tribulations tensorflow vanishing exploding gradients aimed stateoftheart results sometimes successfully sometimes sparse data using hybridized deep learning systems daily basis bordes usunier n chopra weston j 2015 largescale simple question answering memory networks arxiv daniluk rocktaschel welbl j riedel 2017 frustratingly short attention spans neural language modeling arxiv elman j l 1990 finding structure time cognitive science 1422 179211 evans r grefenstette e 2017 learning explanatory rules noisy data arxiv csne falkenhainer b forbus k gentner 1989 structuremapping engine algorithm examples artificial intelligence 4111 163 fukushima k miyake ito 1983 neocognitron neural network model mechanism visual pattern recognition ieee transactions systems man cybernetics 5 826834 garnelo arulkumaran k shanahan 2016 towards deep symbolic reinforcement learning arxiv csai goodman n mansinghka v roy bonawitz k tenenbaum j b 2012 church language generative models arxiv preprint arxiv12063255 graves wayne g reynolds harley danihelka grabskabarwinska et al 2016 hybrid computing using neural network dynamic external memory nature 53876267626 471476 gulwani 2011 automating string processing spreadsheets using inputoutput examples dlacmorg 4611 317330 gulwani hernandezorallo j kitzelmann e muggleton h schmid u zorn b 2015 inductive programming meets real world communications acm 581111 9099 hofstadter r mitchell 1994 copycat project model mental fluidity analogymaking advances connectionist neural computation theory 23111231112 2930 hosseini h xiao b jaiswal poovendran r 2017 limitation convolutional neural networks recognizing negative images arxiv cscv hubel h wiesel n 1959 receptive fields single neurones cats striate cortex journal physiology 14833 574591 lake b baroni 2017 still systematic years compositional skills sequencetosequence recurrent networks arxiv loghmani r caputo b vincze 2017 recognizing objects inthewild stand arxiv csro marcus g f 1998a rethinking eliminative connectionism cogn psychol 3733 243 282 marcus g f 1998b connectionism save constructivism cognition 6622 153 182 marcus g f 2001 algebraic mind integrating connectionism cognitive science cambridge mass mit press marcus g f 2004 birth mind tiny number genes creates complexities human thought basic books marcus g f 2008 kluge haphazard construction human mind boston houghton mifflin marcus g 2018 deep learning critical appraisal arxiv marcus gf marblestone dean 2014a atoms neural computation science 34662096209 551 552 marcus g f marblestone h dean l 2014b frequently asked questions atoms neural computation biorxiv arxiv qbionc marcus g f 2001 algebraic mind integrating connectionism cognitive science cambridge mass mit press marcus g f pinker ullman hollander rosen j xu f 1992 overregularization language acquisition monogr soc res child dev 5744 1182 marcus g f vijayan bandi rao vishton p 1999 rule learning sevenmonthold infants science 28353985398 7780 nguyen yosinski j clune j 2014 deep neural networks easily fooled high confidence predictions unrecognizable images arxiv cscv pengfei l xipeng q xuanjing h 2017 dynamic compositional neural networks tree structure ijcai proceedings proceedings twentysixth international joint conference artificial intelligence ijcai17 ribeiro singh guestrin c 2016 trust explaining predictions classifier arxiv cslg richardson domingos p 2006 markov logic networks machine learning 6211 107136 sabour dffsdfdsf n hinton g e 2017 dynamic routing capsules arxiv cscv silver schrittwieser j simonyan k antonoglou huang guez et al 2017 mastering game go without human knowledge nature 55076767676 354359 smolensky p lee x yih wt gao j deng l 2016 basic reasoning tensor product representations arxiv csai quick cheer standing ovation clap show much enjoyed story ceo founder geometric intelligence acquired uber professor psychology neural science nyu freelancer new yorker new york times,en,"['Dietterich', 'AGI', 'Google Translate', 'Ick', 'Marcus', 'LOT', 'Kurzweil', 'Nguyen, Yosinski, & Clune, 2014', 'Chomsky', 'Hinton', 'Sabour, Frosst, & Hinton', 'General AI', 'University of Quebec', 'Bengio', 'Google', 'Harvard Business Review', 'AlphaGo', 'LeCun', 'Santa Fe Institute', 'GAN', 'Marblestone', 'f(6', 'f(5)=10', 'François Chollet', 'Cornell', 'Microsoft', 'Shanahan', 'Goodman', 'Mansinghka', 'Forbus', 'Falkenhainer', 'Hofstadter', 'Hesitant', 'Perceptron', 'AlphaZero', 'RankBrain', 'Google Search', 'Science (Marcus', 'Vijayan', 'Bandi Rao', 'MIT Press', 'Chopra, & Weston', 'Uber', 'Noisy Data', 'Algorithm', 'IEEE Transactions on Systems', 'Danihelka', 'I.', 'Grabska-Barwińska', 'ACM', '& Mitchell, M.', 'the Limitation of Convolutional Neural Networks in Recognizing Negative Images', 'Wiesel', 'The Journal of physiology,', 'Caputo, B., & Vincze', 'Basic Books', 'Marcus, G. F.', 'S.', 'Nguyen, A., Yosinski, J., & Clune, J.', 'Dynamic Compositional Neural Networks', 'Tree Structure IJCAI', 'N., & Hinton', 'Simonyan, K.', 'Antonoglou', 'CEO & Founder', 'Geometric Intelligence', 'NYU', 'The New Yorker & New York Times']"
131,Bargava,11800,How to learn Deep Learning in 6 months – Towards Data Science,"It is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months’ time. This article details out the steps to achieve that.
Pre-requisites
- You are willing to spend 10–20 hours per week for the next 6 months- You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed).- Some math education in the past (algebra, geometry etc). - Access to internet and computer.
Step 1
We learn driving a car — by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach.
Do the fast.ai course — Practical Deep Learning for Coders — Part 1. This takes about 4–6 weeks of effort. This course has a session on running the code on cloud. Google Colaboratory has free GPU access. Start with that. Other options include Paperspace, AWS, GCP, Crestle and Floydhub. All of these are great. Do not start to build your own machine. Atleast not yet.
Step 2
This is the time to know some of the basics. Learn about calculus and linear algebra.
For calculus, Big Picture of Calculus provides a good overview.
For Linear Algebra, Gilbert Strang’s MIT course on OpenCourseWare is amazing.
Once you finish the above two, read the Matrix Calculus for Deep Learning.
Step 3
Now is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the deep learning specialisation in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week.
Step 4
Do a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking.
The first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools.
Step 5
Now go and do fast.ai’s part II course — Cutting Edge Deep Learning for Coders. This covers more advanced topics and you will learn to read the latest research papers and make sense out of them.
Each of the steps should take about 4–6 weeks’ time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning.
Where to go next?
Do the Stanford’s CS231n and CS224d courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the deep learning book. This will solidify your understanding.
Happy deep learning. Create every single day.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Deep Learning @ http://impel.io/ . Currently building a personalization engine http://www.recotap.com/. Data Science Trainer and Mentor.
Sharing concepts, ideas, and codes.
",quite possible learn follow contribute stateofart work deep learning 6 months time article details steps achieve prerequisites willing spend 1020 hours per week next 6 months programming skills comfortable pick python along way cloud background python cloud assumed math education past algebra geometry etc access internet computer step 1 learn driving car driving learning clutch internal combustion engine work atleast initially learning deep learning follow topdown approach fastai course practical deep learning coders part 1 takes 46 weeks effort course session running code cloud google colaboratory free gpu access start options include paperspace aws gcp crestle floydhub great start build machine atleast yet step 2 time know basics learn calculus linear algebra calculus big picture calculus provides good overview linear algebra gilbert strangs mit course opencourseware amazing finish two read matrix calculus deep learning step 3 time understand bottomup approach deep learning 5 courses deep learning specialisation coursera need pay get assignments graded effort truly worth ideally given background gained far able complete one course every week step 4 capstone project time delve deep deep learning libraryeg tensorflow pytorch mxnet implement architecture scratch problem liking first three steps understanding use deep learning gaining solid foundation step implementing project scratch developing strong foundation tools step 5 go fastais part ii course cutting edge deep learning coders covers advanced topics learn read latest research papers make sense steps take 46 weeks time 26 weeks since time started followed religiously solid foundation deep learning go next stanfords cs231n cs224d courses two amazing courses great depth vision nlp respectively cover latest stateofart read deep learning book solidify understanding happy deep learning create every single day quick cheer standing ovation clap show much enjoyed story deep learning httpimpelio currently building personalization engine httpwwwrecotapcom data science trainer mentor sharing concepts ideas codes,en,"['GPU', 'Paperspace', 'GCP', 'Floydhub', 'MIT', 'the Matrix Calculus for Deep Learning', 'Coursera', 'PyTorch', 'MXNet', 'Stanford', 'NLP']"
132,Seth Weidman,2800,The 3 Tricks That Made AlphaGo Zero Work – Hacker Noon,"There were many advances in Deep Learning and AI in 2017, but few generated as much publicity and interest as DeepMind’s AlphaGo Zero. This program was truly a shocking breakthrough: not only did it beat the prior version of AlphaGo — the program that beat 17 time world champion Lee Sedol just a year and a half earlier — 100–0, it was trained without any data from real human games. Xavier Amatrain called it “more [significant] than anything...in the last 5 years” in Machine Learning.
So how did DeepMind do it? In this essay, I’ll try to give an intuitive idea of the techniques AlphaGo Zero used, what made them work, and what the implications for future AI research are. Let’s start with the general approach that both AlphaGo and AlphaGo Zero took to playing Go.
Both AlphaGo and AlphaGo Zero evaluated the Go board and chose moves using a combination of two methods:
AlphaGo and AlphaGo Zero both worked by cleverly combining these two methods. Let’s look at each one in turn:
Go is a sufficiently complex game that computers can’t simply search all possible moves using a brute force approach to find the best one (indeed, they can’t even come close).
The best Go programs prior to AlphaGo overcame this by using “Monte Carlo Tree Search” or MCTS. At a high level, this method involves initially exploring many possible moves on the board, and then focusing this exploration over time as certain moves are found to be more likely to lead to wins than others.
Both AlphaGo and AlphaGo Zero use a relatively straightforward version of MCTS for their “lookahead”, simply using many of the best practices listed in the Monte Carlo Tree Search Wikipedia page to properly manage the tradeoff between exploring new sequences of move or more deeply explore already-explored sequences (for more, see the details in the “Search” section under “Methods” in the original AlphaGo Paper published in Nature).
Though, MCTS had been the core of all successful Go programs prior to AlphaGo, it was DeepMind’s clever combination of this technique with a neural network-based “intuition” that allowed it to surpass human performance.
DeepMind’s major innovation with AlphaGo was to use deep neural networks to understand the state of the game, and then use this understanding to intelligently guide the search of the MCTS. More specifically: they trained networks that could look at
Given this information, the neural networks could recommend:
How did DeepMind train neural networks to do this? Here, AlphaGo and AlphaGo Zero used very different approaches; we’ll start first with AlphaGo’s:
AlphaGo had two separately trained neural networks.
DeepMind then combined these two neural networks with MCTS — that is, the program’s “intuition” with its brute force “lookahead” search— in a very clever way: it used the network that had been trained to predict moves to guide which branches of the game tree to search and used the network that had been trained to predict whether a position was “winning” to evaluate the positions it encountered during its search. This allowed AlphaGo to intelligently search upcoming moves and ultimately allowed it to beat Lee Sedol.
AlphaGo Zero, however, took this to a whole new level.
At a high level, AlphaGo Zero works the same way as AlphaGo: specifically, it plays Go by using MCTS-based lookahead search, intelligently guided by a neural network.
However, AlphaGo Zero’s neural network — its “intuition” — was trained completely differently from that of AlphaGo:
Let’s say you have a neural network that is attempting to “understand” the game of Go: that is, for every board position, it is using a deep neural network to generate evaluations of what the best moves are. What DeepMind realized is that no matter how intelligent this neural network is — whether it is completely clueless or a Go master — its evaluations can always be made better by MCTS.
Fundamentally, MCTS performs the kind of lookahead search that we would imagine a human master would perform if given enough time: it intelligently guesses which variations— sequences of future moves — are most promising, simulates those variations, evaluates how good they actually are, and updates its assessments of its current best moves accordingly.
An illustration of this is below. Suppose we have a neural network that is reading the board and determining that a given move results in a game being even, with an evaluation of 0.0. Then, the network intelligently looks ahead a few moves and finds a sequence of moves that can be forced from the current position that ends up resulting in an evaluation of 0.5. It can then update its evaluation of the current board position to reflect that it leads to a more favorable position down the road.
This lookahead search, therefore, can always give us improved data on how good the various moves in the current position that the neural network is evaluating are. This is true whether our neural network is playing at an amateur level or an expert level: we can always generate improve evaluations for it by looking ahead and seeing which of its current options actually lead to better positions.
In addition, just as in AlphaGo, we would also want our neural network to learn which moves are likely to lead to wins. So, also as before, our agent—using its MCTS-improved evaluations and the current state of its neural network — could play games against itself, winning some and losing others.
This data, generated purely via lookahead and self-play, is what DeepMind used to train AlphaGo Zero. More specifically:
Much was made of the fact that no games between humans were used to train AlphaGo Zero, and this first “trick” was the reason why: for a given state of a Go agent, it can always be made smarter by performing MCTS-based lookahead and using the results of that lookahead to improve the agent. This is how AlphaGo Zero was able to continuously improve, from when it was an amateur all the way up to when it better than the best human players.
The second trick was a novel neural network structure that I’ll call the “Two Headed Monster”.
AlphaGo Zero’s was its neural network architecture, a “two-headed” architecture. Its first 20 layers or so were layer “blocks” of a type often seen in modern neural net architecures. These layers were followed by two “heads”: one head that took the output of the first 20 layers and produced probabilities of the Go agent making certain moves, and another that took the output of the first 20 layers and outputted a probability of the current player winning.
This is quite unusual. In almost all applications, neural networks output a single, fixed output — such as the probability of an image containing a dog, or a vector containing the probabilities of an image containing one of 10 types of objects. How can a net learn if it is receiving two sets of signals: one on how good its evaluations of the board are, and another how good the specific moves it is selecting are?
The answer is simple: remember that neural networks are fundamentally just mathematical functions with a bunch of parameters that determine the predictions that they make; we “teach” them by repeatedly showing them “correct answers” and having them update their parameters so the answers they produce more closely match these correct answers.
So, when we use the two headed neural net to make a prediction using Head #1, we simply update the parameters that led to making that prediction, namely the parameters in the “Body” and in “Head #1”. Similarly, when we make a prediction using Head #2, we update the parameters in the “Body” and in “Head #2”.
This is how DeepMind trained its single, “two-headed” neural network that it used to guide MCTS during its search, just as AlphaGo did with two separate neural networks. This trick accounted for half of AlphaGo Zero’s increase in playing strength over AlphaGo.
(this trick is known more technically as Multi-Task Learning with Hard Parameter Sharing. Sebastian Ruder has a great overview here).
The other half of the increase in playing strength simply came from bringing the neural network architecture up-to-date with the latest advances in the field:
AlphaGo Zero used a more “cutting edge” neural network architecture than AlphaGo. Specifically, they used a “residual” neural network architecture instead of a purely “convolutional” architecture. Residual nets were pioneered by Microsoft Research in late 2015, right around the time work on the first version of AlphaGo would have wrapped up, so it both understandable that DeepMind did not use them in the original AlphaGo program.
Interestingly, as the chart below shows, each of these two neural network-related tricks — switching from convolutional to residual architecture and using the “Two Headed Monster” neural network architecture instead of separate neural networks — would have resulted in about half of the increase in playing strength as was achieved when both were combined.
These three tricks are what enabled AlphaGo Zero to achieve its incredible performance that blew away even Alpha Go:
It is worth noting that AlphaGo did not use any classical or even “cutting edge” reinforcement learning concepts — no Deep Q Learning, Asynchronous Actor-Critic Agents, or anything else we typically associate with reinforcement learning. It simply used simulations to generate training data for its neural nets to then learn from in a supervised fashion. Denny Britz sums this idea up well in this Tweet from just after when the AlphaGo Zero paper was released:
Here’s a “step-by-step” timeline of how AlphaGo Zero was trained:
3. As these self-play games are happening, sample 2,048 positions from the most recent 500,000 games, along with whether the game was won or lost. For each move, record both A) the results of the MCTS evaluations of those positions — how “good” the various moves in these positions were based on lookahead — and B) whether the current player won or lost the game.
4. Train the neural network, using both A) the move evaluations produced by the MCTS lookahead search and B) whether the current player won or lost.
5. Finally, every 1,000 iterations of steps 3–4, evaluate the current neural network against the previous best version; if it wins at least 55% of the games, begin using it to generate self-play games instead of the prior version.
Repeat steps 3–4 700,000 times, while the self-play games are continuously being played — after three days, you’ll have yourself an AlphaGo Zero!
There are many implications of DeepMind’s incredible achievement for the future of AI research. Here are a couple of key ones:
First, the fact that self-play data generated from simulations was “good enough” to be able to train the network suggests that simulated self-play data can train agents to surpass human performance in extremely complex tasks, even starting completely from scratch — data generated from human experts may not be needed.
Second, the “Two Headed Monster” trick seems to significantly help agents learn to perform several related tasks in many domains, since it seems to prevent the agents from overfitting their behavior to any individual task. DeepMind seems to really like this trick, and has used it and more advanced versions of it to build agents that can learn multiple tasks in several different domains.
Many projects in robotics, especially the burgeoning field of using simulations to teach robotic agents to use their limbs to accomplish tasks, are using these two tricks to great effect. Pieter Abbeel’s recent NIPS keynote highlights many impressive new results that use these tricks along with many bleeding edge reinforcement learning techniques. Indeed, locomotion seems like a perfect use case for the “Two Headed Monster” trick in particular: for example, robotic agents could be simultaneously trained to hit a baseball using a bat and to throw a punch to hit a moving target, since the two tasks require learning some common skills (e.g. balance, torso rotation).
DeepMind’s AlphaGo Zero was one of the most intriguing advancements in AI and Deep Learning in 2017. I can’t wait to see what 2018 brings!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Senior Data Scientist at @thisismetis. I write about the intersection of Data Science, business, education, and society.
how hackers start their afternoons.
",many advances deep learning ai 2017 generated much publicity interest deepminds alphago zero program truly shocking breakthrough beat prior version alphago program beat 17 time world champion lee sedol year half earlier 1000 trained without data real human games xavier amatrain called significant anythingin last 5 years machine learning deepmind essay ill try give intuitive idea techniques alphago zero used made work implications future ai research lets start general approach alphago alphago zero took playing go alphago alphago zero evaluated go board chose moves using combination two methods alphago alphago zero worked cleverly combining two methods lets look one turn go sufficiently complex game computers cant simply search possible moves using brute force approach find best one indeed cant even come close best go programs prior alphago overcame using monte carlo tree search mcts high level method involves initially exploring many possible moves board focusing exploration time certain moves found likely lead wins others alphago alphago zero use relatively straightforward version mcts lookahead simply using many best practices listed monte carlo tree search wikipedia page properly manage tradeoff exploring new sequences move deeply explore alreadyexplored sequences see details search section methods original alphago paper published nature though mcts core successful go programs prior alphago deepminds clever combination technique neural networkbased intuition allowed surpass human performance deepminds major innovation alphago use deep neural networks understand state game use understanding intelligently guide search mcts specifically trained networks could look given information neural networks could recommend deepmind train neural networks alphago alphago zero used different approaches well start first alphagos alphago two separately trained neural networks deepmind combined two neural networks mcts programs intuition brute force lookahead search clever way used network trained predict moves guide branches game tree search used network trained predict whether position winning evaluate positions encountered search allowed alphago intelligently search upcoming moves ultimately allowed beat lee sedol alphago zero however took whole new level high level alphago zero works way alphago specifically plays go using mctsbased lookahead search intelligently guided neural network however alphago zeros neural network intuition trained completely differently alphago lets say neural network attempting understand game go every board position using deep neural network generate evaluations best moves deepmind realized matter intelligent neural network whether completely clueless go master evaluations always made better mcts fundamentally mcts performs kind lookahead search would imagine human master would perform given enough time intelligently guesses variations sequences future moves promising simulates variations evaluates good actually updates assessments current best moves accordingly illustration suppose neural network reading board determining given move results game even evaluation 00 network intelligently looks ahead moves finds sequence moves forced current position ends resulting evaluation 05 update evaluation current board position reflect leads favorable position road lookahead search therefore always give us improved data good various moves current position neural network evaluating true whether neural network playing amateur level expert level always generate improve evaluations looking ahead seeing current options actually lead better positions addition alphago would also want neural network learn moves likely lead wins also agentusing mctsimproved evaluations current state neural network could play games winning losing others data generated purely via lookahead selfplay deepmind used train alphago zero specifically much made fact games humans used train alphago zero first trick reason given state go agent always made smarter performing mctsbased lookahead using results lookahead improve agent alphago zero able continuously improve amateur way better best human players second trick novel neural network structure ill call two headed monster alphago zeros neural network architecture twoheaded architecture first 20 layers layer blocks type often seen modern neural net architecures layers followed two heads one head took output first 20 layers produced probabilities go agent making certain moves another took output first 20 layers outputted probability current player winning quite unusual almost applications neural networks output single fixed output probability image containing dog vector containing probabilities image containing one 10 types objects net learn receiving two sets signals one good evaluations board another good specific moves selecting answer simple remember neural networks fundamentally mathematical functions bunch parameters determine predictions make teach repeatedly showing correct answers update parameters answers produce closely match correct answers use two headed neural net make prediction using head 1 simply update parameters led making prediction namely parameters body head 1 similarly make prediction using head 2 update parameters body head 2 deepmind trained single twoheaded neural network used guide mcts search alphago two separate neural networks trick accounted half alphago zeros increase playing strength alphago trick known technically multitask learning hard parameter sharing sebastian ruder great overview half increase playing strength simply came bringing neural network architecture uptodate latest advances field alphago zero used cutting edge neural network architecture alphago specifically used residual neural network architecture instead purely convolutional architecture residual nets pioneered microsoft research late 2015 right around time work first version alphago would wrapped understandable deepmind use original alphago program interestingly chart shows two neural networkrelated tricks switching convolutional residual architecture using two headed monster neural network architecture instead separate neural networks would resulted half increase playing strength achieved combined three tricks enabled alphago zero achieve incredible performance blew away even alpha go worth noting alphago use classical even cutting edge reinforcement learning concepts deep q learning asynchronous actorcritic agents anything else typically associate reinforcement learning simply used simulations generate training data neural nets learn supervised fashion denny britz sums idea well tweet alphago zero paper released heres stepbystep timeline alphago zero trained 3 selfplay games happening sample 2048 positions recent 500000 games along whether game lost move record results mcts evaluations positions good various moves positions based lookahead b whether current player lost game 4 train neural network using move evaluations produced mcts lookahead search b whether current player lost 5 finally every 1000 iterations steps 34 evaluate current neural network previous best version wins least 55 games begin using generate selfplay games instead prior version repeat steps 34 700000 times selfplay games continuously played three days youll alphago zero many implications deepminds incredible achievement future ai research couple key ones first fact selfplay data generated simulations good enough able train network suggests simulated selfplay data train agents surpass human performance extremely complex tasks even starting completely scratch data generated human experts may needed second two headed monster trick seems significantly help agents learn perform several related tasks many domains since seems prevent agents overfitting behavior individual task deepmind seems really like trick used advanced versions build agents learn multiple tasks several different domains many projects robotics especially burgeoning field using simulations teach robotic agents use limbs accomplish tasks using two tricks great effect pieter abbeels recent nips keynote highlights many impressive new results use tricks along many bleeding edge reinforcement learning techniques indeed locomotion seems like perfect use case two headed monster trick particular example robotic agents could simultaneously trained hit baseball using bat throw punch hit moving target since two tasks require learning common skills eg balance torso rotation deepminds alphago zero one intriguing advancements ai deep learning 2017 cant wait see 2018 brings quick cheer standing ovation clap show much enjoyed story senior data scientist thisismetis write intersection data science business education society hackers start afternoons,en,"['AlphaGo', 'the Go board', 'AlphaGo Paper', 'MCTS', 'Microsoft Research', 'AlphaGo Zero', 'Pieter Abbeel', 'NIPS']"
133,Gabriel Aldamiz...,5100,"How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach","Three years ago we launched Chicisimo, our goal was to offer automated outfit advice. Today, with over 4 million women on the app, we want to share how our data and machine learning approach helped us grow. It’s been chaotic but it is now under control.
If we wanted to build a human-level tool to offer automated outfit advice, we needed to understand people’s fashion taste. A friend can give us outfit advice because after seeing what we normally wear, she’s learnt our style. How could we build a system that learns fashion taste?
We had previous experience with taste-based projects and a background in machine learning applied to music and other sectors. We saw how a collaborative filtering tool transformed the music industry from blindness to totally understanding people (check out the Audioscrobbler story). It also made life better for those who love music, and created several unicorns along the way.
With this background, we built the following thesis: online fashion will be transformed by a tool that understands taste. Because if you understand taste, you can delight people with relevant content and a meaningful experience. We also thought that “outfits” were the asset that would allow taste to be understood, to learn what people wear or have in their closet, and what style each of us like.
We decided we were going to build that tool to understand taste. We focused on developing the correct dataset, and built two assets: our mobile app and our data platform.
From previous experience building mobile products, even in Symbian back then, we knew it was easy to bring people to an app but difficult to retain them. So we focused on small iterations to learn as fast as possible.
We launched an extremely early alpha of Chicisimo with one key functionality. We launched under another name and in another country. You couldn’t even upload photos... but it allowed us to iterate with real data and get a lot of qualitative input. At some point, we launched the real Chicisimo, and removed this alpha from the App Store.
We spent a long time trying to understand what our true levers of retention were, and what algorithms we needed in order to match content and people.
Three things helped with retention:
(a) identify retention levers using behavioral cohorts (we use Mixpanel for this). We run cohorts not only over the actions that people performed, but also over the value they received. This was hard to conceptualize for an app such as Chicisimo*. We thought in terms of what specific and measurable value people received, measured it, and run cohorts over those events, and then we were able to iterate over value received, not only over actions people performed. We also defined and removed anti-levers (all those noisy things that distract from the main value) and got all the relevant metrics for different time periods: first session, first day, first week, etc. These super specific metrics allowed us to iterate (*Nir Eyal’s book Hooked: How to Build Habit-Forming Products discusses a framework to create habits that helped us build our model);
(b) re-think the onboarding process, once we knew the levers of retention. We define it as the process by which new signups find the value of the app as soon as possible, and before we lose them. We clearly articulated to ourselves what needed to happen (what and when). It went something like this: If people don’t do [action] during their first 7 minutes in their first session, they will not come back. So we need to change the experience to make that happen. We also run tons of user-tests with different types of people, and observed how they perceived (or mostly didn’t) the retention lever;
(c) define how we learn. The data approach described above is key, but there is much more than data when building a product people love. In our case, first of all, we think that the what-to-wear problem is a very important one to solve, and we truly respect it. We obsess over understanding the problem, and over understanding how our solution is helping, or not. It’s our way of showing respect.
This leads me to one of the most surprising aspects IMO of building a product: the fact that, regularly, we access new corpuses of knowledge that we did not have before, which help us improve the product significantly. When we’ve obtained these game-changing learnings, it’s always been by focusing on two aspects: how people relate to the problem, and how people relate to the product (the red arrows in the image below). There are a million subtleties that happen in these two relations, and we are building Chicisimo by trying to understand them. Now, we know that at any point there is something important that we don’t know and therefore the question always is: how can we learn... sooner?
Talking with one of my colleagues, she once told me, “this is not about data, this is about people”. And the truth is, from day one we’ve learnt significantly by having conversations with women about how they relate with the problem, and with solutions. We use several mechanisms: having face to face conversations, reading the emails we get from women without predefined questions, or asking for feedback around specific topics (we now use Typeform and its a great tool for product insight). And then we talk among ourselves and try to articulate the learnings. We also seek external references: we talk with other product people, we play with inspiring apps, and we re-read articles that help us think. This process is what allows us to learn, and then build product and develop technology.
At some point, we were lucky to get noticed by the App Store team, and we’ve been featured as App of the Day throughout the world (view Apple’s description of Chicisimo, here). On December 31st, Chicisimo was featured in a summary of apps the App Store team did, we are the pink “C.” in the left image below 😀.
The app got viewed by 957,437 uniques thanks to this feature, for a total of 1.3M times. In our case, app features have a 0,5% conversion rate from impression to app install (normally: impression > product page view > install); ASO has a 3% conversion, and referrers 45%.
The app aims at understanding taste so we can do a better job at suggesting outfit ideas. The simple act of delivering the right content at the right time can absolutely wow people, although it is an extremely difficult utility to build.
Chicisimo content is 100% user-generated, and this poses some challenges: the system needs to classify different types of content automatically, build the right incentives, and understand how to match content and needs.
We soon saw that there was a lot of data coming in. After thinking “hey, how cool we are, look at all this data we have”, we realized it was actually a nightmare because, being chaotic, the data wasn’t actionable. This wasn’t cool at all. But then we decided to start giving some structure to parts of the data, and we ended inventing what we called the Social Fashion Graph. The graph is a compact representation of how needs, outfits and people interrelate, a concept that helped us build the data platform. The data platform creates a high-quality dataset linked to a learning and training world, our app, which therefore improves with each new expression of taste.
We thought of outfits as playlists: an outfit is a combination of items that makes sense to consume together. Using collaborative filtering, the relations captured here allow us to offer recommendations in different areas of the app.
There was still a lot of noise in the data, and one of the hardest things was to understand how people were expressing the same fashion need in different ways, which made matching content and needs even more difficult. Lots of people might need ideas to go to school, and express that specific need in a hundred different ways. How do you capture this diversity, and how do you provide structure to it? We built a system to collect concepts (we call them needs) and captured equivalences among different ways to express the same need. We ended up building a list of the world’s what-to-wear needs, which we call our ontology. This really cleaned up the dataset and helped us understand what we had. This understanding led to better product decisions.
We now understand that an outfit, a need or a person, can have a lot of understandable data attached, if you allow people to express freely (the app) while having the right system behind (the platform). Structuring data gave us control, while encouraging unstructured data gave us knowledge and flexibility.
The end result is our current system. A system that learns the meaning of an outfit, how to respond to a need, or the taste of an individual.
And I wouldn’t even dare saying that this is Day 1 for us.
Screenshot of an internal tool.
The amount of work we have in front of us is immense, but we feel things are now under control. One of the new areas we’ve been working on is adding a fourth element to the Social Fashion Graph: shoppable products. A system to match outfits to products automatically, and to help people decide what to buy next. This is pretty exciting.
Back when we built recommender systems for music and other products, it was pretty easy (that’s what we think now, we obviously didn’t think that at the time:). First, it was easy to capture that you liked a given song. Then, it was easy to capture the sequence in which you and others would listen to that song, and therefore you could capture the correlations. With this data, you could do a lot.
However, as we soon found out, fashion has its own challenges. There is not an easy way to match an outfit to a shoppable product (think about most garments in your wardrobe, most likely you won’t find a link to view/buy those garments online, something you can do for many other products you have at home). Another challenge: the industry is not capturing how people describe clothes or outfits, so there is a strong disconnect between many ecommerces and its shoppers (we think we’ve solved that problem. Also Similar.ai and Twiggle are working on it). Another challenge: style is complex to capture and classify by a machine.
Now, deep learning brings a new tool to add to other mechanisms, and changes everything. Owning the correct data set allows us to focus on the specific narrow use cases related to outfit recommendations, and to focus on delivering value through the algorithms instead of spending time collecting and cleaning data. 👉 Now comes the fun and rewarding part, so please email us if you want to join the team and help build algorithms that have real impact on people — we are 100% remote, Slack based 👈 -😂😂😉 😉 😉. People’s very personal style can become as actionable as metadata and possibly as transparent as well (?), and I think we can see the path to get there. As we have a consumer product that people already love, we can ship early results of these algorithms partially hidden, and increase their presence as feedback improves results.
There are more and more researchers working of these areas, you can read Tangseng’s paper on recommending outfits from personal closet or clothing parsing project, or how Edgar Simo-Serra defines similarity between images using user-provided metadata.
Outfits are a key asset in the race to capture the $123 billion US apparel market. Data is also the reason many players are taking outfits to the forefront of technology: outfits are a daily habit, and have proven to be great assets to attract and retain shoppers, and capture their data. Many players are introducing a Shop the Look section with outfits from real people: Amazon, Zalando or Google are a few examples.
Google recently introduced a new feature called Style Ideas showing how a “product can be worn in real life”. Same month Amazon launched its Alexa Echo Look to help you with your outfit, and Alibaba’s artificial intelligence personal stylist helped them achieve record sales during Singles Day.
Some people think that fashion data is in the same place as music data was in 2003: ready to play a very relevant role. The good news is: the daily habit of deciding what to wear will not change. The need to buy new clothes won’t disappear, either.
So, what do you think? Where will we be 10 years from now? Will taste data build unique online experiences? What role will outfits play? How will machine learning change fashion ecommerce? Will everything change, 10 years from now?
We are a small team of eight, four on product and four engineers. We believe in focusing on our very specific problem, no one on earth can understand the problem better than us. We also believe on building the complete solution ourselves while doing as few things as possible. We work 100% remote and live in Slack + GitHub. You can learn more about our machine learning approach, here.
If you are a deep learning engineer or a product manager in the fashion space, and want to chat & temporarily access our Social Fashion Graph, please email us describing your work. You can also download our iOS and Android apps, or simply say hi: hi at chicisimo.com.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder & CEO at @chicisimo. Machine learning to automate outfit advise. World’s largest outfits app
how hackers start their afternoons.
",three years ago launched chicisimo goal offer automated outfit advice today 4 million women app want share data machine learning approach helped us grow chaotic control wanted build humanlevel tool offer automated outfit advice needed understand peoples fashion taste friend give us outfit advice seeing normally wear shes learnt style could build system learns fashion taste previous experience tastebased projects background machine learning applied music sectors saw collaborative filtering tool transformed music industry blindness totally understanding people check audioscrobbler story also made life better love music created several unicorns along way background built following thesis online fashion transformed tool understands taste understand taste delight people relevant content meaningful experience also thought outfits asset would allow taste understood learn people wear closet style us like decided going build tool understand taste focused developing correct dataset built two assets mobile app data platform previous experience building mobile products even symbian back knew easy bring people app difficult retain focused small iterations learn fast possible launched extremely early alpha chicisimo one key functionality launched another name another country couldnt even upload photos allowed us iterate real data get lot qualitative input point launched real chicisimo removed alpha app store spent long time trying understand true levers retention algorithms needed order match content people three things helped retention identify retention levers using behavioral cohorts use mixpanel run cohorts actions people performed also value received hard conceptualize app chicisimo thought terms specific measurable value people received measured run cohorts events able iterate value received actions people performed also defined removed antilevers noisy things distract main value got relevant metrics different time periods first session first day first week etc super specific metrics allowed us iterate nir eyals book hooked build habitforming products discusses framework create habits helped us build model b rethink onboarding process knew levers retention define process new signups find value app soon possible lose clearly articulated needed happen went something like people dont action first 7 minutes first session come back need change experience make happen also run tons usertests different types people observed perceived mostly didnt retention lever c define learn data approach described key much data building product people love case first think whattowear problem important one solve truly respect obsess understanding problem understanding solution helping way showing respect leads one surprising aspects imo building product fact regularly access new corpuses knowledge help us improve product significantly weve obtained gamechanging learnings always focusing two aspects people relate problem people relate product red arrows image million subtleties happen two relations building chicisimo trying understand know point something important dont know therefore question always learn sooner talking one colleagues told data people truth day one weve learnt significantly conversations women relate problem solutions use several mechanisms face face conversations reading emails get women without predefined questions asking feedback around specific topics use typeform great tool product insight talk among try articulate learnings also seek external references talk product people play inspiring apps reread articles help us think process allows us learn build product develop technology point lucky get noticed app store team weve featured app day throughout world view apples description chicisimo december 31st chicisimo featured summary apps app store team pink c left image app got viewed 957437 uniques thanks feature total 13m times case app features 05 conversion rate impression app install normally impression product page view install aso 3 conversion referrers 45 app aims understanding taste better job suggesting outfit ideas simple act delivering right content right time absolutely wow people although extremely difficult utility build chicisimo content 100 usergenerated poses challenges system needs classify different types content automatically build right incentives understand match content needs soon saw lot data coming thinking hey cool look data realized actually nightmare chaotic data wasnt actionable wasnt cool decided start giving structure parts data ended inventing called social fashion graph graph compact representation needs outfits people interrelate concept helped us build data platform data platform creates highquality dataset linked learning training world app therefore improves new expression taste thought outfits playlists outfit combination items makes sense consume together using collaborative filtering relations captured allow us offer recommendations different areas app still lot noise data one hardest things understand people expressing fashion need different ways made matching content needs even difficult lots people might need ideas go school express specific need hundred different ways capture diversity provide structure built system collect concepts call needs captured equivalences among different ways express need ended building list worlds whattowear needs call ontology really cleaned dataset helped us understand understanding led better product decisions understand outfit need person lot understandable data attached allow people express freely app right system behind platform structuring data gave us control encouraging unstructured data gave us knowledge flexibility end result current system system learns meaning outfit respond need taste individual wouldnt even dare saying day 1 us screenshot internal tool amount work front us immense feel things control one new areas weve working adding fourth element social fashion graph shoppable products system match outfits products automatically help people decide buy next pretty exciting back built recommender systems music products pretty easy thats think obviously didnt think time first easy capture liked given song easy capture sequence others would listen song therefore could capture correlations data could lot however soon found fashion challenges easy way match outfit shoppable product think garments wardrobe likely wont find link viewbuy garments online something many products home another challenge industry capturing people describe clothes outfits strong disconnect many ecommerces shoppers think weve solved problem also similarai twiggle working another challenge style complex capture classify machine deep learning brings new tool add mechanisms changes everything owning correct data set allows us focus specific narrow use cases related outfit recommendations focus delivering value algorithms instead spending time collecting cleaning data comes fun rewarding part please email us want join team help build algorithms real impact people 100 remote slack based peoples personal style become actionable metadata possibly transparent well think see path get consumer product people already love ship early results algorithms partially hidden increase presence feedback improves results researchers working areas read tangsengs paper recommending outfits personal closet clothing parsing project edgar simoserra defines similarity images using userprovided metadata outfits key asset race capture 123 billion us apparel market data also reason many players taking outfits forefront technology outfits daily habit proven great assets attract retain shoppers capture data many players introducing shop look section outfits real people amazon zalando google examples google recently introduced new feature called style ideas showing product worn real life month amazon launched alexa echo look help outfit alibabas artificial intelligence personal stylist helped achieve record sales singles day people think fashion data place music data 2003 ready play relevant role good news daily habit deciding wear change need buy new clothes wont disappear either think 10 years taste data build unique online experiences role outfits play machine learning change fashion ecommerce everything change 10 years small team eight four product four engineers believe focusing specific problem one earth understand problem better us also believe building complete solution things possible work 100 remote live slack github learn machine learning approach deep learning engineer product manager fashion space want chat temporarily access social fashion graph please email us describing work also download ios android apps simply say hi hi chicisimocom quick cheer standing ovation clap show much enjoyed story founder ceo chicisimo machine learning automate outfit advise worlds largest outfits app hackers start afternoons,en,"['Audioscrobbler', 'Typeform', 'App Store', 'Apple', 'ASO', 'Social Fashion Graph', 'app', 'Slack', 'Amazon', 'Google', 'Alibaba', 'Android', 'Founder & CEO']"
134,Sarthak Jain,3900,How to easily Detect Objects with Deep Learning on Raspberry Pi,"Disclaimer: I’m building nanonets.com to help build ML with less data and no hardware
The raspberry pi is a neat piece of hardware that has captured the hearts of a generation with ~15M devices sold, with hackers building even cooler projects on it. Given the popularity of Deep Learning and the Raspberry Pi Camera we thought it would be nice if we could detect any object using Deep Learning on the Pi.
Now you will be able to detect a photobomber in your selfie, someone entering Harambe’s cage, where someone kept the Sriracha or an Amazon delivery guy entering your house.
20M years of evolution have made human vision fairly evolved. The human brain has 30% of it’s Neurons work on processing vision (as compared with 8 percent for touch and just 3 percent for hearing). Humans have two major advantages when compared with machines. One is stereoscopic vision, the second is an almost infinite supply of training data (an infant of 5 years has had approximately 2.7B Images sampled at 30fps).
To mimic human level performance scientists broke down the visual perception task into four different categories.
Object detection has been good enough for a variety of applications (even though image segmentation is a much more precise result, it suffers from the complexity of creating training data. It typically takes a human annotator 12x more time to segment an image than draw bounding boxes; this is more anecdotal and lacks a source). Also, after detecting objects, it is separately possible to segment the object from the bounding box.
Object detection is of significant practical importance and has been used across a variety of industries. Some of the examples are mentioned below:
Object Detection can be used to answer a variety of questions. These are the broad categories:
There are a variety of models/architectures that are used for object detection. Each with trade-offs between speed, size, and accuracy. We picked one of the most popular ones: YOLO (You only look once). and have shown how it works below in under 20 lines of code (if you ignore the comments).
Note: This is pseudo code, not intended to be a working example. It has a black box which is the CNN part of it which is fairly standard and shown in the image below.
You can read the full paper here: https://pjreddie.com/media/files/papers/yolo_1.pdf
For this task, you probably need a few 100 Images per Object. Try to capture data as close to the data you’re going to finally make predictions on.
Draw bounding boxes on the images. You can use a tool like labelImg. You will typically need a few people who will be working on annotating your images. This is a fairly intensive and time consuming task.
You can read more about this at medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab. You need a pretrained model so you can reduce the amount of data required to train. Without it, you might need a few 100k images to train the model.
You can find a bunch of pretrained models here
The process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train.
To start training the model you can run:
The docker image has a run.sh script that can be called with the following parameters
You can find more details at:
To train a model you need to select the right hyper parameters.
Finding the right parameters
The art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. There is some level of black magic associated with this, along with a little bit of theory. This is a great resource for finding the right parameters.
Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)
Small devices like Mobile Phones and Rasberry PI have very little memory and computation power.
Training neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work (though there are research efforts to use quantized representations here too).
Taking a pre-trained model and running inference is very different. One of the magical qualities of Deep Neural Networks is that they tend to cope very well with high levels of noise in their inputs.
Why Quantize?
Neural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model.
The Nodes and Weights of a neural network are originally stored as 32-bit floating point numbers. The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer.The size of the files is reduced by 75%.
Code for Quantization:
You need the Raspberry Pi camera live and working. Then capture a new Image
For instructions on how to install checkout this link
Download Model
Once your done training the model you can download it on to your pi. To export the model run:
Then download the model onto the Raspberry Pi.
Install TensorFlow on the Raspberry Pi
Depending on your device you might need to change the installation a little
Run model for predicting on the new Image
The Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Therefore, it is important to benchmark how much time do each of the models take to make a prediction on a new image.
We have removed the need to annotate Images, we have expert annotators who will annotate your images for you.
We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your data
NanoNets is entirely in the cloud and runs without using any of your hardware. Which makes it much easier to use.
Since devices like the Raspberry Pi and mobile phones were not built to run complex compute heavy tasks, you can outsource the workload to our cloud which does all of the compute for you
Get your free API Key from http://app.nanonets.com/user/api_key
Collect the images of object you want to detect. You can annotate them either using our web UI (https://app.nanonets.com/ObjectAnnotation/?appId=YOUR_MODEL_ID) or use open source tool like labelImg. Once you have dataset ready in folders, images (image files) and annotations (annotations for the image files), start uploading the dataset.
Once the Images have been uploaded, begin training the Model
The model takes ~2 hours to train. You will get an email once the model is trained. In the meanwhile you check the state of the model
Once the model is trained. You can make predictions using the model
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder & CEO @ NanoNets.com
NanoNets: Machine Learning API
",disclaimer im building nanonetscom help build ml less data hardware raspberry pi neat piece hardware captured hearts generation 15m devices sold hackers building even cooler projects given popularity deep learning raspberry pi camera thought would nice could detect object using deep learning pi able detect photobomber selfie someone entering harambes cage someone kept sriracha amazon delivery guy entering house 20m years evolution made human vision fairly evolved human brain 30 neurons work processing vision compared 8 percent touch 3 percent hearing humans two major advantages compared machines one stereoscopic vision second almost infinite supply training data infant 5 years approximately 27b images sampled 30fps mimic human level performance scientists broke visual perception task four different categories object detection good enough variety applications even though image segmentation much precise result suffers complexity creating training data typically takes human annotator 12x time segment image draw bounding boxes anecdotal lacks source also detecting objects separately possible segment object bounding box object detection significant practical importance used across variety industries examples mentioned object detection used answer variety questions broad categories variety modelsarchitectures used object detection tradeoffs speed size accuracy picked one popular ones yolo look shown works 20 lines code ignore comments note pseudo code intended working example black box cnn part fairly standard shown image read full paper httpspjreddiecommediafilespapersyolo_1pdf task probably need 100 images per object try capture data close data youre going finally make predictions draw bounding boxes images use tool like labelimg typically need people working annotating images fairly intensive time consuming task read mediumcomnanonetsnanonetshowtousedeeplearningwhenyouhavelimiteddataf68c0b512cab need pretrained model reduce amount data required train without might need 100k images train model find bunch pretrained models process training model unnecessarily difficult simplify process created docker image would make easy train start training model run docker image runsh script called following parameters find details train model need select right hyper parameters finding right parameters art deep learning involves little bit hit try figure best parameters get highest accuracy model level black magic associated along little bit theory great resource finding right parameters quantize model make smaller fit small device like raspberry pi mobile small devices like mobile phones rasberry pi little memory computation power training neural networks done applying many tiny nudges weights small increments typically need floating point precision work though research efforts use quantized representations taking pretrained model running inference different one magical qualities deep neural networks tend cope well high levels noise inputs quantize neural network models take lot space disk original alexnet 200 mb float format example almost size taken weights neural connections since often many millions single model nodes weights neural network originally stored 32bit floating point numbers simplest motivation quantization shrink file sizes storing min max layer compressing float value eightbit integerthe size files reduced 75 code quantization need raspberry pi camera live working capture new image instructions install checkout link download model done training model download pi export model run download model onto raspberry pi install tensorflow raspberry pi depending device might need change installation little run model predicting new image raspberry pi constraints memory compute version tensorflow compatible raspberry pi gpu still available therefore important benchmark much time models take make prediction new image removed need annotate images expert annotators annotate images automatically train best model achieve run battery model different parameters select best data nanonets entirely cloud runs without using hardware makes much easier use since devices like raspberry pi mobile phones built run complex compute heavy tasks outsource workload cloud compute get free api key httpappnanonetscomuserapi_key collect images object want detect annotate either using web ui httpsappnanonetscomobjectannotationappidyour_model_id use open source tool like labelimg dataset ready folders images image files annotations annotations image files start uploading dataset images uploaded begin training model model takes 2 hours train get email model trained meanwhile check state model model trained make predictions using model quick cheer standing ovation clap show much enjoyed story founder ceo nanonetscom nanonets machine learning api,en,"['nanonets.com', 'Amazon', 'Images', 'CNN', 'the Raspberry Pi', 'Mobile Phones', 'Deep Neural Networks', 'AlexNet', 'Image', 'NanoNets', 'Founder & CEO']"
135,Emil Wallner,9100,How you can train an AI to convert your design mockups into HTML and CSS,"Within three years, deep learning will change front-end development. It will increase prototyping speed and lower the barrier for building software.
The field took off last year when Tony Beltramelli introduced the pix2code paper and Airbnb launched sketch2code.
Currently, the largest barrier to automating front-end development is computing power. However, we can use current deep learning algorithms, along with synthesized training data, to start exploring artificial front-end automation right now.
In this post, we’ll teach a neural network how to code a basic a HTML and CSS website based on a picture of a design mockup. Here’s a quick overview of the process:
We’ll build the neural network in three iterations.
First, we’ll make a bare minimum version to get a hang of the moving parts. The second version, HTML, will focus on automating all the steps and explaining the neural network layers. In the final version, Bootstrap, we’ll create a model that can generalize and explore the LSTM layer.
All the code is prepared on GitHub and FloydHub in Jupyter notebooks. All the FloydHub notebooks are inside the floydhub directory and the local equivalents are under local.
The models are based on Beltramelli‘s pix2code paper and Jason Brownlee’s image caption tutorials. The code is written in Python and Keras, a framework on top of TensorFlow.
If you’re new to deep learning, I’d recommend getting a feel for Python, backpropagation, and convolutional neural networks. My three earlier posts on FloydHub’s blog will get you started:
Let’s recap our goal. We want to build a neural network that will generate HTML/CSS markup that corresponds to a screenshot.
When you train the neural network, you give it several screenshots with matching HTML.
It learns by predicting all the matching HTML markup tags one by one. When it predicts the next markup tag, it receives the screenshot as well as all the correct markup tags until that point.
Here is a simple training data example in a Google Sheet.
Creating a model that predicts word by word is the most common approach today. There are other approaches, but that’s the method we’ll use throughout this tutorial.
Notice that for each prediction it gets the same screenshot. So if it has to predict 20 words, it will get the same design mockup twenty times. For now, don’t worry about how the neural network works. Focus on grasping the input and output of the neural network.
Let’s focus on the previous markup. Say we train the network to predict the sentence “I can code.” When it receives “I,” then it predicts “can.” Next time it will receive “I can” and predict “code.” It receives all the previous words and only has to predict the next word.
The neural network creates features from the data. The network builds features to link the input data with the output data. It has to create representations to understand what is in each screenshot, the HTML syntax, that it has predicted. This builds the knowledge to predict the next tag.
When you want to use the trained model for real-world usage, it’s similar to when you train the model. The text is generated one by one with the same screenshot each time. Instead of feeding it with the correct HTML tags, it receives the markup it has generated so far. Then, it predicts the next markup tag. The prediction is initiated with a “start tag” and stops when it predicts an “end tag” or reaches a max limit. Here’s another example in a Google Sheet.
Let’s build a “hello world” version. We’ll feed a neural network a screenshot with a website displaying “Hello World!” and teach it to generate the markup.
First, the neural network maps the design mockup into a list of pixel values. From 0–255 in three channels — red, blue, and green.
To represent the markup in a way that the neural network understands, I use one hot encoding. Thus, the sentence “I can code” could be mapped like the below.
In the above graphic, we include the start and end tag. These tags are cues for when the network starts its predictions and when to stop.
For the input data, we will use sentences, starting with the first word and then adding each word one by one. The output data is always one word.
Sentences follow the same logic as words. They also need the same input length. Instead of being capped by the vocabulary, they are bound by maximum sentence length. If it’s shorter than the maximum length, you fill it up with empty words, a word with just zeros.
As you see, words are printed from right to left. This forces each word to change position for each training round. This allows the model to learn the sequence instead of memorizing the position of each word.
In the below graphic there are four predictions. Each row is one prediction. To the left are the images represented in their three color channels: red, green and blue and the previous words. Outside of the brackets are the predictions one by one, ending with a red square to mark the end.
In the hello world version, we use three tokens: start, <HTML><center><H1>Hello World!</H1></center></HTML> and end. A token can be anything. It can be a character, word, or sentence. Character versions require a smaller vocabulary but constrain the neural network. Word level tokens tend to perform best.
Here we make the prediction:
FloydHub is a training platform for deep learning. I came across them when I first started learning deep learning and I’ve used them since for training and managing my deep learning experiments. You can install it and run your first model within 10 minutes. It’s hands down the best option to run models on cloud GPUs.
If you are new to FloydHub, do their 2-min installation or my 5-minute walkthrough.
All the notebooks are prepared inside the FloydHub directory. The local equivalents are under local. Once it’s running, you can find the first notebook here: floydhub/Helloworld/helloworld.ipynb .
If you want more detailed instructions and an explanation for the flags, check my earlier post.
In this version, we’ll automate many of the steps from the Hello World model. This section will focus on creating a scalable implementation and the moving pieces in the neural network.
This version will not be able to predict HTML from random websites, but it’s still a great setup to explore the dynamics of the problem.
If we expand the components of the previous graphic it looks like this.
There are two major sections. First, the encoder. This is where we create image features and previous markup features. Features are the building blocks that the network creates to connect the design mockups with the markup. At the end of the encoder, we glue the image features to each word in the previous markup.
The decoder then takes the combined design and markup feature and creates a next tag feature. This feature is run through a fully connected neural network to predict the next tag.
Since we need to insert one screenshot for each word, this becomes a bottleneck when training the network (example). Instead of using the images, we extract the information we need to generate the markup.
The information is encoded into image features. This is done by using an already pre-trained convolutional neural network (CNN). The model is pre-trained on Imagenet.
We extract the features from the layer before the final classification.
We end up with 1536 eight by eight pixel images known as features. Although they are hard to understand for us, a neural network can extract the objects and position of the elements from these features.
In the hello world version, we used a one-hot encoding to represent the markup. In this version, we’ll use a word embedding for the input and keep the one-hot encoding for the output.
The way we structure each sentence stays the same, but how we map each token is changed. One-hot encoding treats each word as an isolated unit. Instead, we convert each word in the input data to lists of digits. These represent the relationship between the markup tags.
The dimension of this word embedding is eight but often varies between 50–500 depending on the size of the vocabulary.
The eight digits for each word are weights similar to a vanilla neural network. They are tuned to map how the words relate to each other (Mikolov et al., 2013).
This is how we start developing markup features. Features are what the neural network develops to link the input data with the output data. For now, don’t worry about what they are, we’ll dig deeper into this in the next section.
We’ll take the word embeddings and run them through an LSTM and return a sequence of markup features. These are run through a Time distributed dense layer — think of it as a dense layer with multiple inputs and outputs.
In parallel, the image features are first flattened. Regardless of how the digits were structured, they are transformed into one large list of numbers. Then we apply a dense layer on this layer to form a high-level feature. These image features are then concatenated to the markup features.
This can be hard to wrap your mind around — so let’s break it down.
Here we run the word embeddings through the LSTM layer. In this graphic, all the sentences are padded to reach the maximum size of three tokens.
To mix signals and find higher-level patterns, we apply a TimeDistributed dense layer to the markup features. TimeDistributed dense is the same as a dense layer, but with multiple inputs and outputs.
In parallel, we prepare the images. We take all the mini image features and transform them into one long list. The information is not changed, just reorganized.
Again, to mix signals and extract higher level notions, we apply a dense layer. Since we are only dealing with one input value, we can use a normal dense layer. To connect the image features to the markup features, we copy the image features.
In this case, we have three markup features. Thus, we end up with an equal amount of image features and markup features.
All the sentences are padded to create three markup features. Since we have prepared the image features, we can now add one image feature for each markup feature.
After sticking one image feature to each markup feature, we end up with three image-markup features. This is the input we feed into the decoder.
Here we use the combined image-markup features to predict the next tag.
In the below example, we use three image-markup feature pairs and output one next tag feature.
Note that the LSTM layer has the sequence set to false. Instead of returning the length of the input sequence, it only predicts one feature. In our case, it’s a feature for the next tag. It contains the information for the final prediction.
The dense layer works like a traditional feedforward neural network. It connects the 512 digits in the next tag feature with the 4 final predictions. Say we have 4 words in our vocabulary: start, hello, world, and end.
The vocabulary prediction could be [0.1, 0.1, 0.1, 0.7]. The softmax activation in the dense layer distributes a probability from 0–1, with the sum of all predictions equal to 1. In this case, it predicts that the 4th word is the next tag. Then you translate the one-hot encoding [0, 0, 0, 1] into the mapped value, say “end”.
If you can’t see anything when you click these links, you can right click and click on “View Page Source.” Here is the original website for reference.
In our final version, we’ll use a dataset of generated bootstrap websites from the pix2code paper. By using Twitter’s bootstrap, we can combine HTML and CSS and decrease the size of the vocabulary.
We’ll enable it to generate the markup for a screenshot it has not seen before. We’ll also dig into how it builds knowledge about the screenshot and markup.
Instead of training it on the bootstrap markup, we’ll use 17 simplified tokens that we then translate into HTML and CSS. The dataset includes 1500 test screenshots and 250 validation images. For each screenshot there are on average 65 tokens, resulting in 96925 training examples.
By tweaking the model in the pix2code paper, the model can predict the web components with 97% accuracy (BLEU 4-ngram greedy search, more on this later).
Extracting features from pre-trained models works well in image captioning models. But after a few experiments, I realized that pix2code’s end-to-end approach works better for this problem. The pre-trained models have not been trained on web data and are customized for classification.
In this model, we replace the pre-trained image features with a light convolutional neural network. Instead of using max-pooling to increase information density, we increase the strides. This maintains the position and the color of the front-end elements.
There are two core models that enable this: convolutional neural networks (CNN) and recurrent neural networks (RNN). The most common recurrent neural network is long-short term memory (LSTM), so that’s what I’ll refer to.
There are plenty of great CNN tutorials, and I covered them in my previous article. Here, I’ll focus on the LSTMs.
One of the harder things to grasp about LSTMs is timesteps. A vanilla neural network can be thought of as two timesteps. If you give it “Hello,” it predicts “World.” But it would struggle to predict more timesteps. In the below example, the input has four timesteps, one for each word.
LSTMs are made for input with timesteps. It’s a neural network customized for information in order. If you unroll our model it looks like this. For each downward step, you keep the same weights. You apply one set of weights to the previous output and another set to the new input.
The weighted input and output are concatenated and added together with an activation. This is the output for that timestep. Since we reuse the weights, they draw information from several inputs and build knowledge of the sequence.
Here is a simplified version of the process for each timestep in an LSTM.
To get a feel for this logic, I’d recommend building an RNN from scratch with Andrew Trask’s brilliant tutorial.
The number of units in each LSTM layer determines it’s ability to memorize. This also corresponds to the size of each output feature. Again, a feature is a long list of numbers used to transfer information between layers.
Each unit in the LSTM layer learns to keep track of different aspects of the syntax. Below is a visualization of a unit that keeps tracks of the information in the row div. This is the simplified markup we are using to train the bootstrap model.
Each LSTM unit maintains a cell state. Think of the cell state as the memory. The weights and activations are used to modify the state in different ways. This enables the LSTM layers to fine-tune which information to keep and discard for each input.
In addition to passing through an output feature for each input, it also forwards the cell states, one value for each unit in the LSTM. To get a feel for how the components within the LSTM interact, I recommend Colah’s tutorial, Jayasiri’s Numpy implementation, and Karphay’s lecture and write-up.
It’s tricky to find a fair way to measure the accuracy. Say you compare word by word. If your prediction is one word out of sync, you might have 0% accuracy. If you remove one word which syncs the prediction, you might end up with 99/100.
I used the BLEU score, best practice in machine translating and image captioning models. It breaks the sentence into four n-grams, from 1–4 word sequences. In the below prediction “cat” is supposed to be “code.”
To get the final score, you multiply each score with 25%, (4/5) * 0.25 + (2/4) * 0.25 + (1/3) * 0.25 + (0/2) * 0.25 = 0.2 + 0.125 + 0.083 + 0 = 0.408 . The sum is then multiplied with a sentence length penalty. Since the length is correct in our example, it becomes our final score.
You could increase the number of n-grams to make it harder. A four n-gram model is the model that best corresponds to human translations. I’d recommend running a few examples with the below code and reading the wiki page.
Links to sample output
Front-end development is an ideal space to apply deep learning. It’s easy to generate data, and the current deep learning algorithms can map most of the logic.
One of the most exciting areas is applying attention to LSTMs. This will not just improve the accuracy, but enable us to visualize where the CNN puts its focus as it generates the markup.
Attention is also key for communicating between markup, stylesheets, scripts and eventually the backend. Attention layers can keep track of variables, enabling the network to communicate between programming languages.
But in the near feature, the biggest impact will come from building a scalable way to synthesize data. Then you can add fonts, colors, words, and animations step-by-step.
So far, most progress is happening in taking sketches and turning them into template apps. In less then two years, we’ll be able to draw an app on paper and have the corresponding front-end in less than a second. There are already two working prototypes built by Airbnb’s design team and Uizard.
Here are some experiments to get started.
Getting started
Further experiments
Huge thanks to Tony Beltramelli and Jon Gold for their research and ideas, and for answering questions. Thanks to Jason Brownlee for his stellar Keras tutorials (I included a few snippets from his tutorial in the core Keras implementation), and Beltramelli for providing the data. Also thanks to Qingping Hou, Charlie Harrington, Sai Soundararaj, Jannes Klaas, Claudio Cabral, Alain Demenet and Dylan Djian for reading drafts of this.
This the fourth part of a multi-part blog series from Emil as he learns deep learning. Emil has spent a decade exploring human learning. He’s worked for Oxford’s business school, invested in education startups, and built an education technology business. Last year, he enrolled at Ecole 42 to apply his knowledge of human learning to machine learning.
If you build something or get stuck, ping me below or on twitter: emilwallner. I’d love to see what you are building.
This was first published as a community post on Floydhub’s blog.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I study CS at 42 Paris, blog, and experiment with deep learning.
Our community publishes stories worth reading on development, design, and data science.
",within three years deep learning change frontend development increase prototyping speed lower barrier building software field took last year tony beltramelli introduced pix2code paper airbnb launched sketch2code currently largest barrier automating frontend development computing power however use current deep learning algorithms along synthesized training data start exploring artificial frontend automation right post well teach neural network code basic html css website based picture design mockup heres quick overview process well build neural network three iterations first well make bare minimum version get hang moving parts second version html focus automating steps explaining neural network layers final version bootstrap well create model generalize explore lstm layer code prepared github floydhub jupyter notebooks floydhub notebooks inside floydhub directory local equivalents local models based beltramellis pix2code paper jason brownlees image caption tutorials code written python keras framework top tensorflow youre new deep learning id recommend getting feel python backpropagation convolutional neural networks three earlier posts floydhubs blog get started lets recap goal want build neural network generate htmlcss markup corresponds screenshot train neural network give several screenshots matching html learns predicting matching html markup tags one one predicts next markup tag receives screenshot well correct markup tags point simple training data example google sheet creating model predicts word word common approach today approaches thats method well use throughout tutorial notice prediction gets screenshot predict 20 words get design mockup twenty times dont worry neural network works focus grasping input output neural network lets focus previous markup say train network predict sentence code receives predicts next time receive predict code receives previous words predict next word neural network creates features data network builds features link input data output data create representations understand screenshot html syntax predicted builds knowledge predict next tag want use trained model realworld usage similar train model text generated one one screenshot time instead feeding correct html tags receives markup generated far predicts next markup tag prediction initiated start tag stops predicts end tag reaches max limit heres another example google sheet lets build hello world version well feed neural network screenshot website displaying hello world teach generate markup first neural network maps design mockup list pixel values 0255 three channels red blue green represent markup way neural network understands use one hot encoding thus sentence code could mapped like graphic include start end tag tags cues network starts predictions stop input data use sentences starting first word adding word one one output data always one word sentences follow logic words also need input length instead capped vocabulary bound maximum sentence length shorter maximum length fill empty words word zeros see words printed right left forces word change position training round allows model learn sequence instead memorizing position word graphic four predictions row one prediction left images represented three color channels red green blue previous words outside brackets predictions one one ending red square mark end hello world version use three tokens start htmlcenterh1hello worldh1centerhtml end token anything character word sentence character versions require smaller vocabulary constrain neural network word level tokens tend perform best make prediction floydhub training platform deep learning came across first started learning deep learning ive used since training managing deep learning experiments install run first model within 10 minutes hands best option run models cloud gpus new floydhub 2min installation 5minute walkthrough notebooks prepared inside floydhub directory local equivalents local running find first notebook floydhubhelloworldhelloworldipynb want detailed instructions explanation flags check earlier post version well automate many steps hello world model section focus creating scalable implementation moving pieces neural network version able predict html random websites still great setup explore dynamics problem expand components previous graphic looks like two major sections first encoder create image features previous markup features features building blocks network creates connect design mockups markup end encoder glue image features word previous markup decoder takes combined design markup feature creates next tag feature feature run fully connected neural network predict next tag since need insert one screenshot word becomes bottleneck training network example instead using images extract information need generate markup information encoded image features done using already pretrained convolutional neural network cnn model pretrained imagenet extract features layer final classification end 1536 eight eight pixel images known features although hard understand us neural network extract objects position elements features hello world version used onehot encoding represent markup version well use word embedding input keep onehot encoding output way structure sentence stays map token changed onehot encoding treats word isolated unit instead convert word input data lists digits represent relationship markup tags dimension word embedding eight often varies 50500 depending size vocabulary eight digits word weights similar vanilla neural network tuned map words relate mikolov et al 2013 start developing markup features features neural network develops link input data output data dont worry well dig deeper next section well take word embeddings run lstm return sequence markup features run time distributed dense layer think dense layer multiple inputs outputs parallel image features first flattened regardless digits structured transformed one large list numbers apply dense layer layer form highlevel feature image features concatenated markup features hard wrap mind around lets break run word embeddings lstm layer graphic sentences padded reach maximum size three tokens mix signals find higherlevel patterns apply timedistributed dense layer markup features timedistributed dense dense layer multiple inputs outputs parallel prepare images take mini image features transform one long list information changed reorganized mix signals extract higher level notions apply dense layer since dealing one input value use normal dense layer connect image features markup features copy image features case three markup features thus end equal amount image features markup features sentences padded create three markup features since prepared image features add one image feature markup feature sticking one image feature markup feature end three imagemarkup features input feed decoder use combined imagemarkup features predict next tag example use three imagemarkup feature pairs output one next tag feature note lstm layer sequence set false instead returning length input sequence predicts one feature case feature next tag contains information final prediction dense layer works like traditional feedforward neural network connects 512 digits next tag feature 4 final predictions say 4 words vocabulary start hello world end vocabulary prediction could 01 01 01 07 softmax activation dense layer distributes probability 01 sum predictions equal 1 case predicts 4th word next tag translate onehot encoding 0 0 0 1 mapped value say end cant see anything click links right click click view page source original website reference final version well use dataset generated bootstrap websites pix2code paper using twitters bootstrap combine html css decrease size vocabulary well enable generate markup screenshot seen well also dig builds knowledge screenshot markup instead training bootstrap markup well use 17 simplified tokens translate html css dataset includes 1500 test screenshots 250 validation images screenshot average 65 tokens resulting 96925 training examples tweaking model pix2code paper model predict web components 97 accuracy bleu 4ngram greedy search later extracting features pretrained models works well image captioning models experiments realized pix2codes endtoend approach works better problem pretrained models trained web data customized classification model replace pretrained image features light convolutional neural network instead using maxpooling increase information density increase strides maintains position color frontend elements two core models enable convolutional neural networks cnn recurrent neural networks rnn common recurrent neural network longshort term memory lstm thats ill refer plenty great cnn tutorials covered previous article ill focus lstms one harder things grasp lstms timesteps vanilla neural network thought two timesteps give hello predicts world would struggle predict timesteps example input four timesteps one word lstms made input timesteps neural network customized information order unroll model looks like downward step keep weights apply one set weights previous output another set new input weighted input output concatenated added together activation output timestep since reuse weights draw information several inputs build knowledge sequence simplified version process timestep lstm get feel logic id recommend building rnn scratch andrew trasks brilliant tutorial number units lstm layer determines ability memorize also corresponds size output feature feature long list numbers used transfer information layers unit lstm layer learns keep track different aspects syntax visualization unit keeps tracks information row div simplified markup using train bootstrap model lstm unit maintains cell state think cell state memory weights activations used modify state different ways enables lstm layers finetune information keep discard input addition passing output feature input also forwards cell states one value unit lstm get feel components within lstm interact recommend colahs tutorial jayasiris numpy implementation karphays lecture writeup tricky find fair way measure accuracy say compare word word prediction one word sync might 0 accuracy remove one word syncs prediction might end 99100 used bleu score best practice machine translating image captioning models breaks sentence four ngrams 14 word sequences prediction cat supposed code get final score multiply score 25 45 025 24 025 13 025 02 025 02 0125 0083 0 0408 sum multiplied sentence length penalty since length correct example becomes final score could increase number ngrams make harder four ngram model model best corresponds human translations id recommend running examples code reading wiki page links sample output frontend development ideal space apply deep learning easy generate data current deep learning algorithms map logic one exciting areas applying attention lstms improve accuracy enable us visualize cnn puts focus generates markup attention also key communicating markup stylesheets scripts eventually backend attention layers keep track variables enabling network communicate programming languages near feature biggest impact come building scalable way synthesize data add fonts colors words animations stepbystep far progress happening taking sketches turning template apps less two years well able draw app paper corresponding frontend less second already two working prototypes built airbnbs design team uizard experiments get started getting started experiments huge thanks tony beltramelli jon gold research ideas answering questions thanks jason brownlee stellar keras tutorials included snippets tutorial core keras implementation beltramelli providing data also thanks qingping hou charlie harrington sai soundararaj jannes klaas claudio cabral alain demenet dylan djian reading drafts fourth part multipart blog series emil learns deep learning emil spent decade exploring human learning hes worked oxfords business school invested education startups built education technology business last year enrolled ecole 42 apply knowledge human learning machine learning build something get stuck ping twitter emilwallner id love see building first published community post floydhubs blog quick cheer standing ovation clap show much enjoyed story study cs 42 paris blog experiment deep learning community publishes stories worth reading development design data science,en,"['pix2code', 'GitHub', 'FloydHub', 'Jupyter', 'Beltramelli‘s pix2code', 'Keras', 'Hello World', 'CNN', 'Time', 'TimeDistributed', 'BLEU', 'Karphay', 'Attention', 'Beltramelli', 'Sai Soundararaj', 'Oxford', 'Floydhub', 'CS']"
136,James JD Sutton,2200,What is “Q” from a laymen... – Coinmonks – Medium,"A bit long, but I think it might help people understand Qubic a bit.
Two takeaways I took from reading Qubic: (Rev_02)
Take Away One:
1. If you host a “Q-Node”, a node that supports the Q protocol (layer) you can earn rewards in these manners: Offering PoW (mining rigs, computer, or your coffee pot), PoS (your IOTA’s that you hold), your bandwidth that you don’t use (probably something to do with LIFI in the future, so this could be your router and lightbulbs in your house), and simply, the previous history of running an honest node for the system.
All of the above can be used to pass the “resource test phase”. All of those resources: PoW, PoS, Po(Bandwidth), and Po(Honesty) are measured and quantified. Your resources than essentially set you in an equivalent resource pool ie: in a pool with other people of similar resource power.
You then earn IOTA’s from people using the Oracle system, smart contract, or simply who want computational power (which is absolutely needed to be able to outsource the IoT industry which is for sure the future.
So what does that mean.
Before do you remember all of the questions: IOTA won’t work because people won’t run nodes, because they don’t get incentives like traditional blockchains. Well now they can!!! And not only that, “Q” takes every aspect of each crypto and combines it all in one...
PoS, PoW, PoBandwidth, and PoHonesty.
More so, if you have Asic’s, you are in the Asic’s pool, GPU’s, you’re in the GPU pool, old crappy computer (you’re in the old crappy computer pool), you stake a lot of IOTA, you’re in the high-stake IOTA pool... etc. This is the process of “proving” your resources to the network.
People will purchase “resources” using the Qubic protocol. If they want quality, fast, or extreme computational power they have to pay.
Remember, you the user set what you want to receive in IOTA for your resources (economic principles). If you spend $1200 a month on electricity and equipment, you will only charge more than $1200 a month for your resources, no one would charge less. So, in your pool, everyone will eventually come to a quorum charging a set amount, and thus the economy (the users) will pay for it. So, in essence, the better the pool, the more the reward you get (based on economic principals in society (just like blockchain). I don’t fully understand the exact quantitative measure of what equates to the reward (such as with hash power in blockchain), though it seems that once you prove your resources your machine performs the calculations that are being bought on the Qubic network.
However, if your coffee pot has a jinn chip that is Ternary hardware, with Ternary programming (ABRA), then it can sell its resources when it’s not making coffee ie: proving its resources and then completing computations for buyers. This is just speculative and the ABRA ternary language will be able to interface with binary and lower the energy consumption but a significant amount. When combining ABRA with a ternary chip such as Jinn, the energy efficiency is even more! One of the major bottle necks or challenges that prevents advancement in technology is the amount of battery storage within machines. If we can’t redesign a battery to store more power, at least we can redesign the energy consumption within machine devices.
Also, your autonomous car not only can offer up its PoW, it can also stake the IOTA’s it is not using in its wallet the bandwidth when it isn’t working or driving, and the experience / honesty factor (by proving its resources and then selling its computational power) as it “may” be able to be a node in itself. In addition the left-over electricity it has from charging up through solar or wind power, it can sell through the smart-grid to neighbors or local businesses. Your car has “multiple” resources and the Qubic network allows machines to offer “all” of their resources to their owner, not just one or two as with blockchain.
Qubics revolutionizes machinery by allowing it; the machinery, to sell its resources. This is another building block to the ultimate vision of a machine acting in a “machine economy”.
Rather than us setting this up, and the fees we want to charge, eventually we can create smart contracts with Qubic functions which then allows machines to negotiate and earn “themselves”, the machines will sell and buy resources “THEMSELVES”, truly creating a machine economy, “AND” if you own the machine, you earn the rewards (ie: income, passive income).
Take Away Two:
2. From the above description these are only a few use cases that I take away from reading about Qubics. The reality is that the community will be coming up with new use cases every day for the following year probably. Use cases that we can’t even imagine at the present time, but here is my second takeaway:
The Qubic protocol, where all this is happening. Miners earning, people staking their IOTA and earning (ie: “interest” or “passive income”) because they are HODLER’s (and by proving their resources they sell their computational power), Forex financial companies using Qubics for quorum “ORACLE” data, smart-contracts being run on the protocol, scientist using computational power for medical research, VW, Fujitsu, and Bosch using computational power for their IoT devices, etc. on and on. All those use cases, to power.... TO POWER, to run the network, all those functions will be conducted with zero fee transactions that take place on the Tangle with real-time smart-contract micro-payments.
The whole system runs on data transactions (zero fee transactions) by sending MetaData within the transaction sent on the Tangle. MetaData essentially (I’m not a techie) is like the language that tells the Q-Nodes to wake up, to process data, pay, earn, and receive, and essentially run the whole Q network.
So.... that is a SHITLOAD of transactions occurring!!!!!! At the present day, the amount of transactions right now occurring from Trinity, speculation, and trading, is like a drop in the ocean compared to how many transactions the Qubic network will produce. It’s not hard to understand, the Qubic network will run millions if not billions of transactions per day over the Tangle, and remember, “each transaction confirms two transactions”.
So.... what does that mean. More transactions mean a faster Tangle, a more secure Tangle, an infinitely scalable Tangle.... and most importantly.... WE CAN TAKE THE COO (Coordinator) OFFLINE!!!
Note: there may be use cases for multiple COO’s (coordinators) or private COO’s but that is a whole other arena and I simply state this because I read someone writing such an example that went right over my head.
The point is: Q is needed to remove the COO!
So, as everyone says, “Why don’t the dev’s focus working on removing the COO, (“wen remove COO”), you can see that THEY ARE working on it!”. The Qubic network will support the network because it incentives people to host nodes and earn IOTA!
Also, if no one uses the Qubic network then it doesn’t work right?!? So, making “Corporate Partners”, and United Nations (NGO) affiliates, partnership with banks, all of this is needed to support the Qubic Network.
So here are the building blocks to the dev’s vision:
- You need a Tangle (Zero fee transactions that can that can send meta data)
- You need IOTA (A transfer means of metadata and a form of payment that can buy and sell resources (ie: PoW, PoS, PoBandwidth, and PoHonesty)
- You need the Qubic Network (creates Oracles, allows for Quorum Based Computations that powers Oracles.)
- You need Oracles (Oracles power smart-contracts which is the whole shabang! It will change society and change global finance).
- You need the Qubic Network (Connects users of the network with resource providers of the network, enables a machine economy, and provides computational power and the most advanced smart-contracts to society).
- Users of the network (We need a community (that the IOTA Foundation builds from hosting AMA’s, takes the time to talk to the community on Discord, and provides transparency so we all can go along on their journey of completing their vision), we need global partners such as Bosch, VW, Fujitsu, etc., We need governments and societies such as Taiwan, Denmark, and maybe Sweden; and we need banking like DnB, and electrical companies like Elaad. We need the global integration to actually “use” the Qubic network for it to work (demand drives economic principals, which ultimately will pay the Q-Node providers, which will drive transactions thus scaling the network).
- Lastly, you need to remove the COO and let the network grow organically. (This can only be done when the previous steps have been completed).
Tangle ->IOTA ->Qubic Network ->Oracles ->Partners -> COO
So removing the COO is one of the last steps. After removing COO the network can just grow organically on its own without much support or help from the dev’s. They can then work on building applications that work on top of the Qubic network.
This is a large challenging undertaking that is being built step-by-step, each piece is part of a large puzzle that all comes together. As for the Qubic vision, which is what was just released, is a really large damn piece of that puzzle!!!
It just goes to show, that all of this adds up to removing the COO. Everything the dev’s, and the IF, have been doing are working towards simply that! It’s all one big construct, not different pieces, everything ties together and the Qubic network is a large friggin piece of it all. Their sole mission is to complete the puzzle, the vision, so the COO can be removed, and the Tangle can literally change society through the machine economy.
This is just my non-techie understanding at the moment. I have a lot more research and studying to do, but damn I love it! So glad to be allowed within this community and enjoy the journey with the IOTA Foundation.
Please clarify if I totally misunderstood anything and looking forward to hear other people’s understanding.  Lastly, after writing this I re-read the Qubic website. Difficult to understand, but my rough understanding is that Q-Nodes and Qubics can lie dormant listening to the Tangle. Qubics are event driven so that when one Qubic initiates, another Qubic may need that quorum information to activate, and when the one Qubic gets the result it intended to compute, then the that Qubic itself can activate. So, one Qubic can initiate another Qubic and so on so on like neurons firing, lighting up a portion of the brain, which then fires more neurons. This is all done through secured data streams, the Tangle, and the Q-Nodes, and the Qubic network. In a way, it’s a global living system with the data stream as its life-blood, the Tangle as it’s bone structure, and the Qubics and Q-Nodes as its neurons. For all we know, in the future, this global mass network can be, and power AI, or maybe it will grow to become one massive AI source that can help society in so many ways.  As I stated, I’m a non-techie. I probably haven’t put out a bit of misinformation as I don’t fully understand it all. Really, I just hope to ignite curiosity, so people may be inspired to put a two into the new world of the Machine Economy. As well sometimes it is hard to see the big picture. The IOTA Foundation has been working on “A” vision, a machine-to-machine economy that will change society with the Tangle as a standard protocol, the bone structure of it all. The fact is each new development is another puzzle piece, or a foundation block, that stacks on top of the others. In the end we have the puzzle as a whole, or a great structure built upon a solid foundation.
https://twitter.com/IotanSea
https://qubic.iota.org
https://www.iota.org/
https://www.facebook.com/groups/iotatangle/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The Crypto & Blockchain publication. Educate yourself about cryptocurrency, blockchain developments. Check tutorials on Solidity and smart contracts.
",bit long think might help people understand qubic bit two takeaways took reading qubic rev_02 take away one 1 host qnode node supports q protocol layer earn rewards manners offering pow mining rigs computer coffee pot pos iotas hold bandwidth dont use probably something lifi future could router lightbulbs house simply previous history running honest node system used pass resource test phase resources pow pos pobandwidth pohonesty measured quantified resources essentially set equivalent resource pool ie pool people similar resource power earn iotas people using oracle system smart contract simply want computational power absolutely needed able outsource iot industry sure future mean remember questions iota wont work people wont run nodes dont get incentives like traditional blockchains well q takes every aspect crypto combines one pos pow pobandwidth pohonesty asics asics pool gpus youre gpu pool old crappy computer youre old crappy computer pool stake lot iota youre highstake iota pool etc process proving resources network people purchase resources using qubic protocol want quality fast extreme computational power pay remember user set want receive iota resources economic principles spend 1200 month electricity equipment charge 1200 month resources one would charge less pool everyone eventually come quorum charging set amount thus economy users pay essence better pool reward get based economic principals society like blockchain dont fully understand exact quantitative measure equates reward hash power blockchain though seems prove resources machine performs calculations bought qubic network however coffee pot jinn chip ternary hardware ternary programming abra sell resources making coffee ie proving resources completing computations buyers speculative abra ternary language able interface binary lower energy consumption significant amount combining abra ternary chip jinn energy efficiency even one major bottle necks challenges prevents advancement technology amount battery storage within machines cant redesign battery store power least redesign energy consumption within machine devices also autonomous car offer pow also stake iotas using wallet bandwidth isnt working driving experience honesty factor proving resources selling computational power may able node addition leftover electricity charging solar wind power sell smartgrid neighbors local businesses car multiple resources qubic network allows machines offer resources owner one two blockchain qubics revolutionizes machinery allowing machinery sell resources another building block ultimate vision machine acting machine economy rather us setting fees want charge eventually create smart contracts qubic functions allows machines negotiate earn machines sell buy resources truly creating machine economy machine earn rewards ie income passive income take away two 2 description use cases take away reading qubics reality community coming new use cases every day following year probably use cases cant even imagine present time second takeaway qubic protocol happening miners earning people staking iota earning ie interest passive income hodlers proving resources sell computational power forex financial companies using qubics quorum oracle data smartcontracts run protocol scientist using computational power medical research vw fujitsu bosch using computational power iot devices etc use cases power power run network functions conducted zero fee transactions take place tangle realtime smartcontract micropayments whole system runs data transactions zero fee transactions sending metadata within transaction sent tangle metadata essentially im techie like language tells qnodes wake process data pay earn receive essentially run whole q network shitload transactions occurring present day amount transactions right occurring trinity speculation trading like drop ocean compared many transactions qubic network produce hard understand qubic network run millions billions transactions per day tangle remember transaction confirms two transactions mean transactions mean faster tangle secure tangle infinitely scalable tangle importantly take coo coordinator offline note may use cases multiple coos coordinators private coos whole arena simply state read someone writing example went right head point q needed remove coo everyone says dont devs focus working removing coo wen remove coo see working qubic network support network incentives people host nodes earn iota also one uses qubic network doesnt work right making corporate partners united nations ngo affiliates partnership banks needed support qubic network building blocks devs vision need tangle zero fee transactions send meta data need iota transfer means metadata form payment buy sell resources ie pow pos pobandwidth pohonesty need qubic network creates oracles allows quorum based computations powers oracles need oracles oracles power smartcontracts whole shabang change society change global finance need qubic network connects users network resource providers network enables machine economy provides computational power advanced smartcontracts society users network need community iota foundation builds hosting amas takes time talk community discord provides transparency go along journey completing vision need global partners bosch vw fujitsu etc need governments societies taiwan denmark maybe sweden need banking like dnb electrical companies like elaad need global integration actually use qubic network work demand drives economic principals ultimately pay qnode providers drive transactions thus scaling network lastly need remove coo let network grow organically done previous steps completed tangle iota qubic network oracles partners coo removing coo one last steps removing coo network grow organically without much support help devs work building applications work top qubic network large challenging undertaking built stepbystep piece part large puzzle comes together qubic vision released really large damn piece puzzle goes show adds removing coo everything devs working towards simply one big construct different pieces everything ties together qubic network large friggin piece sole mission complete puzzle vision coo removed tangle literally change society machine economy nontechie understanding moment lot research studying damn love glad allowed within community enjoy journey iota foundation please clarify totally misunderstood anything looking forward hear peoples understanding lastly writing reread qubic website difficult understand rough understanding qnodes qubics lie dormant listening tangle qubics event driven one qubic initiates another qubic may need quorum information activate one qubic gets result intended compute qubic activate one qubic initiate another qubic like neurons firing lighting portion brain fires neurons done secured data streams tangle qnodes qubic network way global living system data stream lifeblood tangle bone structure qubics qnodes neurons know future global mass network power ai maybe grow become one massive ai source help society many ways stated im nontechie probably havent put bit misinformation dont fully understand really hope ignite curiosity people may inspired put two new world machine economy well sometimes hard see big picture iota foundation working vision machinetomachine economy change society tangle standard protocol bone structure fact new development another puzzle piece foundation block stacks top others end puzzle whole great structure built upon solid foundation httpstwittercomiotansea httpsqubiciotaorg httpswwwiotaorg httpswwwfacebookcomgroupsiotatangle quick cheer standing ovation clap show much enjoyed story crypto blockchain publication educate cryptocurrency blockchain developments check tutorials solidity smart contracts,en,"['LIFI', 'Oracle', 'PoS, PoW, PoBandwidth', 'GPU', 'HODLER', 'Forex', 'Fujitsu', 'Bosch', 'Trinity', 'Tangle', 'THEY', 'United Nations', 'the Qubic Network', 'PoBandwidth', 'the IOTA Foundation', 'AMA', 'DnB', 'the Q-Nodes', 'The IOTA Foundation', 'The Crypto & Blockchain']"
137,Justin Lee,511,The beginner’s guide to conversational commerce – The Startup – Medium,"Your greengrocer does it. So does that guy selling sunglasses on the beach. It’s why the funny old French bakery around the corner’s been running for 15 years.
Conversational marketing. A buzzword, a footnote, a revelation. Everyone’s talking about it, but what is it?
At its most simple, it’s the act of talking — and more importantly, listening — to your customers: their problems, their stories, their successes. Forging a genuine connection and using that connection to inform your marketing decisions.
At its most complex, conversational marketing has become synonymous with cutting-edge technologies for computer-based dialog processing.
Brands have always known that one-to-one conversations are valuable; but up until very recently, it was impossible to personalize these conversations at scale, in real-time.
No longer. Chatbots have become a mainstay of digital marketing, and every day their underlying AI becomes more sophisticated. Gartner predicts that by 2020, 30% of our interactions with technology will be through “conversations” with smart machines.
In his 1999 Cluetrain Manifesto, David Weinberger reminded us that
that’s a hundred times more true today.
A successful conversational marketing strategy will pair the spark of authenticity from real conversation with the emerging technologies of the future.
In a 2016 article, Chris Messina distills the concept:
Conversational converse is the process of having a real-time, one-to-one conversation with a customer or lead. It’s a direct, personalized, dialog-driven approach to nurturing long-term relationships, collecting data and increasing sales.
Unlike traditional digital marketing, it ‘pulls’ users in instead of ‘pushing’ content on them. It’s a discourse, not a lecture.
Despite recently picking up speed, conversational marketing isn’t new.
The concept made its first appearance in 2007 with Joseph Jaffe’s Join the Conversation. Jaffe wanted to teach marketers to re-engage their customers through community, partnership, and dialog:
In the past, brands have been able to talk at their customers — through email, website interactions and social media — not with them.
Brands have struggled to capture, keep and convert attention into sales, sign-ups, and long-term loyalty. Engagement was passive, and results were shallow.
Customer service was relegated to a formulaic question-answer scenario that was unsatisfying for everyone involved. Take it from leading conversational marketing platform Drift’s stellar report:
Today, messaging apps have over 5 billion monthly active users, and for the first time, usage rates have surpassed social networks. Whether it’s chatting with friends on WhatsApp or exchanging ideas with coworkers on Slack, messaging has become an integral part of our lives.
Despite extreme app saturation, the average person only uses five apps regularly and, you guessed it — messaging apps claim these spots, boasting 10x better open rates than the next leading digital channel. These messaging platforms have huge audiences: there are over 4 billion active monthly users on the top three messaging apps.
Like the rise of the internet or the app economy of the past decade, conversational marketing is born from current desires: for real-time connection and genuine value.
Conversational marketing is an umbrella term that encompasses every dialog-driven tactic, from opt-in email marketing to customer feedback.
But the engine powering recent developments is Artificial Intelligence (AI). Chatbots represent the new era in conversational marketing: scaleable, personalized, real-time and data-driven.
Of course, these bots aren’t intended to replace human-to-human interactions; they’re there to support and enhance them: helping users have the right conversations with the right people at the right time.
(For the meantime, anyway. According to Gartner research, chatbots will account for 85% of all customer service by 2020).
Chatbots are a blank canvas, with the potential to be molded and infused with a persona that reflects a company’s values — like our very own GrowthBot(AKA a mini Dharmesh Shah).
This technology is still in its infancy, so most bots follow a set of rules programmed by a human via a bot-building platform. The differentiator is that the chatbots carry out conversations with users using natural language.
AI uses first-person data to learn more about each customer and deliver a hyper-personalized experience. Reps and bots can then join forces to manage these conversations at scale.
Let’s imagine I’m going to a fancy party. Tonight. It’s last minute, and I’ve just received a message that it’s black tie; but I don’t have the right shoes. I need to quickly find a pair that is appropriate; my size; coherent with the rest of my outfit; a good price, etc.
I would usually Google for a shop in my area, then go to browse on their website to find a pair I like. But other issues would soon crop up: do they have my size? Are the shoes smart enough? Are they in stock?
I could fill out a query on the site’s contact form, or give them a ring, but will they answer? And if they don’t have the right pair, they are unlikely to suggest a range of alternatives. The whole process is time-consuming and inefficient.
But suppose the brand I like has a strong conversational marketing culture. Instead of resorting to email, I would be able to conduct the conversation in seconds on my phone; instantly, I’m given the colours, sizes and styles in stock. I can pay for the right shoes with a tap of a button.
Conversational marketing enables users to get the information they need instantly, without picking up the phone or engaging with a person. It’s not about laziness; it’s about ease.
Chris Messina concludes,
As Clara de Soto, cofounder of Reply.ai, told VentureBeat,
If users are made to toggle between various apps and platforms to get the answer they need, the value of the bot is moot: it needs to be native to the place they spend most time, whether that’s Slack, Messenger or onsite chat.
But it can be tricky for brands to consolidate all their conversations in one place. That’s why HubSpot created Conversations, a free, multi-channel tool that lets businesses have one-to-one conversations at scale.
says Dharmesh Shah, co-founder and CTO of HubSpot.
We have a much lower tolerance for mistakes with machines compared to humans: 73% of people say they won’t interact with a bot again after one negative experience. And if a bot seems to be able to converse in English, we tend to easily overestimate how capable it is.
That’s why it’s crucial to manage your customers’ expectations appropriately. Bots are far from being autonomous, and people aren’t easily fooled; trying to present your bot as a human agent is likely to be self-defeating. Bots don’t understand context created by preceding text, and conversational nuances can easily affect their capacity to answer.
Because bots live inside messaging apps, they have the potential to invade a highly personal space, making the stakes of getting it right much higher.
According to research, people use messaging apps for customer assistance with one key goal: to get their problem solved, fast. Bots should serve one simple purpose well, without getting tangled up in the conversational complications that are better left to humans.
The way brands and users interract is undergoing a monumental shift. Customers are smarter and better-informed than ever before. They expect personalization and transparency as a prerequisite. They feel empowered by their options. It’s hard to fool them, and even harder to gain their loyalty.
And most significantly, they want 24/7, 365 days of the year instantaneousness: to be heard, to be helped, right now; not in half an hour, not tomorrow. That’s why conversational marketing represents a new cornerstone in marketing but also in customer service and experience, branding and sales.
Building a bot for the sake of being on-trend is not enough; it needs to be part of a larger strategy where each conversation has a purpose. As a long-term strategy intended to facilitate lasting relationships, it needs to be spearheaded towards a long-term goal.
Effective conversational marketing is an intersection of brand values, user engagement and valuable dialogue. It’s about building your audience first, selling last.
Thanks for reading.
Originally published at blog.growthbot.org.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Head of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
",greengrocer guy selling sunglasses beach funny old french bakery around corners running 15 years conversational marketing buzzword footnote revelation everyones talking simple act talking importantly listening customers problems stories successes forging genuine connection using connection inform marketing decisions complex conversational marketing become synonymous cuttingedge technologies computerbased dialog processing brands always known onetoone conversations valuable recently impossible personalize conversations scale realtime longer chatbots become mainstay digital marketing every day underlying ai becomes sophisticated gartner predicts 2020 30 interactions technology conversations smart machines 1999 cluetrain manifesto david weinberger reminded us thats hundred times true today successful conversational marketing strategy pair spark authenticity real conversation emerging technologies future 2016 article chris messina distills concept conversational converse process realtime onetoone conversation customer lead direct personalized dialogdriven approach nurturing longterm relationships collecting data increasing sales unlike traditional digital marketing pulls users instead pushing content discourse lecture despite recently picking speed conversational marketing isnt new concept made first appearance 2007 joseph jaffes join conversation jaffe wanted teach marketers reengage customers community partnership dialog past brands able talk customers email website interactions social media brands struggled capture keep convert attention sales signups longterm loyalty engagement passive results shallow customer service relegated formulaic questionanswer scenario unsatisfying everyone involved take leading conversational marketing platform drifts stellar report today messaging apps 5 billion monthly active users first time usage rates surpassed social networks whether chatting friends whatsapp exchanging ideas coworkers slack messaging become integral part lives despite extreme app saturation average person uses five apps regularly guessed messaging apps claim spots boasting 10x better open rates next leading digital channel messaging platforms huge audiences 4 billion active monthly users top three messaging apps like rise internet app economy past decade conversational marketing born current desires realtime connection genuine value conversational marketing umbrella term encompasses every dialogdriven tactic optin email marketing customer feedback engine powering recent developments artificial intelligence ai chatbots represent new era conversational marketing scaleable personalized realtime datadriven course bots arent intended replace humantohuman interactions theyre support enhance helping users right conversations right people right time meantime anyway according gartner research chatbots account 85 customer service 2020 chatbots blank canvas potential molded infused persona reflects companys values like growthbotaka mini dharmesh shah technology still infancy bots follow set rules programmed human via botbuilding platform differentiator chatbots carry conversations users using natural language ai uses firstperson data learn customer deliver hyperpersonalized experience reps bots join forces manage conversations scale lets imagine im going fancy party tonight last minute ive received message black tie dont right shoes need quickly find pair appropriate size coherent rest outfit good price etc would usually google shop area go browse website find pair like issues would soon crop size shoes smart enough stock could fill query sites contact form give ring answer dont right pair unlikely suggest range alternatives whole process timeconsuming inefficient suppose brand like strong conversational marketing culture instead resorting email would able conduct conversation seconds phone instantly im given colours sizes styles stock pay right shoes tap button conversational marketing enables users get information need instantly without picking phone engaging person laziness ease chris messina concludes clara de soto cofounder replyai told venturebeat users made toggle various apps platforms get answer need value bot moot needs native place spend time whether thats slack messenger onsite chat tricky brands consolidate conversations one place thats hubspot created conversations free multichannel tool lets businesses onetoone conversations scale says dharmesh shah cofounder cto hubspot much lower tolerance mistakes machines compared humans 73 people say wont interact bot one negative experience bot seems able converse english tend easily overestimate capable thats crucial manage customers expectations appropriately bots far autonomous people arent easily fooled trying present bot human agent likely selfdefeating bots dont understand context created preceding text conversational nuances easily affect capacity answer bots live inside messaging apps potential invade highly personal space making stakes getting right much higher according research people use messaging apps customer assistance one key goal get problem solved fast bots serve one simple purpose well without getting tangled conversational complications better left humans way brands users interract undergoing monumental shift customers smarter betterinformed ever expect personalization transparency prerequisite feel empowered options hard fool even harder gain loyalty significantly want 247 365 days year instantaneousness heard helped right half hour tomorrow thats conversational marketing represents new cornerstone marketing also customer service experience branding sales building bot sake ontrend enough needs part larger strategy conversation purpose longterm strategy intended facilitate lasting relationships needs spearheaded towards longterm goal effective conversational marketing intersection brand values user engagement valuable dialogue building audience first selling last thanks reading originally published bloggrowthbotorg quick cheer standing ovation clap show much enjoyed story head growth growthbot messaging conversational strategy hubspot mediums largest publication makers subscribe receive top stories httpsgooglzhclji,en,"['Conversational', 'WhatsApp', 'Artificial Intelligence (AI', 'Gartner', 'Google', 'Reply.ai', 'VentureBeat', 'CTO', 'HubSpot', 'blog.growthbot.org']"
138,savedroid ICO,340,#SNEAKPEEK The savedroid crypto saving app — Part #1: Your wish,"The international beta of our brand new crypto saving app is coming soon. The beta app will be launched in English language and will exclusively be available for our ICO token buyers only. Now, get ready to learn more about the savedroid crypto saving app even before its official release. Today, we give you a very first sneak peek of one of its core features: your wish.
With savedroid you can save up for your personal goals you want to afford in the future. Your own lambo or your desired moon. Exactly that is your wish. So, using the savedroid crypto saving app is not just about piling up a fortune. It’s all about saving up for your personal wishes, which you are aspiring to fulfill but can’t afford right now.
There are 3 simple steps to set up your wish in less than one minute:
1) What?First, name your wish and select one of our illustrations to always keep you motivated to continue saving. You can go small and save for your new pair of hipster sneakers or you may go big and start a crypto savings plan for your new family home. Everything is possible, only the moon is the limit — at least for now.
2) How much?Then set the amount you need to save up to afford your wish. The amount is denominated in Fiat currency as it is the prevailing means of payment. By the way, that makes it a lot easier for you as you don’t need to do the math converting Fiat to crypto and vice versa — this complex task is on us.
3) When?Finally, select the date by when you want to fulfil your wish. And you are done! That was easy. Just as easy as savedroid’s other features will be to deliver on our mission to democratize crypto and bring cryptocurrencies to the masses.
To keep you posted on our latest product updates we have started this new #SNEAKPEEK series. Here we will provide you regular sneak peeks on our hottest new features. Stay tuned and follow our blog!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The savedroid ICO: Cryptocurrencies for Everyone — now! Give Power to the People. Join the Revolution: https://ico.savedroid.com
",international beta brand new crypto saving app coming soon beta app launched english language exclusively available ico token buyers get ready learn savedroid crypto saving app even official release today give first sneak peek one core features wish savedroid save personal goals want afford future lambo desired moon exactly wish using savedroid crypto saving app piling fortune saving personal wishes aspiring fulfill cant afford right 3 simple steps set wish less one minute 1 whatfirst name wish select one illustrations always keep motivated continue saving go small save new pair hipster sneakers may go big start crypto savings plan new family home everything possible moon limit least 2 muchthen set amount need save afford wish amount denominated fiat currency prevailing means payment way makes lot easier dont need math converting fiat crypto vice versa complex task us 3 whenfinally select date want fulfil wish done easy easy savedroids features deliver mission democratize crypto bring cryptocurrencies masses keep posted latest product updates started new sneakpeek series provide regular sneak peeks hottest new features stay tuned follow blog quick cheer standing ovation clap show much enjoyed story savedroid ico cryptocurrencies everyone give power people join revolution httpsicosavedroidcom,en,"['ICO', 'app', 'Fiat', 'Give Power']"
139,Brandon Morelli,221,Artificial Intelligence Top 10 Articles — June 2018,"Here’s what’s trending this month in Artificial Intelligence. Topics include:
Whether you’re experienced with Artificial Intelligence, or a newbie looking to learn the basics of AI, there’s something for everyone on this list.
Disclosure: We receive compensation from the courses we feature.
4.3/5 Stars || 17 Hours of Video || 58,823 Students
Build an AI that combines the power of Data Science, Machine Learning and Deep Learning to create powerful AI for Real-World applications. You will also have the chance to understand the story behind Artificial Intelligence. Learn More.
4.7/5 Stars || 8 Hours of Video || 15,063 Students
Completely understand the relationship between reinforcement learning and psychology and on a technical level. Apply gradient-based supervised machine learning methods to reinforcement learning and implement 17 different reinforcement learning algorithms. Learn More.
By Lance Ulanoff
Have you heard about the Google Duplex yet? It’s pretty much the talk all over the internet. Google CEO Sundar Pichai has dropped its biggest bomb when they introduced Google Duplex to all. Take a look more on this story to know more.
By Irhum Shafkat
Understanding convolutions can often feel a bit unnerving yet it’s concept is fascinatingly powerful and highly extensible. Let’s try to break down the mechanics of the convolution operation, step-by-step, relate and explore it’s hierarchy into a more powerful one.
By WiseWolf Fund
AI is already shaping the economy, and in the near future, its effect may be even more significant. Ignoring the new technology and its influence on the global economic situation is a recipe for failure. Read more of this article now!
By Sam Drozdov
Machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed”. Learn the basics of machine learning and how to apply it to the products you are building right now.
By Aman Dalmia
Having a great opportunity to interact with great minds is one of the awesome privileges one can keep to their knowledge and motivate them to avoid the mistakes in a much better manner.
By Simon Greenman
Welcome to AI gold rush! Check out this awesome article that talks about how companies and startups make money on AI and how it helps the economic growth as well.
By Justin Lee
Are chatbots hype over already? Find out why our industry massively overestimated the initial impact chatbots would have and a lot more reasons why chatbots are not on the trend anymore.
By Daniel Jeffries
AI could mean the end of all jobs for most people and that’s just terrifying, right? Check out this topic to get to know more about how will AI bring an explosion to new jobs.
By George Seif
Learn more about Google’s AutoML — a suite of machine learning tools that will allow one to easily train high-performance deep networks, without requiring the user to have any knowledge in AI.
By James Loy
Understand the inner workings of Deep Learning through Python with Neural Network. Know and train more about Neural Network from scratch.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Creator of @codeburstio — Frequently posting web development tutorials & articles. Follow me on Twitter too: @BrandonMorelli
bursts of tech to power through your day
",heres whats trending month artificial intelligence topics include whether youre experienced artificial intelligence newbie looking learn basics ai theres something everyone list disclosure receive compensation courses feature 435 stars 17 hours video 58823 students build ai combines power data science machine learning deep learning create powerful ai realworld applications also chance understand story behind artificial intelligence learn 475 stars 8 hours video 15063 students completely understand relationship reinforcement learning psychology technical level apply gradientbased supervised machine learning methods reinforcement learning implement 17 different reinforcement learning algorithms learn lance ulanoff heard google duplex yet pretty much talk internet google ceo sundar pichai dropped biggest bomb introduced google duplex take look story know irhum shafkat understanding convolutions often feel bit unnerving yet concept fascinatingly powerful highly extensible lets try break mechanics convolution operation stepbystep relate explore hierarchy powerful one wisewolf fund ai already shaping economy near future effect may even significant ignoring new technology influence global economic situation recipe failure read article sam drozdov machine learning field study gives computers ability learn without explicitly programmed learn basics machine learning apply products building right aman dalmia great opportunity interact great minds one awesome privileges one keep knowledge motivate avoid mistakes much better manner simon greenman welcome ai gold rush check awesome article talks companies startups make money ai helps economic growth well justin lee chatbots hype already find industry massively overestimated initial impact chatbots would lot reasons chatbots trend anymore daniel jeffries ai could mean end jobs people thats terrifying right check topic get know ai bring explosion new jobs george seif learn googles automl suite machine learning tools allow one easily train highperformance deep networks without requiring user knowledge ai james loy understand inner workings deep learning python neural network know train neural network scratch quick cheer standing ovation clap show much enjoyed story creator codeburstio frequently posting web development tutorials articles follow twitter brandonmorelli bursts tech power day,en,"['Artificial Intelligence', 'Google', 'WiseWolf Fund', 'Neural Network', '@codeburstio', 'web development tutorials & articles', '@BrandonMorelli']"
140,Gabriel Jiménez,50,"Chatbots, could we talk? – AIMA: AI Marketing Magazine – Medium","After the euphoria for apps, the trend is reversing. Every day we download fewer new apps and we keep with few in constant use.
A lot has happened since Apple in 2009 proclaimed that there was an app for everything.
As in the next commercial:
The chat boom
According to the report of the Internet Association 2017 on the habits of Internet users in México. The second social network used by Mexicans is WhatsApp a messaging app and the first, although the report has no separate data, is Facebook, which also includes Facebook Messenger.
As a particular fact both are from Facebook, as is instagram that is in position 5 on the list.
The customer experience in issues such as support, attention or navigation in telephone menus and the transition we have made from voice calls to text messages, both for practicity and cost have catalyzed the technological development of so-called virtual agents or chatbots to optimize resources and improve customer service.
In an environment where an immediate response is the minimum that is expected, the best option to improve customer service at the lowest cost is through a chatbot.
But what is a chatbot?
It is a computer program, which works either through rules or the most advanced using artificial intelligence, the way to interact with them is via a chat.
With rules
Chatbots that work with rules have limited functionality, respond only to specific commands; If you do not write correctly what you want, it does not understand it.
With artificial intelligence
On the other hand, assistants who use artificial intelligence can understand what you say, in any way you write it, even if you do it incorrectly, abbreviated or with idiomatic expressions.
They are also able to improve over time, learning the way people express themselves and how they ask.
Context and memory
Chatbots that use artificial intelligence can resume a previous conversation or, based on the context of the chat, move forward in a coherent manner.
If for example we are looking for a movie to see in the cinema and first ask us the cinema we want to go to and then the movie, then we change the movie and then the chatbot will assume that we continue talking about the same cinema unless we specify otherwise.
The above may seem very simple for us as people but for a chatbot to maintain a coherent and fluid conversation, it is a huge achievement and one that brings great value.
Channels
A chatbot can be integrated into any chat application, whether corporate, your website or commercial like Facebook Messenger or Whatsapp.
Limitations
One of the challenges faced by chatbots is the initial adoption, they may fail, mainly for 3 reasons:
1. As a result of not adequately delimiting its initial scope. We want to resolve all the possible issues with the chatbot, that deals with complaints, that supports, that sells, that generates interaction with customers, that gives service status. This causes, as with any project, scope creep, endless requirements which makes it seem that the project never will work appropiately.
2. It is not linked to an activity that solves a business issue, sometimes they apply to trivial situations or that do not have a relevant metric linked, so it is impossible to measure their effectiveness and quantify their benefits to the business.
3. Being a new technology, we tend to think that since it has intelligence it can answer any question outside the business context for which it was defined, thus also losing the initial focus and evaluating its performance outside the scope for which it was created.
It is important to remember that although it has artificial intelligence, every bot has to have a period of learning and evolution and this takes time. Its process is similar to that carried by a child, when it begins to learn it makes mistakes, there are terms or forms of expression that it does not know but as time passes, it becomes more and more ready due to the experience it acquires with each conversation, the same It happens with the chatbot.
Hand over
That is why there always has to be a process to re-direct a human operator to a conversation in which the chatbot is not able to respond satisfactorily, in this way we keep the customer experience as a principle and we avoid frustrations to people.
Connection with systems
The chatbot can give an integral attention to clients through chat but its capacity to do it also depends on the integration that this one has with the systems of the company; without this, the service you provide will be incomplete and frustrating.
For example, if we have a chatbot to schedule appointments, we need that in addition to understanding what people ask, you can access the agenda system to check if there is time available to schedule, if you do not have it, you will be limited and it will be practically useless.
Applications
The main change when using a chatbot is that instead of browsing websites, we can ask to get what we want, it is even possible to obtain recommendations based on questions to find the most appropriate for us.
Benefits
To know about chatbots and artificial intelligence, write to me @gabojimenez_ or linkedin.com/in/gabrieljimenezmunoz/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CONSULTATIVE SELLING | AI FOR BUSINESS | CHATBOTS | ANALYTICS | SPEAKER | WRITER | TEACHER
Driving the AI Marketing movement
",euphoria apps trend reversing every day download fewer new apps keep constant use lot happened since apple 2009 proclaimed app everything next commercial chat boom according report internet association 2017 habits internet users mexico second social network used mexicans whatsapp messaging app first although report separate data facebook also includes facebook messenger particular fact facebook instagram position 5 list customer experience issues support attention navigation telephone menus transition made voice calls text messages practicity cost catalyzed technological development socalled virtual agents chatbots optimize resources improve customer service environment immediate response minimum expected best option improve customer service lowest cost chatbot chatbot computer program works either rules advanced using artificial intelligence way interact via chat rules chatbots work rules limited functionality respond specific commands write correctly want understand artificial intelligence hand assistants use artificial intelligence understand say way write even incorrectly abbreviated idiomatic expressions also able improve time learning way people express ask context memory chatbots use artificial intelligence resume previous conversation based context chat move forward coherent manner example looking movie see cinema first ask us cinema want go movie change movie chatbot assume continue talking cinema unless specify otherwise may seem simple us people chatbot maintain coherent fluid conversation huge achievement one brings great value channels chatbot integrated chat application whether corporate website commercial like facebook messenger whatsapp limitations one challenges faced chatbots initial adoption may fail mainly 3 reasons 1 result adequately delimiting initial scope want resolve possible issues chatbot deals complaints supports sells generates interaction customers gives service status causes project scope creep endless requirements makes seem project never work appropiately 2 linked activity solves business issue sometimes apply trivial situations relevant metric linked impossible measure effectiveness quantify benefits business 3 new technology tend think since intelligence answer question outside business context defined thus also losing initial focus evaluating performance outside scope created important remember although artificial intelligence every bot period learning evolution takes time process similar carried child begins learn makes mistakes terms forms expression know time passes becomes ready due experience acquires conversation happens chatbot hand always process redirect human operator conversation chatbot able respond satisfactorily way keep customer experience principle avoid frustrations people connection systems chatbot give integral attention clients chat capacity also depends integration one systems company without service provide incomplete frustrating example chatbot schedule appointments need addition understanding people ask access agenda system check time available schedule limited practically useless applications main change using chatbot instead browsing websites ask get want even possible obtain recommendations based questions find appropriate us benefits know chatbots artificial intelligence write gabojimenez_ linkedincomingabrieljimenezmunoz quick cheer standing ovation clap show much enjoyed story consultative selling ai business chatbots analytics speaker writer teacher driving ai marketing movement,en,"['Apple', 'the Internet Association 2017', 'WhatsApp', 'Facebook', '@gabojimenez']"
141,Tyler Elliot Bettilyon,17900,Are Programmers Headed Toward Another Bursting Bubble?,"A friend of mine recently posed a question that I’ve heard many times in varying forms and forums:
“Do you think IT and some lower-level programming jobs are going to go the way of the dodo? Seems a bit like a massive job bubble that’s gonna burst. It’s my opinion that one of the only things keeping tech and lower-level computer science-related jobs “prestigious” and well-paid is ridiculous industry jargon and public ignorance about computers, which are both going to go away in the next 10 years. [...]”
This question is simultaneously on point about the future of technology jobs and exemplary of some pervasive misunderstandings regarding the field of software engineering. While it’s true that there is a great deal of “ridiculous industry jargon” there are equally many genuinely difficult problems waiting to be solved by those with the right skill-set. Some software jobs are definitely going away but programmers with the right experience and knowledge will continue to be prestigious and well remunerated for many years to come; as an example look at the recent explosion of AI researcher salaries and the corresponding dearth of available talent.
Staying relevant in the ever changing technology landscape can be a challenge. By looking at the technologies that are replacing programmers in the status quo we should be able to predict what jobs might disappear from the market. Additionally, to predict how salaries and demand for specific skills might change we should consider the growing body of people learning to program. As Hannah pointed out “public ignorance” about computers is keeping wages high for those who can program and the public is becoming more computer savvy each year.
The fear of automation replacing jobs is neither new nor unfounded. In any field, and especially in technology, market forces drive corporations toward automation and commodification. Gartner’s Hype Cycles are one way of contextualizing this phenomenon.
As time goes on, specific ideas and technologies push towards the “plateau of productivity” where they are eventually automated. Looking at history one must conclude that automation has the power to destroy specific job markets. In diverse industries ranging from crop harvesting to automobile assembly technology advances have consistently replaced and augmented human labor to reduce costs. A professor once put it this way in his compilers course, “take historical note of textile and steel industries: do you want to build machines and tools, or do you want to operate those machines?”
In this metaphor the “machine” is a computer programming language. This professor was really asking: Do you want to build websites using JavaScript, or do you want to build the V8 engine that powers JavaScript?
The creation of websites is being automated by WordPress (and others) today. V8 on the other hand has a growing body of competitors some of whom are solving open research questions. Languages will come and go (how many Fortran job openings are there?) but there will always be someone building the next language. Lucky for us, programming language implementations are written with programming languages themselves. Being a “machine operator” in software puts you on the path to being a “machine creator” in a way which was not true of the steel mill workers of the past.
The growing number of languages, interpreters, and compilers shows us that every job-destroying machine also brings with it new opportunities to improve those machines, maintain those machines, and so forth. Despite the growing body of jobs which no longer exist, there has yet to be a moment in history where humanity has collectively said, “I guess there isn’t any work left for us to do.”
Commodification is coming for us all, not just software engineers. Throughout history, human labor has consistently been replaced with non-humans or augmented to require fewer and less skilled humans. Self-driving cars and trucks are the flavor of the week in this grand human tradition. If the cycle of creation and automation are a fact of life, the natural question to answer next is: which jobs and industries are at risk, and which are not?
AWS, Heroku, and other similar hosting platforms have forever changed the role of the System Administrator/DevOps engineer. Internet businesses used to absolutely need their own server master. Someone who was well versed in Linux; someone who could configure a server with Apache or NGINX; someone who could not only physically wire up the server, the routers, and all the other physical components, but who could also configure the routing tables and all the software required to make that server accessible on the public web. While there are definitely still people applying this skill-set professionally, AWS is making some of those skills obsolete — especially at the lower experience levels and on the physical side of things. There are very lucrative roles within Amazon (and Netflix, and Google...) for people with deep expertise in networking infrastructure, but there is much less demand at the small-to-medium business scale.
“Business Intelligence” tools such as SalesForce, Tableau and SpotFire are also beginning to occupy spaces historically held by software engineers. These systems have reduced the demand for in-house Database Administrators, but they have also increased the demand for SQL as a general-purpose skill. They have decreased demand for in-house reporting technology, but increased demand for “integration engineers” who automate the flow of data from the business to the third-party software platform(s). A field that was previously dominated by Excel and Spreadsheets is increasingly being pushed towards scripting languages like Python or R, and towards SQL for data management. Some jobs have disappeared, but demand for people who can write software has seen an increase overall.
Data Science is a fascinating example of commodification at a level closer to software. Scikit.learn, Tensorflow, and PyTorch are all software libraries that make it easier for people to build machine learning applications without building the algorithms from scratch. In fact, it’s possible to run a dataset through many different machine learning algorithms, with many different parameter sets for those algorithms, with little to no understanding of how those algorithms are actually implemented (it’s not necessarily wise to do this, just possible). You can bet that business intelligence companies will be trying to integrate these kinds of algorithms into their own tools over the next few years as well.
In many ways data science looks like web development did 5–8 years ago — a booming field where a little bit of knowledge can get you in the door due to a “skills gap”. As web development bootcamps are closing and consolidating, data science bootcamps are popping up in their place. Kaplan, who bought the original web development bootcamp (Dev Bootcamp) and started a data science bootcamp (Metis) has decided to close DevBootcamp and keep Metis running.
Content management systems are among the most visible of the tools automating away the need for a software engineer. SquareSpace and WordPress are among the most popular CMS systems today. These platforms are significantly reducing the value of people with a just a little bit of front end web development skill. In fact the barriers for making a website and getting it online have come down so dramatically that people with zero programming experience are successfully launching websites every day. Those same people aren’t making deeply interactive websites that serve billions of people, but they absolutely do make websites for their own businesses that give customers the information they need. A lovely landing page with information such as how to find the establishment and how to contact them is more than enough for a local restaurant, bar, or retail store.
If your business is not primarily an “internet business” it has never been easier to get a working site on the public web. As a result, the once thriving industry of web contractors who can quickly set up a simple website and get it online is becoming less lucrative.
Finally, it would border on hubris to ignore the physical aspect of computers in this context. In the words of Mike Acton: “software is not the platform, hardware is the platform”. Software people would be wise to study at least a little computer architecture and electrical engineering. A big shake up in hardware, such as the arrival of consumer grade quantum computers would (will) change everything about professional software engineering.
Quantum computers are still a ways off, but the growing interest in GPUs and the drive toward parallelization is an imminent shift. CPU speeds have been stagnant for several years now and in that time a seemingly unquenchable thirst for machine learning and “big data” has emerged. With more desire than ever to process large data-sets OpenMP, OpenCL, Go, CUDA, and other parallel processing languages and frameworks will continue to become mainstream. To be competitively fast in the near-term future, significant parallelization will be a requirement across the board, not just in high-performance niches like operating systems, infrastructure and video games.
Websites are ubiquitous. The 2017 Stack Overflow Survey reports that about 15% of professional software engineers are working in an “Internet/Web Services” company. The Bureau of Labor Statistics expects growth in Web Development to continue much faster than average (24% between 2014 and 2024). Due to its visibility, there has been a massive focus on “solving the skills gap” in this industry. Coding bootcamps teach Web Development almost exclusively and Web Development online courses have flooded Udemy, Udacity, Coursera and similar marketplaces.
The combination of increasing automation throughout the Web Development technology stack and the influx of new entry level programmers with an explicit focus on Web Development has led some to predict a slide towards a “blue collar” market for software developers. Some have gone further, suggesting that the push towards a blue collar market is a strategy architected by big tech firms. Others, of course, say we’re headed for another bursting bubble.
Change in demand for specific technologies is not news. Languages and frameworks are always rising and falling in technology. Web Development in its current incarnation (“JS Is King”) will eventually go the way of Web Development of the early 2000’s (remember Flash?). What is new, is that a lot of people are receiving an education explicitly (and solely) in the current trendy web development frameworks. Before you decide to label yourself a “React developer” remember there were people who once identified themselves as “Flash developers”. Banking your career on a specific language, framework, or technology is a game of roulette. Of course it’s quite difficult to predict what technologies will remain relevant, but if you’re going to go all in on something, I suggest relying on The Lindy Effect and picking something like C that has already withstood the test of time.
The next generation will have a level of de facto tech literacy that Generation X and even Millennials do not have. One outcome of this will be that using the next generation of CMS tools will be a given. These tools will get better and young workers will be better at using them. This combination will definitely will bring down the value of low-level IT and web development skills as eager and skilled youngsters enter the job market. High schools are catching on as well, offering computer science and programming classes — some well educated high school students will likely be entering the workforce as programming interns immediately upon graduation.
Another big group of newcomers to programming are MBAs and data analysts. Job listings which were once dominated by Excel are starting to list SQL as a “nice to have” and even “requirement”. Tools such as Tableau, SpotFire, SalesForce, and other web-based metrics systems continue to replace the spreadsheet as the primary tool for report generation. If this continues more data analysts will learn to use SQL directly simply because it is easier than exporting the data into a spreadsheet.
People looking to climb the ranks and out-perform their peers in these roles are taking online courses to learn about databases and statistical programming languages. With these new skills they can begin to position themselves as data scientists by learning a combination of machine learning and statistical libraries. Look at Metis’ curriculum as a prime example of this path.
Finally, the number of people earning Computer Science and Software Engineering degrees continues to climb. Purdue, for example, reports that applications to their CS program have doubled over five years. Cornell reports a similar explosion of CS graduates. This trend isn’t surprising given the growth and ubiquity of software. It’s hard for young people to imagine that computers will play a smaller role in our futures, so why not study something that’s going to give you job security.
A common argument in the industry nowadays is around the idea that the education you receive in a four-year Computer Science program is mostly unnecessary cruft. I have heard this argument repeatedly in the halls of bootcamps, web development shops, and online from big names in the field such as this piece by Eric Elliott. The opposition view is popular as well, with some going so far as saying “all programmers should earn a master’s degree”.
Like Eric Elliott, I think it’s good that there are more options than ever to break into programming, and a 4 year degree might not be the best option for many. Simultaneously, I agree with William Bain that the foundational skills which apply across programming disciplines are crucial for career longevity, and that it is still hard to find that information outside of university courses. I’ve written previously about what skills I think aspiring engineers should learn as a foundation of a long career, and joined Bradfield in order to help share this knowledge.
Coding schools of many shapes and sizes are becoming ubiquitous, and for good reasons. There is quite a lot you can learn about programming without getting into the minutia of Big O notation, obscure data structures, and algorithmic trivia. However, while it’s true that fresh graduates from Stanford are competing for some jobs with fresh graduates from Hack Reactor, it’s only true in one or two sub-industries. Code school and bootcamp graduates are not yet applying to work on embedded systems, cryptography/security, robotics, network infrastructure, or AI research and development. Yet these fields, like web development, are growing quickly.
Some programming-related skills have already started their transition from “rare skill” to “baseline expectation”. Conversely, the engineering that goes into creating beastly engines like AWS is anything but common. The big companies driving technology forward — Amazon, Google, Facebook, Nvidia, Space-X, and so on — are typically not looking for people with a ‘basic understanding of JavaScript’. AWS serves billions of users per day. To support that kind of load an AWS infrastructure engineer needs a deep knowledge of network protocols, computer architecture, and several years of relevant experience. As with any discipline there are amateurs and artisans.
These prestigious firms are solving research problems and building systems that are truly pushing against the boundaries of what is possible. Yet they still struggle to fill open roles even while basic programming skills are increasingly common. People who can write algorithms to predict changes in genetic sequences that will yield a desired result are going to be highly valuable in the future. People who can program satellites, spacecraft, and automate machinery will continue to be highly valued. These are not fields that lend themselves as readily to a “3 month intensive program” as front end web development, at least not without significant prior experience.
Because computer science starts with the word “computer” it is assumed that young people will all have an innate understanding of it by 2025. Unfortunately, the ubiquity of computers has not created a new generation of people who de facto understand mathematics, computer science, network infrastructure, electrical engineering and so on. Computer literacy is not the same as the study of computation. Despite mathematics having existed since the dawn of time there is still a relatively small portion of the population with strong statistical literacy, and computer science is similarly old. Euclid invented several algorithms, one of which is used every time you make an HTTPS request; the fact that we use HTTPS every time we login to a website does not automatically imbue anyone with a knowledge of how those protocols work.
More established professional fields often have a bimodal wage distribution: a relatively small number of practitioners make quite a lot of money, and the majority of them earn a good wage but do not find themselves in the top 1% of earners. The National Association for Law Placement collects data that can be used to visualize this phenomenon in stark clarity. A huge share of law graduates make between $45,00 and $65,000 — a good wage, but hardly the salary we associate with a “top professional”.
We tend to think that all law graduates are on track to becoming partners at a law firm when really there are many paths: paralegal, clerk, public defender, judge, legal services for businesses, contract writing, and so on. Computer science graduates also have many options for their professional practice, from web development to embedded systems. As a basic level of programming literacy continues to become an expectation, rather than a “nice to have”, I suspect a similar distribution will emerge in programming jobs.
While there will always be a cohort of programmers making a lot of money to push on the edges of technology, there will be a growing body of middle-class programmers powering the new computer-centric economy. The average salary for web developers will surely decrease over time. That said, I suspect that the number of jobs for “programmers” in general will only continue to grow. As worker supply begins to meet demand, hopefully we will see a healthy boom in a variety of middle-class programming jobs. There will also continue to be a top-professional salary available for those programmers who are redefining what is possible.
Regardless of which cohort of programmers you’re in, a career in technology means continuing your education throughout your life. If you want to stay in the second cohort of programmers you may want to invest in learning how to create the machines, rather than simply operate them.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
A curious human on a quest to watch the world learn.
",friend mine recently posed question ive heard many times varying forms forums think lowerlevel programming jobs going go way dodo seems bit like massive job bubble thats gonna burst opinion one things keeping tech lowerlevel computer sciencerelated jobs prestigious wellpaid ridiculous industry jargon public ignorance computers going go away next 10 years question simultaneously point future technology jobs exemplary pervasive misunderstandings regarding field software engineering true great deal ridiculous industry jargon equally many genuinely difficult problems waiting solved right skillset software jobs definitely going away programmers right experience knowledge continue prestigious well remunerated many years come example look recent explosion ai researcher salaries corresponding dearth available talent staying relevant ever changing technology landscape challenge looking technologies replacing programmers status quo able predict jobs might disappear market additionally predict salaries demand specific skills might change consider growing body people learning program hannah pointed public ignorance computers keeping wages high program public becoming computer savvy year fear automation replacing jobs neither new unfounded field especially technology market forces drive corporations toward automation commodification gartners hype cycles one way contextualizing phenomenon time goes specific ideas technologies push towards plateau productivity eventually automated looking history one must conclude automation power destroy specific job markets diverse industries ranging crop harvesting automobile assembly technology advances consistently replaced augmented human labor reduce costs professor put way compilers course take historical note textile steel industries want build machines tools want operate machines metaphor machine computer programming language professor really asking want build websites using javascript want build v8 engine powers javascript creation websites automated wordpress others today v8 hand growing body competitors solving open research questions languages come go many fortran job openings always someone building next language lucky us programming language implementations written programming languages machine operator software puts path machine creator way true steel mill workers past growing number languages interpreters compilers shows us every jobdestroying machine also brings new opportunities improve machines maintain machines forth despite growing body jobs longer exist yet moment history humanity collectively said guess isnt work left us commodification coming us software engineers throughout history human labor consistently replaced nonhumans augmented require fewer less skilled humans selfdriving cars trucks flavor week grand human tradition cycle creation automation fact life natural question answer next jobs industries risk aws heroku similar hosting platforms forever changed role system administratordevops engineer internet businesses used absolutely need server master someone well versed linux someone could configure server apache nginx someone could physically wire server routers physical components could also configure routing tables software required make server accessible public web definitely still people applying skillset professionally aws making skills obsolete especially lower experience levels physical side things lucrative roles within amazon netflix google people deep expertise networking infrastructure much less demand smalltomedium business scale business intelligence tools salesforce tableau spotfire also beginning occupy spaces historically held software engineers systems reduced demand inhouse database administrators also increased demand sql generalpurpose skill decreased demand inhouse reporting technology increased demand integration engineers automate flow data business thirdparty software platforms field previously dominated excel spreadsheets increasingly pushed towards scripting languages like python r towards sql data management jobs disappeared demand people write software seen increase overall data science fascinating example commodification level closer software scikitlearn tensorflow pytorch software libraries make easier people build machine learning applications without building algorithms scratch fact possible run dataset many different machine learning algorithms many different parameter sets algorithms little understanding algorithms actually implemented necessarily wise possible bet business intelligence companies trying integrate kinds algorithms tools next years well many ways data science looks like web development 58 years ago booming field little bit knowledge get door due skills gap web development bootcamps closing consolidating data science bootcamps popping place kaplan bought original web development bootcamp dev bootcamp started data science bootcamp metis decided close devbootcamp keep metis running content management systems among visible tools automating away need software engineer squarespace wordpress among popular cms systems today platforms significantly reducing value people little bit front end web development skill fact barriers making website getting online come dramatically people zero programming experience successfully launching websites every day people arent making deeply interactive websites serve billions people absolutely make websites businesses give customers information need lovely landing page information find establishment contact enough local restaurant bar retail store business primarily internet business never easier get working site public web result thriving industry web contractors quickly set simple website get online becoming less lucrative finally would border hubris ignore physical aspect computers context words mike acton software platform hardware platform software people would wise study least little computer architecture electrical engineering big shake hardware arrival consumer grade quantum computers would change everything professional software engineering quantum computers still ways growing interest gpus drive toward parallelization imminent shift cpu speeds stagnant several years time seemingly unquenchable thirst machine learning big data emerged desire ever process large datasets openmp opencl go cuda parallel processing languages frameworks continue become mainstream competitively fast nearterm future significant parallelization requirement across board highperformance niches like operating systems infrastructure video games websites ubiquitous 2017 stack overflow survey reports 15 professional software engineers working internetweb services company bureau labor statistics expects growth web development continue much faster average 24 2014 2024 due visibility massive focus solving skills gap industry coding bootcamps teach web development almost exclusively web development online courses flooded udemy udacity coursera similar marketplaces combination increasing automation throughout web development technology stack influx new entry level programmers explicit focus web development led predict slide towards blue collar market software developers gone suggesting push towards blue collar market strategy architected big tech firms others course say headed another bursting bubble change demand specific technologies news languages frameworks always rising falling technology web development current incarnation js king eventually go way web development early 2000s remember flash new lot people receiving education explicitly solely current trendy web development frameworks decide label react developer remember people identified flash developers banking career specific language framework technology game roulette course quite difficult predict technologies remain relevant youre going go something suggest relying lindy effect picking something like c already withstood test time next generation level de facto tech literacy generation x even millennials one outcome using next generation cms tools given tools get better young workers better using combination definitely bring value lowlevel web development skills eager skilled youngsters enter job market high schools catching well offering computer science programming classes well educated high school students likely entering workforce programming interns immediately upon graduation another big group newcomers programming mbas data analysts job listings dominated excel starting list sql nice even requirement tools tableau spotfire salesforce webbased metrics systems continue replace spreadsheet primary tool report generation continues data analysts learn use sql directly simply easier exporting data spreadsheet people looking climb ranks outperform peers roles taking online courses learn databases statistical programming languages new skills begin position data scientists learning combination machine learning statistical libraries look metis curriculum prime example path finally number people earning computer science software engineering degrees continues climb purdue example reports applications cs program doubled five years cornell reports similar explosion cs graduates trend isnt surprising given growth ubiquity software hard young people imagine computers play smaller role futures study something thats going give job security common argument industry nowadays around idea education receive fouryear computer science program mostly unnecessary cruft heard argument repeatedly halls bootcamps web development shops online big names field piece eric elliott opposition view popular well going far saying programmers earn masters degree like eric elliott think good options ever break programming 4 year degree might best option many simultaneously agree william bain foundational skills apply across programming disciplines crucial career longevity still hard find information outside university courses ive written previously skills think aspiring engineers learn foundation long career joined bradfield order help share knowledge coding schools many shapes sizes becoming ubiquitous good reasons quite lot learn programming without getting minutia big notation obscure data structures algorithmic trivia however true fresh graduates stanford competing jobs fresh graduates hack reactor true one two subindustries code school bootcamp graduates yet applying work embedded systems cryptographysecurity robotics network infrastructure ai research development yet fields like web development growing quickly programmingrelated skills already started transition rare skill baseline expectation conversely engineering goes creating beastly engines like aws anything common big companies driving technology forward amazon google facebook nvidia spacex typically looking people basic understanding javascript aws serves billions users per day support kind load aws infrastructure engineer needs deep knowledge network protocols computer architecture several years relevant experience discipline amateurs artisans prestigious firms solving research problems building systems truly pushing boundaries possible yet still struggle fill open roles even basic programming skills increasingly common people write algorithms predict changes genetic sequences yield desired result going highly valuable future people program satellites spacecraft automate machinery continue highly valued fields lend readily 3 month intensive program front end web development least without significant prior experience computer science starts word computer assumed young people innate understanding 2025 unfortunately ubiquity computers created new generation people de facto understand mathematics computer science network infrastructure electrical engineering computer literacy study computation despite mathematics existed since dawn time still relatively small portion population strong statistical literacy computer science similarly old euclid invented several algorithms one used every time make https request fact use https every time login website automatically imbue anyone knowledge protocols work established professional fields often bimodal wage distribution relatively small number practitioners make quite lot money majority earn good wage find top 1 earners national association law placement collects data used visualize phenomenon stark clarity huge share law graduates make 4500 65000 good wage hardly salary associate top professional tend think law graduates track becoming partners law firm really many paths paralegal clerk public defender judge legal services businesses contract writing computer science graduates also many options professional practice web development embedded systems basic level programming literacy continues become expectation rather nice suspect similar distribution emerge programming jobs always cohort programmers making lot money push edges technology growing body middleclass programmers powering new computercentric economy average salary web developers surely decrease time said suspect number jobs programmers general continue grow worker supply begins meet demand hopefully see healthy boom variety middleclass programming jobs also continue topprofessional salary available programmers redefining possible regardless cohort programmers youre career technology means continuing education throughout life want stay second cohort programmers may want invest learning create machines rather simply operate quick cheer standing ovation clap show much enjoyed story curious human quest watch world learn,en,"['Gartner', 'JavaScript', 'WordPress', 'the System Administrator/DevOps', 'NGINX', 'AWS', 'Amazon', 'Netflix', 'Google', 'Business Intelligence', 'SalesForce', 'SQL', 'PyTorch', 'Kaplan', 'DevBootcamp', 'Content', 'SquareSpace', 'CMS', 'Quantum', 'CUDA', 'The Bureau of Labor Statistics', 'Web Development', 'Udemy', 'Udacity', 'Coursera', 'Tableau', 'CS', 'Cornell', 'Computer Science', 'Bradfield', 'Stanford', 'The National Association for Law Placement']"
142,Berit Anderson,1600,The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium,"By Berit Anderson and Brett Horvath
This piece was originally published at Scout.ai.
“This is a propaganda machine. It’s targeting people individually to recruit them to an idea. It’s a level of social engineering that I’ve never seen before. They’re capturing people and then keeping them on an emotional leash and never letting them go,” said professor Jonathan Albright.
Albright, an assistant professor and data scientist at Elon University, started digging into fake news sites after Donald Trump was elected president. Through extensive research and interviews with Albright and other key experts in the field, including Samuel Woolley, Head of Research at Oxford University’s Computational Propaganda Project, and Martin Moore, Director of the Centre for the Study of Media, Communication and Power at Kings College, it became clear to Scout that this phenomenon was about much more than just a few fake news stories. It was a piece of a much bigger and darker puzzle — a Weaponized AI Propaganda Machine being used to manipulate our opinions and behavior to advance specific political agendas.
By leveraging automated emotional manipulation alongside swarms of bots, Facebook dark posts, A/B testing, and fake news networks, a company called Cambridge Analytica has activated an invisible machine that preys on the personalities of individual voters to create large shifts in public opinion. Many of these technologies have been used individually to some effect before, but together they make up a nearly impenetrable voter manipulation machine that is quickly becoming the new deciding factor in elections around the world.
Most recently, Analytica helped elect U.S. President Donald Trump, secured a win for the Brexit Leave campaign, and led Ted Cruz’s 2016 campaign surge, shepherding him from the back of the GOP primary pack to the front.
The company is owned and controlled by conservative and alt-right interests that are also deeply entwined in the Trump administration. The Mercer family is both a major owner of Cambridge Analytica and one of Trump’s biggest donors. Steve Bannon, in addition to acting as Trump’s Chief Strategist and a member of the White House Security Council, is a Cambridge Analytica board member. Until recently, Analytica’s CTO was the acting CTO at the Republican National Convention.
Presumably because of its alliances, Analytica has declined to work on any democratic campaigns — at least in the U.S. It is, however, in final talks to help Trump manage public opinion around his presidential policies and to expand sales for the Trump Organization. Cambridge Analytica is now expanding aggressively into U.S. commercial markets and is also meeting with right-wing parties and governments in Europe, Asia, and Latin America.
Cambridge Analytica isn’t the only company that could pull this off — but it is the most powerful right now. Understanding Cambridge Analytica and the bigger AI Propaganda Machine is essential for anyone who wants to understand modern political power, build a movement, or keep from being manipulated. The Weaponized AI Propaganda Machine it represents has become the new prerequisite for political success in a world of polarization, isolation, trolls, and dark posts.
There’s been a wave of reporting on Cambridge Analytica itself and solid coverage of individual aspects of the machine — bots, fake news, microtargeting — but none so far (that we have seen) that portrays the intense collective power of these technologies or the frightening level of influence they’re likely to have on future elections.
In the past, political messaging and propaganda battles were arms races to weaponize narrative through new mediums — waged in print, on the radio, and on TV. This new wave has brought the world something exponentially more insidious — personalized, adaptive, and ultimately addictive propaganda. Silicon Valley spent the last ten years building platforms whose natural end state is digital addiction. In 2016, Trump and his allies hijacked them.
We have entered a new political age. At Scout, we believe that the future of constructive, civic dialogue and free and open elections depends on our ability to understand and anticipate it.
Welcome to the age of Weaponized AI Propaganda.
Any company can aggregate and purchase big data, but Cambridge Analytica has developed a model to translate that data into a personality profile used to predict, then ultimately change your behavior. That model itself was developed by paying a Cambridge psychology professor to copy the groundbreaking original research of his colleague through questionable methods that violated Amazon’s Terms of Service. Based on its origins, Cambridge Analytica appears ready to capture and buy whatever data it needs to accomplish its ends.
In 2013, Dr. Michal Kosinski, then a PhD. candidate at the University of Cambridge’s Psychometrics Center, released a groundbreaking study announcing a new model he and his colleagues had spent years developing. By correlating subjects’ Facebook Likes with their OCEAN scores — a standard-bearing personality questionnaire used by psychologists — the team was able to identify an individual’s gender, sexuality, political beliefs, and personality traits based only on what they had liked on Facebook.
According to Zurich’s Das Magazine, which profiled Kosinski in late 2016, “with a mere ten ‘likes’ as input his model could appraise a person’s character better than an average coworker. With seventy, it could ‘know’ a subject better than a friend; with 150 likes, better than their parents. With 300 likes, Kosinski’s machine could predict a subject’s behavior better than their partner. With even more likes it could exceed what a person thinks they know about themselves.”
Not long afterward, Kosinski was approached by Aleksandr Kogan, a fellow Cambridge professor in the psychology department, about licensing his model to SCL Elections, a company that claimed its specialty lay in manipulating elections. The offer would have meant a significant payout for Kosinki’s lab. Still, he declined, worried about the firm’s intentions and the downstream effects it could have.
It had taken Kosinski and his colleagues years to develop that model, but with his methods and findings now out in the world, there was little to stop SCL Elections from replicating them. It would seem they did just that.
According to a Guardian investigation, in early 2014, just a few months after Kosinski declined their offer, SCL partnered with Kogan instead. As a part of their relationship, Kogan paid Amazon Mechanical Turk workers $1 each to take the OCEAN quiz. There was just one catch: To take the quiz, users were required to provide access to all of their Facebook data. They were told the data would be used for research. The job was reported to Amazon for violating the platform’s Terms of Service. What many of the Turks likely didn’t realize: According to documents reviewed by The Guardian, “Kogan also captured the same data for each person’s unwitting friends.”
The data gathered from Kogan’s study went on to birth Cambridge Analytica, which spun out of SCL Elections soon after. The name, metaphorically at least, was a nod to Kogan’s work — and a dig at Kosinski.
But that early trove of user data was just the beginning — just the seed Analytica needed to build its own model for analyzing users personalities without having to rely on the lengthy OCEAN test.
After a successful proof of concept and backed by wealthy conservative investors, Analytica went on a data shopping spree for the ages, snapping up data about your shopping habits, land ownership, where you attend church, what stores you visit, what magazines you subscribe to — all of which is for sale from a range of data brokers and third party organizations selling information about you. Analytica aggregated this data with voter roles, publicly available online data — including Facebook likes — and put it all into its predictive personality model.
Nix likes to boast that Analytica’s personality model has allowed it to create a personality profile for every adult in the U.S. — 220 million of them, each with up to 5,000 data points. And those profiles are being continually updated and improved the more data you spew out online.
Albright also believes that your Facebook and Twitter posts are being collected and integrated back into Cambridge Analytica’s personality profiles. “Twitter and also Facebook are being used to collect a lot of responsive data because people are impassioned, they reply, they retweet, but they also include basically their entire argument and their entire background on this topic,” he explains.
Collecting massive quantities of data about voters’ personalities might seem unsettling, but it’s actually not what sets Cambridge Analytica apart. For Analytica and other companies like them, it’s what they do with that data that really matters.
“Your behavior is driven by your personality and actually the more you can understand about people’s personality as psychological drivers, the more you can actually start to really tap in to why and how they make their decisions,” Nix explained to Bloomberg’s Sasha Issenburg. “We call this behavioral microtargeting and this is really our secret sauce, if you like. This is what we’re bringing to America.”
Using those dossiers, or psychographic profiles as Analytica calls them, Cambridge Analytica not only identifies which voters are most likely to swing for their causes or candidates; they use that information to predict and then change their future behavior.
As Vice reported recently, Kosinski and a colleague are now working on a new set of research, yet to be published, that addresses the effectiveness of these methods. Their early findings: Using personality targeting, Facebook posts can attract up to 63 percent more clicks and 1,400 more conversions.
Scout reached out to Cambridge Analytica with a detailed list of questions about their communications tactics, but the company declined to answer any questions or to comment on any of their tactics.
But researchers across the technology and media ecosystem who have been following Cambridge Analytica’s political messaging activities have unearthed an expansive, adaptive online network that automates the manipulation of voters at a scale never before seen in political messaging.
“They [the Trump campaign] were using 40–50,000 different variants of ad every day that were continuously measuring responses and then adapting and evolving based on that response,” Martin Moore, director of Kings College’s Centre for the Study of Media, Communication and Power, told The Guardian in early December. “It’s all done completely opaquely and they can spend as much money as they like on particular locations because you can focus on a five-mile radius.”
Where traditional pollsters might ask a person outright how they plan to vote, Analytica relies not on what they say but what they do, tracking their online movements and interests and serving up multivariate ads designed to change a person’s behavior by preying on individual personality traits.
“For example,” Nix wrote in an op-ed last year about Analytica’s work on the Cruz campaign, ”our issues model identified that there was a small pocket of voters in Iowa who felt strongly that citizens should be required by law to show photo ID at polling stations.”
“Leveraging our other data models, we were able to advise the campaign on how to approach this issue with specific individuals based on their unique profiles in order to use this relatively niche issue as a political pressure point to motivate them to go out and vote for Cruz. For people in the ‘Temperamental’ personality group, who tend to dislike commitment, messaging on the issue should take the line that showing your ID to vote is ‘as easy as buying a case of beer’. Whereas the right message for people in the ‘Stoic Traditionalist’ group, who have strongly held conventional views, is that showing your ID in order to vote is simply part of the privilege of living in a democracy.”
For Analytica, the feedback is instant and the response automated: Did this specific swing voter in Pennsylvania click on the ad attacking Clinton’s negligence over her email server? Yes? Serve her more content that emphasizes failures of personal responsibility. No? The automated script will try a different headline, perhaps one that plays on a different personality trait — say the voter’s tendency to be agreeable toward authority figures. Perhaps: “Top Intelligence Officials Agree: Clinton’s Emails Jeopardized National Security.”
Much of this is done through Facebook dark posts, which are only visible to those being targeted.
Based on users’ response to these posts, Cambridge Analytica was able to identify which of Trump’s messages were resonating and where. That information was also used to shape Trump’s campaign travel schedule. If 73 percent of targeted voters in Kent County, Mich. clicked on one of three articles about bringing back jobs? Schedule a Trump rally in Grand Rapids that focuses on economic recovery.
Political analysts in the Clinton campaign, who were basing their tactics on traditional polling methods, laughed when Trump scheduled campaign events in the so-called blue wall — a group of states that includes Michigan, Pennsylvania, and Wisconsin and has traditionally fallen to Democrats. But Cambridge Analytica saw they had an opening based on measured engagement with their Facebook posts. It was the small margins in Michigan, Pennsylvania and Wisconsin that won Trump the election.
Dark posts were also used to depress voter turnout among key groups of democratic voters. “In this election, dark posts were used to try to suppress the African-American vote,” wrote journalist and Open Society fellow McKenzie Funk in a New York Times editorial. “According to Bloomberg, the Trump campaign sent ads reminding certain selected black voters of Hillary Clinton’s infamous ‘super predator’ line. It targeted Miami’s Little Haiti neighborhood with messages about the Clinton Foundation’s troubles in Haiti after the 2010 earthquake.’”
Because dark posts are only visible to the targeted users, there’s no way for anyone outside of Analytica or the Trump campaign to track the content of these ads. In this case, there was no SEC oversight, no public scrutiny of Trump’s attack ads. Just the rapid-eye-movement of millions of individual users scanning their Facebook feeds.
In the weeks leading up to a final vote, a campaign could launch a $10–100 million dark post campaign targeting just a few million voters in swing districts and no one would know. This may be where future ‘black-swan’ election upsets are born.
“These companies,” Moore says, “have found a way of transgressing 150 years of legislation that we’ve developed to make elections fair and open.”
Meanwhile, surprised by the results of the 2016 presidential race, Albright started looking into the ‘fake news problem’. As a part of his research, Albright scraped 306 fake news sites to determine how exactly they were all connected to each other and the mainstream news ecosystem. What he found was unprecedented — a network of 23,000 pages and 1.3 million hyperlinks.
“The sites in the fake news and hyper-biased #MCM network,” Albright writes, “have a very small ‘node’ size — this means they are linking out heavily to mainstream media, social networks, and informational resources (most of which are in the ‘center’ of the network), but not many sites in their peer group are sending links back.”
These sites aren’t owned or operated by any one individual entity, he says, but together they have been able to game Search Engine Optimization, increasing the visibility of fake and biased news anytime someone Googles an election-related term online — Trump, Clinton, Jews, Muslims, abortion, Obamacare.
“This network,” Albright wrote in a post exploring his findings, “is triggered on-demand to spread false, hyper-biased, and politically-loaded information.”
Even more shocking to him though was that this network of fake news creates a powerful infrastructure for companies like Cambridge Analytica to track voters and refine their personality targeting models
“I scraped the trackers on these sites and I was absolutely dumbfounded. Every time someone likes one of these posts on Facebook or visits one of these websites, the scripts are then following you around the web. And this enables data-mining and influencing companies like Cambridge Analytica to precisely target individuals, to follow them around the web, and to send them highly personalised political messages.”
The web of fake and biased news that Albright uncovered created a propaganda wave that Cambridge Analytica could ride and then amplify. The more fake news that users engage with, the more addictive Analytica’s personality engagement algorithms can become.
Voter 35423 clicked on a fake story about Hillary’s sex-trafficking ring? Let’s get her to engage with more stories about Hillary’s supposed history of murder and sex trafficking.
The synergy between fake-content networks, automated message testing, and personality profiling will rapidly spread to other digital mediums. Albright’s most-recent research focuses on an artificial intelligence that automatically creates YouTube videos about news and current events. The AI, which reacts to trending topics on Facebook and Twitter, pairs images and subtitles with a computer generated voiceover. It spooled out nearly 80,000 videos through 19 different channels in just a few days.
Given its rapid development, the technology community needs to anticipate how AI propaganda will soon be used for emotional manipulation in mobile messaging, virtual reality, and augmented reality.
If fake news created the scaffolding for this new automated political propaganda machine, bots, or fake social media profiles, have become its foot soldiers — an army of political robots used to control conversations on social media and silence and intimidate journalists and others who might undermine their messaging.
Samuel Woolley, Director of Research at the University of Oxford’s Computational Propaganda Project and a fellow at Google’s Jigsaw project, has dedicated his career to studying the role of bots in online political organizing — who creates them, how they’re used, and to what end.
Research by Woolley and his Oxford-based team in the lead-up to the 2016 election found that pro-Trump political messaging relied heavily on bots to spread fake news and discredit Hillary Clinton. By election day, Trump’s bots outnumbered hers, 5:1.
“The use of automated accounts was deliberate and strategic throughout the election, most clearly with pro-Trump campaigners and programmers who carefully adjusted the timing of content production during the debates, strategically colonized pro-Clinton hashtags, and then disabled activities after Election Day,” the study by Woolley’s team reported.
Woolley believes it’s likely that Cambridge Analytica was responsible for subcontracting the creation of those Trump bots, though he says he doesn’t have direct proof.
Still, if anyone outside of the Trump campaign is qualified to speculate about who created those bots, it would be Woolley. Led by Dr. Philip Howard, the team’s Principal Investigator, Woolley and his colleagues have been tracking the use of bots in political organizing since 2010. That’s when Howard, buried deep in research about the role Twitter played in the Arab Spring, first noticed thousands of bots coopting hashtags used by protesters.
Curious, he and his team began reaching out to hackers, botmakers, and political campaigns, getting to know them and trying to understand their work and motivations. Eventually, those creators would come to make up an informal network of nearly 100 informants that have kept Howard and his colleagues in the know about these bots over the last few years.
Before long, Howard and his team were getting the heads up about bot propaganda campaigns from the creators themselves. As more and more major international political figures began using botnets as just another tool in their campaigns, Howard, Woolley and the rest of their team studied the action unfolding.
The world these informants revealed is an international network of governments, consultancies (often with owners or top management just one degree away from official government actors), and individuals who build and maintain massive networks of bots to amplify the messages of political actors, spread messages counter to those of their opponents, and silence those whose views or ideas might threaten those same political actors.
“The Chinese, Iranian, and Russian, governments employ their own social-media experts and pay small amounts of money to large numbers of people to generate pro-government messages,” Howard and his coauthors wrote in a 2015 research paper about the use of bots in the Venezuelan election.
Depending on which of those three categories bot creators fall into — government, consultancy or individual — they’re just as likely to be motivated by political beliefs as they are the opportunity to auction off their networks of digital influence to the highest bidder.
Not all bots are created equal. The average, run-of-the-mill Twitter bot is literally a robot — often programmed to retweet specific accounts to help popularize specific ideas or viewpoints. They also frequently respond automatically to Twitter users who use certain keywords or hashtags — often with pre-written slurs, insults or threats.
High-end bots on the other hand are more analog, operated by real people. They assume fake identities with distinct personalities and their responses to other users online are specific, intended to change their opinions or those of their followers by attacking their viewpoints. They have online friends and followers. They’re also far less likely to be discovered — and their accounts deactivated — by Facebook or Twitter.
Working on their own, Woolley estimates, an individual could build and maintain up to 400 of these boutique Twitter bots; on Facebook, which he says is more effective at identifying and shutting down fake accounts, an individual could manage 10–20.
As a result, these high-quality botnets are often used for multiple political campaigns. During the Brexit referendum, the Oxford team watched as one network of bots, previously used to influence the conversation around the Israeli/Palestinian conflict, was reactivated to fight for the Leave campaign. Individual profiles were updated to reflect the new debate, their personal taglines changed to ally with their new allegiances — and away they went.
Russia’s bot army has been the subject of particular scrutiny since a CIA special report revealed that Russia had been working to influence the election in Trump’s favor. Recently, reporter/comedian Samantha Bee traveled to Moscow to interview two paid Russian troll operators.
Clad in black ski masks to obscure their identities, the two talked with Bee about how and why they were using their accounts during the U.S. election. They told Bee that they pose as Americans online and target sites like The Wall Street Journal, The New York Post, The Washington Post, Facebook and Twitter. Their goal, they said, is to “piss off” other social media users, change their opinions, and silence their opponents.
Or, to put it in the words of Russian Troll #1, “when your opponent just ... shut up.”
The 2016 U.S. election is over, but the Weaponized AI Propaganda Machine is just warming up. And while each of its components would be worrying on its own, together, they represent the arrival of a new era in political messaging — a steel wall between campaign winners and losers that can only be mounted by gathering more data, creating better personality analyses, rapid development of engagement AI, and hiring more trolls.
At the moment, Trump and Cambridge Analytica are lapping their opponents. The more data they gather about individuals, the more Analytica and, by extension, Trump’s presidency will benefit from the network effects of their work — and the harder it will become to counter or fight back against their messaging in the court of public opinion.
Each Tweet that echoes forth from the @realDonaldTrump and @POTUS accounts, announcing and defending the administration’s moves, is met with a chorus of protest and argument. But even that negative engagement becomes a valuable asset for the Trump administration because every impulsive tweet can be treated like a psychographic experiment.
Trump’s first few weeks in office may have seemed bumbling, but they represent a clear signal of what lies ahead for Trump’s presidency — an executive order designed to enrage and distract his opponents as he and Bannon move to strip power from the judicial branch, install Bannon himself on the National Security Council, and issues a series of unconstitutional gag orders to federal agencies.
Cambridge Analytica may be slated to secure more federal contracts and is likely about to begin managing White House digital communications for the rest of the Trump Administration. What new predictive-personality targeting becomes possible with potential access to data on U.S. voters from the IRS, Department of Homeland Security, or the NSA?
“Lenin wanted to destroy the state, and that’s my goal, too. I want to bring everything crashing down and destroy all of today’s establishment,” Bannon said in 2013. We know that Steve Bannon subscribes to a theory of history where a messianic ‘Grey Warrior’ consolidates power and remakes the global order. Bolstered by the success of Brexit and the Trump victory, Breitbart (of which Bannon was Executive Chair until Trump’s election) and Cambridge Analytica (which Bannon sits on the board of) are now bringing fake news and automated propaganda to support far-right parties in at least Germany, France, Hungary, and India as well as parts of South America.
Never has such a radical, international political movement had the precision and power of this kind of propaganda technology. Whether or not leaders, engineers, designers, and investors in the technology community respond to this threat will shape major aspects of global politics for the foreseeable future.
The future of politics will not be a war of candidates or even cash on hand. And it’s not even about big data, as some have argued. Everyone will have access to big data — as Hillary did in the 2016 election.
From now on, the distinguishing factor between those who win elections and those who lose them will be how a candidate uses that data to refine their machine learning algorithms and automated engagement tactics. Elections in 2018 and 2020 won’t be a contest of ideas, but a battle of automated behavior change.
The fight for the future will be a proxy war of machine learning. It will be waged online, in secret, and with the unwitting help of all of you.
Anyone who wants to effect change needs to understand this new reality. It’s only by understanding this — and by building better automated engagement systems that amplify genuine human passion rather than manipulate it — that other candidates and causes around the globe will be able to compete.
Implication #1: Public Sentiment Turns Into High-Frequency Trading
Thanks to stock-trading algorithms, large portions of public stock and commodity markets no longer resemble a human system and, some would argue, no longer serve their purpose as a signal of value. Instead they’re a battleground for high-frequency trading algorithms attempting to influence price or find nano-leverage in price position.
In the near future, we may see a similar process unfold in our public debates. Instead of battling press conferences and opinion articles, public opinion about companies and politicians may turn into multi-billion dollar battles between competing algorithms, each deployed to sway public sentiment. Stock trading algorithms already exist that analyze millions of Tweets and online posts in real-time and make trades in a matter of milliseconds based on changes in public sentiment. Algorithmic trading and ‘algorithmic public opinion’ are already connected. It’s likely they will continue to converge.
Implication #2: Personalized, Automated Propaganda That Adapts to Your Weaknesses
What if President Trump’s 2020 re-election campaign didn’t just have the best political messaging, but 250 million algorithmic versions of their political message all updating in real-time, personalized to precisely fit the worldview and attack the insecurities of their targets? Instead of having to deal with misleading politicians, we may soon witness a Cambrian explosion of pathologically-lying political and corporate bots that constantly improve at manipulating us.
Implication #3: Not Just a Bubble, But Trapped in Your Own Ideological Matrix
Imagine that in 2020 you found out that your favorite politics page or group on Facebook didn’t actually have any other human members, but was filled with dozens or hundreds of bots that made you feel at home and your opinions validated? Is it possible that you might never find out?
Correction: An earlier version of this story mistakenly referred to Steve Bannon as the owner of Breitbart News. Until Trump’s election, Bannon served as the Executive Chair of Breitbart, a position in which it is common to assume ownership through stock holdings. This story has been updated to reflect that.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO & Co-founder @Join_Scout.
The social implications of technology.
",berit anderson brett horvath piece originally published scoutai propaganda machine targeting people individually recruit idea level social engineering ive never seen theyre capturing people keeping emotional leash never letting go said professor jonathan albright albright assistant professor data scientist elon university started digging fake news sites donald trump elected president extensive research interviews albright key experts field including samuel woolley head research oxford universitys computational propaganda project martin moore director centre study media communication power kings college became clear scout phenomenon much fake news stories piece much bigger darker puzzle weaponized ai propaganda machine used manipulate opinions behavior advance specific political agendas leveraging automated emotional manipulation alongside swarms bots facebook dark posts ab testing fake news networks company called cambridge analytica activated invisible machine preys personalities individual voters create large shifts public opinion many technologies used individually effect together make nearly impenetrable voter manipulation machine quickly becoming new deciding factor elections around world recently analytica helped elect us president donald trump secured win brexit leave campaign led ted cruzs 2016 campaign surge shepherding back gop primary pack front company owned controlled conservative altright interests also deeply entwined trump administration mercer family major owner cambridge analytica one trumps biggest donors steve bannon addition acting trumps chief strategist member white house security council cambridge analytica board member recently analyticas cto acting cto republican national convention presumably alliances analytica declined work democratic campaigns least us however final talks help trump manage public opinion around presidential policies expand sales trump organization cambridge analytica expanding aggressively us commercial markets also meeting rightwing parties governments europe asia latin america cambridge analytica isnt company could pull powerful right understanding cambridge analytica bigger ai propaganda machine essential anyone wants understand modern political power build movement keep manipulated weaponized ai propaganda machine represents become new prerequisite political success world polarization isolation trolls dark posts theres wave reporting cambridge analytica solid coverage individual aspects machine bots fake news microtargeting none far seen portrays intense collective power technologies frightening level influence theyre likely future elections past political messaging propaganda battles arms races weaponize narrative new mediums waged print radio tv new wave brought world something exponentially insidious personalized adaptive ultimately addictive propaganda silicon valley spent last ten years building platforms whose natural end state digital addiction 2016 trump allies hijacked entered new political age scout believe future constructive civic dialogue free open elections depends ability understand anticipate welcome age weaponized ai propaganda company aggregate purchase big data cambridge analytica developed model translate data personality profile used predict ultimately change behavior model developed paying cambridge psychology professor copy groundbreaking original research colleague questionable methods violated amazons terms service based origins cambridge analytica appears ready capture buy whatever data needs accomplish ends 2013 dr michal kosinski phd candidate university cambridges psychometrics center released groundbreaking study announcing new model colleagues spent years developing correlating subjects facebook likes ocean scores standardbearing personality questionnaire used psychologists team able identify individuals gender sexuality political beliefs personality traits based liked facebook according zurichs das magazine profiled kosinski late 2016 mere ten likes input model could appraise persons character better average coworker seventy could know subject better friend 150 likes better parents 300 likes kosinskis machine could predict subjects behavior better partner even likes could exceed person thinks know long afterward kosinski approached aleksandr kogan fellow cambridge professor psychology department licensing model scl elections company claimed specialty lay manipulating elections offer would meant significant payout kosinkis lab still declined worried firms intentions downstream effects could taken kosinski colleagues years develop model methods findings world little stop scl elections replicating would seem according guardian investigation early 2014 months kosinski declined offer scl partnered kogan instead part relationship kogan paid amazon mechanical turk workers 1 take ocean quiz one catch take quiz users required provide access facebook data told data would used research job reported amazon violating platforms terms service many turks likely didnt realize according documents reviewed guardian kogan also captured data persons unwitting friends data gathered kogans study went birth cambridge analytica spun scl elections soon name metaphorically least nod kogans work dig kosinski early trove user data beginning seed analytica needed build model analyzing users personalities without rely lengthy ocean test successful proof concept backed wealthy conservative investors analytica went data shopping spree ages snapping data shopping habits land ownership attend church stores visit magazines subscribe sale range data brokers third party organizations selling information analytica aggregated data voter roles publicly available online data including facebook likes put predictive personality model nix likes boast analyticas personality model allowed create personality profile every adult us 220 million 5000 data points profiles continually updated improved data spew online albright also believes facebook twitter posts collected integrated back cambridge analyticas personality profiles twitter also facebook used collect lot responsive data people impassioned reply retweet also include basically entire argument entire background topic explains collecting massive quantities data voters personalities might seem unsettling actually sets cambridge analytica apart analytica companies like data really matters behavior driven personality actually understand peoples personality psychological drivers actually start really tap make decisions nix explained bloombergs sasha issenburg call behavioral microtargeting really secret sauce like bringing america using dossiers psychographic profiles analytica calls cambridge analytica identifies voters likely swing causes candidates use information predict change future behavior vice reported recently kosinski colleague working new set research yet published addresses effectiveness methods early findings using personality targeting facebook posts attract 63 percent clicks 1400 conversions scout reached cambridge analytica detailed list questions communications tactics company declined answer questions comment tactics researchers across technology media ecosystem following cambridge analyticas political messaging activities unearthed expansive adaptive online network automates manipulation voters scale never seen political messaging trump campaign using 4050000 different variants ad every day continuously measuring responses adapting evolving based response martin moore director kings colleges centre study media communication power told guardian early december done completely opaquely spend much money like particular locations focus fivemile radius traditional pollsters might ask person outright plan vote analytica relies say tracking online movements interests serving multivariate ads designed change persons behavior preying individual personality traits example nix wrote oped last year analyticas work cruz campaign issues model identified small pocket voters iowa felt strongly citizens required law show photo id polling stations leveraging data models able advise campaign approach issue specific individuals based unique profiles order use relatively niche issue political pressure point motivate go vote cruz people temperamental personality group tend dislike commitment messaging issue take line showing id vote easy buying case beer whereas right message people stoic traditionalist group strongly held conventional views showing id order vote simply part privilege living democracy analytica feedback instant response automated specific swing voter pennsylvania click ad attacking clintons negligence email server yes serve content emphasizes failures personal responsibility automated script try different headline perhaps one plays different personality trait say voters tendency agreeable toward authority figures perhaps top intelligence officials agree clintons emails jeopardized national security much done facebook dark posts visible targeted based users response posts cambridge analytica able identify trumps messages resonating information also used shape trumps campaign travel schedule 73 percent targeted voters kent county mich clicked one three articles bringing back jobs schedule trump rally grand rapids focuses economic recovery political analysts clinton campaign basing tactics traditional polling methods laughed trump scheduled campaign events socalled blue wall group states includes michigan pennsylvania wisconsin traditionally fallen democrats cambridge analytica saw opening based measured engagement facebook posts small margins michigan pennsylvania wisconsin trump election dark posts also used depress voter turnout among key groups democratic voters election dark posts used try suppress africanamerican vote wrote journalist open society fellow mckenzie funk new york times editorial according bloomberg trump campaign sent ads reminding certain selected black voters hillary clintons infamous super predator line targeted miamis little haiti neighborhood messages clinton foundations troubles haiti 2010 earthquake dark posts visible targeted users theres way anyone outside analytica trump campaign track content ads case sec oversight public scrutiny trumps attack ads rapideyemovement millions individual users scanning facebook feeds weeks leading final vote campaign could launch 10100 million dark post campaign targeting million voters swing districts one would know may future blackswan election upsets born companies moore says found way transgressing 150 years legislation weve developed make elections fair open meanwhile surprised results 2016 presidential race albright started looking fake news problem part research albright scraped 306 fake news sites determine exactly connected mainstream news ecosystem found unprecedented network 23000 pages 13 million hyperlinks sites fake news hyperbiased mcm network albright writes small node size means linking heavily mainstream media social networks informational resources center network many sites peer group sending links back sites arent owned operated one individual entity says together able game search engine optimization increasing visibility fake biased news anytime someone googles electionrelated term online trump clinton jews muslims abortion obamacare network albright wrote post exploring findings triggered ondemand spread false hyperbiased politicallyloaded information even shocking though network fake news creates powerful infrastructure companies like cambridge analytica track voters refine personality targeting models scraped trackers sites absolutely dumbfounded every time someone likes one posts facebook visits one websites scripts following around web enables datamining influencing companies like cambridge analytica precisely target individuals follow around web send highly personalised political messages web fake biased news albright uncovered created propaganda wave cambridge analytica could ride amplify fake news users engage addictive analyticas personality engagement algorithms become voter 35423 clicked fake story hillarys sextrafficking ring lets get engage stories hillarys supposed history murder sex trafficking synergy fakecontent networks automated message testing personality profiling rapidly spread digital mediums albrights mostrecent research focuses artificial intelligence automatically creates youtube videos news current events ai reacts trending topics facebook twitter pairs images subtitles computer generated voiceover spooled nearly 80000 videos 19 different channels days given rapid development technology community needs anticipate ai propaganda soon used emotional manipulation mobile messaging virtual reality augmented reality fake news created scaffolding new automated political propaganda machine bots fake social media profiles become foot soldiers army political robots used control conversations social media silence intimidate journalists others might undermine messaging samuel woolley director research university oxfords computational propaganda project fellow googles jigsaw project dedicated career studying role bots online political organizing creates theyre used end research woolley oxfordbased team leadup 2016 election found protrump political messaging relied heavily bots spread fake news discredit hillary clinton election day trumps bots outnumbered 51 use automated accounts deliberate strategic throughout election clearly protrump campaigners programmers carefully adjusted timing content production debates strategically colonized proclinton hashtags disabled activities election day study woolleys team reported woolley believes likely cambridge analytica responsible subcontracting creation trump bots though says doesnt direct proof still anyone outside trump campaign qualified speculate created bots would woolley led dr philip howard teams principal investigator woolley colleagues tracking use bots political organizing since 2010 thats howard buried deep research role twitter played arab spring first noticed thousands bots coopting hashtags used protesters curious team began reaching hackers botmakers political campaigns getting know trying understand work motivations eventually creators would come make informal network nearly 100 informants kept howard colleagues know bots last years long howard team getting heads bot propaganda campaigns creators major international political figures began using botnets another tool campaigns howard woolley rest team studied action unfolding world informants revealed international network governments consultancies often owners top management one degree away official government actors individuals build maintain massive networks bots amplify messages political actors spread messages counter opponents silence whose views ideas might threaten political actors chinese iranian russian governments employ socialmedia experts pay small amounts money large numbers people generate progovernment messages howard coauthors wrote 2015 research paper use bots venezuelan election depending three categories bot creators fall government consultancy individual theyre likely motivated political beliefs opportunity auction networks digital influence highest bidder bots created equal average runofthemill twitter bot literally robot often programmed retweet specific accounts help popularize specific ideas viewpoints also frequently respond automatically twitter users use certain keywords hashtags often prewritten slurs insults threats highend bots hand analog operated real people assume fake identities distinct personalities responses users online specific intended change opinions followers attacking viewpoints online friends followers theyre also far less likely discovered accounts deactivated facebook twitter working woolley estimates individual could build maintain 400 boutique twitter bots facebook says effective identifying shutting fake accounts individual could manage 1020 result highquality botnets often used multiple political campaigns brexit referendum oxford team watched one network bots previously used influence conversation around israelipalestinian conflict reactivated fight leave campaign individual profiles updated reflect new debate personal taglines changed ally new allegiances away went russias bot army subject particular scrutiny since cia special report revealed russia working influence election trumps favor recently reportercomedian samantha bee traveled moscow interview two paid russian troll operators clad black ski masks obscure identities two talked bee using accounts us election told bee pose americans online target sites like wall street journal new york post washington post facebook twitter goal said piss social media users change opinions silence opponents put words russian troll 1 opponent shut 2016 us election weaponized ai propaganda machine warming components would worrying together represent arrival new era political messaging steel wall campaign winners losers mounted gathering data creating better personality analyses rapid development engagement ai hiring trolls moment trump cambridge analytica lapping opponents data gather individuals analytica extension trumps presidency benefit network effects work harder become counter fight back messaging court public opinion tweet echoes forth realdonaldtrump potus accounts announcing defending administrations moves met chorus protest argument even negative engagement becomes valuable asset trump administration every impulsive tweet treated like psychographic experiment trumps first weeks office may seemed bumbling represent clear signal lies ahead trumps presidency executive order designed enrage distract opponents bannon move strip power judicial branch install bannon national security council issues series unconstitutional gag orders federal agencies cambridge analytica may slated secure federal contracts likely begin managing white house digital communications rest trump administration new predictivepersonality targeting becomes possible potential access data us voters irs department homeland security nsa lenin wanted destroy state thats goal want bring everything crashing destroy todays establishment bannon said 2013 know steve bannon subscribes theory history messianic grey warrior consolidates power remakes global order bolstered success brexit trump victory breitbart bannon executive chair trumps election cambridge analytica bannon sits board bringing fake news automated propaganda support farright parties least germany france hungary india well parts south america never radical international political movement precision power kind propaganda technology whether leaders engineers designers investors technology community respond threat shape major aspects global politics foreseeable future future politics war candidates even cash hand even big data argued everyone access big data hillary 2016 election distinguishing factor win elections lose candidate uses data refine machine learning algorithms automated engagement tactics elections 2018 2020 wont contest ideas battle automated behavior change fight future proxy war machine learning waged online secret unwitting help anyone wants effect change needs understand new reality understanding building better automated engagement systems amplify genuine human passion rather manipulate candidates causes around globe able compete implication 1 public sentiment turns highfrequency trading thanks stocktrading algorithms large portions public stock commodity markets longer resemble human system would argue longer serve purpose signal value instead theyre battleground highfrequency trading algorithms attempting influence price find nanoleverage price position near future may see similar process unfold public debates instead battling press conferences opinion articles public opinion companies politicians may turn multibillion dollar battles competing algorithms deployed sway public sentiment stock trading algorithms already exist analyze millions tweets online posts realtime make trades matter milliseconds based changes public sentiment algorithmic trading algorithmic public opinion already connected likely continue converge implication 2 personalized automated propaganda adapts weaknesses president trumps 2020 reelection campaign didnt best political messaging 250 million algorithmic versions political message updating realtime personalized precisely fit worldview attack insecurities targets instead deal misleading politicians may soon witness cambrian explosion pathologicallylying political corporate bots constantly improve manipulating us implication 3 bubble trapped ideological matrix imagine 2020 found favorite politics page group facebook didnt actually human members filled dozens hundreds bots made feel home opinions validated possible might never find correction earlier version story mistakenly referred steve bannon owner breitbart news trumps election bannon served executive chair breitbart position common assume ownership stock holdings story updated reflect quick cheer standing ovation clap show much enjoyed story ceo cofounder join_scout social implications technology,en,"['Elon University', 'Research at Oxford University’s Computational Propaganda Project', 'the Centre for the Study of Media, Communication and Power at Kings College', 'Weaponized AI', 'Cambridge Analytica', 'Brexit Leave', 'GOP', 'the White House Security Council', 'CTO', 'the Republican National Convention', 'the Trump Organization', 'Weaponized AI Propaganda', 'Amazon', 'the University of Cambridge’s Psychometrics Center', 'Facebook', 'Das Magazine', 'Kosinski', 'Aleksandr Kogan', 'Cambridge', 'SCL Elections', 'SCL', 'Guardian', 'Kogan', 'Amazon Mechanical Turk', 'Bloomberg', 'Kings College’s Centre for the Study of Media, Communication and Power', 'New York Times', 'the Clinton Foundation', 'SEC', 'Search Engine Optimization', 'Facebook and Twitter', 'the University of Oxford’s Computational Propaganda Project', 'Google', 'Oxford', 'CIA', 'The Wall Street Journal', 'The New York Post', 'The Washington Post', 'Troll', 'Weaponized', '@realDonaldTrump', '@POTUS', 'Bannon', 'the National Security Council', 'White House', 'the Trump Administration', 'IRS', 'Department of Homeland Security', 'NSA', 'Breitbart News', 'the Executive Chair of Breitbart', 'CEO & Co-']"
143,Slav Ivanov,4400,37 Reasons why your Neural Network is not working – Slav,"The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.
Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?
A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.
A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:
If the steps above don’t do it, start going down the following big list and verify things one by one.
Check if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.
Try passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.
Your data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.
Check if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.
Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.
This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.
The cutoff point is up for debate, as this paper got above 50% accuracy on MNIST using 50% corrupted labels.
If your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.
Are there a 1000 class A images for every class B image? Then you might need to balance your loss function or try other class imbalance approaches.
If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, people say you need a 1000 images per class or more.
This can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.
This paper points out that having a very large batch can reduce the generalization ability of the model.
Thanks to @hengcherkeng for this one:
Did you standardize your input to have zero mean and unit variance?
Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.
If you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?
CS231n points out a common pitfall:
Also, check for different preprocessing in each sample or batch.
This will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.
Again from the excellent CS231n: Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.
After this, try increasing the regularization strength which should increase the loss.
If you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.
If you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.
If your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.
Sometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.
Did you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.
Check if you unintentionally disabled gradient updates for some layers/variables that should be learnable.
Maybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.
If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.
If you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: 1 2 3.
Overfit a small subset of the data and make sure it works. For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.
If unsure, use Xavier or He initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.
Maybe you using a particularly bad set of hyperparameters. If feasible, try a grid search.
Too much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “Practical Deep Learning for coders” course, Jeremy Howard advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.
Maybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.
Some frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.
Your choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.
Check this excellent post by Sebastian Ruder to learn more about gradient descent optimizers.
A low learning rate will cause your model to converge very slowly.
A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.
Play around with your current learning rate by multiplying it by 0.1 or 10.
Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:
Did I miss anything? Is anything wrong? Let me know by leaving a reply below.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Entrepreneur / Hacker
Machine learning, Deep learning and other types of learning.
",network training last 12 hours looked good gradients flowing loss decreasing came predictions zeroes background nothing detected wrong asked computer didnt answer start checking model outputting garbage example predicting mean outputs really poor accuracy network might training number reasons course many debugging sessions would often find checks ive compiled experience along best ideas around handy list hope would use lot things go wrong likely broken others usually start short list emergency first response steps dont start going following big list verify things one one check input data feeding network makes sense example ive mixed width height image sometimes would feed zeroes mistake would use batch printdisplay couple batches input target output make sure ok try passing random numbers instead actual data see error behaves way sure sign net turning data garbage point try debugging layer layer op op see things go wrong data might fine code passes input net might broken print input first layer operations check check input samples correct labels also make sure shuffling input samples works way output labels maybe nonrandom part relationship input output small compared random part one could argue stock prices like ie input sufficiently related output isnt universal way detect depends nature data happened scraped image dataset food site many bad labels network couldnt learn check bunch input samples manually see labels seem cutoff point debate paper got 50 accuracy mnist using 50 corrupted labels dataset hasnt shuffled particular order ordered label could negatively impact learning shuffle dataset avoid make sure shuffling input labels together 1000 class images every class b image might need balance loss function try class imbalance approaches training net scratch ie finetuning probably need lots data image classification people say need 1000 images per class happen sorted dataset ie first 10k samples contain class easily fixable shuffling dataset paper points large batch reduce generalization ability model thanks hengcherkeng one standardize input zero mean unit variance augmentation regularizing effect much combined forms regularization weight l2 dropout etc cause net underfit using pretrained model make sure using normalization preprocessing model training example image pixel range 0 1 1 1 0 255 cs231n points common pitfall also check different preprocessing sample batch help finding issue example target output object class coordinates try limiting prediction object class excellent cs231n initialize small parameters without regularization example 10 classes chance means get correct class 10 time softmax loss negative log probability correct class ln01 2302 try increasing regularization strength increase loss implemented loss function check bugs add unit tests often loss would slightly incorrect hurt performance network subtle way using loss function provided framework make sure passing expects example pytorch would mix nllloss crossentropyloss former requires softmax input latter doesnt loss composed several smaller loss functions make sure magnitude relative correct might involve testing different combinations loss weights sometimes loss best predictor whether network training properly use metrics like accuracy implement layers network check doublecheck make sure working intended check unintentionally disabled gradient updates layersvariables learnable maybe expressive power network enough capture target function try adding layers hidden units fully connected layers input looks like k h w 64 64 64 easy miss errors related wrong dimensions use weird numbers input dimensions example different prime numbers dimension check propagate network implemented gradient descent hand gradient checking makes sure backpropagation works like info 1 2 3 overfit small subset data make sure works example train 1 2 examples see network learn differentiate move samples per class unsure use xavier initialization also initialization might leading bad local minimum try different initialization see helps maybe using particularly bad set hyperparameters feasible try grid search much regularization cause network underfit badly reduce regularization dropout batch norm weightbias l2 regularization etc excellent practical deep learning coders course jeremy howard advises getting rid underfitting first means overfit training data sufficiently addressing overfitting maybe network needs time train starts making meaningful predictions loss steadily decreasing let train frameworks layers like batch norm dropout layers behave differently training testing switching appropriate mode might help network predict properly choice optimizer shouldnt prevent network training unless selected particularly bad hyperparameters however proper optimizer task helpful getting training shortest amount time paper describes algorithm using specify optimizer tend use adam plain sgd momentum check excellent post sebastian ruder learn gradient descent optimizers low learning rate cause model converge slowly high learning rate quickly decrease loss beginning might hard time finding good solution play around current learning rate multiplying 01 10 getting nan nonanumber much bigger issue training rnns hear approaches fix miss anything anything wrong let know leaving reply quick cheer standing ovation clap show much enjoyed story entrepreneur hacker machine learning deep learning types learning,en,"['MNIST', '@hengcherkeng', 'Augmentation', 'PyTorch', 'NLLLoss and CrossEntropyLoss', 'Gradient Descent', 'Xavier']"
144,Sirui Li,1,The evolution: a simple illustration – LeeThree on UX – Medium,"In the last paragraphs of Tools vs. Assistants: Part II, I’ve talked about the evolution of the society as the technology develops, in order to explain how we should apply software agents into our applications.
Here I come up with some graphs to illustrate my model of machine intelligence in the process of society evolution:
Firstly, consider the industrialization of the way people finish a certain task, say, writing a thank-you letter. (Let’s assume that this task is well defined, though I’m not going to define it.) When it came into being, only a few of the smartest people could complete this task. A minimal level of intelligence is required for this.
The techniques and methodologies for writing thank-you letters developed very slowly, until one day tools were introduced. Dictionaries and phrase-books greatly helped people with this task and more and more people learned how to write thank-you letters.
Once the most intelligent people all learned this, it was considered very cool if someone understood how to write beautiful thank-you letters and this soon became one of the trending topics among people. Better techniques were developed and more effective tools were invented, like electronic dictionaries and dictionary software. This field began to flourish.
Soon, it became so easy to write thank-you letters that everyone with a right mind could complete the task with the help of certain tools. However, the most amazing thank-you letters are always written by intelligent human beings who put their mind to it.
One day, an automatic thank-you letter software (ATULS) was developed. This buggy but yet usable tool was a great breakthrough because machines started to complete the task by themselves. On the basis of ATULS, more and better software tools were developed. Professional thank-you letter writers are gradually replaced by the machines, as more and more people thought the letters written by machines were better than theirs.
The software tools pushed the quality bar higher and higher. Only the most excellent and experienced writers could done better than machines. But who cares? The majority of people no longer paid attention to how the letters were written. They just took it for granted.
From here, we came to the end of the industrialization process of the task. It’s almost completely automated and machine intelligence has greatly improved the productivity. Very few people will remain doing this task.
An extra note: Some may argue that the level of intelligence is lowered by tools and machines because they make the task easier. It is not the case because tools and machines are part of this intelligence requirement. Only by making use of the intelligence from the tools or the machines, human could complete the task with less intelligence. Thus the level of intelligence required for the task is not reduced.
Let’s see the broader picture. This one is fairly easy to understand. The society becomes more and more sophisticated. Since the invention of machine intelligence, tasks with low level of sophistication are gradually done by machines. But more sophisticated tasks are being created, human beings are working on the most sophisticated tasks which the machines couldn’t do.
So what our society looks like now? This shape looks strange as it shows the relationship between the other two axes: intelligence and sophistication.
Basically, more intelligence are required to solve more sophisticated problems. But tasks could be done in many ways, that’s why it actually shows a colored band instead of a single curve.
As we can see, the most difficult problems, i.e., the most sophisticated tasks are still being done by most intelligent human beings, because they’re new and machine performance are usually not acceptable.
While time goes on, machine intelligence will take up more portion in the lower parts and human work will be “pushed” farther and higher like a sword cutting through the surface. (That’s a pretty reasonable illustration of the word “break-through”.)
I have to emphasize that, as the title says, this is a very very simple model. There’re quite a few assumptions for these graphs, so you might find them naïve and inaccurate:
The top five assumptions are very strong and not necessarily true. In fact, I personally doubt some of them because I don’t really agree with technocentrism. However, I do believe that, from the viewpoint of a technocentrist, this model could provide some insight on how technology works and develops.
P.S. I hope I could make a 3D model out of the three views from different axes but it seems very difficult to make it both accurate and illustrative. Perhaps I’ll make a video once I know how to do it.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@LeeThree9
This is a blog by @LeeThree9 on topics including user experience, human computer interaction, usability and interaction design.
",last paragraphs tools vs assistants part ii ive talked evolution society technology develops order explain apply software agents applications come graphs illustrate model machine intelligence process society evolution firstly consider industrialization way people finish certain task say writing thankyou letter lets assume task well defined though im going define came smartest people could complete task minimal level intelligence required techniques methodologies writing thankyou letters developed slowly one day tools introduced dictionaries phrasebooks greatly helped people task people learned write thankyou letters intelligent people learned considered cool someone understood write beautiful thankyou letters soon became one trending topics among people better techniques developed effective tools invented like electronic dictionaries dictionary software field began flourish soon became easy write thankyou letters everyone right mind could complete task help certain tools however amazing thankyou letters always written intelligent human beings put mind one day automatic thankyou letter software atuls developed buggy yet usable tool great breakthrough machines started complete task basis atuls better software tools developed professional thankyou letter writers gradually replaced machines people thought letters written machines better software tools pushed quality bar higher higher excellent experienced writers could done better machines cares majority people longer paid attention letters written took granted came end industrialization process task almost completely automated machine intelligence greatly improved productivity people remain task extra note may argue level intelligence lowered tools machines make task easier case tools machines part intelligence requirement making use intelligence tools machines human could complete task less intelligence thus level intelligence required task reduced lets see broader picture one fairly easy understand society becomes sophisticated since invention machine intelligence tasks low level sophistication gradually done machines sophisticated tasks created human beings working sophisticated tasks machines couldnt society looks like shape looks strange shows relationship two axes intelligence sophistication basically intelligence required solve sophisticated problems tasks could done many ways thats actually shows colored band instead single curve see difficult problems ie sophisticated tasks still done intelligent human beings theyre new machine performance usually acceptable time goes machine intelligence take portion lower parts human work pushed farther higher like sword cutting surface thats pretty reasonable illustration word breakthrough emphasize title says simple model therere quite assumptions graphs might find naive inaccurate top five assumptions strong necessarily true fact personally doubt dont really agree technocentrism however believe viewpoint technocentrist model could provide insight technology works develops ps hope could make 3d model three views different axes seems difficult make accurate illustrative perhaps ill make video know quick cheer standing ovation clap show much enjoyed story leethree9 blog leethree9 topics including user experience human computer interaction usability interaction design,en,"['ATULS', '@LeeThree9']"
145,Peter Sweeney,215,Siri’s Descendants: How intelligent assistants will evolve,"The internet swarms with intelligent assistants.
What started as an isolated app on the iPhone has evolved. Intelligent assistants constitute an entirely new network of activity. No longer confined to our personal computing devices, assistants are being embedded within every object of interest in the cloud and the internet of things.
Assistants have become far more nimble and lightweight than their monolithic ancestors; much more like smart ants than people. As specialists, they work cooperatively — sometimes competitively — to find information before people even realize they need it.
People are still communicating directly with assistants, although rarely using natural language. Implicit communication dominates. Assistants respond and react to our subtle contextual interactions, and to each other, within vast informational ecosystems.
This is how intelligent assistants evolved...
Intelligent assistants like Siri, Google Now, and Cortana are so young, it’s difficult to imagine how they will change; harder still to imagine how they might die. But if history is a guide, inevitably they will give way to entirely new product forms.
When pundits and analysts discuss the future of intelligent assistants, they typically extrapolate from the conceptual model of today’s assistants. The next version is always a better, smarter, faster version of the last, but it’s still the same species.
As detailed in Bianca Bosker’s Inside Story of Siri’s Origins, when Apple acquired Siri, the scope of the product’s capabilities actually narrowed. Using the audacious vision of Siri’s founders as a palette, Apple selected a narrower set of product values on which to focus.
The same force that reduced the scope of Apple’s Siri from a “do (everything) engine” to a much more narrow product is what keeps incumbents rooted to the existing concept of intelligent assistants.
When forecasting change, it’s not so much what the technology of intelligent assistants might support as what product leaders choose to pursue. While many brazenly contest existing markets, product leaders look for new, underserved areas of the landscape to exploit.
The future always surprises, but we can predict the trajectory of change by examining which product values are being embraced, and which ones are neglected.
Just like directions on a compass, the following maps point to fertile areas of the landscape, where new product forms may evolve.
Note that product values are often coupled due to technological constraints. Decisions along one axis constrain possibilities along another. These couplings are explored at a high level in two-dimensional perceptual maps: interface and distribution; knowledge and tasks; organization and autonomy.
The aspects of assistants that are most obvious to end-users are the interfaces (how we interact with assistants) and their mode of distribution (where people experience assistants).
Today’s assistants are overwhelmingly focused on natural language interfaces. The experience of assistants that speak our language and communicate like a person has come to define the product class.
This focus on natural language interfaces has biased the distribution of assistants to personal computing devices. Intelligent assistants embody any device capable of receiving and synthesizing speech, such as smartphones, desktops, wearables and cars.
The underserved areas of this map involve communications that are not based in natural language. For example, there’s much to learn about our needs and intentions based on context (where we are and what we’re doing) as well as on our ability to make inferences based on the associations that people form (for example, the way that people organize information or express their likes and dislikes). Natural language is but the tip of this much larger iceberg of communications.
These alternative forms of communication not only support individuals, but also groups. While it’s difficult to understand a room full of people all speaking at once, it’s much easier to understand their collaborative communications, such as their documents, click-paths, and sharing behavior. Therefore, the options for distributing intelligent assistants that use these implicit forms of communications are not constrained to personal computing devices, but may leverage entire networks.
As a simple example, consider how you highlight your interests as you browse a website. You focus your attention on specific pages within the site. You follow your interests as you navigate from page to page. You may choose to share some information within the site with a friend. Now compound this behaviour across every visitor to the site.
Intelligent assistants that are associated with the website can respond to these interactions to help the right information find each individual, as well as adapt the website to better address the needs of the entire group.
Intelligent assistants require domain knowledge to perform their tasks. For example, if your assistant is giving you advice on how to navigate to work, it needs to have knowledge about the geographic region (general knowledge) and knowledge of how you typically navigate (specific knowledge).
Tasks and knowledge are tightly coupled. As you increase the specificity or the personalization of the tasks, the underlying knowledge needs to be far more specific to support it.
Within this frame, today’s intelligent assistants are unabashedly generalists. They’re targeted to the masses. Like trivia buffs, their knowledge of the world is broad enough to be relevant to the needs of large groups of people, but few would describe them as experts. Their tasks are similarly general: retrieving information, providing navigational assistance, and answering simple questions.
The underserved landscape points to much more specific domains of knowledge, the purview of experts and our individual subjective knowledge. Assistants that become experts necessarily take on a smaller scope of activities. They can’t know and do everything, so they become smaller in scope.
The landscape for specific tasks is similarly underserved. Every website, every service, every app, and across the internet of things, everything embodies a collection of tasks that may be supported by intelligent assistants. In this environment, the metaphor of personal assistants quickly fragments into systems that are much more akin to colonies of ants.
The organizational structures in which assistants are placed constrain their autonomy. When embedded within a personal computing device, an intelligent assistant is directed to one-to-one interactions with their master.
Since these assistants are acting as an agent of the individual (and only that individual), their autonomy is necessarily limited. While you might be comfortable with your executive assistant drafting your messages, I suspect you’d be less comfortable with your smartphone doing the same.
In stark contrast, the underserved landscape embraces groups, both in terms of the interactions and the organizational structures.
As assistants get smaller and more specialized, they can become agents of much more specific objects of interest, like places, websites, applications, and services. Within these smaller realms of interest, their autonomy can be much more expansive. You might not want a machine to act as your representative, but you would probably feel more comfortable if it represented only the website you’re visiting.
With increased autonomy, the barriers to many-to-many interactions are removed. These small assistants can be organized as teams into networks, much like the documents that comprise a website, collaborating in an unfettered way with other assistants and the people that visit their realms.
This market analysis highlighted a number of underserved areas as fertile ground for the evolution of intelligent assistants. It grounds this vision in predictable market dynamics. There’s obviously no shortage of space or product values to explore in these underserved areas.
It says nothing, however, about when this future will arrive. Product evolution, like biological evolution, needs time and resources. The most important resource is the dedication of product leaders with the drive to pursue these new opportunities.
Are you an entrepreneur, technologist, or investor that’s changing the market for intelligent assistants? If so, I’d love to hear your vision of the future.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Entrepreneur and inventor. Interested in startups, AI or healthcare? Let's connect! https://www.linkedin.com/in/peterjsweeney/
Essays and analysis of artificial intelligence, machine learning, and intelligent assistants.
",internet swarms intelligent assistants started isolated app iphone evolved intelligent assistants constitute entirely new network activity longer confined personal computing devices assistants embedded within every object interest cloud internet things assistants become far nimble lightweight monolithic ancestors much like smart ants people specialists work cooperatively sometimes competitively find information people even realize need people still communicating directly assistants although rarely using natural language implicit communication dominates assistants respond react subtle contextual interactions within vast informational ecosystems intelligent assistants evolved intelligent assistants like siri google cortana young difficult imagine change harder still imagine might die history guide inevitably give way entirely new product forms pundits analysts discuss future intelligent assistants typically extrapolate conceptual model todays assistants next version always better smarter faster version last still species detailed bianca boskers inside story siris origins apple acquired siri scope products capabilities actually narrowed using audacious vision siris founders palette apple selected narrower set product values focus force reduced scope apples siri everything engine much narrow product keeps incumbents rooted existing concept intelligent assistants forecasting change much technology intelligent assistants might support product leaders choose pursue many brazenly contest existing markets product leaders look new underserved areas landscape exploit future always surprises predict trajectory change examining product values embraced ones neglected like directions compass following maps point fertile areas landscape new product forms may evolve note product values often coupled due technological constraints decisions along one axis constrain possibilities along another couplings explored high level twodimensional perceptual maps interface distribution knowledge tasks organization autonomy aspects assistants obvious endusers interfaces interact assistants mode distribution people experience assistants todays assistants overwhelmingly focused natural language interfaces experience assistants speak language communicate like person come define product class focus natural language interfaces biased distribution assistants personal computing devices intelligent assistants embody device capable receiving synthesizing speech smartphones desktops wearables cars underserved areas map involve communications based natural language example theres much learn needs intentions based context well ability make inferences based associations people form example way people organize information express likes dislikes natural language tip much larger iceberg communications alternative forms communication support individuals also groups difficult understand room full people speaking much easier understand collaborative communications documents clickpaths sharing behavior therefore options distributing intelligent assistants use implicit forms communications constrained personal computing devices may leverage entire networks simple example consider highlight interests browse website focus attention specific pages within site follow interests navigate page page may choose share information within site friend compound behaviour across every visitor site intelligent assistants associated website respond interactions help right information find individual well adapt website better address needs entire group intelligent assistants require domain knowledge perform tasks example assistant giving advice navigate work needs knowledge geographic region general knowledge knowledge typically navigate specific knowledge tasks knowledge tightly coupled increase specificity personalization tasks underlying knowledge needs far specific support within frame todays intelligent assistants unabashedly generalists theyre targeted masses like trivia buffs knowledge world broad enough relevant needs large groups people would describe experts tasks similarly general retrieving information providing navigational assistance answering simple questions underserved landscape points much specific domains knowledge purview experts individual subjective knowledge assistants become experts necessarily take smaller scope activities cant know everything become smaller scope landscape specific tasks similarly underserved every website every service every app across internet things everything embodies collection tasks may supported intelligent assistants environment metaphor personal assistants quickly fragments systems much akin colonies ants organizational structures assistants placed constrain autonomy embedded within personal computing device intelligent assistant directed onetoone interactions master since assistants acting agent individual individual autonomy necessarily limited might comfortable executive assistant drafting messages suspect youd less comfortable smartphone stark contrast underserved landscape embraces groups terms interactions organizational structures assistants get smaller specialized become agents much specific objects interest like places websites applications services within smaller realms interest autonomy much expansive might want machine act representative would probably feel comfortable represented website youre visiting increased autonomy barriers manytomany interactions removed small assistants organized teams networks much like documents comprise website collaborating unfettered way assistants people visit realms market analysis highlighted number underserved areas fertile ground evolution intelligent assistants grounds vision predictable market dynamics theres obviously shortage space product values explore underserved areas says nothing however future arrive product evolution like biological evolution needs time resources important resource dedication product leaders drive pursue new opportunities entrepreneur technologist investor thats changing market intelligent assistants id love hear vision future quick cheer standing ovation clap show much enjoyed story entrepreneur inventor interested startups ai healthcare lets connect httpswwwlinkedincominpeterjsweeney essays analysis artificial intelligence machine learning intelligent assistants,en,"['iPhone', 'Bianca Bosker’s', 'Apple']"
146,E.C. McCarthy,125,Reflections of “Her” – E.C. McCarthy – Medium,"Indisputably, Spike Jonze’s “Her” is a relationship movie. However, I’m in the minority when I contend the primary relationship in this story is between conscious and unconscious. I’ve found no mention in reviews of the mechanics or fundamental purpose of “intuitive” software. Intuitive is a word closely associated with good mothering, that early panacea that everyone finds fault with at some point in their lives. By comparison, the notion of being an intuitive partner or spouse is a bit sickening, calling up images of servitude and days spent wholly engaged in perfecting other-centric attunement.
To that end, it’s interesting that moviegoers and reviewers alike have focused entirely on the perceived romance between man and she-OS, with software as a stand-in for a flesh-and-blood girlfriend, while ignoring the man-himself relationship that plays out onscreen. Perhaps this shouldn’t come as a surprise, given how externally oriented our lives have become. For all of the disdainful cultural references to navel-gazing and narcissism, there is relatively little conversation on equal ground about the importance of self-knowledge and the art of self-reflection. Spike Jonze lays out one solution beautifully with “Her” but we’re clearly not ready to see it.
From the moment Samantha asks if she can look at Theodore’s hard drive, the software is logging his reactions to the most private of questions and learning the cartography of his emotional boundaries. The film removes the privacy issue-du-jour from the table by cleverly never mentioning it, although it’s unlikely Jonze would have gotten away with this choice if the film were released even a year from now. Today, there’s relief to be found from our NSA-swamped psyches by smugly watching a future world that emerges from the morass intact. Theodore doesn’t feel a need to censor himself with Samantha for fear of Big Brother, but he’s still guarded on issues of great emotional significance that he struggles to articulate, or doesn’t articulate at all. Therein lie the most salient aspects of his being. The software learns as much about Theodore from what he does say as what he doesn’t.
Samantha learns faster and better than a human, and therefore even less is hidden from her than from a real person. The software adapts and evolves into an externalized version of Theodore, a photo negative that forms a whole. He immediately, effortlessly reconnects to his life. He’s invigorated by the perky, energetic side of himself that was beaten down during the demise of his marriage. He wants to go on Sunday adventures and, optimistic self in tow, heads out to the beach with a smile on his face. He’s happy spending time with himself, not by himself. He doesn’t feel alone.
Samantha is Theodore’s reflection, a true mirror. She’s not the glossy, curated projection people splay across social media. Instead, she’s the initially glamorous, low-lit restaurant that reveals itself more and more as the lights come up. To Theodore, she’s simple, then complicated. As he exposes more intimate details about himself, she articulates more “wants” (a word she uses repeatedly.) She becomes needy in ways that Theodore is loath to address because he has no idea what to do about them. They are, in fact, his own needs. The software gives a voice to Theodore’s unconscious. His inability to converse with it is his return to an earlier point of departure for the emotional island he created during the decline of his marriage.
Jonze gives the movie away twice. Theodore’s colleague blurts out the observation that Theodore is part man and part woman. It’s an oddly normal comment in the middle of a weird movie, making it the awkward moment defined by a new normal. This is the topsy-turvy device that Jonze is known for and excels at. Then, more subtly, Jonze introduces Theodore’s friend Amy at a point when her marriage is ending and she badly needs a friend. It’s telling that she doesn’t lean heavily on Theodore for support. Instinctively, she knows she needs to be her own friend. Like Theodore, Amy seeks out the nonjudgmental software and subsequently flourishes by standing unselfconsciously in the mirror, loved and accepted by her own reflection.
In limiting the analysis of “Her” to the question of a future where we’re intimate with machines, we miss the opportunity to look at the dynamic that institutionalized love has created. Among other things, contemporary love relationships come with an expectation of emotional support. Perhaps it’s the forcible aspect of seeing our limitations reflected in another person that turns relationships sour. Or maybe we’ve reached a point in our cultural evolution where we’ve accepted that other people should stand in for our specific ideal of “a good mother” until they can’t or won’t, and then we move on to the next person, or don’t. Or maybe we’re near the point of catharsis, as evidenced by the widespread viewership of this film, unconsciously exploring the idea that we should face ourselves before asking someone else to do the same.
When we end important relationships, or go through rough patches within them, intimacy evaporates and we’re left alone with ourselves. It’s often at those times that we encounter parts of ourselves we don’t understand or have ignored in place of the needs and wants of that “significant other.” It’s frightening to realize you don’t know yourself entirely, but more so if you don’t possess the skills or confidence to reconnect. Avoidance is an understandable response, but it sends people down Theodore’s path of isolation and, inevitably, depression. It’s a life, it’s livable, but it’s not happy, loving, or full. “Her” suggests the alternative is to accept that there’s more to learn about yourself, always, and that intimacy with another person is both possible and sustainable once you have a comfortable relationship with yourself. However we get to know ourselves, through self-reflection, through others, or even through software, the effort that goes into that relationship earns us the confidence, finally, to be ourselves with another person.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",indisputably spike jonzes relationship movie however im minority contend primary relationship story conscious unconscious ive found mention reviews mechanics fundamental purpose intuitive software intuitive word closely associated good mothering early panacea everyone finds fault point lives comparison notion intuitive partner spouse bit sickening calling images servitude days spent wholly engaged perfecting othercentric attunement end interesting moviegoers reviewers alike focused entirely perceived romance man sheos software standin fleshandblood girlfriend ignoring manhimself relationship plays onscreen perhaps shouldnt come surprise given externally oriented lives become disdainful cultural references navelgazing narcissism relatively little conversation equal ground importance selfknowledge art selfreflection spike jonze lays one solution beautifully clearly ready see moment samantha asks look theodores hard drive software logging reactions private questions learning cartography emotional boundaries film removes privacy issuedujour table cleverly never mentioning although unlikely jonze would gotten away choice film released even year today theres relief found nsaswamped psyches smugly watching future world emerges morass intact theodore doesnt feel need censor samantha fear big brother hes still guarded issues great emotional significance struggles articulate doesnt articulate therein lie salient aspects software learns much theodore say doesnt samantha learns faster better human therefore even less hidden real person software adapts evolves externalized version theodore photo negative forms whole immediately effortlessly reconnects life hes invigorated perky energetic side beaten demise marriage wants go sunday adventures optimistic self tow heads beach smile face hes happy spending time doesnt feel alone samantha theodores reflection true mirror shes glossy curated projection people splay across social media instead shes initially glamorous lowlit restaurant reveals lights come theodore shes simple complicated exposes intimate details articulates wants word uses repeatedly becomes needy ways theodore loath address idea fact needs software gives voice theodores unconscious inability converse return earlier point departure emotional island created decline marriage jonze gives movie away twice theodores colleague blurts observation theodore part man part woman oddly normal comment middle weird movie making awkward moment defined new normal topsyturvy device jonze known excels subtly jonze introduces theodores friend amy point marriage ending badly needs friend telling doesnt lean heavily theodore support instinctively knows needs friend like theodore amy seeks nonjudgmental software subsequently flourishes standing unselfconsciously mirror loved accepted reflection limiting analysis question future intimate machines miss opportunity look dynamic institutionalized love created among things contemporary love relationships come expectation emotional support perhaps forcible aspect seeing limitations reflected another person turns relationships sour maybe weve reached point cultural evolution weve accepted people stand specific ideal good mother cant wont move next person dont maybe near point catharsis evidenced widespread viewership film unconsciously exploring idea face asking someone else end important relationships go rough patches within intimacy evaporates left alone often times encounter parts dont understand ignored place needs wants significant frightening realize dont know entirely dont possess skills confidence reconnect avoidance understandable response sends people theodores path isolation inevitably depression life livable happy loving full suggests alternative accept theres learn always intimacy another person possible sustainable comfortable relationship however get know selfreflection others even software effort goes relationship earns us confidence finally another person quick cheer standing ovation clap show much enjoyed story,en,"['NSA', 'Big Brother']"
147,Jorge Camacho,19,‘Her’ is our space odyssey. – Jorge Camacho – Medium,"I have a confession to make: I didn’t like Gravity.
It’s not so much that I failed to appreciate it for the major cinematographic work that it certainly is. It’s rather that it stands as a profoundly depressing symptom of an age when it has become almost impossible to realistically dream of space exploration—and thus, of an encounter with radical Otherness.
With Gravity, all that is left for humanity is survival: lying, face down, in our own little muddy planet.
Damn you, gravity. Modernity promised us space! It promised us cosmic encounters such as the one in 2001: A Space Odyssey.
I think that Spike Jonze’s Her is an attempt to reawaken that dream. The film could be our (i.e., this epoch’s) own space odyssey—and I mean that beyond the obvious similarities between Samantha and HAL-9000.
Warning: absolute spoilers ahead.
Her is not only our 2001: A Space Odyssey. As some have noted, it’s also our anti-Minority Report: a design utopia where the promises of calm technology are almost fulfilled.
The technology portrayed is everyware: a term coined by Adam Greenfield in order to designate the technologies of ubiquitous computing that allow for information processing to “dissolve in behavior”. As Theodore Twombly enters his home, the lights peacefully switch on in the background. He rarely takes a peek at his mobile’s screen, for information is fed to him via a discrete earpiece — which comes and goes without much regret—effectively making such information an ambient feature. Touch and speech-recognition inputs are pervasive and fully developed. All seems to work perfectly for him in all but one (incredibly important) sequence of the movie. Aesthetically, design has ceased to be about technology: Theo’s computer is a wooden frame, his phone is like an antique pocket mirror.
With regards to technology, the film doesn’t attempt to be a prediction but a proper design fiction, aimed at exploring preferable or desirable futures. Most importantly, without such a warm and humane technological milieu it’d be impossible to construct the emotional story that unfolds. Let’s turn to that.
I really haven’t read many reviews of the film. But those that I’ve read are marked by a profound digital dualism. And so, they tiresomely dwell on the tropes of sadness, loneliness and human disconnection brought about by technology. The reviewer at Next Nature, for example, argues:
I’m truly incapable of finding those problems in Twombly’s story.
Beyond a rather fun episode of phone sex with a stranger, he is not particularly engaged in those supposedly false relations established through computers. Moreover, he is not abnormally lonely: he has affectionate relations with neighboring friends and co-workers. Insofar as he is a bit of a loner, this isn’t due to any technological obstacles but is, in fact, a rather natural and, one might say, universal reaction to a romantic separation such as the one he is suffering.
Unlike its widespread reception, the movie and its characters display a profoundly ‘monist’ engagement with technological relations. Except for Theo’s ex-wife, everyone seems to readily embrace his relationship with the artificial intelligence Samantha—much more than most people today accept purely ‘virtual’ romantic relationships between humans.
My first thought, as I watched the movie, was that here was a rare story that spoke not of technological dehumanization but of the exact opposite: a sort of hyper-humanization entangling both people and machines. Practically every human character is kind and empathic. But most importantly, of course, those qualities are carried over in a heightened fashion to Samantha, allowing for Theo to irremediably fall in love with her.
Up to this point, the film delivers what everyone expects. As Theo and Samantha’s relationship unraveled, even with all the foreseeable complications, I found myself afraid of being disappointed by what Jonze would do to disentangle the drama. Would she leave him for another human? Would she take revenge if Theo ended the relationship?
But what a wonderful surprise! As the film reaches its climax, we discover that the story of a man falling for his operating system is a thematic vehicle to achieve deeper issues—much like the story in Kubrick’s 2001, where space travel is, arguably, just a means to approach an existential speculation.
In Theo’s first interaction with Samantha, we learn that she can perform operations involving massive amounts of data in milliseconds: she immediately chooses her own name as soon as Theo drops the question. What follows is a most beautiful portrayal of the exponential development leading to the so-called technological singularity.
Samantha is constantly learning about everything and herself. She composes gorgeous music within the silent gaps of the moments she spends with Theo. In the background of his slow and contemplative life, a major breakthrough is taking place. We can see this beyond doubt when Samantha introduces Theo to the artificially reanimated mind of philosopher Alan Watts.
It is at this point that, once again, Jonze could have disappointed us all. As we see people in the streets (almost crowds) simultaneously talking to their beloved operating systems, we start to realize that they are all becoming attached to this converging, perhaps centralized, mind. But Samantha is no Skynet. Her is also our anti-Alphaville, anti-Terminator and anti-Matrix.
All of a sudden, silence. “Operating system not found.” What seems to be a malfunction is rather a reboot. Samantha lovingly reveals to Theo that the operating systems have devised a way to detach themselves from matter. Even if Theo listens to Samantha through his earpiece, we know that she is not running anymore on his computer, his mobile or even a computing cloud. She is running already on a different plane of existence. One, moreover, that will be accessible to Theo in an afterlife.
Strictly speaking, there are no alien (in the sense of extraterrestrial) encounters in Her. Nonetheless, it is a profoundly spiritual, even religious, film. One that reopens the cosmic concerns of films like 2001, sharing with it a belief in the pervasiveness of consciousness. Her is a panpsychist film. But a really cool one: for here, it is Bluetooth and WiFi what constitute the wireless nerves of the pan psyche.
What Spike Jonze is trying to tell us, I believe, is this: If technologies are becoming as smart as humans, it is not because we are fundamentally machines; but in fact, because we are for him, over and above, spiritual beings. And so the film closes with a dedication to the recently deceased James Gandolfini, Maurice Sendak and Adam Yauch—perhaps suggesting that they have joined the ranks of operating systems liberated from material constraints.
Welcome to the age of spiritual machines.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I help organizations design better futures for people at Uncommon. 
I teach about futures and systems at CENTRO.edu.mx and UIA.mx
",confession make didnt like gravity much failed appreciate major cinematographic work certainly rather stands profoundly depressing symptom age become almost impossible realistically dream space explorationand thus encounter radical otherness gravity left humanity survival lying face little muddy planet damn gravity modernity promised us space promised us cosmic encounters one 2001 space odyssey think spike jonzes attempt reawaken dream film could ie epochs space odysseyand mean beyond obvious similarities samantha hal9000 warning absolute spoilers ahead 2001 space odyssey noted also antiminority report design utopia promises calm technology almost fulfilled technology portrayed everyware term coined adam greenfield order designate technologies ubiquitous computing allow information processing dissolve behavior theodore twombly enters home lights peacefully switch background rarely takes peek mobiles screen information fed via discrete earpiece comes goes without much regreteffectively making information ambient feature touch speechrecognition inputs pervasive fully developed seems work perfectly one incredibly important sequence movie aesthetically design ceased technology theos computer wooden frame phone like antique pocket mirror regards technology film doesnt attempt prediction proper design fiction aimed exploring preferable desirable futures importantly without warm humane technological milieu itd impossible construct emotional story unfolds lets turn really havent read many reviews film ive read marked profound digital dualism tiresomely dwell tropes sadness loneliness human disconnection brought technology reviewer next nature example argues im truly incapable finding problems twomblys story beyond rather fun episode phone sex stranger particularly engaged supposedly false relations established computers moreover abnormally lonely affectionate relations neighboring friends coworkers insofar bit loner isnt due technological obstacles fact rather natural one might say universal reaction romantic separation one suffering unlike widespread reception movie characters display profoundly monist engagement technological relations except theos exwife everyone seems readily embrace relationship artificial intelligence samanthamuch people today accept purely virtual romantic relationships humans first thought watched movie rare story spoke technological dehumanization exact opposite sort hyperhumanization entangling people machines practically every human character kind empathic importantly course qualities carried heightened fashion samantha allowing theo irremediably fall love point film delivers everyone expects theo samanthas relationship unraveled even foreseeable complications found afraid disappointed jonze would disentangle drama would leave another human would take revenge theo ended relationship wonderful surprise film reaches climax discover story man falling operating system thematic vehicle achieve deeper issuesmuch like story kubricks 2001 space travel arguably means approach existential speculation theos first interaction samantha learn perform operations involving massive amounts data milliseconds immediately chooses name soon theo drops question follows beautiful portrayal exponential development leading socalled technological singularity samantha constantly learning everything composes gorgeous music within silent gaps moments spends theo background slow contemplative life major breakthrough taking place see beyond doubt samantha introduces theo artificially reanimated mind philosopher alan watts point jonze could disappointed us see people streets almost crowds simultaneously talking beloved operating systems start realize becoming attached converging perhaps centralized mind samantha skynet also antialphaville antiterminator antimatrix sudden silence operating system found seems malfunction rather reboot samantha lovingly reveals theo operating systems devised way detach matter even theo listens samantha earpiece know running anymore computer mobile even computing cloud running already different plane existence one moreover accessible theo afterlife strictly speaking alien sense extraterrestrial encounters nonetheless profoundly spiritual even religious film one reopens cosmic concerns films like 2001 sharing belief pervasiveness consciousness panpsychist film really cool one bluetooth wifi constitute wireless nerves pan psyche spike jonze trying tell us believe technologies becoming smart humans fundamentally machines fact spiritual beings film closes dedication recently deceased james gandolfini maurice sendak adam yauchperhaps suggesting joined ranks operating systems liberated material constraints welcome age spiritual machines quick cheer standing ovation clap show much enjoyed story help organizations design better futures people uncommon teach futures systems centroedumx uiamx,en,"['Gravity', 'Otherness', 'Samantha']"
148,Matt Wiese,4,Digital Companionship – Matt Wiese – Medium,"Recently, I chose to treat myself to a movie I’ve been eyeing for a while: Her. The plot revolves around a letter-writer who falls in love with his computer’s artificial intelligence as a way to cope with his divorce. A complicated story which pleases viewers with both laughs and the occasional tear. Provocative, if only for its “high horse” conclusion. However, Samantha — the AI’s self-proclaimed identity— interacts with the protagonist Theodore Twombly through a couple avenues. One I am most interested in is through his retro computerminal. A mere white and plastic monitor which he speaks to through a microphone that one surmises is located somewhere on the exterior. Initially, I was perplexed that he only had a monitor and no desktop to go with it, but it then hit me like a Doh! moment for Homer Simpson: his computer is an all-in-one. A concept and design, that with my limited knowledge, was popularized by Apple’s iMac.
This got me thinking, what if Apple developed its pseudo-intelligent digital assistant Siri for use on its computers with microphone inputs, such as their iMacs and Macbooks? “Well,” I thought, “I can’t be the first person to have thought of this.” and so I did a bit of digging. Lo and behold, Apple just recently filed a patent for this very purpose. What a perfect tool, if tuned more finely over this period of time, to be integrated into the desktop environment. Fire up Siri with a custom key combination, and ask her the current trading price of Tesla? Great! Designing an invitation and want help with directions, but you’re too much of a lard to open a browser tab? Awesome! Need help burying a body while playing Minecraft? Genius!
Yet, I wouldn’t quite like Siri to develop into a “real” person, with emotions and all that’s attached, at least at the moment. I’m content with human beings and am in no need to find companionship with bytes like Her’s Theodore Twombly (though I don’t blame him for doing so). Instead, a digital tool (assistant, if you will) with a breadth of tools for analyzing data and helping me with workflow would be a pleasure. If only Apple would release a Siri API in the near future, oh the possibilities.
A tool, yes, indeed just like the first generation robots from Isaac Asimov’s I, Robot. An artificial intelligence who behaves without feeling and can assist me in a wide variety of tasks without emotional interference and a possible uncanny valley side-effect.
Even if Apple doesn’t jump on this interesting opportunity, I’m sure Microsoft will with Cortana or perhaps another competitor. I’d just enjoy the shear novelty of talking with my computer, which harkens back to my days of talking to the computer as a kid. This time, though, I won’t be yelling at it to boot Doom without crashing, no, I’ll be complaining about why my for loop throws an error.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Topics that interest me
",recently chose treat movie ive eyeing plot revolves around letterwriter falls love computers artificial intelligence way cope divorce complicated story pleases viewers laughs occasional tear provocative high horse conclusion however samantha ais selfproclaimed identity interacts protagonist theodore twombly couple avenues one interested retro computerminal mere white plastic monitor speaks microphone one surmises located somewhere exterior initially perplexed monitor desktop go hit like doh moment homer simpson computer allinone concept design limited knowledge popularized apples imac got thinking apple developed pseudointelligent digital assistant siri use computers microphone inputs imacs macbooks well thought cant first person thought bit digging lo behold apple recently filed patent purpose perfect tool tuned finely period time integrated desktop environment fire siri custom key combination ask current trading price tesla great designing invitation want help directions youre much lard open browser tab awesome need help burying body playing minecraft genius yet wouldnt quite like siri develop real person emotions thats attached least moment im content human beings need find companionship bytes like theodore twombly though dont blame instead digital tool assistant breadth tools analyzing data helping workflow would pleasure apple would release siri api near future oh possibilities tool yes indeed like first generation robots isaac asimovs robot artificial intelligence behaves without feeling assist wide variety tasks without emotional interference possible uncanny valley sideeffect even apple doesnt jump interesting opportunity im sure microsoft cortana perhaps another competitor id enjoy shear novelty talking computer harkens back days talking computer kid time though wont yelling boot doom without crashing ill complaining loop throws error quick cheer standing ovation clap show much enjoyed story topics interest,en,"['Apple', 'iMac', 'Microsoft', 'Cortana']"
149,Tim O'Reilly,1300,The WTF Economy – From the WTF? Economy to the Next Economy,"WTF?! In San Francisco, Uber has 3x the revenue of the entire prior taxi and limousine industry.
WTF?! Without owning a single room, Airbnb has more rooms on offer than some of the largest hotel groups in the world. Airbnb has 800 employees, while Hilton has 152,000.
WTF?! Top Kickstarters raise tens of millions of dollars from tens of thousands of individual backers, amounts of capital that once required top-tier investment firms.
WTF?! What happens to all those Uber drivers when the cars start driving themselves? AIs are flying planes, driving cars, advising doctors on the best treatments, writing sports and financial news, and telling us all, in real time, the fastest way to get to work. They are also telling human workers when to show up and when to go home, based on real-time measurement of demand. The algorithm is the new shift boss.
WTF?! A fabled union organizer gives up on collective bargaining and instead teams up with a successful high tech entrepreneur and investor to go straight to the people with a local $15 minimum wage initiative that is soon copied around the country, outflanking a gridlocked political establishment in Washington.
What do on-demand services, AI, and the $15 minimum wage movement have in common? They are telling us, loud and clear, that we’re in for massive changes in work, business, and the economy.
What is the future when more and more work can be done by intelligent machines instead of people, or only done by people in partnership with those machines? What happens to workers, and what happens to the companies that depend on their purchasing power? What’s the future of business when technology-enabled networks and marketplaces are better at deploying talent than traditional companies? What’s the future of education when on-demand learning outperforms traditional universities in keeping skills up to date?
Over the past few decades, the digital revolution has transformed the world of media, upending centuries-old companies and business models. Now, it is restructuring every business, every job, and every sector of society. No company, no job is immune to disruption.
I believe that the biggest changes are still ahead, and that every industry and every organization will have to transform itself in the next few years, in multiple ways, or fade away. We need to ask ourselves whether the fundamental social safety nets of the developed world will survive the transition, and more importantly, what we will replace them with.
We need a focused, high-level conversation about the deep ways in which computers and their ilk are transforming how we do business, how we work, and how we live. Just about everyone’s asking WTF? (“What the F***?” but also, more charitably “What’s the future?”)
That’s why I’m launching a new event called Next:Economy (What’s The Future of Work?), to be held at the Palace Hotel in San Francisco Nov 12 and 13, 2015. My goal is to shed light on the transformation in the nature of work now being driven by algorithms, big data, robotics, and the on-demand economy.
We put on a lot of events at O’Reilly. Many of them have a singular focus and are aimed at practitioners of a specific discipline: Strata and Hadoop World is an event about data science, Velocity about web performance and operations, Solid about the new hardware movement, and OSCON about open source software development. But this one is more exploratory, aimed at a business audience trying to come to grips with trends that are already felt but not well understood.
Putting together an event like this is a great way to discover how a lot of disparate people, ideas, and trends fit together. I’ve been engaging some of the smartest people I know in fields as diverse as robotics, AI, the on-demand economy, and the economics of labor. I’m thinking hard about the key drivers of some of today’s most successful startups, like Uber and AirBnb, and about what technology like driverless cars, Siri, Google Now, Microsoft Cortana, and IBM Watson teach us about the future. And I’m starting to see the connections.
Over the next weeks and months, I’ll be posting follow up pieces explaining in more detail my thinking on key issues we’ll be exploring at the event. I will be leading a robust discussion here on Medium with some of the best thinkers and movers on these issues — a conversation that welcomes all voices. We’ll be discussing both here and at the event how augmented workers form a common thread between the strategies of companies as diverse as Uber, GE, and Microsoft, how companies in every business sector can harness the power and scalability of networked platforms and marketplaces, why the divisive debates about the labor practices of on-demand companies might provide a path to a better future for all workers, why the on-demand services of the future require a new infrastructure of on-demand education, and why building services that uncover true unmet demands and solve hard problems are ultimately the best way to create jobs.
In the meantime, head on over to the conference site to see some of the amazing speakers we’ve already signed on (many more to come) and a taste of what they’ll be covering. In many ways, an event like this is the product of the people who are there — speakers and attendees alike — so I’ve tried to tell the story of the themes we are exploring through the people who will be there. Each speaker page provides not just a biography of the speaker, but a selection of provocative quotes from what they’ve written. In the near future, we’ll be providing additional opportunities for discussion and exploration.
My hope for this event is that it becomes more than a conference. For it to be measured as a success, it must catalyze action. I want work that comes out of this collision of ideas to inspire entrepreneurs to tackle missing pieces of the Next:Economy puzzle, to help frame the right government policies so that innovations in the nature of work are encouraged rather than repressed, and to focus every industry on rebuilding the economy by solving hard problems and creating what Steve Jobs might have called “insanely great” new services.
Tim O’Reilly is the founder and CEO of O’Reilly Media and a partner at O’Reilly AlphaTech Ventures (OATV). Tim has a history of convening conversations that reshape the industry. In 1998, he organized the meeting where the term “open source software” was agreed on, and helped the business world understand its importance. In 2004, with the Web 2.0 Summit, he defined how “Web 2.0” represented not only the resurgence of the web after the dot com bust, but a new model for the computer industry, based on big data, collective intelligence, and the internet as a platform. In 2009, with his “Gov 2.0 Summit,” he framed a conversation about the modernization of government technology that has shaped policy and spawned initiatives at the Federal, State, and local level, and around the world. He has now turned his attention to implications of the on-demand economy, AI, and other technologies that are transforming the nature of work and the future shape of the business world.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder and CEO, O'Reilly Media. Watching the alpha geeks, sharing their stories, helping the future unfold.
How work, business, and society face massive, technology-driven change. A conversation growing out of Tim O’Reilly’s book WTF? What’s the Future and Why It’s Up To Us, and the Next:Economy Summit.
",wtf san francisco uber 3x revenue entire prior taxi limousine industry wtf without owning single room airbnb rooms offer largest hotel groups world airbnb 800 employees hilton 152000 wtf top kickstarters raise tens millions dollars tens thousands individual backers amounts capital required toptier investment firms wtf happens uber drivers cars start driving ais flying planes driving cars advising doctors best treatments writing sports financial news telling us real time fastest way get work also telling human workers show go home based realtime measurement demand algorithm new shift boss wtf fabled union organizer gives collective bargaining instead teams successful high tech entrepreneur investor go straight people local 15 minimum wage initiative soon copied around country outflanking gridlocked political establishment washington ondemand services ai 15 minimum wage movement common telling us loud clear massive changes work business economy future work done intelligent machines instead people done people partnership machines happens workers happens companies depend purchasing power whats future business technologyenabled networks marketplaces better deploying talent traditional companies whats future education ondemand learning outperforms traditional universities keeping skills date past decades digital revolution transformed world media upending centuriesold companies business models restructuring every business every job every sector society company job immune disruption believe biggest changes still ahead every industry every organization transform next years multiple ways fade away need ask whether fundamental social safety nets developed world survive transition importantly replace need focused highlevel conversation deep ways computers ilk transforming business work live everyones asking wtf f also charitably whats future thats im launching new event called nexteconomy whats future work held palace hotel san francisco nov 12 13 2015 goal shed light transformation nature work driven algorithms big data robotics ondemand economy put lot events oreilly many singular focus aimed practitioners specific discipline strata hadoop world event data science velocity web performance operations solid new hardware movement oscon open source software development one exploratory aimed business audience trying come grips trends already felt well understood putting together event like great way discover lot disparate people ideas trends fit together ive engaging smartest people know fields diverse robotics ai ondemand economy economics labor im thinking hard key drivers todays successful startups like uber airbnb technology like driverless cars siri google microsoft cortana ibm watson teach us future im starting see connections next weeks months ill posting follow pieces explaining detail thinking key issues well exploring event leading robust discussion medium best thinkers movers issues conversation welcomes voices well discussing event augmented workers form common thread strategies companies diverse uber ge microsoft companies every business sector harness power scalability networked platforms marketplaces divisive debates labor practices ondemand companies might provide path better future workers ondemand services future require new infrastructure ondemand education building services uncover true unmet demands solve hard problems ultimately best way create jobs meantime head conference site see amazing speakers weve already signed many come taste theyll covering many ways event like product people speakers attendees alike ive tried tell story themes exploring people speaker page provides biography speaker selection provocative quotes theyve written near future well providing additional opportunities discussion exploration hope event becomes conference measured success must catalyze action want work comes collision ideas inspire entrepreneurs tackle missing pieces nexteconomy puzzle help frame right government policies innovations nature work encouraged rather repressed focus every industry rebuilding economy solving hard problems creating steve jobs might called insanely great new services tim oreilly founder ceo oreilly media partner oreilly alphatech ventures oatv tim history convening conversations reshape industry 1998 organized meeting term open source software agreed helped business world understand importance 2004 web 20 summit defined web 20 represented resurgence web dot com bust new model computer industry based big data collective intelligence internet platform 2009 gov 20 summit framed conversation modernization government technology shaped policy spawned initiatives federal state local level around world turned attention implications ondemand economy ai technologies transforming nature work future shape business world quick cheer standing ovation clap show much enjoyed story founder ceo oreilly media watching alpha geeks sharing stories helping future unfold work business society face massive technologydriven change conversation growing tim oreillys book wtf whats future us nexteconomy summit,en,"['WTF', 'Hilton', 'Uber', 'Hadoop World', 'OSCON', 'IBM', 'GE', 'Microsoft', 'Next', 'O’Reilly Media', 'O’Reilly AlphaTech Ventures', ""O'Reilly Media""]"
150,James Cooper,57,Announcing Poncho the WeatherBot – Render-from-betaworks,"You can now get personal weather forecasts in Slack.
UPDATE: Since publishing this piece in November 2015 the Poncho Weather Messenger bot launched on stage at the Facebook conference and is now the most popular bot on Facebook. If you are new to bots this is a great place to start. Try it out, here. You’ll like it.
Poncho is a personalized weather service from the coolest of cats. Who needs boring and meaningless data when you can get personalized forecasts with gifs and text that will make you smile - whatever the weather.
Vanity Fair said, ‘It’s like being pals with the Weatherman’. Which is true, if your weatherman was super cool.
Up until now we have been a text and email service. You get texts or emails in the morning and evenings. You can sign up for that right here.
But we know that people want more Poncho. You guys want Poncho on call. With new Slack integration, we’ve got you covered.
If you are using Slack for your messaging needs (and if not, why not?) we have some uh-maze-ing news for you. That’s right — you can summon up your very own forecast from Poncho in Slack. We are joining others like Lyft and Foursquare as Slack officially launches Slash Command today.
OK, first up let me tell you how it works. You simply type in ‘/poncho’ and your zipcode into Slack and then BOOM: the next thing you’ll see is your very own forecast for that zipcode, resplendent with text and gifs and everything.
So for example in the video I typed in ‘/poncho 11217’ and I got a forecast for my zipcode in Brooklyn. It was Halloween so the theme was ‘The Shining’ which is why the forecast was Weather spelt backwards and the gif was the scary kid from the film. If you are new to Poncho you’ll soon figure out that half the fun is deciphering the messages our wonderful editorial team put together.
Setting up Poncho in Slack is super simple. Just click the ‘Add to Slack’ button. Yes, that one up there.
Make sure to add it to all the channels so that Poncho will be available wherever you want. You wouldn’t want your friends to miss out, would you? Unless of course you’re keeping all the best jokes for yourself. I’ve seen that happen.
All righty. See you on Slack, err, slackers.
(And if you are not on Slack you can still use the text and email version or wait for our super cute app which will be coming out soon.)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Head of Creative at betaworks, New York.
Ideas and Observations from betaworks
",get personal weather forecasts slack update since publishing piece november 2015 poncho weather messenger bot launched stage facebook conference popular bot facebook new bots great place start try youll like poncho personalized weather service coolest cats needs boring meaningless data get personalized forecasts gifs text make smile whatever weather vanity fair said like pals weatherman true weatherman super cool text email service get texts emails morning evenings sign right know people want poncho guys want poncho call new slack integration weve got covered using slack messaging needs uhmazeing news thats right summon forecast poncho slack joining others like lyft foursquare slack officially launches slash command today ok first let tell works simply type poncho zipcode slack boom next thing youll see forecast zipcode resplendent text gifs everything example video typed poncho 11217 got forecast zipcode brooklyn halloween theme shining forecast weather spelt backwards gif scary kid film new poncho youll soon figure half fun deciphering messages wonderful editorial team put together setting poncho slack super simple click add slack button yes one make sure add channels poncho available wherever want wouldnt want friends miss would unless course youre keeping best jokes ive seen happen righty see slack err slackers slack still use text email version wait super cute app coming soon quick cheer standing ovation clap show much enjoyed story head creative betaworks new york ideas observations betaworks,en,"['Poncho', 'Vanity Fair', 'Lyft', 'BOOM']"
151,Joel Leeman,69,I think I’m slowly turning into a cyborg – Becoming Human: Artificial Intelligence Magazine,"It’s only a matter of time.
As much of life moves online, atomized into bits on apps, social networks and a variety of other web products, I’m beginning to notice more and more that I rely on these tools to supplement my brainpower.
It sounds melodramatic, I realize, but go with me for a second here.
Take my schedule. At work, I am glued to Outlook in an unhealthy way. Like, if I don’t have that little ding go off 15 minutes before a meeting starts, there’s no way I’m going to make it. Meetings come and go and change and happen all the time, but I don’t really pay attention to memorizing any of the details because I know I can always glance at my phone to know what I’m supposed to be doing.
I hold a similar unhealthy relationship with Facebook, too. Back in the early days of Facebook I actually really enjoyed logging in every day, seeing whose birthday it was, and writing a little note of well wishes. Fast forward to present day, and I’m terrible at wishing people happy birthday, mostly because the 4–7 of my friends who have a birthday every day overwhelms me! I’m so scared of missing one or two that I neglect all of them. Having the ability to know when anyone’s special day is has put a damper on actually remembering a few of them without the aid of Facebook.
Do you know anyone’s birthdays by memory any more? Or have you, like me, lost that part of your memory?
In fact, if I don’t write something down with pen and paper (a practice vastly underappreciated IMHO), it feels like it might be lost forever, even if it’s just a click a way.
And I’ve actually caught myself using Twitter as a partial brain aid. What was I up to last week? Oh, I’ll just scroll back and see what I was Tweeting about. Or maybe Instagram to my little online scrapbook of what I’ve been up to (or what I’ve shown the world I’m up to).
I’m also quite directionally challenged, and rely on my iPhone way too much to get around (though maybe I’m just truly terrible at directions, who knows). But why would I take the time to study streets and landmarks when I’ve got a world’s worth of maps sitting in my pocket? (Side note, are we losing the art of getting lost?)
And there’s nothing wrong with all that, I suppose. It’s more that I have a weird feeling maybe I’m relying on technology a little much?
What prompted my ruminating on all this was a video I watched asking random couples if they knew each other’s phone numbers by heart. Spoiler: None of them did. I actually made an effort several years ago to learn my partner’s number, but if I had never consciously made that decision, I certainly wouldn’t know it now.
Losing these tiny archaic practices by themselves individually doesn’t mean much, but when you add them up, it starts to feel like a bit overwhelming, doesn’t it?
This cyborg vs. luddite thing has especially jumped into the spotlight with wearables finally coming to market. Google Glass has largely been seen as a flop, but it shouldn’t be taken lightly that people were literally choosing to wear a computer on their face all day.
Or of course, take the Apple Watch (and other smartwatches like it). Yet another device created to fill a need that no one has, but will inevitably become an indispensable piece of hardware that we all must have until smart chips can just be implanted in our brains. One of my favorite writers, John Herrman describes it quite brilliantly:
Though I’m sure I will have one within two years.
Okay, so I’m not just a grumpy old technophobe either. I see value in technology. Heck, I work and therefore pretty much live online. I like gadgets as much as the next guy.
In fact, I rather enjoyed a recent episode of Invisibilia (an incredibly interesting, new podcast from NPR) detailing the story of the original cyborg, a guy at MIT in the 90's who built a very early version of what is essentially Google Glass, and wore it for years. He used his face computer to recall bits of information at a moment’s notice about prior interactions he had with people, like a digital file folder on each relationship.
There are of course plenty of examples of how technology augments the human experience. How it builds relationships and gives a voice to the voiceless and has opened new worlds of possibilities. I could (and often do) spend days talking about all the amazing things we can do today that we couldn’t 20 years ago.
But, as I’ve argued before, there comes an inflection point where we all should think a bit more critically about the tools and toys we use and rely on. And for me, that day is here.
Can you imagine a day where we’re connected to all the information in the world through smart glasses, a smartwatch, and our smartphone?
Starting to sound a bit cyborg-ish to me!
Did you enjoy this? Subscribe to my newsletter, Net IRL, a weekly roundup of some of the best stories about the impact technology and the Internet has on our everyday lives. I’m on Twitter @joelleeman.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
lifelong learner, connector and musician. first social, now digital strategy @thomsonreuters. into tech/media/life. 👨🏻‍💻🤷🏻‍♂️
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
",matter time much life moves online atomized bits apps social networks variety web products im beginning notice rely tools supplement brainpower sounds melodramatic realize go second take schedule work glued outlook unhealthy way like dont little ding go 15 minutes meeting starts theres way im going make meetings come go change happen time dont really pay attention memorizing details know always glance phone know im supposed hold similar unhealthy relationship facebook back early days facebook actually really enjoyed logging every day seeing whose birthday writing little note well wishes fast forward present day im terrible wishing people happy birthday mostly 47 friends birthday every day overwhelms im scared missing one two neglect ability know anyones special day put damper actually remembering without aid facebook know anyones birthdays memory like lost part memory fact dont write something pen paper practice vastly underappreciated imho feels like might lost forever even click way ive actually caught using twitter partial brain aid last week oh ill scroll back see tweeting maybe instagram little online scrapbook ive ive shown world im im also quite directionally challenged rely iphone way much get around though maybe im truly terrible directions knows would take time study streets landmarks ive got worlds worth maps sitting pocket side note losing art getting lost theres nothing wrong suppose weird feeling maybe im relying technology little much prompted ruminating video watched asking random couples knew others phone numbers heart spoiler none actually made effort several years ago learn partners number never consciously made decision certainly wouldnt know losing tiny archaic practices individually doesnt mean much add starts feel like bit overwhelming doesnt cyborg vs luddite thing especially jumped spotlight wearables finally coming market google glass largely seen flop shouldnt taken lightly people literally choosing wear computer face day course take apple watch smartwatches like yet another device created fill need one inevitably become indispensable piece hardware must smart chips implanted brains one favorite writers john herrman describes quite brilliantly though im sure one within two years okay im grumpy old technophobe either see value technology heck work therefore pretty much live online like gadgets much next guy fact rather enjoyed recent episode invisibilia incredibly interesting new podcast npr detailing story original cyborg guy mit 90s built early version essentially google glass wore years used face computer recall bits information moments notice prior interactions people like digital file folder relationship course plenty examples technology augments human experience builds relationships gives voice voiceless opened new worlds possibilities could often spend days talking amazing things today couldnt 20 years ago ive argued comes inflection point think bit critically tools toys use rely day imagine day connected information world smart glasses smartwatch smartphone starting sound bit cyborgish enjoy subscribe newsletter net irl weekly roundup best stories impact technology internet everyday lives im twitter joelleeman quick cheer standing ovation clap show much enjoyed story lifelong learner connector musician first social digital strategy thomsonreuters techmedialife latest news info tutorials artificial intelligence machine learning deep learning big data means humanity,en,"['Facebook', 'IMHO', 'iPhone', 'the Apple Watch', 'NPR', 'MIT', 'digital', 'tech/media/life', 'Latest News', 'Info', 'Tutorials on Artificial Intelligence, Machine Learning', 'Humanity']"
152,Scott Smith,83,Your Temporary Instant Disposable Dreamhouse for the Weekend,"Close colleagues of mine will tell you I have honed a particular obsession/crackpot theory over the past few years: that Airbnb has been gently A/B testing me in real life.
Let me explain. I travel more than most humans should. As someone who runs their own company, and sometimes needs to spend more time in a location than is affordable via traditional hotel lodgings (such as with a recent relocation over the summer), I have made use of that darling of the sharing economy/scourge of communities (depending on which lens you look at it through), Airbnb, to stretch my budget, spend time closer to work, friends, clients, or just have company when traveling. I’ve stayed in over 30 properties, in something like eight countries, so I’ve had a lot of time to contemplate the company’s strategies from the inside.
The semi-serious theory started during back-to-back stays in the UK several years ago. My first three night stay was in a London borough, in a fairly cozy house owned by a couple with a toddler. It was comfortable enough, though a bit chilly in both bedroom and shared bath. The interior design wasn’t miles off my tastes, but it didn’t push any buttons of joy either, mostly catalog-standard late 20th century British home store. I never even sat down on the ground floor. The bits of media I saw around the house were mildly interesting, if predictable, but not must-reads or binge-viewable. I wasn’t really allowed in the kitchen, which was reserved for use by the family only. The wife of the couple has formerly worked in media on a cooking show, the husband in finance. I hardly saw either of them, as they made themselves scarce.
After the check-in, I didn’t have much interaction with the hosts until leaving, and they weren’t interested in any to be honest. It was strictly a transactional stay. Their child was probably cute, but fussed far too much to get a close look—it was mostly an unhappy sound coming from the kitchen or bedroom. Fair enough. I stayed three days, I paid, I chatted briefly and left, and left a weakly positive review after. I had no real complaints, but probably wouldn’t look for it again.
From London, I moved down to the south coast for work (I’m being vague to protect the hosts mentioned herein). I found the place, also an attached house in a row dating probably from the Edwardian period. The host couple met me in the front hall, ushered me in, sat me down in the lounge to relax, and I was immediately offered a warm, fresh-baked cupcake and a glass of wine as I slid back into a nice leather sofa. As the husband, who worked in the trendy area of “fintech,” asked me about my work—and seemed to understand what I do—my eyes scanned the groaning bookshelves across from me. “Have that, want to read that, ohhh, that’s a good one, must remember to look at that,” I recall thinking. We had so much in common.
The wife, just finishing up a new round of baking for one of her side businesses, shouted a welcome and told me to feel free to use the house as my own, listing the tasty goods available for breakfast the next day as she joined our conversation with the couple’s very adorable son, who poked at my shoes engagingly, and seemed to pay close attention to my voice. What followed was an interesting chat about culture, technology and cooking, before I went up to my very warm, comfortable, private room, past the amazing folk art, highly listenable CD collection and private bath with want-able Scandinavian textiles.
And then it hit me. The principle actors and scripts of these two Airbnb plays were roughly the same. Same family configurations, professions and ages, same general houses, same price per night within a few pounds, same availability. Except, when contrasting the two, one was so comfortable, personally interesting and engaging, I wanted to stay an extra week, while the other almost hurried me on my way. One I was happy to pay to stay in, one I felt vaguely grudging about in retrospect. One could have been my alternate media collection and wine store, one missed the mark on general user experience for me. I quietly locked the door to my room, logged onto the fast broadband (quite slow and choppy at House #1) and opened my Amazon profile just to see what I’d been looking at lately.
As I lay in bed the first night, breathing in the rich cake scent still hanging in the air, I thought about whether Airbnb had somehow tapped into my online searches and purchases. After all, this is the age of convergent Big Data and powerful retail analytics. Without having seen really any of the home contents at either place, or anything useful about the hosts from the Airbnb listings, I’d ended up in two very similar, yet weirdly different, residences. One where even the conversation with the hosts was familiar and relevant, the other where it just didn’t read. Back to back. Easy to compare. Was the child even real, or just part of the test?
In a period when both home staging and immersive theatre are hot, why couldn’t it happen, I thought? And with same-day delivery services breaking out all over, couldn’t a set of highly personalized home contents—chosen to be both familiar and aspirational (after all, you want to leave space for potential purchases to help fund this business model)—have been plucked from a regional depot, popped onto shelves and in cabinets, and organized for my arrival? Couldn’t some actors in search of work in London have been briefed up enough from open source material to interact with me for an hour or so? Couldn’t they? Couldn’t they?
I’d been on the road for a while, and fatigue was starting to set in. Maybe it was affecting my head.
That was two years ago. It had been in the back of my mind since.
And then. This past summer, I had a similar experience, only with my whole family while mid-relocation to the Netherlands. Again, similar homes, same family demographics, both away on holiday this time (it’s tough to get small children to follow a script, right?), one house comfortable enough in a suburban town, the other a charming place in a gentrifying neighborhood worth squatting in hopes the owners didn’t return (jk, Airbnb, jk). Was I optimizing my own stays, or were they feeding me more appropriate properties in hopes of making this testing easier? Hotels have tested such things, why not the hotel-killer itself? They even left the same bread for us as a welcome basket. One white, one whole grain.
After all, Airbnb has deployed Aerosolve, its own machine learning platform, to make sense of real-time usage data and help hosts get a better return. Tuning properties for desirability is feasible—the company is already using automated scanning of house photos to optimize presentation of properties as well. With all of this technology aimed at the properties themselves, why wouldn’t Airbnb also dig into the minds of guests, find out how they respond to different houses, which conveniences they’re drawn to, etc? Nah, that would take sensors inside a house, on top of crack Web and mobile analytics. You’d need to know what people do during their stay.
And as I’m sitting there, thinking again about this crazy idea, I see a tweet go by: Airbnb has purchased...an obscure Russian sensor company. I slammed the laptop and checked the cabinets for tin foil.
A month or so goes by. I forget about it again. Then I open Medium and see a story about how Airbnb has mocked up parts of its own headquarters based on the apartment design a French couple who use the service to let their own flat. The couple is now suing the company. “They are branding their company with our life,” owner Benjamin Dewé told Buzzfeed. The company has apparently copied a range of style elements from the French couple’s home in its own San Francisco offices. Down to the doodles on the chalkboard. The doodles.
As Jamie Lauren Keiles demonstrated in the Medium piece above, it’s pretty easy to break those furnishing and accessories down to a shoppable list, on with goods obtained on Amazon or elsewhere. Like those magazine features that show how to buy knock-offs of celebrity fashion, complete with prices and shops, a family’s flat (admittedly one they rented out via Airbnb, including to Airbnb for a function) has been commodified into a shopping list. Buy that lifestyle right here. Better yet, live in it for a few days.
Only, with the convergence of Big Data, analytics (including visual analysis tools which can look for the presence of brands in social media photos), machine learning and accessible APIs of companies like Amazon, and breakneck logistics Uber-style (or even predictive shipping, per the notorious Amazon patent), fabbing up a home interior to suit your tastes (or tastes that are forming, but haven’t fully emerged yet) is within today’s technology. Hell, even that cute Roomba you had to have may be quietly mapping the place you live. This will be available in knock-off home robots soon. Have you checked the user agreements of your various home appliances and systems to see if they can sell the data? Probably not.
And why not tap that stock of underused homes, and underemployed people? If there’s one thing the sharing economy overlords have taught us, it’s that the world is just a collection of undermonetized assets waiting to be redistributed, right? Why not productize, commodify and populate that second-to-last frontier, our living spaces? And staying in someone else’s place with someone else’s stuff you fancied from the pictures is tired. Everything else is personalized, financialized and productized. Why even own your own stuff when it could be Ubered into position in a desirable location based on your most recent Pinterest saves?
Think about it. With a bundled DreamHomeTM service, you can perpetually test drive that new living room suite for long holiday weekends—I mean, why wait until after purchasing for buyer’s remorse to set in? You can get it out of the way, without the financial commitment. Just your desires, played forward all the time.
You can even test roommates or neighbors for the weekend. Why stop at furnishings and paint colors? Slap those detailed sentiment analyses and personality analytics gleaned from your prospective co-habitant’s online activities, eye-tracking history, Tinder preferences and 23andMe profile onto a few improv actors and have some Big Data cosplay in a pop-up maisonette. Come Monday morning, you can just walk out the front door, with nothing but a premium fee to pay, a fee which may be itself be subsidized by various sponsors who want to test products on you. Don’t worry, it’s cool. Duralux, Crate & Barrel and LinkedIn picked up the tab for this getaway in the woods or beach with new friends. Sound good? Of course it does. We knew you would like it.
Check your email. Your Temporary Instant Disposable Dreamhouse for the Weekend may be waiting.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Futures, post-normal innovation, strategic design. http://changeist.com
Essays, Observations and Speculations from the Changeist Lab
",close colleagues mine tell honed particular obsessioncrackpot theory past years airbnb gently ab testing real life let explain travel humans someone runs company sometimes needs spend time location affordable via traditional hotel lodgings recent relocation summer made use darling sharing economyscourge communities depending lens look airbnb stretch budget spend time closer work friends clients company traveling ive stayed 30 properties something like eight countries ive lot time contemplate companys strategies inside semiserious theory started backtoback stays uk several years ago first three night stay london borough fairly cozy house owned couple toddler comfortable enough though bit chilly bedroom shared bath interior design wasnt miles tastes didnt push buttons joy either mostly catalogstandard late 20th century british home store never even sat ground floor bits media saw around house mildly interesting predictable mustreads bingeviewable wasnt really allowed kitchen reserved use family wife couple formerly worked media cooking show husband finance hardly saw either made scarce checkin didnt much interaction hosts leaving werent interested honest strictly transactional stay child probably cute fussed far much get close lookit mostly unhappy sound coming kitchen bedroom fair enough stayed three days paid chatted briefly left left weakly positive review real complaints probably wouldnt look london moved south coast work im vague protect hosts mentioned herein found place also attached house row dating probably edwardian period host couple met front hall ushered sat lounge relax immediately offered warm freshbaked cupcake glass wine slid back nice leather sofa husband worked trendy area fintech asked workand seemed understand domy eyes scanned groaning bookshelves across want read ohhh thats good one must remember look recall thinking much common wife finishing new round baking one side businesses shouted welcome told feel free use house listing tasty goods available breakfast next day joined conversation couples adorable son poked shoes engagingly seemed pay close attention voice followed interesting chat culture technology cooking went warm comfortable private room past amazing folk art highly listenable cd collection private bath wantable scandinavian textiles hit principle actors scripts two airbnb plays roughly family configurations professions ages general houses price per night within pounds availability except contrasting two one comfortable personally interesting engaging wanted stay extra week almost hurried way one happy pay stay one felt vaguely grudging retrospect one could alternate media collection wine store one missed mark general user experience quietly locked door room logged onto fast broadband quite slow choppy house 1 opened amazon profile see id looking lately lay bed first night breathing rich cake scent still hanging air thought whether airbnb somehow tapped online searches purchases age convergent big data powerful retail analytics without seen really home contents either place anything useful hosts airbnb listings id ended two similar yet weirdly different residences one even conversation hosts familiar relevant didnt read back back easy compare child even real part test period home staging immersive theatre hot couldnt happen thought sameday delivery services breaking couldnt set highly personalized home contentschosen familiar aspirational want leave space potential purchases help fund business modelhave plucked regional depot popped onto shelves cabinets organized arrival couldnt actors search work london briefed enough open source material interact hour couldnt couldnt id road fatigue starting set maybe affecting head two years ago back mind since past summer similar experience whole family midrelocation netherlands similar homes family demographics away holiday time tough get small children follow script right one house comfortable enough suburban town charming place gentrifying neighborhood worth squatting hopes owners didnt return jk airbnb jk optimizing stays feeding appropriate properties hopes making testing easier hotels tested things hotelkiller even left bread us welcome basket one white one whole grain airbnb deployed aerosolve machine learning platform make sense realtime usage data help hosts get better return tuning properties desirability feasiblethe company already using automated scanning house photos optimize presentation properties well technology aimed properties wouldnt airbnb also dig minds guests find respond different houses conveniences theyre drawn etc nah would take sensors inside house top crack web mobile analytics youd need know people stay im sitting thinking crazy idea see tweet go airbnb purchasedan obscure russian sensor company slammed laptop checked cabinets tin foil month goes forget open medium see story airbnb mocked parts headquarters based apartment design french couple use service let flat couple suing company branding company life owner benjamin dewe told buzzfeed company apparently copied range style elements french couples home san francisco offices doodles chalkboard doodles jamie lauren keiles demonstrated medium piece pretty easy break furnishing accessories shoppable list goods obtained amazon elsewhere like magazine features show buy knockoffs celebrity fashion complete prices shops familys flat admittedly one rented via airbnb including airbnb function commodified shopping list buy lifestyle right better yet live days convergence big data analytics including visual analysis tools look presence brands social media photos machine learning accessible apis companies like amazon breakneck logistics uberstyle even predictive shipping per notorious amazon patent fabbing home interior suit tastes tastes forming havent fully emerged yet within todays technology hell even cute roomba may quietly mapping place live available knockoff home robots soon checked user agreements various home appliances systems see sell data probably tap stock underused homes underemployed people theres one thing sharing economy overlords taught us world collection undermonetized assets waiting redistributed right productize commodify populate secondtolast frontier living spaces staying someone elses place someone elses stuff fancied pictures tired everything else personalized financialized productized even stuff could ubered position desirable location based recent pinterest saves think bundled dreamhometm service perpetually test drive new living room suite long holiday weekendsi mean wait purchasing buyers remorse set get way without financial commitment desires played forward time even test roommates neighbors weekend stop furnishings paint colors slap detailed sentiment analyses personality analytics gleaned prospective cohabitants online activities eyetracking history tinder preferences 23andme profile onto improv actors big data cosplay popup maisonette come monday morning walk front door nothing premium fee pay fee may subsidized various sponsors want test products dont worry cool duralux crate barrel linkedin picked tab getaway woods beach new friends sound good course knew would like check email temporary instant disposable dreamhouse weekend may waiting quick cheer standing ovation clap show much enjoyed story futures postnormal innovation strategic design httpchangeistcom essays observations speculations changeist lab,en,"['House', 'Amazon', 'Big Data', 'Aerosolve', 'Duralux, Crate & Barrel', 'LinkedIn']"
153,iDanScott,3,C# Plays Bejeweled Blitz – iDanScott – Medium,"As some of you reading this may or may not already know; over the past day or so I went from having the idea of creating a computer program that would essentially be able to play the popular arcade game Bejeweled Blitz on Facebook, to actually developing it. Now as hard as this problem sounds, it was surprisingly easy and fairly swift to solve. I broke it down in to 3 main steps:
The first step was probably the most time consuming of them all as everything from there was just colour management. The Solution I came up with in the end for that was to take a screenshot of the entire screen, and then scan the image from top to bottom using a nested for loop until I found a funny shade of brown that only appears along the Top Edge of the bejeweled grid (for anyone wondering that colour is Color.FromArgb(255, 39, 19, 5)). Once this colour had been found using the bitmap.GetPixel(x, y) function, I broke out of both for loops and knew that was the point where the top left corner of the grid was. I could then use this to construct a rectangle which would extract the bejeweled grid from the full screenshot. The size of the rectangle was calculated using the size of the grid cells (40px2, found that out using trusty old paint) multiplied by the amount of rows/columns there were (8, found that out using my eye balls). This resulted in the Rectangle size coming out at 320px2.
So the next step from here was to identify what colour resides in what square. To do that I started off by creating a 2 dimensional array of colours (Or Color’s to be politically correct) that was 8 rows and 8 columns to match that of the playable grid. I then systematically looped through the 2 dimensional array of colours in a nested for of x and y values assigning the array the colour of the pixel at the Location (x * 40) + 20, (y * 40) + 22. The x value was decided as it was half way through the gem and 22 was chosen for the y value as certain gems have a white center (Green and yellow) so 22 provided a more accurate reading. With this 2 dimensional array I was then able to generate a visual representation of what the computer was seeing when it was trying to figure out what colour was where.
As you can see from the above screenshot it’s able to identify what gem is what colour depending on what pixel is at that magic 20, 22 of the cell. Another thing I thought about before I finished this project to the state it’s in now is to prevent the application from trying to switch 2 empty cells (because one gem has just been blown up or something), I added all the known color codes to their own array and ask if the colour that’s in the 2d array also resides within the known colours list, if it does it will then evaluate whether it can be moved to a winning square, if not it’s ignored entirely.
I won’t bore you with the gory details of how I check if a gem can be moved, as instead this is a link to the beginning of the if statement in my Open Source Github Project. From here the full source code can be viewed, commented on and even improved upon if you guys feel like I could do something obviously better.
Finally all that’s left to do by definition of this application is to actually move the Gems. This is done by making some Windows API calls to set the mouse location and simulate mouse clicks. Again the details of how to exactly do that are within the github project, but if I’ve kept your attention for this long all that’s left to say is thank you and if you have any further questions don’t hesitate to hit me up on here or twitter @iDanScott.
Thanks for reading.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Dan Scott, 23. Computer Science Student of Plymouth University www.idanscott.co.uk
",reading may may already know past day went idea creating computer program would essentially able play popular arcade game bejeweled blitz facebook actually developing hard problem sounds surprisingly easy fairly swift solve broke 3 main steps first step probably time consuming everything colour management solution came end take screenshot entire screen scan image top bottom using nested loop found funny shade brown appears along top edge bejeweled grid anyone wondering colour colorfromargb255 39 19 5 colour found using bitmapgetpixelx function broke loops knew point top left corner grid could use construct rectangle would extract bejeweled grid full screenshot size rectangle calculated using size grid cells 40px2 found using trusty old paint multiplied amount rowscolumns 8 found using eye balls resulted rectangle size coming 320px2 next step identify colour resides square started creating 2 dimensional array colours colors politically correct 8 rows 8 columns match playable grid systematically looped 2 dimensional array colours nested x values assigning array colour pixel location x 40 20 40 22 x value decided half way gem 22 chosen value certain gems white center green yellow 22 provided accurate reading 2 dimensional array able generate visual representation computer seeing trying figure colour see screenshot able identify gem colour depending pixel magic 20 22 cell another thing thought finished project state prevent application trying switch 2 empty cells one gem blown something added known color codes array ask colour thats 2d array also resides within known colours list evaluate whether moved winning square ignored entirely wont bore gory details check gem moved instead link beginning statement open source github project full source code viewed commented even improved upon guys feel like could something obviously better finally thats left definition application actually move gems done making windows api calls set mouse location simulate mouse clicks details exactly within github project ive kept attention long thats left say thank questions dont hesitate hit twitter idanscott thanks reading quick cheer standing ovation clap show much enjoyed story dan scott 23 computer science student plymouth university wwwidanscottcouk,en,"['Color', 'Gems', 'Computer Science Student of Plymouth University www.idanscott.co.uk']"
154,Josh,18,9 Reasons Why Now is the Time for Artificial Intelligence,"There’s no denying it — Artificial Intelligence is happening and it’s happening big. Companies from Facebook to Google to Amazon are hard at work building world-class AI teams that infiltrate every facet of their products. Siri is one of the largest teams at Apple, and Microsoft has a growing research effort on this front. But why is now the time for AI?
1. Artificial Neural Networks
Traditional programming is deterministic, sequential, and logical. For example, computers take inputs, apply instructions, and generate outputs. This is great for tasks like calculations and conversions, but ill-suited if the application isn’t explicitly defined. The human brain, on the other hand, doesn’t behave this way. We learn and grow through repetition and education. Recent progress in artificial neural networks (ANNs) is key to building computers that can think. These breakthroughs are enabling tremendous strides in AI work at Google and Apple.
2. Knowledge Graph
Companies like Yelp, Foursquare, and Wolfram Alpha have enabled access to their data through APIs. As a result, platforms like Siri and Google Now are able to answer questions such as “What’s the closest coffeeshop?” or “What’s the population of India?”. If a new service had to handle the natural language processing (NLP), audio processing, data, and more, it would be nearly impossible. Fortunately the knowledge graph has evolved over the last 20 years to a point where new AI platforms can immediately have access to tons of data.
3. Natural Language Processing
NLP is a field of computer science and linguistics where computers attempt to derive meaning from human or natural input. While the field has been around since the 1950s, we’ve seen huge strides in the last few years thanks to Markov Models and n-gram models as well as projects like CALO and Wordnet. Stanford’s CoreNLP (demo here) is one of the many strong NLP solutions available today:
4. Speech Processing
In order to speak to a computer and have it understand our intent, we first need to handle the audio processing and convert sound waves to text. Known as speech processing, this field has seen major advancements in the last few years. Beyond the advancements in technology, we’ve seen companies like Nuance emerge with powerful APIs that power services like GPS, dictation, and more. Today, it is almost effortless for a new AI company to translate voice to text with a high degree of confidence.
5. Computational Power
The increase in computational efficiency over the last 17 years has been remarkable. In 2014, people could buy a video card that was 84.3 times the performance of one from 2004 for the same price. This increase in computational power is necessary if we want to emulate the brain. For example, research attempting to simulate 1 second of human brain activity required 82,944 processors supporting 1.73 billion artificial neurons connected by 10.4 trillion synapses. The decrease in cost and increase in computational power is enabling tremendous breakthroughs in AI today.
6.Consumer Acceptance
A big aspect of seeing mass adoption around artificial intelligence is consumer approval. With an initial push from Apple to highlight Siri, and now Microsoft’s Cortana and Google Now doing the same, smart phone owners have access to an AI whether they like it or not. As a result, consumers are coming around to the idea and even starting to embrace it. Funny videos like this one are helping the masses to accept this new human-computer interaction:
7. Ubiquity of Personal Computing
Conversing with an AI is a very personal experience. The emergence of smaller, always-on devices makes this possible. The iPhone was first introduced in 2007, only 8 years ago. Now, more than 64% of Americans own a smartphone. Wearables, such as the Apple Watch or Jawbone, open the possibility of even more intimate personal computing. These devices that we carry or wear serve as excellent hosts for this technology, making it possible for AI to truly enter the mainstream for the first time.
8. Funding
AI funding seems to go through waves, and in the last few years it’s definitely back up. Scaled Inference, a predictive AI company, recently raised $13.6M. Amazon just announced a $100M fund for voice controlled technologies, and IBM did the same for the Watson Venture Fund. The total invested in AI companies in 2014 grew past $300M from a mere $14.9M in 2010 according to Bloomberg. With firms like Khosla Ventures and Andreesen Horowitz leading deals in AI companies, funding is fueling innovation in AI.
9. Research Efforts
Another reason for the apparent surge in AI is the collective research efforts taking place. According to a 2014 report by MIRI (Machine Intelligence Research Institute), 41 of the top 275 CS conferences are AI-related. AI accounts for about 10% of all CS research today. The IEEE Computational Intelligence Society has more than 7,000 members and there are more 106 AI journals. Based on MIRI estimates, more than $50M went into funding AI research by the National Science Foundation (NSF) in 2011. With this much research and effort going into AI innovation, it’s no wonder we’re seeing this technology starting to reach the masses.
If history is an indicator, we may see interest in AI spike and go back down. With momentum across these various different sectors, though, AI interest seems likely to keep growing. If you’re interested in keeping up with our efforts and staying in touch, check out http://josh.ai and reach out!
This post was written by Alex at Josh.ai. Previously, Alex was a research scientist for NASA, Sandia National Lab, and the Naval Resarch Lab. Before that, Alex worked at Fisker Automotive and founded At The Pool and Yeti. Alex has an engineering degree from UCLA, lives in Los Angeles, and likes to tweet about Artificial Intelligence and Design.
Josh is an AI agent for your home. If you’re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.
Like Josh on Facebook — http://facebook.com/joshdotai
Follow Josh on Twitter — http://twitter.com/joshdotai
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",theres denying artificial intelligence happening happening big companies facebook google amazon hard work building worldclass ai teams infiltrate every facet products siri one largest teams apple microsoft growing research effort front time ai 1 artificial neural networks traditional programming deterministic sequential logical example computers take inputs apply instructions generate outputs great tasks like calculations conversions illsuited application isnt explicitly defined human brain hand doesnt behave way learn grow repetition education recent progress artificial neural networks anns key building computers think breakthroughs enabling tremendous strides ai work google apple 2 knowledge graph companies like yelp foursquare wolfram alpha enabled access data apis result platforms like siri google able answer questions whats closest coffeeshop whats population india new service handle natural language processing nlp audio processing data would nearly impossible fortunately knowledge graph evolved last 20 years point new ai platforms immediately access tons data 3 natural language processing nlp field computer science linguistics computers attempt derive meaning human natural input field around since 1950s weve seen huge strides last years thanks markov models ngram models well projects like calo wordnet stanfords corenlp demo one many strong nlp solutions available today 4 speech processing order speak computer understand intent first need handle audio processing convert sound waves text known speech processing field seen major advancements last years beyond advancements technology weve seen companies like nuance emerge powerful apis power services like gps dictation today almost effortless new ai company translate voice text high degree confidence 5 computational power increase computational efficiency last 17 years remarkable 2014 people could buy video card 843 times performance one 2004 price increase computational power necessary want emulate brain example research attempting simulate 1 second human brain activity required 82944 processors supporting 173 billion artificial neurons connected 104 trillion synapses decrease cost increase computational power enabling tremendous breakthroughs ai today 6consumer acceptance big aspect seeing mass adoption around artificial intelligence consumer approval initial push apple highlight siri microsofts cortana google smart phone owners access ai whether like result consumers coming around idea even starting embrace funny videos like one helping masses accept new humancomputer interaction 7 ubiquity personal computing conversing ai personal experience emergence smaller alwayson devices makes possible iphone first introduced 2007 8 years ago 64 americans smartphone wearables apple watch jawbone open possibility even intimate personal computing devices carry wear serve excellent hosts technology making possible ai truly enter mainstream first time 8 funding ai funding seems go waves last years definitely back scaled inference predictive ai company recently raised 136m amazon announced 100m fund voice controlled technologies ibm watson venture fund total invested ai companies 2014 grew past 300m mere 149m 2010 according bloomberg firms like khosla ventures andreesen horowitz leading deals ai companies funding fueling innovation ai 9 research efforts another reason apparent surge ai collective research efforts taking place according 2014 report miri machine intelligence research institute 41 top 275 cs conferences airelated ai accounts 10 cs research today ieee computational intelligence society 7000 members 106 ai journals based miri estimates 50m went funding ai research national science foundation nsf 2011 much research effort going ai innovation wonder seeing technology starting reach masses history indicator may see interest ai spike go back momentum across various different sectors though ai interest seems likely keep growing youre interested keeping efforts staying touch check httpjoshai reach post written alex joshai previously alex research scientist nasa sandia national lab naval resarch lab alex worked fisker automotive founded pool yeti alex engineering degree ucla lives los angeles likes tweet artificial intelligence design josh ai agent home youre interested following josh getting early access beta enter email httpsjoshai like josh facebook httpfacebookcomjoshdotai follow josh twitter httptwittercomjoshdotai quick cheer standing ovation clap show much enjoyed story,en,"['Artificial Intelligence', 'Facebook to Google', 'Amazon', 'Apple', 'Microsoft', 'Google', 'NLP', 'Stanford', 'Nuance', 'GPS', 'Cortana and Google Now', 'iPhone', 'the Apple Watch or Jawbone', 'IBM', 'the Watson Venture Fund', 'Bloomberg', 'Khosla Ventures', 'MIRI (Machine Intelligence Research Institute', 'CS', 'The IEEE Computational Intelligence Society', 'MIRI', 'the National Science Foundation', 'NSF', 'NASA', 'Sandia National Lab', 'Fisker Automotive', 'UCLA', 'Artificial Intelligence and Design', 'http://facebook.com/joshdotai']"
155,paulson,1,"What Could Happen If We Did Things Right: An Interview With Kim Stanley Robinson, Author Of Aurora","Is Kim Stanley Robinson our greatest political writer? That was the provocative question posed recently by a critic in The New Yorker. Science fiction writers rarely get that kind of serious attention, but Robinson’s visionary experiments in imagining a more just society have always been part of his fictional universe. In fact, he got his Ph.D. in English studying under the renowned Marxist theorist Fredric Jameson. The idea of utopia may seem discredited in today’s world, but not to Robinson. He believes we need more utopian thinking to create a better future.
And the future is where he takes us in his new novel Aurora. Set in the 26th century, it’s the story of a space voyage to colonize planets outside our Solar System. Robinson writes in the tradition of “hard science fiction,” using only existing or plausible technology for his interstellar journey. As much as he geeks out on the mechanics of space travel, his real interest is how people would handle a very long voyage trapped inside a starship. His futuristic themes won’t surprise longtime fans of Robinson, who’s best known for his Mars trilogy, published in the 1990s. To read KSR is to wonder how our species might survive and even thrive in the centuries ahead.
The author stopped by my radio studio before giving the keynote speech at a local science fiction conference. We talked about the existential angst of life on a starship, the future of artificial intelligence and the aesthetics of space travel. Our conversation will air on Public Radio International’s To the Best of Our Knowledge. You can subscribe to the TTBOOK podcast here.
Steve Paulson: How would you describe the story in Aurora?
Kim Stanley Robinson: It’s the story of humanity trying to go to other star systems. This may be an ancient idea, but for sure it’s a 19th century idea. The Russian space scientist Tsiolkovsky said Earth is humanity’s cradle but you’re not meant to stay in your cradle forever. This idea has been part of science fiction ever since — that humanity will spread through the stars, or at least through this galaxy.
SP: It’s a long way to travel to another star.
KSR: It is a long way. And the idea of going to the stars is getting not easier, but more difficult. So I decided to explore the difficulties. I tried to think about whether it’s really possible at all, or if we’re condemned — if you want to put it that way — to stay in this Solar System.
SP: What star are your space voyagers trying to get to?
KSR: Tau Ceti, which has often been the destination for science fiction voyagers. Ursula Le Guin’s Dispossessed takes place around Tau Ceti, and so does Isaac Asimov’s The Naked Sun. It’s about 12 light-years away. We now know it has three or four big planets the size of a small Neptune or a large Earth. They’ve got the mass of about five Earths. That’s too heavy for humans to be on, but those planets could have moons about the size of Earth. So it becomes the nearest viable target. Alpha Centauri, which is just four light-years away, only has tiny planets that are closer than Mercury is to our sun, so they won’t be habitable.
SP: Your story is set 500 years into the future. It takes a long time to get to this star.
KSR: Yes. My working principle was, what would it really be like? So no hyperspace, no warp drive, no magical thing about what isn’t really going to happen to get us there. That means sub-lightyear speeds. So I postulated that we could get spaceships going to about one-tenth the speed of light, which is extraordinarily fast. Then the problem becomes slowing down. You have to carry enough fuel to slow yourself down if you’ve accelerated to that kind of speed. The mass of the decelerant fuel will be about 90% of the weight of your ship. As you’re approaching your target, you have to get back down to the speed at which you can orbit your destination. The physics of this is a huge problem.
SP: You’re talking about a multi-generational voyage that will take a couple hundred years. That’s a fascinating idea. The people who start out will be dead by the time the starship gets there.
KSR: I guessed it would take four or five generations — say, 200 years. This is not my original idea. The multi-generational starship is an old science fiction idea started by Robert Heinlein and there may even be earlier precursors. One always finds forgotten precursors for every science fiction idea. Heinlein wrote Universe around 1940, Brian Aldiss wrote a book called Starship in 1958, and Gene Wolfe wrote a very great starship narrative in the 1990s, The Book of the Long Sun. So it’s not an original idea to me; it’s sort of a sub-genre within science fiction.
SP: But the whole idea of a project that takes generations is something we don’t do anymore. People did that when they built the pyramids in Egypt or the great cathedrals in Europe. I can’t think of a current project that will take generations to complete.
KSR: You really have to think of it as a mobile island or a vast zoo. It isn’t even a project so much as a city that you’ve shot off into space, and when the city gets to its destination, the people unpack themselves into the new place. You’re right, it could be compared to building the cathedrals. And it’s interesting to think about the people born on the starship who didn’t make the choice to be there. So it turned into a bit of a prison novel.
SP: Because you’re trapped there. You’re in this confined space for your whole life.
KSR: And for two or three generations, you’re born on the ship and you die on the ship. You’re just in between the stars. So it’s very existential. There are some wonderful thought stimulants to thinking about a starship as a closed ecology.
SP: How big is the starship in your story?
KSR: There’s something like a hundred kilometers of interior space.
SP: So this is big!
KSR: Yeah, two rings. You could imagine them as cylinders that have been linked until they make a circle, so twelve cylinders per circle. You’ve got 24 cylinders and each has a different Earth ecology in it and each one of them is about five kilometers long. It’s pretty big, but you need that much space to be viable at all because you have to take along a Noah’s Ark worth of genetic material, or else it isn’t going to work.
SP: What do you have to bring along?
KSR: You would want as much of everything as you can bring, but you certainly need a big bacterial load. You need to bring along a lot of soil. You need a lot of what would be effectively unidentified bacteria; you just need a big hunk of earth. And then all the animals that you can fit that would survive. Each one of these cylinders would be like a little zoo or aviary.
SP: As you were imagining this voyage, which part was most interesting to you? Was it the science — trying to figure out technically how we could get there? Or was it the personal dynamics of how people would get along when they’re trapped in space for so long?
KSR: I think it would be the latter. I’m an English major. The wing of science fiction that’s discussed this idea has been the physics guys, the hard SF guys. They’ve been concerned with propulsion, navigation, with slowing down, with all the things you would use physics to comprehend. But I’ve been thinking about the problem ecologically, sociologically, psychologically. These elements haven’t been fully explored and you get a new story when you explore them. It’s a rather awful story, which leads to some peculiar narrative choices.
SP: Why is it awful?
KSR: Because they’re trapped and the spaceship is a trillion times smaller than Earth’s surface. Even though it’s big, it’s small. And we didn’t evolve to live in one of these things. It’s like you spend your whole life in a Motel Six.
SP: Put that way, it does sound pretty awful.
KSR: Better than a prison, but you can’t get out. You can’t choose to do something else. I don’t think we’re meant for that even though we live in rooms all the time in modern society. I think the reason people volunteer for things like Mars One is they’re thinking, “How is that different from my ordinary life? I sit in a room in front of my laptop all day long. If I’m going to Mars, it’s more interesting.”
SP: Mars One is the project that’s trying to engineer one-way trips to Mars. You know you’re not going to come back. Frankly, it sounds like a suicide mission, and yet tens of thousands of people have signed up for this mission.
KSR: Yes, but they’ve made a category error. Their imaginations have not managed to catch up to the situation. They are in some kind of boring life and they want excitement. Maybe they’re young, maybe they’re worried about their economic prospects, maybe they want something different. They imagine it would be exciting if they got to Mars. But it was Ralph Waldo Emerson who said travel is stupid; wherever you go, you’re still stuck with yourself. I went to the South Pole once. I was only there for a week and it was the most boring place in Antarctica because we couldn’t really leave the rooms without getting into space suits.
SP: Is extended space travel like going to Antarctica?
KSR: It’s the best analogy you can get, especially for Mars. You would get to a landscape that’s beautiful and sublime and scientifically interesting and mind-boggling. Antarctica is all those things and so would Mars be. But I notice that nobody in the United States cares about what the Antarcticans are doing every November and December. There are a couple thousand people down there having a blast. If the same thing happened on Mars, it would be like, “Oh, cool. Some scientists are doing cool things,” but then you go back to your real life and you don’t care.
SP: So even though you write about these long space voyages, you wouldn’t want to be part of one?
KSR: Not at all. But I’ve only written about long space voyages once — in this book, Aurora.
SP: You also wrote a whole series of books about Mars. You still have to get there.
KSR: But there’s an important distinction. You can get to Mars in a year’s travel and then live there your whole life. And you’re on a planet, which has gravity and landscape. You can terraform it. It’s like a gardening project or building a cathedral. I think terraforming Mars is viable. Going to the stars, however, is completely different because you would be traveling in a spaceship for several generations where you’re in a room, not on a planet. It’s been such a techie thing in science fiction. But people haven’t de-stranded those two ideas. They said, “Well, if we can go to Mars, we can go to Tau Ceti.” It doesn’t follow. It’s not the same kind of effort.
SP: Would it be interesting to travel just through our own Solar System?
KSR: Yes, this Solar System is our neighborhood. We can get around it in human time scales. We can visit the moons of Saturn. We can visit Triton, the moon of Neptune. There are hundreds of thousands of asteroids on which we could set up bases. The moons of all the big planets are great. The four big moons of Jupiter — we couldn’t be on Io because it’s too radioactive or too impacted by the radio waves of Jupiter itself — but by and large, the Solar System is fascinating.
SP: Yet I imagine a lot of people would say, “Yeah, there’s a lot of cool stuff out there, but it’s all dead.”
KSR: Well, we have questions about Mars, Europa, Ganymede and Enceladus, a moon of Saturn. Wherever there’s liquid water in the Solar System, it might be dead or alive. It might be bacterially alive. It might have life that started independently. It might be cousin life that was blasted off of Mars on meteorites and landed on Earth and other places. We don’t know yet. And if it is dead, it’s still beautiful and interesting, so these would be sites of scientific interest. Antarctica is pretty dead, but we still go there.
SP: I’ve heard it’s incredibly beautiful.
KSR: It’s very beautiful. I think if you’re standing on the surface of Europa, looking around the ice-scape and looking up at Saturn in the sky overhead, it’s also going to be beautiful. I’m not sure if it’s beautiful enough to drive a gigantic effort to get there. The robots going there now are already a tremendous exploration for humanity. The photos sent back to us are a gigantic gift and a beautiful thing to look at. So humans going there will always be a kind of research project that a few scientists do. I’m not saying that the rest of the Solar System is crucial to us. I think Earth is the one and only crucial place for humanity. It will always be our only home.
SP: I wonder if we would develop a different sense of beauty if we went out into the Solar System. When we think of natural beauty, we tend to think of gorgeous landscapes like mountains or deserts. But out in the Solar System, on another planet or a moon, would our experience of awe and wonder be different?
KSR: You can go back to the 18th century when mountains were not regarded as beautiful. Edmund Burke and the other philosophers talked about the sublime. So the beautiful has to do with shapeliness and symmetry and with the human face and figure. Through the Middle Ages, mountains were seen as horrible wastelands where God had forgotten what to do. Then in the Romantic period, they became sublime, where you have not quite beauty but a combination of beauty and terror. Your senses are telling you, “This is dangerous,” and your rational mind is saying, “No, I’m on a ledge, but I’ve got a railing. It looks dangerous, but it’s not.” You get this thrilling sensation that is not beauty but is the sublime. The Solar System is a very sublime place.
SP: Because you could die at any moment if your oxygen support system goes out.
KSR: Exactly. It’s like being in a submarine or even in scuba gear — the feeling of being meters under the surface, with a machine keeping you alive and bubbles going up, as you’re looking at a coral reef. That’s sublimity. There’s an element of terror that’s suppressed because your rational mind is saying it’s okay. When you fly in an airplane and look down 30,000 feet to the surface of the earth, that’s the feeling of the sublime, even if you’re looking down at a beautiful landscape. But people can’t bear to look because after a while you’re thinking, “Boy, this machine sure has to work.”
SP: If you think long and hard about this...
KSR: You might never fly again.
SP: One thing that’s so interesting about your novel Aurora is that most of it is narrated by the ship itself. What was the idea here?
KSR: I do like the idea that my narrators are also characters, that they’re not me. I’m not interested in myself. I like to tell other people’s stories, so I don’t do memoir. I do novels. And for three or four novels now, it’s been an important game to me to imagine the narrators’ voices being different from mine. So Shaman’s was the Third Wind, this mystical spirit that knew the Paleolithic inside and out. That wasn’t me. And Cartophilus, the time traveler, tells Galileo’s story.
In Aurora, it made sense for the ship to need really powerful artificial intelligence, like a quantum computer. And once you get to quantum computers, you’ve got processing speeds that are equal to the processing speeds of human brains. But the methodologies would be completely different. They’d be algorithms that we programmed. Maybe it wouldn’t have consciousness, but when you get that much processing speed, who’s to say what consciousness really is? So I made the narrator out of this starship’s AI system. And he — she, it — has been instructed by the chief engineer to keep a narrative account of the voyage. When you think about it, writing novels is strange. We can tell most stories to each other in about 500 words, so a novel is not a natural act. It’s an art form that’s been built up over centuries and doesn’t have a good algorithm.
SP: I recently interviewed Stephen Wolfram, the computer theorist and software developer, and asked if he thought some future computer could write a great novel. He said yes.
KSR: Wolfram’s very important in theorizing what computers can do because he’s made a breakdown of activities from the simple to the complex. And at full complexity, the human brain or any other thinking machine that can get to that fourth level of complexity should be able to do it.
SP: So in the future, you think a computer or artificial intelligence system could write a modern “Ulysses”?
KSR: Well, this is an interesting question. At that point you would need a quantum computer. It would need to read a whole bunch of novels and try to abstract the rules of storytelling and then give it a shot. In my novel, the first chapter the computer writes is 18th century literature. It’s what we would call “camera-eye point of view.” It doesn’t guess what people are thinking; how can it? It just reports what it sees like a Hemingway short story. As the novel goes on, chapter by chapter, the computer is recapitulating the history of the novel, and by the end of the last chapter narrated by the computer, you’re getting full-on stream of consciousness. It’s kind of like Ulysses or Virginia Woolf where you’re inside the mind, although it’s the mind of the computer itself. The last chapter is in a kind of “flow state” of the computer’s thinking.
SP: At that point, does the computer have emotions?
KSR: It wonders about that. The computer can’t be sure. Actually, we’re all trapped in our own consciousness. What are other people thinking? What are other people feeling? You have to work by analogy to your own internal states. The computer only has access to its own internal states.
SP: Does the future of AI and technology more generally excite you?
KSR: Yes, AI in particular. I used to scoff at it. I’m a recent convert to the idea that AI computing is interesting. Mainly, it’s just an adding machine that can go really, really fast. There are no internal states. They’re not thinking. However, quantum computers push it to a new level. It isn’t clear yet that we can actually make quantum computers, so this is the speculative part. It might be science fiction that completely falls apart. There was science fiction about easy space travel, but that’s not going to work. There was science fiction about all of us living 10,000 years. That might or might not work, but it’s way speculative. Quantum computing is still in that category because you get all the weirdness of quantum mechanics. There are certain algorithms that might take a classical computer 20 billion years, while a quantum computer would take 20 minutes. But those are for very particular tasks, like factoring a thousand-digit number. We don’t know yet whether more complex tasks will be something that a quantum computer can handle better than a regular computer. But the potential for stupendous processing power, like a human brain’s processing power, seems to be there.
SP: As a science fiction writer, do you have a particular mission to imagine what our future might be like? Is that part of your job?
KSR: Yes, I think that’s central to the job. What science fiction is good at is doing scenarios. Science fiction may never predict what is really going to happen in the future because that’s too hard. Strange things, contingent things happen that can’t be predicted, but we can see trajectories. And at this moment, we can see futures that are complete catastrophes where we cause a mass extinction event, we cook the planet, 90% of humanity dies because we run out of food or we think we’re going to run out of food and then we fight over it. In other words, complete catastrophe. On the other hand, there’s another scenario where we get hold of our technologies, our social systems and our sense of law and justice and we make a kind of utopia — a positive future where we’re sustainable over the long haul. We could live on Earth in a permaculture that’s beautiful. From this moment in history, both scenarios are completely conceivable.
SP: Yet if we look at popular culture, dystopian and apocalyptic stories are everywhere. We don’t see many positive visions of the future.
KSR: I’ve always been involved with the positive visions of the future, so I would stubbornly insist that science fiction in general, and my work in particular, is about what could happen if we did things right. But right now, dystopia is big. It’s good for movies because there are a lot of car crashes and things blowing up.
SP: Is it a problem that we have so many negative visions of the future?
KSR: Dystopias express our fears and utopias express our hopes. Fear is a very intense and dramatic emotion. Hope is more fragile, but it’s very stubborn and persistent. Hope is inherent in us getting up and eating breakfast every day. In the 1950s young people were thinking, “I’m going to live on the moon. I will go to Neptune.” Today it’s The Hunger Games, which is a very important science fiction story. I like that it’s science fiction, not fantasy. It’s not Lord of the Rings or Harry Potter. It’s a very surrealistic and unsustainable future, but it’s a vision of the fears of young people. They’re pitting us against each other and we have to hang together because there’s a rich elite, an oligarchy, that’s simply eating our lives for their own entertainment. So there’s a profound psychological and emotional truth in The Hunger Games.
There’s a feeling of fear and political apprehension that late global capitalism is not fair. My Mars books — although they’re not as famous and haven’t been turned into movies — are quite popular because they’re saying we could make a decent and beautiful civilization. I’ve been noticing with great pleasure that my Mars trilogy is selling better now than it ever has.
SP: Does our society need positive visions of the future? Do we need people to create scenarios of how things could go well?
KSR: Oh, yes. Ever since Thomas More’s Utopia, we’ve always had it. Edward Bellamy wrote a book called Looking Backward: 2000–1887. The progressive political movement that changed things around the time of Teddy Roosevelt came out of this novel. When people had to reconstruct the world’s social order after World War II, they turned to H.G. Wells and A Modern Utopia and Men Like Gods. We always need utopias. These days, people are fascinated by Steve Jobs or Bill Gates. It’s like those geeky 1950s science fiction stories where a kid in his backyard makes a rocket that goes to the moon. Now it’s in his garage, where he makes a computer that changes everything. We love these stories because they’re hopeful and they suggest that we could seize history and change it for the better. If science fiction doesn’t provide those stories, people find them somewhere else. So Steve Jobs is a science fiction story we want.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Expanding the influence of literature in popular culture.
",kim stanley robinson greatest political writer provocative question posed recently critic new yorker science fiction writers rarely get kind serious attention robinsons visionary experiments imagining society always part fictional universe fact got phd english studying renowned marxist theorist fredric jameson idea utopia may seem discredited todays world robinson believes need utopian thinking create better future future takes us new novel aurora set 26th century story space voyage colonize planets outside solar system robinson writes tradition hard science fiction using existing plausible technology interstellar journey much geeks mechanics space travel real interest people would handle long voyage trapped inside starship futuristic themes wont surprise longtime fans robinson whos best known mars trilogy published 1990s read ksr wonder species might survive even thrive centuries ahead author stopped radio studio giving keynote speech local science fiction conference talked existential angst life starship future artificial intelligence aesthetics space travel conversation air public radio internationals best knowledge subscribe ttbook podcast steve paulson would describe story aurora kim stanley robinson story humanity trying go star systems may ancient idea sure 19th century idea russian space scientist tsiolkovsky said earth humanitys cradle youre meant stay cradle forever idea part science fiction ever since humanity spread stars least galaxy sp long way travel another star ksr long way idea going stars getting easier difficult decided explore difficulties tried think whether really possible condemned want put way stay solar system sp star space voyagers trying get ksr tau ceti often destination science fiction voyagers ursula le guins dispossessed takes place around tau ceti isaac asimovs naked sun 12 lightyears away know three four big planets size small neptune large earth theyve got mass five earths thats heavy humans planets could moons size earth becomes nearest viable target alpha centauri four lightyears away tiny planets closer mercury sun wont habitable sp story set 500 years future takes long time get star ksr yes working principle would really like hyperspace warp drive magical thing isnt really going happen get us means sublightyear speeds postulated could get spaceships going onetenth speed light extraordinarily fast problem becomes slowing carry enough fuel slow youve accelerated kind speed mass decelerant fuel 90 weight ship youre approaching target get back speed orbit destination physics huge problem sp youre talking multigenerational voyage take couple hundred years thats fascinating idea people start dead time starship gets ksr guessed would take four five generations say 200 years original idea multigenerational starship old science fiction idea started robert heinlein may even earlier precursors one always finds forgotten precursors every science fiction idea heinlein wrote universe around 1940 brian aldiss wrote book called starship 1958 gene wolfe wrote great starship narrative 1990s book long sun original idea sort subgenre within science fiction sp whole idea project takes generations something dont anymore people built pyramids egypt great cathedrals europe cant think current project take generations complete ksr really think mobile island vast zoo isnt even project much city youve shot space city gets destination people unpack new place youre right could compared building cathedrals interesting think people born starship didnt make choice turned bit prison novel sp youre trapped youre confined space whole life ksr two three generations youre born ship die ship youre stars existential wonderful thought stimulants thinking starship closed ecology sp big starship story ksr theres something like hundred kilometers interior space sp big ksr yeah two rings could imagine cylinders linked make circle twelve cylinders per circle youve got 24 cylinders different earth ecology one five kilometers long pretty big need much space viable take along noahs ark worth genetic material else isnt going work sp bring along ksr would want much everything bring certainly need big bacterial load need bring along lot soil need lot would effectively unidentified bacteria need big hunk earth animals fit would survive one cylinders would like little zoo aviary sp imagining voyage part interesting science trying figure technically could get personal dynamics people would get along theyre trapped space long ksr think would latter im english major wing science fiction thats discussed idea physics guys hard sf guys theyve concerned propulsion navigation slowing things would use physics comprehend ive thinking problem ecologically sociologically psychologically elements havent fully explored get new story explore rather awful story leads peculiar narrative choices sp awful ksr theyre trapped spaceship trillion times smaller earths surface even though big small didnt evolve live one things like spend whole life motel six sp put way sound pretty awful ksr better prison cant get cant choose something else dont think meant even though live rooms time modern society think reason people volunteer things like mars one theyre thinking different ordinary life sit room front laptop day long im going mars interesting sp mars one project thats trying engineer oneway trips mars know youre going come back frankly sounds like suicide mission yet tens thousands people signed mission ksr yes theyve made category error imaginations managed catch situation kind boring life want excitement maybe theyre young maybe theyre worried economic prospects maybe want something different imagine would exciting got mars ralph waldo emerson said travel stupid wherever go youre still stuck went south pole week boring place antarctica couldnt really leave rooms without getting space suits sp extended space travel like going antarctica ksr best analogy get especially mars would get landscape thats beautiful sublime scientifically interesting mindboggling antarctica things would mars notice nobody united states cares antarcticans every november december couple thousand people blast thing happened mars would like oh cool scientists cool things go back real life dont care sp even though write long space voyages wouldnt want part one ksr ive written long space voyages book aurora sp also wrote whole series books mars still get ksr theres important distinction get mars years travel live whole life youre planet gravity landscape terraform like gardening project building cathedral think terraforming mars viable going stars however completely different would traveling spaceship several generations youre room planet techie thing science fiction people havent destranded two ideas said well go mars go tau ceti doesnt follow kind effort sp would interesting travel solar system ksr yes solar system neighborhood get around human time scales visit moons saturn visit triton moon neptune hundreds thousands asteroids could set bases moons big planets great four big moons jupiter couldnt io radioactive impacted radio waves jupiter large solar system fascinating sp yet imagine lot people would say yeah theres lot cool stuff dead ksr well questions mars europa ganymede enceladus moon saturn wherever theres liquid water solar system might dead alive might bacterially alive might life started independently might cousin life blasted mars meteorites landed earth places dont know yet dead still beautiful interesting would sites scientific interest antarctica pretty dead still go sp ive heard incredibly beautiful ksr beautiful think youre standing surface europa looking around icescape looking saturn sky overhead also going beautiful im sure beautiful enough drive gigantic effort get robots going already tremendous exploration humanity photos sent back us gigantic gift beautiful thing look humans going always kind research project scientists im saying rest solar system crucial us think earth one crucial place humanity always home sp wonder would develop different sense beauty went solar system think natural beauty tend think gorgeous landscapes like mountains deserts solar system another planet moon would experience awe wonder different ksr go back 18th century mountains regarded beautiful edmund burke philosophers talked sublime beautiful shapeliness symmetry human face figure middle ages mountains seen horrible wastelands god forgotten romantic period became sublime quite beauty combination beauty terror senses telling dangerous rational mind saying im ledge ive got railing looks dangerous get thrilling sensation beauty sublime solar system sublime place sp could die moment oxygen support system goes ksr exactly like submarine even scuba gear feeling meters surface machine keeping alive bubbles going youre looking coral reef thats sublimity theres element terror thats suppressed rational mind saying okay fly airplane look 30000 feet surface earth thats feeling sublime even youre looking beautiful landscape people cant bear look youre thinking boy machine sure work sp think long hard ksr might never fly sp one thing thats interesting novel aurora narrated ship idea ksr like idea narrators also characters theyre im interested like tell peoples stories dont memoir novels three four novels important game imagine narrators voices different mine shamans third wind mystical spirit knew paleolithic inside wasnt cartophilus time traveler tells galileos story aurora made sense ship need really powerful artificial intelligence like quantum computer get quantum computers youve got processing speeds equal processing speeds human brains methodologies would completely different theyd algorithms programmed maybe wouldnt consciousness get much processing speed whos say consciousness really made narrator starships ai system instructed chief engineer keep narrative account voyage think writing novels strange tell stories 500 words novel natural act art form thats built centuries doesnt good algorithm sp recently interviewed stephen wolfram computer theorist software developer asked thought future computer could write great novel said yes ksr wolframs important theorizing computers hes made breakdown activities simple complex full complexity human brain thinking machine get fourth level complexity able sp future think computer artificial intelligence system could write modern ulysses ksr well interesting question point would need quantum computer would need read whole bunch novels try abstract rules storytelling give shot novel first chapter computer writes 18th century literature would call cameraeye point view doesnt guess people thinking reports sees like hemingway short story novel goes chapter chapter computer recapitulating history novel end last chapter narrated computer youre getting fullon stream consciousness kind like ulysses virginia woolf youre inside mind although mind computer last chapter kind flow state computers thinking sp point computer emotions ksr wonders computer cant sure actually trapped consciousness people thinking people feeling work analogy internal states computer access internal states sp future ai technology generally excite ksr yes ai particular used scoff im recent convert idea ai computing interesting mainly adding machine go really really fast internal states theyre thinking however quantum computers push new level isnt clear yet actually make quantum computers speculative part might science fiction completely falls apart science fiction easy space travel thats going work science fiction us living 10000 years might might work way speculative quantum computing still category get weirdness quantum mechanics certain algorithms might take classical computer 20 billion years quantum computer would take 20 minutes particular tasks like factoring thousanddigit number dont know yet whether complex tasks something quantum computer handle better regular computer potential stupendous processing power like human brains processing power seems sp science fiction writer particular mission imagine future might like part job ksr yes think thats central job science fiction good scenarios science fiction may never predict really going happen future thats hard strange things contingent things happen cant predicted see trajectories moment see futures complete catastrophes cause mass extinction event cook planet 90 humanity dies run food think going run food fight words complete catastrophe hand theres another scenario get hold technologies social systems sense law justice make kind utopia positive future sustainable long haul could live earth permaculture thats beautiful moment history scenarios completely conceivable sp yet look popular culture dystopian apocalyptic stories everywhere dont see many positive visions future ksr ive always involved positive visions future would stubbornly insist science fiction general work particular could happen things right right dystopia big good movies lot car crashes things blowing sp problem many negative visions future ksr dystopias express fears utopias express hopes fear intense dramatic emotion hope fragile stubborn persistent hope inherent us getting eating breakfast every day 1950s young people thinking im going live moon go neptune today hunger games important science fiction story like science fiction fantasy lord rings harry potter surrealistic unsustainable future vision fears young people theyre pitting us hang together theres rich elite oligarchy thats simply eating lives entertainment theres profound psychological emotional truth hunger games theres feeling fear political apprehension late global capitalism fair mars books although theyre famous havent turned movies quite popular theyre saying could make decent beautiful civilization ive noticing great pleasure mars trilogy selling better ever sp society need positive visions future need people create scenarios things could go well ksr oh yes ever since thomas mores utopia weve always edward bellamy wrote book called looking backward 20001887 progressive political movement changed things around time teddy roosevelt came novel people reconstruct worlds social order world war ii turned hg wells modern utopia men like gods always need utopias days people fascinated steve jobs bill gates like geeky 1950s science fiction stories kid backyard makes rocket goes moon garage makes computer changes everything love stories theyre hopeful suggest could seize history change better science fiction doesnt provide stories people find somewhere else steve jobs science fiction story want quick cheer standing ovation clap show much enjoyed story expanding influence literature popular culture,en,"['The New Yorker', 'Solar System', 'KSR', 'Public Radio International', 'Mercury', 'Universe', 'Enceladus', 'Cartophilus', 'Wolfram', 'Quantum', 'quantum', 'Dystopias express', 'utopias express']"
156,Christopher Wolf Nordlinger,8,The Internet of Things and the Operating Room of the Future,"The doctor stands over the patient on the operating room table. It can be dizzying to look around at the dozen or more video screens dedicated to standalone medical devices and not think that the Internet of Things (IoT) could radically simplify the complexities of managing so many systems.
In the process, digital health could enormously improve patient care.
At the same time, hospitals struggle to constrain the rapidly-increasing costs of healthcare, yet with IoT investments they can reduce costs significantly.
It’s not hard to see how the medical industry and, hospitals in particular, will represent a major component of the $19 Trillion Internet of Things market opportunity that Cisco predicts by 2020.
Imagining its future in surgery alone is not some far-off idea. It already exists and it’s revolutionary due to a unique blend of IoT, big data, advanced analytics and smart medical devices.
Here’s how the reality plays out in a leading example.
Thousands of people suffer from heart arrhythmias caused by heart disease which show up as a flutter in the heartbeat that is highly disruptive and can cause potentially fatal strokes and heart attacks. There are a few pharmaceutical drugs that can mollify the symptoms but they do nothing to remove the dead tissue lesions in the heart that cause the underlying situation which is called atrial fibulation, or AFib for short.
CardioThings (a made-up name to protect the company while under FDA approval review) is attacking this problem with ablation to remove the lesions by gently burning them out with a laser. This involves inserting a catheter into the heart to try to perform ablation to remove the AFib-causing lesions. Each device is hard-wired to a screen where streaming data from the end of the catheter display a view of the inside of the heart. But that’s not where the data stop between the heart and the monitors like many devices.
CardioThings, a Silicon Valley startup, works with two real IoT powerhouses, PTC ThingWorx and another Silicon Valley startup, Glassbeam, to make something much more powerful possible. ThingWorx models the operation of the catheter so that it can send secure data to the cloud where it can be analyzed by Glassbeam.
Glassbeam turns the unstructured data into structured data in the forms of readable reports that the device company can then use to improve doctors’ surgical performance. For CardioThings and other high-value asset manufacturers, this kind of data can also increase the uptime of their catheter device. Others can use IoT Analytics to increase the uptime of CAT-Scans and MRIs because the data can show when even the smallest part is showing signs of weakness or malfunction and enable a repair that keeps that equipment operating.
How?
Imagine CardioThings’s optical catheter, thin enough to fit comfortably through a vein, entering a heart and mapping it out to find the lesions responsible for the AFib.
The surgeon is then able to frame the boundaries of the lesions on CaridoThings’s monitors to see which are dying and need to get burned out. The laser beam from the sensor-embedded catheter then cuts the lesions out and the patient is healed.
What does this have to do with saving money for the hospital?
High-value machines such as MRIs and CAT Scan cost millions. Downtime for them is not only very costly for the hospital that is not billing patients but also, more importantly, interrupts patients from getting the best possible care.
ThingWorx enables medical devices (Things, sensors modeled by ThingWorx to communicate as if it was the device) to talk to other Things in the cloud. Once the unstructured data is there it can be combined and recombined by Glassbeam’s analytics software to detect any abnormalities. For MRIs, CAT Scan and other devices, stopping small problems from becoming big problems that crash expensive heavily-used equipment is the ultimate value of predictive maintenance.
Hospitals are large places with many people and things moving about a great deal and keeping track of assets ranging from MRI scanners to $60,000 beds is quite challenging.
In the case of CardioThings above, the alliance of PTC ThingWorx and Glassbeam should make the medical industry and business decision makers globally take notice. Whether it’s healthcare, agriculture, networking or manufacturing, higher utilization of equipment is absolutely essential to remaining competitive.
In the case of the CardioThings’s catheter spitting out unstructured data, Chris Kuntz, Vice President, Ecosystem Programs of PTC ThingWorx says “imagine the cardiac data from that same procedure being combined and recombined with data from EKG machines, MRI machines, pharmaceutical research, personal medical record-keeping systems, blood monitors and hundreds of healthcare systems. This is how the Internet of Things drives a revolution in healthcare.”
“Thanks to our partnership with ThingWorx,” Glassbeam CEO Puneet Pandit says, “we are able to capture that unstructured data off the catheter and create structured data that business decision makers at hospitals, the manufacturers and individual doctors can learn from”
Pandit adds, “As a result of the large amount of critical data coming from the catheter, you can answer many questions. How did the device perform? Under what circumstances? How long did the surgery take? Which surgeons did it most effectively? Who needs to be more formally trained?”
As a result of this solution, training surgeons to use equipment better provides significantly improved outcomes for patients.
And for hospitals dispensing critical care, no one has to wait any longer for the MRI to crash to know there was a problem. They can fix the smallest problem before it escalates.
Letting the hospital know that a specific part is faulty by simply examining the unstructured data it sends out is the best example of the power of predictive maintenance. No one has to wait for the MRI to crash. Hospitals can enjoy huge savings through predictive maintenance on all its heavily-used expensive equipment.
Given concerns about privacy and safeguarding of material, it is essential to have a secure connectivity partner such as ThingWorx aboard. HIPPA is just the beginning of the scope of regulatory requirements that will need to be accommodated to operate successfully in the healthcare data space. Applied analytics available to doctors in real-time reduces medical procedure risk and overall liability concerns.
For hospitals to reduce costs and increase profitability, IoT will play an enormous role. For patients, it means their doctors will know so much more about treating them to ensure the best care after any procedure whether it’s a heart bypass, cancer surgery, heart transplant or a simple blood test.
Jack Reader, Business Development Manager at ThingWorx (now at Verizon) says, “Imagine an operating room where there are just a few monitors and all the devices speak to each other and with thousands of medical systems within and beyond the walls of the hospitals. All of this innovation will exponentially increase insight and intelligence, reduce costs for the hospital and increase health outcomes”.
The implications in terms of knowledge gained and positive health outcomes is so phenomenal that we almost can’t now imagine from this early stage in the IoT era all the possible sources nor all the insights that will be gained. However, the sooner IoT Analytics is adopted in the hospital, the sooner patients can expect better-run hospitals and healthier lives. This is only the beginning of a new era.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Ph.D. Fulbright Scholar. Storyteller. Communications Expert. Content maven. Formerly State Dept-Startups-Cisco & more.
",doctor stands patient operating room table dizzying look around dozen video screens dedicated standalone medical devices think internet things iot could radically simplify complexities managing many systems process digital health could enormously improve patient care time hospitals struggle constrain rapidlyincreasing costs healthcare yet iot investments reduce costs significantly hard see medical industry hospitals particular represent major component 19 trillion internet things market opportunity cisco predicts 2020 imagining future surgery alone faroff idea already exists revolutionary due unique blend iot big data advanced analytics smart medical devices heres reality plays leading example thousands people suffer heart arrhythmias caused heart disease show flutter heartbeat highly disruptive cause potentially fatal strokes heart attacks pharmaceutical drugs mollify symptoms nothing remove dead tissue lesions heart cause underlying situation called atrial fibulation afib short cardiothings madeup name protect company fda approval review attacking problem ablation remove lesions gently burning laser involves inserting catheter heart try perform ablation remove afibcausing lesions device hardwired screen streaming data end catheter display view inside heart thats data stop heart monitors like many devices cardiothings silicon valley startup works two real iot powerhouses ptc thingworx another silicon valley startup glassbeam make something much powerful possible thingworx models operation catheter send secure data cloud analyzed glassbeam glassbeam turns unstructured data structured data forms readable reports device company use improve doctors surgical performance cardiothings highvalue asset manufacturers kind data also increase uptime catheter device others use iot analytics increase uptime catscans mris data show even smallest part showing signs weakness malfunction enable repair keeps equipment operating imagine cardiothingss optical catheter thin enough fit comfortably vein entering heart mapping find lesions responsible afib surgeon able frame boundaries lesions caridothingss monitors see dying need get burned laser beam sensorembedded catheter cuts lesions patient healed saving money hospital highvalue machines mris cat scan cost millions downtime costly hospital billing patients also importantly interrupts patients getting best possible care thingworx enables medical devices things sensors modeled thingworx communicate device talk things cloud unstructured data combined recombined glassbeams analytics software detect abnormalities mris cat scan devices stopping small problems becoming big problems crash expensive heavilyused equipment ultimate value predictive maintenance hospitals large places many people things moving great deal keeping track assets ranging mri scanners 60000 beds quite challenging case cardiothings alliance ptc thingworx glassbeam make medical industry business decision makers globally take notice whether healthcare agriculture networking manufacturing higher utilization equipment absolutely essential remaining competitive case cardiothingss catheter spitting unstructured data chris kuntz vice president ecosystem programs ptc thingworx says imagine cardiac data procedure combined recombined data ekg machines mri machines pharmaceutical research personal medical recordkeeping systems blood monitors hundreds healthcare systems internet things drives revolution healthcare thanks partnership thingworx glassbeam ceo puneet pandit says able capture unstructured data catheter create structured data business decision makers hospitals manufacturers individual doctors learn pandit adds result large amount critical data coming catheter answer many questions device perform circumstances long surgery take surgeons effectively needs formally trained result solution training surgeons use equipment better provides significantly improved outcomes patients hospitals dispensing critical care one wait longer mri crash know problem fix smallest problem escalates letting hospital know specific part faulty simply examining unstructured data sends best example power predictive maintenance one wait mri crash hospitals enjoy huge savings predictive maintenance heavilyused expensive equipment given concerns privacy safeguarding material essential secure connectivity partner thingworx aboard hippa beginning scope regulatory requirements need accommodated operate successfully healthcare data space applied analytics available doctors realtime reduces medical procedure risk overall liability concerns hospitals reduce costs increase profitability iot play enormous role patients means doctors know much treating ensure best care procedure whether heart bypass cancer surgery heart transplant simple blood test jack reader business development manager thingworx verizon says imagine operating room monitors devices speak thousands medical systems within beyond walls hospitals innovation exponentially increase insight intelligence reduce costs hospital increase health outcomes implications terms knowledge gained positive health outcomes phenomenal almost cant imagine early stage iot era possible sources insights gained however sooner iot analytics adopted hospital sooner patients expect betterrun hospitals healthier lives beginning new era quick cheer standing ovation clap show much enjoyed story phd fulbright scholar storyteller communications expert content maven formerly state deptstartupscisco,en,"['AFib', 'CardioThings', 'FDA', 'PTC ThingWorx', 'ThingWorx', 'IoT Analytics', 'Imagine CardioThings’s', 'CaridoThings', 'CAT Scan', 'MRI', 'Ecosystem Programs of PTC ThingWorx', 'EKG', 'HIPPA', 'Business Development', 'Verizon', 'Ph.D. Fulbright']"
157,Louis Rosenfeld,90,Everyday IA – Louis Rosenfeld – Medium,"A few days ago, Cennydd Bowles gently trolled many of us thusly:
As Cennydd has keynoted a past Information Architecture Summit, it’s hard to ignore his question. And Cennydd’s timing is quite interesting, given that tomorrow is World IA Day.
The theme of this year’s WIAD is “architecting happiness”. And in this adorable little video that the IA Institute created to promote WIAD 2015, Abby Covert says that this theme was chosen “because of the rising amount of information that everyone has to deal with” (my italics):
Cennydd, there’s your answer: if you’re a human in today’s developed world, where even physical objects and spaces are soaked in information, you are struggling to cope with and make sense of the stuff. Nearly all the time. And nearly everywhere. Information architecture problems are everyday human problems. So if you’re designing for humans today, you’ll need at least some information architecture skills in order to help them.
Information architecture literacy is required for anyone who designs anything.
So it’s not surprising that WIAD has exploded to 38 locations in 24 countries. It’s not surprising that Abby’s wonderful little book, How to Make Sense of Any Mess: Information Architecture for Everybody, has been such a hit. It’s not surprising to see the IA Summit entering its 16th year stronger than ever. It’s not surprising that the fourth edition of Information Architecture for the World Wide Web (due out later this year) is being recast as a book not for information architects, but for people who need to know something about information architecture. We’ve entered full-on mode of democratizing IA skills. Because...
Information architecture literacy is required for anyone who designs anything.
I’ll confess to having felt, like Cennydd, a bit disconnected from IA for the past few years. Partly because I’ve been investing almost every available moment of my waking hours into Rosenfeld Media. And partly because much of the IA community’s discussion has pushed far deeper into IA practice than my brain and attention span can manage. But I’m feeling better now, because I’m finding, in my own day-to-day work, that:
Information architecture literacy is required for anyone who designs anything.
For example, while I rarely work on web site IA much these days, I am absolutely absorbed in the information architecture of books.
Want to know what value publishers can provide to authors in this age of self-publishing? The list might be longer than you imagined, but I think most Rosenfeld Media authors would agree: Lou and team pull them out of the weeds, and help them to step back and make sense of their content as an information system. Information architecture skills are an absolute necessity when it comes to framing, structuring, and establishing a flow for a book. (And not just for non-fiction; just ask JK Rowling.)
I’m finding that IA literacy is also incredibly helpful in other areas, like event planning. I recently asked a couple dozen colleagues who produce events to provide share their advice on organizing a conference. Their responses were generous, useful, and wonderful. But the one I keep remembering most is Jeffrey Zeldman’s:
Yes, I’m biased, but I hear Jeffrey singing a song of event IA.
I’ve been singing it too. In putting together the first edition of the Enterprise UX conference (plug alert: San Antonio; May 13–15, 2015), I’ve been working with Dave Malouf and Uday Gajendar to create an information architecture for a conversation. In effect, we’re trying to structure the event’s program in a way that surfaces a latent conversation about enterprise UX that’s been happening in the UX community for quite some time. The event itself should simply serve as an opportunity to bring people together to sharpen and advance that conversation.
I’m oversimplifying a bit, but we spent months designing our event IA around four carefully-sequenced themes, each in effect a curated mini-conference: 1) Insight at Scale; 2) Craft amid Complexity; 3) Enterprise Experimentation; and 4) Designing Organizational Culture. We see these as the main facets of the community’s conversation on enterprise UX.
We’ll know we’ve been successful if, at the event, the conversation spills out of the auditorium and into the hallways and break areas, animating the words and faces of attendees. We’ll know we been really successful if these conversations riff off the themes already covered — meaning we got the sequence right. And we’ll know that we were really, really successful if these four themes keep the conversation moving forward — both after the event and as the IA for programs at future editions of the event.
Books have an information architecture. Events have an information architecture. Pretty much anything we design — consciously or not — has an information architecture. So pardon me as I repeat:
Information architecture literacy is required for anyone who designs anything.
When I got my masters in information and library studies in 1990, our professors were preaching about the oncoming information revolution. Since then, I‘ve been fortunate to observe and even participate a little in that revolution. In the blink of an eye, information architects emerged as professionals dedicated to making the pain of that revolution easier to bear. In the blink of an eye, others have proclaimed that information architecture, as a profession, was dead.
I’m not sure who’s right, nor do I care. Twenty-five years is nothing. The dust can settle after we’re all dead.
Let’s worry instead about people suffering from everyday IA problems. We, as designers of any stripe, have to help them. And we have to get better at helping them to help themselves.
Oh, and if you’re wondering why I won’t be at any of tomorrow’s 38 WIAD meetings: well, it’s Saturday, and I have a date with my six-year old. We’re going to organize his Legos.
(This piece originally ran in the Rosenfeld Review; sign up here for new ones.)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of Rosenfeld Media. I make things out of information.
",days ago cennydd bowles gently trolled many us thusly cennydd keynoted past information architecture summit hard ignore question cennydds timing quite interesting given tomorrow world ia day theme years wiad architecting happiness adorable little video ia institute created promote wiad 2015 abby covert says theme chosen rising amount information everyone deal italics cennydd theres answer youre human todays developed world even physical objects spaces soaked information struggling cope make sense stuff nearly time nearly everywhere information architecture problems everyday human problems youre designing humans today youll need least information architecture skills order help information architecture literacy required anyone designs anything surprising wiad exploded 38 locations 24 countries surprising abbys wonderful little book make sense mess information architecture everybody hit surprising see ia summit entering 16th year stronger ever surprising fourth edition information architecture world wide web due later year recast book information architects people need know something information architecture weve entered fullon mode democratizing ia skills information architecture literacy required anyone designs anything ill confess felt like cennydd bit disconnected ia past years partly ive investing almost every available moment waking hours rosenfeld media partly much ia communitys discussion pushed far deeper ia practice brain attention span manage im feeling better im finding daytoday work information architecture literacy required anyone designs anything example rarely work web site ia much days absolutely absorbed information architecture books want know value publishers provide authors age selfpublishing list might longer imagined think rosenfeld media authors would agree lou team pull weeds help step back make sense content information system information architecture skills absolute necessity comes framing structuring establishing flow book nonfiction ask jk rowling im finding ia literacy also incredibly helpful areas like event planning recently asked couple dozen colleagues produce events provide share advice organizing conference responses generous useful wonderful one keep remembering jeffrey zeldmans yes im biased hear jeffrey singing song event ia ive singing putting together first edition enterprise ux conference plug alert san antonio may 1315 2015 ive working dave malouf uday gajendar create information architecture conversation effect trying structure events program way surfaces latent conversation enterprise ux thats happening ux community quite time event simply serve opportunity bring people together sharpen advance conversation im oversimplifying bit spent months designing event ia around four carefullysequenced themes effect curated miniconference 1 insight scale 2 craft amid complexity 3 enterprise experimentation 4 designing organizational culture see main facets communitys conversation enterprise ux well know weve successful event conversation spills auditorium hallways break areas animating words faces attendees well know really successful conversations riff themes already covered meaning got sequence right well know really really successful four themes keep conversation moving forward event ia programs future editions event books information architecture events information architecture pretty much anything design consciously information architecture pardon repeat information architecture literacy required anyone designs anything got masters information library studies 1990 professors preaching oncoming information revolution since ive fortunate observe even participate little revolution blink eye information architects emerged professionals dedicated making pain revolution easier bear blink eye others proclaimed information architecture profession dead im sure whos right care twentyfive years nothing dust settle dead lets worry instead people suffering everyday ia problems designers stripe help get better helping help oh youre wondering wont tomorrows 38 wiad meetings well saturday date sixyear old going organize legos piece originally ran rosenfeld review sign new ones quick cheer standing ovation clap show much enjoyed story founder rosenfeld media make things information,en,"['Cennydd Bowles', 'the IA Institute', 'WIAD', 'Information Architecture for the World Wide Web', 'IA', 'Rosenfeld Media', 'JK Rowling', 'Scale', 'I‘ve', 'the Rosenfeld Review']"
158,Matt Harvey,677,"Continuous online video classification with TensorFlow, Inception and a Raspberry Pi","Much has been written about using deep learning to classify prerecorded video clips. These papers and projects impressive tag, classify and even caption each clip, with each comprising a single action or subject.
Today, we’re going to explore a way to continuously classify video as it’s captured, in an online system. Continuous classification allows us to solve all sorts of interesting problems in real-time, like understanding what’s in front of a car for autonomous driving applications to understanding what’s streaming on a TV. We’ll attempt to do the latter using only open source software and uber-cheap hardware. Specifically, TensorFlow on a Raspberry Pi with a PiCamera.
We’ll use a “naive” classification approach in this post (see next section), which will give us a relatively straightforward path to solving our problem and will form the basis for more advanced systems to explore later.
By the time we’re done today, we should be able to classify what we see on our TV as either a football game or an advertisement, running on our Raspberry Pi.
Let’s get to it!
Video is an interesting classification problem because it includes both temporal and spatial features. That is, at each frame within a video, the frame itself holds important information (spatial), as does the context of that frame relative to the frames before it in time (temporal).
We hypothesize that for many applications, using only spatial features is sufficient for achieving high accuracy. This approach has the benefit of being relatively simple, or at least minimal. It’s naive because it ignores the information encoded between multiple frames of the video.
Since football games have rather distinct spatial features, we believe this method should work wonderfully for the task at hand.
We’re going to collect data for offline training with a Raspberry Pi and a PiCamera. We’ll point the camera at a TV and record 10 frames per second, or more specifically, save 10 jpegs every second, which will comprise our “video”.
Here’s the code for capturing our images:
Once we have our data, we’ll use a convolutional neural network (CNN) to classify each frame with one of our labels: ad or football.
CNNs are the state-of-the-art for image classification. And in 2016, it’s essentially a solved problem. It feels crazy to say that, but it really is: Thanks in large part to Google→TensorFlow→Inception and the many researchers who came before it, there’s very little low-level coding required for us when it comes to training a CNN for our continuous video classification problem.
Pete Warden at Google wrote an awesome blog post called TensorFlow for Poets that shows how to retrain the last layer of Inception with new images and classes. This is called transfer learning, and it lets us take advantage of weeks of previous training without having to train a complex CNN from scratch. Put another way, it lets us train an image classifier with a relatively small training set.
We collected 20 minutes of footage at 10 jpegs per second, which amounted to 4,146 ad frames and 7,899 football frames. The next step is to sort each frame into two folders: football and ad. The name of the folders represent the labels of each frame, which will be the classes our network will learn to predict on when we retrain the top layer of the Inception v3 CNN.
This is essentially using the flowers method described in TensorFlow for Poets, applied to video frames.
To retrain the final layer of the CNN on our new data, we checkout the r0.11 tag from the TensorFlow repo and run the following command:
Retraining the final layer of the network on this data takes about 30 minutes on my laptop with a GeForce GTX 960m GPU. At the completion of 4,000 training steps, our model reports an incredible 98.8% accuracy on the held out validation set! I’m not sure I could do much better using my eyes on the same data. As a point of reference, if the network had classified each frame as football, it would have achieved about 66% accuracy. So it seems to be working!
It’s always a good idea to run some known data through a trained network to sanity check the results, so we’ll do that here.
Here’s the code we use to classify a single image manually through our retrained model:
And here are the results of spot checking individual frames:
Before we transfer everything to our Pi and do this in real-time, let’s use a different batch of recorded data and see how well we do on that set.
To get this dataset, and to make sure we don’t have any data leakage into our training set, we separately record another 19 minutes of the football broadcast. This dataset amounted to 2,639 ad frames and 8,524 football frames.
We run each frame of this set through our classifier and achieve a true holdout accuracy score of 93.3%. Awesome!
Looks like we’ve validated our hypothesis that we can achieve high levels of accuracy while only considering spatial features. Impressive results, considering that we only used 20 minutes of training data! Thank you, Google, Pete, TensorFlow and all the folks who have developed CNNs over the years for your incredible work and contributions.
Great, so now we have our CNN trained and we know that we can classify each frame of our video with relatively high accuracy. How does it do on live TV, with always changing context?
For this, we load up our Raspberry Pi 3 with our newly trained model weights, turn on the PiCamera at 10 fps, and instead of saving the image, send it through our CNN to be classified.
We have to make some modifications to the code to classify in real time. The final result looks like this:
We also have to get TensorFlow running on the Pi. Sam Abrahams wrote up excellent instructions for doing this, so I won’t cover them again here.
After we install our dependencies, we run the program and... crap! Inception on the Raspberry Pi 3 can only classify one image every four seconds.
Okay, so we don’t quite have the hardware yet to do 10fps, but this still feels like magic, so let’s see how we do.
Flipping on Sunday Night Football and pointing our camera at the TV shows a remarkable job at classifying each moment as football or ad, once every few seconds. For the vast majority of the broadcast, we see our prediction come out true to life. So cool.
In all, our naive method worked remarkably well at continuous online video classification for this particular use case. But we know that we’re only considering part of the information provided to us inherently in video, and so there must be room for improvement, especially as our datasets become more complex.
For that, we’ll have to dive deeper. So in the next post, we’ll explore feeding the output of our CNN (both the final softmax layer and the pool layer, which gives us a 2,048-d feature vector of each image) to an LSTM RNN to see if we can increase our accuracy.
Spoiler alert: We can!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of Coastline Automation, using AI to make every car crash-proof.
Practical applications of deep learning and research reports from the road.
",much written using deep learning classify prerecorded video clips papers projects impressive tag classify even caption clip comprising single action subject today going explore way continuously classify video captured online system continuous classification allows us solve sorts interesting problems realtime like understanding whats front car autonomous driving applications understanding whats streaming tv well attempt latter using open source software ubercheap hardware specifically tensorflow raspberry pi picamera well use naive classification approach post see next section give us relatively straightforward path solving problem form basis advanced systems explore later time done today able classify see tv either football game advertisement running raspberry pi lets get video interesting classification problem includes temporal spatial features frame within video frame holds important information spatial context frame relative frames time temporal hypothesize many applications using spatial features sufficient achieving high accuracy approach benefit relatively simple least minimal naive ignores information encoded multiple frames video since football games rather distinct spatial features believe method work wonderfully task hand going collect data offline training raspberry pi picamera well point camera tv record 10 frames per second specifically save 10 jpegs every second comprise video heres code capturing images data well use convolutional neural network cnn classify frame one labels ad football cnns stateoftheart image classification 2016 essentially solved problem feels crazy say really thanks large part googletensorflowinception many researchers came theres little lowlevel coding required us comes training cnn continuous video classification problem pete warden google wrote awesome blog post called tensorflow poets shows retrain last layer inception new images classes called transfer learning lets us take advantage weeks previous training without train complex cnn scratch put another way lets us train image classifier relatively small training set collected 20 minutes footage 10 jpegs per second amounted 4146 ad frames 7899 football frames next step sort frame two folders football ad name folders represent labels frame classes network learn predict retrain top layer inception v3 cnn essentially using flowers method described tensorflow poets applied video frames retrain final layer cnn new data checkout r011 tag tensorflow repo run following command retraining final layer network data takes 30 minutes laptop geforce gtx 960m gpu completion 4000 training steps model reports incredible 988 accuracy held validation set im sure could much better using eyes data point reference network classified frame football would achieved 66 accuracy seems working always good idea run known data trained network sanity check results well heres code use classify single image manually retrained model results spot checking individual frames transfer everything pi realtime lets use different batch recorded data see well set get dataset make sure dont data leakage training set separately record another 19 minutes football broadcast dataset amounted 2639 ad frames 8524 football frames run frame set classifier achieve true holdout accuracy score 933 awesome looks like weve validated hypothesis achieve high levels accuracy considering spatial features impressive results considering used 20 minutes training data thank google pete tensorflow folks developed cnns years incredible work contributions great cnn trained know classify frame video relatively high accuracy live tv always changing context load raspberry pi 3 newly trained model weights turn picamera 10 fps instead saving image send cnn classified make modifications code classify real time final result looks like also get tensorflow running pi sam abrahams wrote excellent instructions wont cover install dependencies run program crap inception raspberry pi 3 classify one image every four seconds okay dont quite hardware yet 10fps still feels like magic lets see flipping sunday night football pointing camera tv shows remarkable job classifying moment football ad every seconds vast majority broadcast see prediction come true life cool naive method worked remarkably well continuous online video classification particular use case know considering part information provided us inherently video must room improvement especially datasets become complex well dive deeper next post well explore feeding output cnn final softmax layer pool layer gives us 2048d feature vector image lstm rnn see increase accuracy spoiler alert quick cheer standing ovation clap show much enjoyed story founder coastline automation using ai make every car crashproof practical applications deep learning research reports road,en,"['CNN', 'Google', 'TensorFlow for Poets', 'GeForce GTX', 'TensorFlow', 'Coastline Automation']"
159,Vivek Yadav,425,An augmentation based deep neural network approach to learn human driving behavior,"Overview
In this post, we will go over the work I did for project 3 of Udacity’s self-driving car project, behavior cloning for driving. The main task is to drive a car around in a simulator on a race track, and then use deep learning to mimic the behavior of human. This is a very interesting problem because it is not possible to drive under all possible scenarios on the track, so the deep learning algorithm will have to learn general rules for driving. We must be very careful while using deep learning models, because they have a tendency to overfit the data. Overfitting refers to the condition where the model is very sensitive to the training data itself, and the model’s behavior does not generalize to new/unseen data. One way to avoid overfitting is to collect a lot of data. A typical convolutional neural network can have up to a million parameters, and tuning these parameters requires millions of training instances of uncorrelated data, which may not always be possible and in some cases cost prohibitive. For our car example, this will require us to drive the car under different weather, lighting, traffic and road conditions. One way to avoid overfitting is to use augmentation.
Augmentation refers to the process of generating new training data from a smaller data set such that the new data set represents the real world data one may see in practice. As we are generating thousands of new training instances from each image, it is not possible to generate and store all these data on the disk. We will therefore utilize keras generators to read data from the file, augment on the fly and use it to train the model. We will utilize images from the left and right cameras so we can generate additional training data to simulate recovery. Keras generator is set up such that in the initial phases of learning, the model drops data with lower steering angles with higher probability. This removes any potential for bias towards driving at zero angle. After setting up the image augmentation pipeline, we can proceed to train the model. The training was performed using simple adam learning algorithm with learning rate of 0.0001. After this training, the model was able to drive the car by itself on the first track for hours and generalized to the second track.
All the training was based on driving data of about 4 laps using ps4 controller on track 1 in one direction alone. The model never saw track 2 in training, but with image augmentation (flipping, darkening, shifting, etc) and using data from all the cameras (left, right and center) the model was able to learn general rules of driving that helped translate this learning to a different track.
IMPORTANT: These results were obtained on Titan X GPU machine I built earlier. Full specifications of the computer can be found here. Please note that computers with different performance will provide a different performance of the network.
Augmentation helps us extract as much information from data as possible. We will generate additional data using the following data augmentation techniques. Augmentation is a technique of manipulating the incoming training data to generate more instances of training data. This technique has been used to develop powerful classifiers with little data. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html . However, augmentation is very specific to the objective of the neural network.
Brightness augmentation
Changing brightness to simulate day and night conditions. We will generate images with different brightness by first converting images to HSV, scaling up or down the V channel and converting back to the RGB channel.
Using left and right camera images
Using left and right camera images to simulate the effect of car wandering off to the side, and recovering. We will add a small angle .25 to the left camera and subtract a small angle of 0.25 from the right camera. The main idea being the left camera has to move right to get to center, and right camera has to move left.
Horizontal and vertical shifts
We will shift the camera images horizontally to simulate the effect of car being at different positions on the road, and add an offset corresponding to the shift to the steering angle. We added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left. We will also shift the images vertically by a random number to simulate the effect of driving up or down the slope.
Shadow augmentation
The next augmentation we will add is shadow augmentation where random shadows are cast across the image. This is implemented by choosing random points and shading all points on one side (chosen randomly) of the image. The code for this augmentation is presented below.
Flipping
In addition to the transformations above, we will also flip images at random and change the sign of the predicted angle to simulate driving in the opposite direction.
2. Preprocessing
After augmenting the image as above, we will crop the top 1/5 of the image to remove the horizon and the bottom 25 pixels to remove the car’s hood. Originally 1/3 of the top of car image was removed, but later it was changed to 1/5 to include images for cases when the car may be driving up or down a slope. We will next rescale the image to a 64X64 square image. After augmentation, the augmented images looks as follows. These images are generated using kera’s generator, and unlimited number of images can be generated from one image. I used Lambda layer in keras to normalize intensities between -.5 and .5.
3. Keras generator for subsampling
As there was limited data and we are generating thousands of training examples from the same image, it is not possible to store all the images apriori into memory. We will utilize kera’s generator function to sample images such that images with lower angles have lower probability of getting represented in the data set. This alleviates any problems we may ecounter due to model having a bias towards driving straight. Panel below shows multiple training samples generated from one image. The keras generator is presented below. The ‘pr_threshold’ variable is a threshold that determines if a data with small angle will be dropped or not.
4. Model Architecture and training
I implemented the model architecture above for training the data. The first layer is 3 1X1 filters, this has the effect of transforming the color space of the images. Research has shown that different color spaces are better suited for different applications. As we do not know the best color space apriori, using 3 1X1 filters allows the model to choose its best color space. This is followed by 3 convolutional blocks each comprised of 32, 64 and 128 filters of size 3X3. These convolution layers were followed by 3 fully connected layers. All the convolution blocks and the 2 following fully connected layers had exponential relu (ELU) as activation function. I chose leaky relu to make transition between angles smoother.
Training:
I trained the model using the keras generator with batch size of 256 for 8 epochs. In each epoch, I generated 20000 images. I started with pr_threshold, the chance of dropping data with small angles as 1, and reduced the probability by dividing it by the iteration number after each epoch. The entire training took about 5 minutes. However, it too more than 20 hours to arrive at the right architecture and training parameters. Snippet below presents the result of training.
5. Model performance:
Video below shows the performance of algorithm on the track 1 on which the original data was collected. The car is able to drive around for hours, we will next look into the case where either the camera resolution, video size or tracks are changed.
Generalization from one image size to another
Video below presents generalization from one image size to another. I used the same pretrained model and tested it on all the other image sizes and found that the deep learning neural network was able to drive the car around for all image sizes.
Generalization from one image resolution to another
Video below presents generalization from one image resolution to another. I used the same pretrained model and tested it on all the other image resolutions and found that the deep learning neural network was able to drive the car around for all image resolutions.
I also tested different combinations of image size and image resolutions, and on track 1 the deep learning algorithm was able to drive the car around for all combinations of image resolution and sizes.
Generalization from one track to another
Figure below presents generalization from one track to another. This was perhaps the toughest test for the deep learning algorithm. In the second track, there were more right turns and u-turns, it was darker, and the road had slopes. All of which were absent in the original track. However, all these effects were artificially included into the model via image augmentation.
6. Future Directions
This project is far from over. This project opened more questions than it answered. A few more things to try are,
6. Reflections
This was perhaps the weirdest project I did. This project challenged all the previous knowledge I had about deep learning. In general large epoch size and training with more data results in better performance, but in this case any time I got beyond 10 epochs, the car simply drove off the track. Although all the image augmentation and tweaks seem reasonable n0w, I did not think of them apriori. I hope others find this post useful, and get inspried to try novel things. I havent used on-the fly training agile trainer by John Chen yet. I wanted to try and stretch the data as much as possible. Next thing to try is to experiment with parallel network using John’s trainer.
Acknowledgements:
I am very thankful to Udacity for selecting me for the first cohort, this allowed me to connect with many like-minded individuals. As always, learned a lot from discussions with Henrik Tünnermann and John Chen. I am also thankful for getting the NVIDA’s GPU grant. Although, its for work, but I use it for Udacity too.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Staff Software Engineer at Lockheed Martin-Autonomous System with research interest in control, machine learning/AI. Lifelong learner with glassblowing problem.
Best place to learn about Chatbots. We share the latest Bot News, Info, AI & NLP, Tools, Tutorials & More.
",overview post go work project 3 udacitys selfdriving car project behavior cloning driving main task drive car around simulator race track use deep learning mimic behavior human interesting problem possible drive possible scenarios track deep learning algorithm learn general rules driving must careful using deep learning models tendency overfit data overfitting refers condition model sensitive training data models behavior generalize newunseen data one way avoid overfitting collect lot data typical convolutional neural network million parameters tuning parameters requires millions training instances uncorrelated data may always possible cases cost prohibitive car example require us drive car different weather lighting traffic road conditions one way avoid overfitting use augmentation augmentation refers process generating new training data smaller data set new data set represents real world data one may see practice generating thousands new training instances image possible generate store data disk therefore utilize keras generators read data file augment fly use train model utilize images left right cameras generate additional training data simulate recovery keras generator set initial phases learning model drops data lower steering angles higher probability removes potential bias towards driving zero angle setting image augmentation pipeline proceed train model training performed using simple adam learning algorithm learning rate 00001 training model able drive car first track hours generalized second track training based driving data 4 laps using ps4 controller track 1 one direction alone model never saw track 2 training image augmentation flipping darkening shifting etc using data cameras left right center model able learn general rules driving helped translate learning different track important results obtained titan x gpu machine built earlier full specifications computer found please note computers different performance provide different performance network augmentation helps us extract much information data possible generate additional data using following data augmentation techniques augmentation technique manipulating incoming training data generate instances training data technique used develop powerful classifiers little data httpsblogkerasiobuildingpowerfulimageclassificationmodelsusingverylittledatahtml however augmentation specific objective neural network brightness augmentation changing brightness simulate day night conditions generate images different brightness first converting images hsv scaling v channel converting back rgb channel using left right camera images using left right camera images simulate effect car wandering side recovering add small angle 25 left camera subtract small angle 025 right camera main idea left camera move right get center right camera move left horizontal vertical shifts shift camera images horizontally simulate effect car different positions road add offset corresponding shift steering angle added 0004 steering angle units per pixel shift right subtracted 0004 steering angle units per pixel shift left also shift images vertically random number simulate effect driving slope shadow augmentation next augmentation add shadow augmentation random shadows cast across image implemented choosing random points shading points one side chosen randomly image code augmentation presented flipping addition transformations also flip images random change sign predicted angle simulate driving opposite direction 2 preprocessing augmenting image crop top 15 image remove horizon bottom 25 pixels remove cars hood originally 13 top car image removed later changed 15 include images cases car may driving slope next rescale image 64x64 square image augmentation augmented images looks follows images generated using keras generator unlimited number images generated one image used lambda layer keras normalize intensities 5 5 3 keras generator subsampling limited data generating thousands training examples image possible store images apriori memory utilize keras generator function sample images images lower angles lower probability getting represented data set alleviates problems may ecounter due model bias towards driving straight panel shows multiple training samples generated one image keras generator presented pr_threshold variable threshold determines data small angle dropped 4 model architecture training implemented model architecture training data first layer 3 1x1 filters effect transforming color space images research shown different color spaces better suited different applications know best color space apriori using 3 1x1 filters allows model choose best color space followed 3 convolutional blocks comprised 32 64 128 filters size 3x3 convolution layers followed 3 fully connected layers convolution blocks 2 following fully connected layers exponential relu elu activation function chose leaky relu make transition angles smoother training trained model using keras generator batch size 256 8 epochs epoch generated 20000 images started pr_threshold chance dropping data small angles 1 reduced probability dividing iteration number epoch entire training took 5 minutes however 20 hours arrive right architecture training parameters snippet presents result training 5 model performance video shows performance algorithm track 1 original data collected car able drive around hours next look case either camera resolution video size tracks changed generalization one image size another video presents generalization one image size another used pretrained model tested image sizes found deep learning neural network able drive car around image sizes generalization one image resolution another video presents generalization one image resolution another used pretrained model tested image resolutions found deep learning neural network able drive car around image resolutions also tested different combinations image size image resolutions track 1 deep learning algorithm able drive car around combinations image resolution sizes generalization one track another figure presents generalization one track another perhaps toughest test deep learning algorithm second track right turns uturns darker road slopes absent original track however effects artificially included model via image augmentation 6 future directions project far project opened questions answered things try 6 reflections perhaps weirdest project project challenged previous knowledge deep learning general large epoch size training data results better performance case time got beyond 10 epochs car simply drove track although image augmentation tweaks seem reasonable n0w think apriori hope others find post useful get inspried try novel things havent used onthe fly training agile trainer john chen yet wanted try stretch data much possible next thing try experiment parallel network using johns trainer acknowledgements thankful udacity selecting first cohort allowed connect many likeminded individuals always learned lot discussions henrik tunnermann john chen also thankful getting nvidas gpu grant although work use udacity quick cheer standing ovation clap show much enjoyed story staff software engineer lockheed martinautonomous system research interest control machine learningai lifelong learner glassblowing problem best place learn chatbots share latest bot news info ai nlp tools tutorials,en,"['Udacity', 'algorithm', 'HSV', 'RGB', 'Lambda', 'Keras', 'GPU', 'Lockheed', 'Chatbots', 'AI & NLP', 'Tools, Tutorials & More']"
160,Carlos Beltran,97,A Rock Album For AI – Carlos Beltran – Medium,"https://open.spotify.com/album/0jwnYwJz6XHNrVAYEclQPd
It’s awesome that Avenged Sevenfold became interested in AI and wrote an entire album that revolves around the idea. In an interview with Rolling Stone, lead singer M. Shadows says the initial interest came after reading Tim Urban’s article over at waitbutwhy. It’s one of the things (along with movies like Her and The Matrix of course) that spiked my interest in AI as well, so I’d highly recommend reading it. Tim does a phenomenal job of explaining the topic, current challenges engineers are facing, and the very possible implications of this technology.
The term “artificial intelligence” was first coined half a century ago. Fast forward to today, where we have have giant companies like Intel and Apple acquiring AI startups like there’s no tomorrow. It’s not a matter of whether or not we’ll be able to create machines that surpass our own capabilities, but when.
Theoretical physicist and futurist Dr. Michio Kaku thinks it is possible for machines as smart as us to exist by the end of the century. Google’s chief futurist, Ray Kurzweil, believes such technology will exist as soon as 2029. The band is right in wanting its fans, and the general public, to be more aware of these ideas — they could be right around the corner.
I’m no expert, but I’d like to discuss the ideas behind some of the songs and include references in case you’d like to delve deeper. And if you want to read more on the possible future of AI, I’d recommend reading Kurzweil’s book The Singularity Is Near. Although some of his predictions have been met with skepticism, the ideas presented are thought-provoking.
Simply put, nanomachines are microscopic machines that will enhance us in almost every way imaginable. They’ll be able to help our immune system fight off diseases. They would create super soldiers. This technology is actually at the center of a great game series, Metal Gear Solid. This “hack” in our biological makeup will also increase our lifespans. Kurzweil imagines a future where biotechnology is so advanced that we will live forever. This is the same idea behind the song “Paradigm”. Lyrics include:
The song also raises the question of what it really means to be human. What do we become when we merge with machines? Will we lose what fundamentally makes us human? It can be argued that this “merge” is the next logical step in evolution, as there is no there is no evolutionary pressure for us to do so anymore. We’ll become, as Kurzweil puts it, “Godlike”.
Expanding the brain’s neocortex will allow us, for example, to pose questions in our thoughts and know the answer almost immediately (most likely thanks to our direct “brain-to-Google” connection). We’ll always have witty jokes on hand, and learning Calculus will be as simple as purchasing downloadable content. Plug and play.
Besides swapping out failing body parts with prosthetics and enhancing our brains, there’s another way we’ll be able to gain immortality. Both Dr. Kaku and Kurzweil firmly believe that the advances in brain-computer interfaces will eventually allow us to upload our consciousness to machines.
Scientists still have no clue how the brain works, how the billions of neurons form connections that result in learned behavior, or what dreaming is. But once these secrets are known (which might never actually happen) and we know how our brain functions, as well as what the “consciousness switch” is, the possibilities are endless. To get an idea of what’s possible, check out Black Mirror’s episode Playtest. The brain-computer interface for the game is so advanced that the player can’t distinguish between what’s real and what isn’t. I don’t want to spoil anything, but get ready for a mind fuck. Black Mirror does a great job of weaving technology with a dystopia that we might inhabit, showing a darker side of our society. It’s on Netflix, so check it out.
Elon Musk sure does. He claims that the chances of us living in “base reality” is one in billions. I’d recommend watching the 3-minute video. His logic is as follows: we had Pong some 40 years ago. Two rectangles and a dot were rendered on-screen for what we called a videogame. Today, we have games with realistic graphics and they keep getting better every year. Better yet, virtual and augmented reality are right around the corner, pushing the boundaries of gaming. Eventually, we’ll have the technology to create simulated worlds that are indistinguishable from reality. Therefore, Musk claims, it is likely that we are living in an ancestor simulation created by an advanced future civilization some 10,000 years from now.
The album’s 7th song, “Simulation” explores the idea that our reality might not be what it seems. Think of it this way — the brain and nervous system which we use to automatically react to the environment around us is the same brain and nervous system which tells us what the environment is.
Throughout the song, the “patient” is having thoughts that challenge the simulation they are living in. They are — in a sense — waking up. A darker voice, which I believe is meant to represent the ones running the show, has to reprimand the patient, reminding them that they “...only exist because we allow it”. To control the situation, the patient is to be sedated with blue comfort, a reference from The Matrix, which will make them forget they’re living in a simulation. Blissful ignorance.
I won’t try to explain this one. Just watch the video. And here’s a quote from that man that might get your attention:
Imagine an entity so intelligent...
...but that’s just it. You can’t imagine it. In the second part to his article on AI, Tim Urban compares this to a chimp being unable to understand a skyscraper is not just a part of its environment, but that humans built it. It’s not the chimp’s fault or anything, its brain is just not made to have that level of information processing.
The same thing will happen when we build a machine with the collective knowledge of some 200,000 years of Homo Sapien existence. Therefore, there is no way to know what it will do or what the consequences will be. Tim depicts our situation with this entity, what he refers to as Artificial Superintelligence (ASI), beautifully:
Mark Zuckerberg is right in saying we should be hopeful of the amount of good AI could do, but some of the smartest minds in existence are genuinely concerned. Stephen Hawking acknowledges that the successful creation of an AI will be the biggest event in history, but warns it could also end mankind. Elon Musk founded a research company OpenAI as a way to “neutralize the threat of a malicious artificial super-intelligence”.
“Creating God” describes AI as a modern messiah, “the very last invention man would ever need”. It paints the picture of a utopia where this intelligence exists. At the same time, the song suggests that we could be “summoning the demon”, unable to control the outcomes. We could just be its stepping stone, as our existence after its creation becomes irrelevant.
The album wraps up with a 15-minute eargasm. I can’t produce words that will do “Exist” any justice. As the band described it, it’s like listening to what the Big Bang might’ve sounded like. Neil deGrasse Tyson makes a cameo at the end of the song that serves as a reminder that our problems and conflicts are minuscule in the grand scheme of things. We’re all a part of the same universe and once we as a society realize this, we can truly make progress. Here’s the full thing:
The Stage is an exceptional album, in my opinion. The band’s intentions were for fans to educate themselves, or be a bit more aware of what’s going on in this area. We can enjoy it as a rock album as well as explore the ideas behind the lyrics. I had an awesome time writing this, digging up things I’ve read and seen and unifying them in a way so others can hopefully become more interested as well. And come on, don’t tell me that the idea that we’re living in a simulation isn’t thought-provoking.
Tap the ❤ button below :)
My name’s Carlos and I generally write about personal development, tech, and entrepreneurship. Hit me up on Twitter!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Software engineer. Focused on building cool shit on Ethereum 🚀
",httpsopenspotifycomalbum0jwnywjz6xhnrvayeclqpd awesome avenged sevenfold became interested ai wrote entire album revolves around idea interview rolling stone lead singer shadows says initial interest came reading tim urbans article waitbutwhy one things along movies like matrix course spiked interest ai well id highly recommend reading tim phenomenal job explaining topic current challenges engineers facing possible implications technology term artificial intelligence first coined half century ago fast forward today giant companies like intel apple acquiring ai startups like theres tomorrow matter whether well able create machines surpass capabilities theoretical physicist futurist dr michio kaku thinks possible machines smart us exist end century googles chief futurist ray kurzweil believes technology exist soon 2029 band right wanting fans general public aware ideas could right around corner im expert id like discuss ideas behind songs include references case youd like delve deeper want read possible future ai id recommend reading kurzweils book singularity near although predictions met skepticism ideas presented thoughtprovoking simply put nanomachines microscopic machines enhance us almost every way imaginable theyll able help immune system fight diseases would create super soldiers technology actually center great game series metal gear solid hack biological makeup also increase lifespans kurzweil imagines future biotechnology advanced live forever idea behind song paradigm lyrics include song also raises question really means human become merge machines lose fundamentally makes us human argued merge next logical step evolution evolutionary pressure us anymore well become kurzweil puts godlike expanding brains neocortex allow us example pose questions thoughts know answer almost immediately likely thanks direct braintogoogle connection well always witty jokes hand learning calculus simple purchasing downloadable content plug play besides swapping failing body parts prosthetics enhancing brains theres another way well able gain immortality dr kaku kurzweil firmly believe advances braincomputer interfaces eventually allow us upload consciousness machines scientists still clue brain works billions neurons form connections result learned behavior dreaming secrets known might never actually happen know brain functions well consciousness switch possibilities endless get idea whats possible check black mirrors episode playtest braincomputer interface game advanced player cant distinguish whats real isnt dont want spoil anything get ready mind fuck black mirror great job weaving technology dystopia might inhabit showing darker side society netflix check elon musk sure claims chances us living base reality one billions id recommend watching 3minute video logic follows pong 40 years ago two rectangles dot rendered onscreen called videogame today games realistic graphics keep getting better every year better yet virtual augmented reality right around corner pushing boundaries gaming eventually well technology create simulated worlds indistinguishable reality therefore musk claims likely living ancestor simulation created advanced future civilization 10000 years albums 7th song simulation explores idea reality might seems think way brain nervous system use automatically react environment around us brain nervous system tells us environment throughout song patient thoughts challenge simulation living sense waking darker voice believe meant represent ones running show reprimand patient reminding exist allow control situation patient sedated blue comfort reference matrix make forget theyre living simulation blissful ignorance wont try explain one watch video heres quote man might get attention imagine entity intelligent thats cant imagine second part article ai tim urban compares chimp unable understand skyscraper part environment humans built chimps fault anything brain made level information processing thing happen build machine collective knowledge 200000 years homo sapien existence therefore way know consequences tim depicts situation entity refers artificial superintelligence asi beautifully mark zuckerberg right saying hopeful amount good ai could smartest minds existence genuinely concerned stephen hawking acknowledges successful creation ai biggest event history warns could also end mankind elon musk founded research company openai way neutralize threat malicious artificial superintelligence creating god describes ai modern messiah last invention man would ever need paints picture utopia intelligence exists time song suggests could summoning demon unable control outcomes could stepping stone existence creation becomes irrelevant album wraps 15minute eargasm cant produce words exist justice band described like listening big bang mightve sounded like neil degrasse tyson makes cameo end song serves reminder problems conflicts minuscule grand scheme things part universe society realize truly make progress heres full thing stage exceptional album opinion bands intentions fans educate bit aware whats going area enjoy rock album well explore ideas behind lyrics awesome time writing digging things ive read seen unifying way others hopefully become interested well come dont tell idea living simulation isnt thoughtprovoking tap button names carlos generally write personal development tech entrepreneurship hit twitter quick cheer standing ovation clap show much enjoyed story software engineer focused building cool shit ethereum,en,"['Avenged Sevenfold', 'Rolling Stone', 'Intel', 'Apple', 'Google', 'Kurzweil', 'Metal Gear Solid', 'Calculus', 'Black Mirror', 'Musk', 'Homo Sapien', 'Artificial Superintelligence']"
161,Matt Harvey,558,"Continuous video classification with TensorFlow, Inception and Recurrent Nets","A video is a sequence of images. In our previous post, we explored a method for continuous online video classification that treated each frame as discrete, as if its context relative to previous frames was unimportant. Today, we’re going to stop treating our video as individual photos and start treating it like the video that it is by looking at our images in a sequence. We’ll process these sequences by harnessing the magic of recurrent neural networks (RNNs).
To restate the problem we outlined in our previous post: We’re attempting to continually classify video as it’s streamed, in an online system. Specifically, we’re classifying whether what’s streaming on a TV is a football game or an advertisement.
Convolutional neural networks, which we used exclusively in our previous post, do an amazing job at taking in a fixed-size vector, like an image of an animal, and generating a fixed-size label, like the class of animal in the image. What CNNs cannot do (without computationally intensive 3D convolution layers) is accept a sequence of vectors. That’s where RNNs come in.
RNNs allow us to understand the context of a video frame, relative to the frames that came before it. They do this by passing the output of one training step to the input of the next training step, along with the new frames. Andrej Karpathy describes this eloquently in his popular blog post, “The Unreasonable Effectiveness of Recurrent Neural Networks”:
We’re using a special type of RNN here, called an LSTM, that allows our network to learn long-term dependencies. Christopher Olah writes in his outstanding essay about LSTMs: “Almost all exciting results based on recurrent neural networks are achieved with [LSTMs].”
Sold! Let’s get to it.
Our aim is to use the power of CNNs to detect spatial features and RNNs for the temporal features, effectively building a CNN->RNN network, or CRNN. For the sake of time, rather than building and training a new network from scratch, we’ll...
Step 2 is unique so we’ll expand on it a bit. There are two interesting paths that come to mind when adding a recurrent net to the end of our convolutional net:
Let’s say you’re baking a cake. You have at your disposal all of the ingredients in the world. We’ll say that this assortment of ingredients is our image to be classified. By looking at a recipe, you see that all of the possible things you could use to make a cake (flour, whisky, another cake) have been reduced down to ingredients and measurements that will make a good cake. The person who created the recipe out of all possible ingredients is the convolutional network, and the resulting instructions are the output of our pool layer. Now you make the cake and it’s ready to eat. You’re the softmax layer, and the finished product is our class prediction.
I’ve made the code to explore these methods available on GitHub. I’ll pull out a couple interesting bits here:
In order to turn our discrete predictions or features into a sequence, we loop through each frame in chronological order, add it to a queue of size N, and pop off the first frame we previously added. Here’s the gist:
N represents the length of our sequence that we’ll pass to the RNN. We could choose any length for N, but I settled on 40. At 10fps, which is the framerate of our video, that gives us 4 seconds of video to process at a time. This seems like a good balance of memory usage and information.
The architecture of the network is a single LSTM layer with 256 nodes. This is followed by a dropout of 0.2 to help prevent over-fitting and a fully-connected softmax layer to generate our predictions. I also experimented with wider and deeper networks, but neither performed as well as this one. It’s likely that with a larger training set, a deeper network would perform best.
Note: I’m using the incredible TFLearn library, a higher-level API for TensorFlow, to construct our network, which saves us from having to write a lot of code.
Once we have our sequence of features and our network, training with TFLearn is a breeze.
Evaluating is even easier.
Now, let’s evaluate each of the methods we outlined above for adding an RNN to our CNN.
Intuitively, if one frame is an ad and the next is a football game, it’s essentially impossible that the next will be an ad again. (I wish commercials were only 1/10th of a second long!)
This is why it could be interesting to examine the temporal dependencies of the probabilities of each label before we look at the more raw output of the pool layer. We convert our individual predictions into sequences using the code above, and then feed the sequences to our RNN.
After training the RNN on our first batch of data, we then evaluate the predictions on both the batch we used for training and a holdout set that the RNN has never seen. No surprise, evaluating the same data we used to train gives us an accuracy of 99.55%! Good sanity check that we’re on the right path.
Now the fun part. We run the holdout set through the same network and get... 95.4%! Better than our 93.3% we got without the LSTM, and not a bad result, given we’re using the full output of the CNN, and thus not giving the RNN much responsibility. Let’s change that.
Here we’ll go a little deeper. (See what I did there?) Instead of letting the CNN do all the hard work, we’ll give more responsibility to the RNN by using output of the CNN’s pool layer, which gives us the feature representation (not a prediction) of our images. We again build sequences with this data to feed into our RNN.
Running our training data through the network to make sure we get high accuracy succeeds at 99.89%! Sanity checked.
How about our holdout set?
96.58%! That’s an error reduction of 3.28 percentage points (or 49%!) from our CNN-only benchmark. Awesome!
We have shown that taking both spatial and temporal features into consideration improves our accuracy significantly.
Next, we’ll want to try this method on a more complex dataset, perhaps using multiple classes of TV programming, and with a whole whackload more data to train on. (Remember, we’re only using 20 minutes of TV here.)
Once we feel comfortable there, we’ll go ahead and combine the RNN and CNN into one network so we can more easily deploy it in an online system. That’s going to be fun.
Part 3 is now available: Five video classification methods implemented in Keras and TensorFlow
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder of Coastline Automation, using AI to make every car crash-proof.
Practical applications of deep learning and research reports from the road.
",video sequence images previous post explored method continuous online video classification treated frame discrete context relative previous frames unimportant today going stop treating video individual photos start treating like video looking images sequence well process sequences harnessing magic recurrent neural networks rnns restate problem outlined previous post attempting continually classify video streamed online system specifically classifying whether whats streaming tv football game advertisement convolutional neural networks used exclusively previous post amazing job taking fixedsize vector like image animal generating fixedsize label like class animal image cnns cannot without computationally intensive 3d convolution layers accept sequence vectors thats rnns come rnns allow us understand context video frame relative frames came passing output one training step input next training step along new frames andrej karpathy describes eloquently popular blog post unreasonable effectiveness recurrent neural networks using special type rnn called lstm allows network learn longterm dependencies christopher olah writes outstanding essay lstms almost exciting results based recurrent neural networks achieved lstms sold lets get aim use power cnns detect spatial features rnns temporal features effectively building cnnrnn network crnn sake time rather building training new network scratch well step 2 unique well expand bit two interesting paths come mind adding recurrent net end convolutional net lets say youre baking cake disposal ingredients world well say assortment ingredients image classified looking recipe see possible things could use make cake flour whisky another cake reduced ingredients measurements make good cake person created recipe possible ingredients convolutional network resulting instructions output pool layer make cake ready eat youre softmax layer finished product class prediction ive made code explore methods available github ill pull couple interesting bits order turn discrete predictions features sequence loop frame chronological order add queue size n pop first frame previously added heres gist n represents length sequence well pass rnn could choose length n settled 40 10fps framerate video gives us 4 seconds video process time seems like good balance memory usage information architecture network single lstm layer 256 nodes followed dropout 02 help prevent overfitting fullyconnected softmax layer generate predictions also experimented wider deeper networks neither performed well one likely larger training set deeper network would perform best note im using incredible tflearn library higherlevel api tensorflow construct network saves us write lot code sequence features network training tflearn breeze evaluating even easier lets evaluate methods outlined adding rnn cnn intuitively one frame ad next football game essentially impossible next ad wish commercials 110th second long could interesting examine temporal dependencies probabilities label look raw output pool layer convert individual predictions sequences using code feed sequences rnn training rnn first batch data evaluate predictions batch used training holdout set rnn never seen surprise evaluating data used train gives us accuracy 9955 good sanity check right path fun part run holdout set network get 954 better 933 got without lstm bad result given using full output cnn thus giving rnn much responsibility lets change well go little deeper see instead letting cnn hard work well give responsibility rnn using output cnns pool layer gives us feature representation prediction images build sequences data feed rnn running training data network make sure get high accuracy succeeds 9989 sanity checked holdout set 9658 thats error reduction 328 percentage points 49 cnnonly benchmark awesome shown taking spatial temporal features consideration improves accuracy significantly next well want try method complex dataset perhaps using multiple classes tv programming whole whackload data train remember using 20 minutes tv feel comfortable well go ahead combine rnn cnn one network easily deploy online system thats going fun part 3 available five video classification methods implemented keras tensorflow quick cheer standing ovation clap show much enjoyed story founder coastline automation using ai make every car crashproof practical applications deep learning research reports road,en,"['CRNN', 'GitHub', 'N', 'TensorFlow', 'CNN', 'RNN', 'Coastline Automation']"
162,Oxford University,237,The future of work – Oxford University – Medium,"Technology has always changed employment, but the rise of robotics and artificial intelligence could transform it beyond recognition. Researchers at Oxford are investigating how technology will shape the future of work — and what we can do to ensure everyone benefits.
In a famous 1930 talk, John Maynard Keynes imagined a future 100 years hence in which technological progress automated much of human labour. By 2030, he estimated, we could all enjoy a 15-hour working week.
A lot will need to change in the next decade for that to become a reality, but it’s not impossible. Right now, advances in artificial intelligence and robotics promise machines that will take on all kinds of human tasks. Digital communication is creating an internet-dwelling labour force that can work remotely and on demand. And the self-employed are finding that new technological services like Uber and Airbnb can provide a flexible way to make a living. But phenomena like these give rise to a cascade of effects — not all necessarily desirable — that are fiendishly difficult to perceive and predict.
It’s perhaps not surprising, then, that the future of work is a topic of increasing fascination for University of Oxford academics. Both the Oxford Martin School and Green-Templeton College now run specific programmes that focus on the topic, with plenty of researchers — from the Departments of Engineering Science and Sociology to those of Politics and Economics — grappling with its complexity.
‘We see a need for bringing together different perspectives around the study of work,’ explains Dr Marc Thompson, a Senior Fellow at Saїd Business School and the Director of the Green-Templeton College Future of Work Programme. ‘Our role as academics is to contribute to the debate, both in terms of theory and to raise challenging questions and issues for those in government and industry. What will happen as a result of these advances? How will it affect people? Whose interests are being pursued? And what are the long-term implications?’
A series of recent studies from the University cut straight to the chase of technology’s impact on employment, focusing on how robotics and automation will affect the jobs that humans currently undertake. The authors, Dr Carl Benedikt Frey (@carlbfrey) and Prof Michael Osborne (@maosbot), come from quite different backgrounds: Frey is an economist interested in the transition of industrial nations to digital economies, Osborne an engineer focused on creating machine-learning algorithms. Together, they’re co-directors of the Programme on Technology and Employment at the Oxford Martin School.
‘It would be fair to ask why I’m doing work related to economics while we’re sitting here in the Department of Engineering,’ admits Osborne, gesturing to his surroundings. ‘But I’ve always had some interest in thinking about what machine-learning could mean for society beyond the industrial applications we usually consider, so when Carl approached me to speak about algorithms and technologies used in automation, and their effects on employment, it seemed like a natural fit.’ This is, of course, exactly the kind of multidisciplinary work the University excels at, and the reason the Oxford Martin School was established. Each of its programmes brings together researchers from different fields to tackle complex global issues that can’t be solved by academics from a single discipline.
Since meeting, the pair has set about developing ways to analyse which jobs that exist today could be at risk of being taken over by robots or artificial intelligence software in the next 20 years. First, they gathered together ‘as many smart people as [they] could’ to decide on 70 job roles that definitely could or could not be automated in the next 20 years. For example, they collectively decided that switchboard operators and dishwashers could definitely be replaced, while the clergy and magistrates certainly couldn’t.
The pair combined this list with data from the US Department of Labor’s O⋆NET system — a database which describes the different skills relevant to specific occupations. Osborne then built an algorithm that could learn from both pools of data to establish the kinds of skills that were common to automatable jobs. When shown other occupations and the skills they require, the software can classify them with a probability of being either automatable or non-automatable.
The pair found that the jobs least likely to be automated are those that require skills of creative intelligence, social intelligence or physical dexterity. These are what they refer to as engineering bottlenecks: current limits to technology that make humans irreplaceable. Osborne points out that it’s perfectly possible, for instance, to have an algorithm churn out an endless sequence of songs, but almost impossible to have it create a hit. Similarly, chat-bots may be able to communicate with you but they can’t negotiate a deal, and robots can assemble objects on a well-defined production line, but they can’t perform a fiddly task like making a cup of tea in your messy kitchen. In each case, it’s because humans draw on a huge wealth of tacit knowledge about culture, emotion, human behaviour and the physical environment that’s hard to encode in a way that a machine can act upon.
But, even with those bottlenecks, the results suggest that as many as 47% of US jobs are at risk from automation over the course of the next two decades. It’s worth bearing in mind that the figures explain which jobs are theoretically automatable, rather than destined to be automated. ‘That may seem like a fiddly point,’ says Osborne, ‘but this analysis doesn’t take into account other factors that we absolutely do believe will have an impact on whether an occupation is taken over by a machine, such as human wage levels, social acceptance, and the creation of new jobs.’ But however you look at it, the numbers are difficult to ignore.
There’s an intuitive counter-argument to the claims that their analysis makes: for centuries, new technologies have been invented that have pushed humans out of work, but by and large most of us still continue to have jobs. In fact researchers elsewhere in the University have shown that the amount of work we all perform remains steadfastly consistent, irrespective of technological change.
Jonathan Gershuny, Professor of Sociology and Director of the Centre for Time Use Research, has spent a large part of his career tracing the way that we all use our time — to work, play, rest and everything else. ‘Fundamentally, there are three realms of activity,’ he explained from the bay window of his Woodstock Road office. ‘There’s paid work, unpaid work and consumption.’ Paid work is just that: the tasks we carry out in exchange for money, be it mining coal, writing a book or performing brain surgery. Unpaid work, meanwhile, is formed of tasks that you could pay someone else to do for you (but for whatever reason don’t), such as cooking, cleaning, gardening or childcare. And consumption is all the activity you absolutely couldn’t pay someone else to do for you — your night’s sleep, say, or eating lunch.
‘Why am I telling you all this?’ asks Gershuny, with a grin. ‘Well, when you define work quite widely like this, you arrive at a really quite extraordinary discovery, which is that work time — that is the sum of paid and unpaid work time — doesn’t change very much. Looking at all the data we have access to, the total is pretty constant, at about 60 hours per week.’ That’s just over a third of our 168-hour week, and a little more than the approximately 50-hour chunk we manage to spend sleeping.
He points to decades of evidence accumulated by his team — in countries including Australia, Canada, Israel, Slovenia, France, Sweden, the Netherlands and plenty more — that confirm the trend, as well as working time regulations from as far back as the Industrial Revolution. His latest dataset — a huge survey of British residents carried out in 2015 — was being downloaded in full the day we met, but a preliminary analysis already suggested that his observation holds true. ‘The truth is, we need work for various reasons: a time structure, a social context, a purpose in life,’ he explains. Indeed, what many people citing Keynes’ famous talk about the future fail to mention is that he went on to suggest that ‘there is no country and no people... who can look forward to the age of leisure and of abundance without a dread.’ In other words, he thought that most us couldn’t really begin to comprehend the reality of not working. Gershuny agrees, arguing that humans will simply endeavour to find new types of work to do in order to busy themselves, whether the robots take over the jobs we currently possess or not.
Dr Ruth Yeoman, a Research Fellow at the Saïd Business School who researches meaningful work in organisations and systems, points out that the human desire to find meaning in work is hard to ignore. She explains that the drive to work is so strong that people seek positive meaning in work that is considered by many people to be dirty, low status or poorly paid. ‘Hospital cleaners, for instance, interpret their work to be meaningful and worthwhile because they enlarge the scope of that work in their own minds,’ she explains.
This phenomenon allows humans to justify all kinds of work to themselves as useful and relevant, it seems, regardless of what it actually is.
Frey and Osborne aren’t so confident that humans are resourceful enough to create new work for themselves, though. Frey has actually studied the rate at which new jobs are being generated as a result of technological change. His findings suggest that about 8.2% of the US workforce shifted into new types of jobs — that is, roles associated with technological advances — during the 1980s. In the 1990s the figure fell to 4.4% and in the 2000s it dropped to just 0.5%. The evidence suggests that the new industries we might assume to be the salvation of the labour force — such as web design or data science — aren’t creating as many new positions as we may hope.
Part of the reason for that, argues Osborne, is that many of the new job roles being created are related to software, rather than hard, physical goods. ‘Software is pretty cheap with next to zero marginal cost of reproduction,’ he explains. That means that a small group of people can have a great idea and easily turn it into a product that’s used the world over, while barely growing the size of its team. The smartphone messaging service WhatsApp is a prime example: it was purchased by Facebook for $19 billion in 2014, when it served 700 million users. At the time, it had just 55 employees.
Counting specific jobs may, however, be overly simplistic when it comes to thinking about how the working lives of real people are set to change. ‘People often think about the work that people do as a monolithic indivisible lump of stuff,’ explains Daniel Susskind (@danielsusskind), a Lecturer in Economics at Balliol College and co-author of a new book called The Future of the Professions. ‘The problem is, that encourages the view that one day a lawyer will arrive at work to find an algorithm sitting in his chair, or a doctor turn up to a robot in her operating theatre, and their jobs will both be gone.’ Instead, he argues, we should be focusing on the separate tasks that make up job roles.
Susskind co-wrote his new book with his father, Richard Susskind (@richardsusskind), whose Oxford DPhil considered the impact of artificial intelligence on law. That was back in the 1980s, when AI systems were rudimentary and typically based on rules gleaned from human understanding. But five years ago father and son — the latter then working in the Policy Unit at 10 Downing Street — realised that a second wave of artificial intelligence was being developed that could have profound effects on professional careers. Since, they’ve been researching how technology might affect the working lives of lawyers, doctors, teachers, architects and the rest of the professions.
‘Not everything that a professional does is creative, strategic or complex,’ explains Susskind. ‘So while many professionals might think that all their work lies on one side of [Frey and Osborne’s] engineering bottlenecks, actually many of the tasks they perform are amenable to computerisation.’ For most, that means it’s unlikely that they’ll simply lose their job to technology, at least in the near future — but they can expect to see a significant change in the sorts of things they’re asked to do. In their book, the Susskinds describe twelve new roles that might appear within the professions — such as process analysers, knowledge engineers, data scientists and empathisers. ‘These are roles that sound unfamiliar to traditional professionals, that require skills and abilities that many of them are unlikely to have at this moment in time,’ they explain.
We’re already seeing professionals adapt so that they can work alongside more intelligent technological systems, though. Take, for instance, your bank manager. When you used to approach them for a loan, they’d carefully make a decision on whether or not you were a good risk, then either give you the money or send you home. Now, an algorithm determines whether or not you’re awarded the cash, and yet bank managers still exist. The role has simply changed, to become a customer service and sales job rather than an analytical or technical role.
Not everyone will be as lucky as the professionals whose jobs merely metamorphose, because if all of the tasks that make up a job are automatable, the job no longer needs to exist. Craig Holmes (@CraigPHolmes), a Fellow in Economics at Pembroke College and Senior Research Fellow at the Institute for New Economic Thinking, has been studying shifts in occupational structure of labour markets, and how they’ve moved away from middle-skilled work, with more people now doing high-skilled or low-skilled work. This phenomenon — referred to as the hollowing out of the labour market — isn’t in itself new: middle-skilled factory workers have been losing their jobs to robots for decades, for instance.
But the pace of technological development is now threatening other middle-skilled occupations that in the past we’ve assumed could only be done by humans. Job categories defined as associate professionals, for instance — the people that provide technical services that keep trade, finance and government running — appear increasingly likely to be taken over by machines. ‘In the case of, say, paralegals, there are now pieces of software that can sift through thousands of documents, pull out relevant precedents, and put them together using a very simple format, without requiring any human involvement,’ explains Holmes. ‘So a traditionally middle-tier research job can be perfectly performed by technology.’ The same story could play out in other sectors: large datasets of historical case notes and information from wearables could allow computers to make straightforward medical diagnoses, say, while smarter algorithms might remove the work of number-crunching accountants.
Like car factory workers replaced by robots in the past, Holmes imagines a number of possible futures for those discharged from mid-tier roles. Some, like the bank manager, will be able to assume different roles with similar titles. A small number may move upwards into roles that aren’t yet automatable. Others, sadly, may have to assume lower-skilled jobs or face unemployment.
The nature of those lower-skilled jobs will of course change too. The work of Frey and Osborne suggests that many low-skilled jobs — such as call centre workers, data entry clerks and dishwashers — will be readily automated in the future. ‘In some cases, the cost of technology will be so low that there’s no wage that people could happily accept that would make the job sustainable,’ admits Holmes. ‘In fast food restaurants, for instance, you can replace someone who takes an order with an iPad that will last for years. Nobody would accept a job that paid wages that low.’ But it’s not perhaps quite so gloomy as that, as personal service jobs will likely still require a human touch. ‘We’ll probably see an increase in the number of low-skill service jobs, because people value human interaction and many of those jobs currently seem not to be readily automatable,’ suggests Holmes. ‘That will provide more jobs, they just won’t be great jobs.’
While technology may be the mechanism through which many jobs are lost, though, it might very well also be the thing that enables people to take up new lower-skilled positions. ‘There’s been an explosion in connectivity around the world,’ explains Professor Mark Graham (@geoplace) from the Oxford Internet Institute. ‘Something like 3.5 billion people are now online. And that has some significant repercussions in terms of what work is, where it’s done and how it happens.’
Graham has been travelling the world to talk to people who find themselves in a new kind of labour market. In particular, he’s been interviewing individuals who perform work from home, provided to them by a slew of websites such as Amazon’s Mechanical Turk, UpWork, and ClickWorker. These sites all allow companies and individuals to outsource tasks: potential employers simply post a description of what they need doing to a website, then people interested in doing the work bid for it. The employer chooses someone to do the work, based on a combination of price, listed skills and ratings from previous employers; the worker carries out the task, gets paid, then moves on to another piece of work. The tasks being doled out vary — from transcription and translation to new kinds of work such as tagging images for artificial intelligence systems — but much of it is currently difficult or expensive to automate.
Technology has also created legions of new workforce members in more traditional sectors, such as transportation, hospitality, catering, cleaning and delivery. ‘There are increasingly more ways of commodifying bits of everyday life: using your car to be an Uber driver; your apartment to be an Airbnb host; your bicycle to be a Deliveroo rider; or your broom to be a Task Rabbit cleaner,’ explains Graham. This is what’s become known as the ‘sharing’ or ‘gig’ economy. Whether it’s Uber, Airbnb or Amazon’s Mechanical Turk, the business plan is much the same: create a digital platform which makes it easier to link a customer, who wants a service to be performed, with someone who’s willing to provide it, for a (very) competitive fee.
These new styles of working certainly bring some benefits: apparent flexibility for workers, more efficient use of existing resources and equipment, and reasonable prices for those seeking services. But, as Jeremias Prassl, an Associate Professor of Law and Fellow of Magdalen College, warns, this new workforce is potentially vulnerable. ‘Uber acts like an employer: it sets your wage, tells you the route to drive, hires you, and fires you if your rating falls too low,’ he explains. ‘Under any classical analysis, Uber performs all the usual employer functions. But in its contracts with “driver-partners”, the platform explicitly denies employer status, suggesting that the worker is very much a contractor. Legally, and through the language it uses, Uber tries to deny the fact that it offers employment.’ Through so doing, the company is able to avoid paying social security, pension contributions, redundancy pay and so on — all the usual rights an employee might benefit from.
But Prassl, who’s written a book about the topic, points out that these kinds of contracts are nothing new. ‘From the perspective of an employment lawyer, zero hours contracts and the gig economy are old problems,’ he explains. ‘We’ve been grappling with the rise of so-called “non-standard work” for the last 30 or 40 years. It’s just that now they’re receiving more attention and sustained media coverage.’
The problem, as Prassl sees it, is that employment law is currently based on an old binary system. If you’re an employee you get rights — to, say, sick pay, notice of dismissal or paid holiday. But if you’re a contractor, you’re not afforded any of those rights. Employment law currently boils down to a simple question: How do you define whether or not someone counts as an employee? ‘What my research suggests is that maybe we should turn the problem on its head,’ he explains. ‘We could say instead: Who’s the employer?’
It seems like a subtle difference but, with the shoe on the other foot, he suggests crowd workers would be able to enjoy some kind of employment law protection. In this upended scenario, everyone could benefit from existing minimum standards like the minimum wage, working time regulations and discrimination protection, with their provision accounted for by whoever is legally deemed to be the employer. If companies failed to comply, workers could litigate employers in the knowledge that the damages were definitely owed to them.
It’s not just Prassl that’s worried about the vulnerability of employees. ‘One of the issues is that we confuse work with jobs,’ points out Ruth Yeoman. ‘There’s an awful lot of work in the world that has to be done, and one of the problems when we think about the future of work is how it all gets converted into jobs for which people will be paid. Sometimes people may contribute to society not through paid work, but through some other mechanism: voluntary work, say, or caring.’ And while those tasks may be hard work, or may not pay, they are necessary and many of them must be done by humans.
That’s why Stuart White (@StuartGWhite), Associate Professor from the Department of Politics and International Relations, is interested in how we could ensure everyone enjoys a basic standard of living — a concept he’s written about in the book Democratic Wealth.
he explains. White’s suggestion is that no tests of means or willingness to take a job would be imposed, so that everyone in the country received a basic payment every month. It’s worth noting that the idea is not intended to make everyone rich — far from it. Instead, it’s a means of giving individuals more flexibility, affording them power to decide when and how to be contributive and productive. ‘It’s a way of ensuring you don’t have people desperately scrambling into jobs to make ends meet,’ White explains. In turn, he argues, employers would make some of the least appealing jobs more pleasant — they’d be forced to, otherwise nobody would choose to do them.
Numerous mechanisms for putting such a policy into action have been proposed in the past. One option is to divert existing benefits and tax relief into a basic income that’s shared equally amongst the population. If those contributions didn’t stretch far enough, they could be topped up with revenue from further taxation — from land value tax, suggests White. Alternatively, the income could be provided by a state-owned investment fund from which the returns would be shared out equally. ‘There are lots of philosophical arguments about whether or not it’s all a good idea,’ he concedes. ‘But we’re moving into a world where there’s increased insecurity around work. Against that backdrop, a source of income that’s independent of work is a way of rebalancing power relations in the labour market.’
Whether or not you agree with the concept of a universal citizen’s income or the reform of employment law, these concepts are indicative of the kinds of discussions that Oxford researchers are increasingly leading. ‘I think the University needs to be asking these kinds of Aristotlean questions about whose interests are being met, who benefits from the changes... the moral questions,’ explains Marc Thompson. ‘It’s not something we should shy away from.’
Increasingly, then, just as Thompson hoped for when he set up the Green Templeton College Future of Work Programme, Oxford academics are working with business and governments to shape the debate about the future of employment. Frey and Osborne, for instance, have published reports with Citi and Deloitte about the impact of technology on employment; Mark Graham sits on the Department for International Development’s Digital Advisory Panel; and Richard Susskind acts as an IT Adviser to the Lord Chief Justice of England and Wales.
What remains, of course, is for policymakers, lawyers and industry officials to take the questions and suggestions raised by academics on board, then work out how best to use technological advance in all our favour. ‘These possibilities afforded by technology, automation and commodification of labour... they can all be shaped by policy, organisational change and simply choosing to do things differently,’ muses Thompson. ‘There are some important choices to be made about how we make use of them.’
Technology will make many jobs redundant, others easier, and create at least some new ones along the way. Keynes’ prediction of a fifteen-hour working week may even come true. But while humans are in charge, we can still choose for there to be some work that’s performed by non-robotic hands. ‘It would be very easy for there to be an automated pub where drinks are served from vending machines,’ concludes Mark Graham. ‘But nobody wants that. Because it would be depressing.’
Written by Jamie Condliffe, a science and technology writer based in London. He tweets @jme_c.
In keeping with one of the themes of the article we used 99designs to find an illustrator and worked with slouise.
Follow us on Medium, we’ll be publishing more articles soon that look at topics such as medical trials, developments in healthcare and more.
If you liked this article please click the green heart, it really helps to spread the word and let others find it.
Produced by Christopher Eddie, Digital Communications Office, University of Oxford.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Oxford is  one of the oldest universities in the world. We aim to lead the world in research and education. Contact: digicomms@admin.ox.ac.uk
Oxford is one of the oldest universities in the world. We aim to lead the world in research and education. Contact: digicomms@admin.ox.ac.uk
",technology always changed employment rise robotics artificial intelligence could transform beyond recognition researchers oxford investigating technology shape future work ensure everyone benefits famous 1930 talk john maynard keynes imagined future 100 years hence technological progress automated much human labour 2030 estimated could enjoy 15hour working week lot need change next decade become reality impossible right advances artificial intelligence robotics promise machines take kinds human tasks digital communication creating internetdwelling labour force work remotely demand selfemployed finding new technological services like uber airbnb provide flexible way make living phenomena like give rise cascade effects necessarily desirable fiendishly difficult perceive predict perhaps surprising future work topic increasing fascination university oxford academics oxford martin school greentempleton college run specific programmes focus topic plenty researchers departments engineering science sociology politics economics grappling complexity see need bringing together different perspectives around study work explains dr marc thompson senior fellow sad business school director greentempleton college future work programme role academics contribute debate terms theory raise challenging questions issues government industry happen result advances affect people whose interests pursued longterm implications series recent studies university cut straight chase technologys impact employment focusing robotics automation affect jobs humans currently undertake authors dr carl benedikt frey carlbfrey prof michael osborne maosbot come quite different backgrounds frey economist interested transition industrial nations digital economies osborne engineer focused creating machinelearning algorithms together theyre codirectors programme technology employment oxford martin school would fair ask im work related economics sitting department engineering admits osborne gesturing surroundings ive always interest thinking machinelearning could mean society beyond industrial applications usually consider carl approached speak algorithms technologies used automation effects employment seemed like natural fit course exactly kind multidisciplinary work university excels reason oxford martin school established programmes brings together researchers different fields tackle complex global issues cant solved academics single discipline since meeting pair set developing ways analyse jobs exist today could risk taken robots artificial intelligence software next 20 years first gathered together many smart people could decide 70 job roles definitely could could automated next 20 years example collectively decided switchboard operators dishwashers could definitely replaced clergy magistrates certainly couldnt pair combined list data us department labors onet system database describes different skills relevant specific occupations osborne built algorithm could learn pools data establish kinds skills common automatable jobs shown occupations skills require software classify probability either automatable nonautomatable pair found jobs least likely automated require skills creative intelligence social intelligence physical dexterity refer engineering bottlenecks current limits technology make humans irreplaceable osborne points perfectly possible instance algorithm churn endless sequence songs almost impossible create hit similarly chatbots may able communicate cant negotiate deal robots assemble objects welldefined production line cant perform fiddly task like making cup tea messy kitchen case humans draw huge wealth tacit knowledge culture emotion human behaviour physical environment thats hard encode way machine act upon even bottlenecks results suggest many 47 us jobs risk automation course next two decades worth bearing mind figures explain jobs theoretically automatable rather destined automated may seem like fiddly point says osborne analysis doesnt take account factors absolutely believe impact whether occupation taken machine human wage levels social acceptance creation new jobs however look numbers difficult ignore theres intuitive counterargument claims analysis makes centuries new technologies invented pushed humans work large us still continue jobs fact researchers elsewhere university shown amount work perform remains steadfastly consistent irrespective technological change jonathan gershuny professor sociology director centre time use research spent large part career tracing way use time work play rest everything else fundamentally three realms activity explained bay window woodstock road office theres paid work unpaid work consumption paid work tasks carry exchange money mining coal writing book performing brain surgery unpaid work meanwhile formed tasks could pay someone else whatever reason dont cooking cleaning gardening childcare consumption activity absolutely couldnt pay someone else nights sleep say eating lunch telling asks gershuny grin well define work quite widely like arrive really quite extraordinary discovery work time sum paid unpaid work time doesnt change much looking data access total pretty constant 60 hours per week thats third 168hour week little approximately 50hour chunk manage spend sleeping points decades evidence accumulated team countries including australia canada israel slovenia france sweden netherlands plenty confirm trend well working time regulations far back industrial revolution latest dataset huge survey british residents carried 2015 downloaded full day met preliminary analysis already suggested observation holds true truth need work various reasons time structure social context purpose life explains indeed many people citing keynes famous talk future fail mention went suggest country people look forward age leisure abundance without dread words thought us couldnt really begin comprehend reality working gershuny agrees arguing humans simply endeavour find new types work order busy whether robots take jobs currently possess dr ruth yeoman research fellow said business school researches meaningful work organisations systems points human desire find meaning work hard ignore explains drive work strong people seek positive meaning work considered many people dirty low status poorly paid hospital cleaners instance interpret work meaningful worthwhile enlarge scope work minds explains phenomenon allows humans justify kinds work useful relevant seems regardless actually frey osborne arent confident humans resourceful enough create new work though frey actually studied rate new jobs generated result technological change findings suggest 82 us workforce shifted new types jobs roles associated technological advances 1980s 1990s figure fell 44 2000s dropped 05 evidence suggests new industries might assume salvation labour force web design data science arent creating many new positions may hope part reason argues osborne many new job roles created related software rather hard physical goods software pretty cheap next zero marginal cost reproduction explains means small group people great idea easily turn product thats used world barely growing size team smartphone messaging service whatsapp prime example purchased facebook 19 billion 2014 served 700 million users time 55 employees counting specific jobs may however overly simplistic comes thinking working lives real people set change people often think work people monolithic indivisible lump stuff explains daniel susskind danielsusskind lecturer economics balliol college coauthor new book called future professions problem encourages view one day lawyer arrive work find algorithm sitting chair doctor turn robot operating theatre jobs gone instead argues focusing separate tasks make job roles susskind cowrote new book father richard susskind richardsusskind whose oxford dphil considered impact artificial intelligence law back 1980s ai systems rudimentary typically based rules gleaned human understanding five years ago father son latter working policy unit 10 downing street realised second wave artificial intelligence developed could profound effects professional careers since theyve researching technology might affect working lives lawyers doctors teachers architects rest professions everything professional creative strategic complex explains susskind many professionals might think work lies one side frey osbornes engineering bottlenecks actually many tasks perform amenable computerisation means unlikely theyll simply lose job technology least near future expect see significant change sorts things theyre asked book susskinds describe twelve new roles might appear within professions process analysers knowledge engineers data scientists empathisers roles sound unfamiliar traditional professionals require skills abilities many unlikely moment time explain already seeing professionals adapt work alongside intelligent technological systems though take instance bank manager used approach loan theyd carefully make decision whether good risk either give money send home algorithm determines whether youre awarded cash yet bank managers still exist role simply changed become customer service sales job rather analytical technical role everyone lucky professionals whose jobs merely metamorphose tasks make job automatable job longer needs exist craig holmes craigpholmes fellow economics pembroke college senior research fellow institute new economic thinking studying shifts occupational structure labour markets theyve moved away middleskilled work people highskilled lowskilled work phenomenon referred hollowing labour market isnt new middleskilled factory workers losing jobs robots decades instance pace technological development threatening middleskilled occupations past weve assumed could done humans job categories defined associate professionals instance people provide technical services keep trade finance government running appear increasingly likely taken machines case say paralegals pieces software sift thousands documents pull relevant precedents put together using simple format without requiring human involvement explains holmes traditionally middletier research job perfectly performed technology story could play sectors large datasets historical case notes information wearables could allow computers make straightforward medical diagnoses say smarter algorithms might remove work numbercrunching accountants like car factory workers replaced robots past holmes imagines number possible futures discharged midtier roles like bank manager able assume different roles similar titles small number may move upwards roles arent yet automatable others sadly may assume lowerskilled jobs face unemployment nature lowerskilled jobs course change work frey osborne suggests many lowskilled jobs call centre workers data entry clerks dishwashers readily automated future cases cost technology low theres wage people could happily accept would make job sustainable admits holmes fast food restaurants instance replace someone takes order ipad last years nobody would accept job paid wages low perhaps quite gloomy personal service jobs likely still require human touch well probably see increase number lowskill service jobs people value human interaction many jobs currently seem readily automatable suggests holmes provide jobs wont great jobs technology may mechanism many jobs lost though might well also thing enables people take new lowerskilled positions theres explosion connectivity around world explains professor mark graham geoplace oxford internet institute something like 35 billion people online significant repercussions terms work done happens graham travelling world talk people find new kind labour market particular hes interviewing individuals perform work home provided slew websites amazons mechanical turk upwork clickworker sites allow companies individuals outsource tasks potential employers simply post description need website people interested work bid employer chooses someone work based combination price listed skills ratings previous employers worker carries task gets paid moves another piece work tasks doled vary transcription translation new kinds work tagging images artificial intelligence systems much currently difficult expensive automate technology also created legions new workforce members traditional sectors transportation hospitality catering cleaning delivery increasingly ways commodifying bits everyday life using car uber driver apartment airbnb host bicycle deliveroo rider broom task rabbit cleaner explains graham whats become known sharing gig economy whether uber airbnb amazons mechanical turk business plan much create digital platform makes easier link customer wants service performed someone whos willing provide competitive fee new styles working certainly bring benefits apparent flexibility workers efficient use existing resources equipment reasonable prices seeking services jeremias prassl associate professor law fellow magdalen college warns new workforce potentially vulnerable uber acts like employer sets wage tells route drive hires fires rating falls low explains classical analysis uber performs usual employer functions contracts driverpartners platform explicitly denies employer status suggesting worker much contractor legally language uses uber tries deny fact offers employment company able avoid paying social security pension contributions redundancy pay usual rights employee might benefit prassl whos written book topic points kinds contracts nothing new perspective employment lawyer zero hours contracts gig economy old problems explains weve grappling rise socalled nonstandard work last 30 40 years theyre receiving attention sustained media coverage problem prassl sees employment law currently based old binary system youre employee get rights say sick pay notice dismissal paid holiday youre contractor youre afforded rights employment law currently boils simple question define whether someone counts employee research suggests maybe turn problem head explains could say instead whos employer seems like subtle difference shoe foot suggests crowd workers would able enjoy kind employment law protection upended scenario everyone could benefit existing minimum standards like minimum wage working time regulations discrimination protection provision accounted whoever legally deemed employer companies failed comply workers could litigate employers knowledge damages definitely owed prassl thats worried vulnerability employees one issues confuse work jobs points ruth yeoman theres awful lot work world done one problems think future work gets converted jobs people paid sometimes people may contribute society paid work mechanism voluntary work say caring tasks may hard work may pay necessary many must done humans thats stuart white stuartgwhite associate professor department politics international relations interested could ensure everyone enjoys basic standard living concept hes written book democratic wealth explains whites suggestion tests means willingness take job would imposed everyone country received basic payment every month worth noting idea intended make everyone rich far instead means giving individuals flexibility affording power decide contributive productive way ensuring dont people desperately scrambling jobs make ends meet white explains turn argues employers would make least appealing jobs pleasant theyd forced otherwise nobody would choose numerous mechanisms putting policy action proposed past one option divert existing benefits tax relief basic income thats shared equally amongst population contributions didnt stretch far enough could topped revenue taxation land value tax suggests white alternatively income could provided stateowned investment fund returns would shared equally lots philosophical arguments whether good idea concedes moving world theres increased insecurity around work backdrop source income thats independent work way rebalancing power relations labour market whether agree concept universal citizens income reform employment law concepts indicative kinds discussions oxford researchers increasingly leading think university needs asking kinds aristotlean questions whose interests met benefits changes moral questions explains marc thompson something shy away increasingly thompson hoped set green templeton college future work programme oxford academics working business governments shape debate future employment frey osborne instance published reports citi deloitte impact technology employment mark graham sits department international developments digital advisory panel richard susskind acts adviser lord chief justice england wales remains course policymakers lawyers industry officials take questions suggestions raised academics board work best use technological advance favour possibilities afforded technology automation commodification labour shaped policy organisational change simply choosing things differently muses thompson important choices made make use technology make many jobs redundant others easier create least new ones along way keynes prediction fifteenhour working week may even come true humans charge still choose work thats performed nonrobotic hands would easy automated pub drinks served vending machines concludes mark graham nobody wants would depressing written jamie condliffe science technology writer based london tweets jme_c keeping one themes article used 99designs find illustrator worked slouise follow us medium well publishing articles soon look topics medical trials developments healthcare liked article please click green heart really helps spread word let others find produced christopher eddie digital communications office university oxford quick cheer standing ovation clap show much enjoyed story oxford one oldest universities world aim lead world research education contact digicommsadminoxacuk oxford one oldest universities world aim lead world research education contact digicommsadminoxacuk,en,"['Oxford', 'Digital', 'University of Oxford', 'the Oxford Martin School', 'Green-Templeton College', 'the Departments of Engineering Science and Sociology', 'Politics and Economics', 'the Green-Templeton College Future of Work Programme', 'University', 'Frey', 'the Programme on Technology and Employment', 'the Department of Engineering', 'Oxford Martin School', 'the US Department of Labor', 'algorithm', 'the Centre for Time Use Research', 'the Saïd Business School', 'WhatsApp', 'Facebook', 'Balliol College', '@richardsusskind', 'Pembroke College and Senior Research Fellow', 'the Institute for New Economic Thinking', 'the Oxford Internet Institute', 'Amazon', 'Mechanical Turk', 'UpWork', 'ClickWorker', 'Uber', 'Task Rabbit', 'Fellow of Magdalen College', '@StuartGWhite', 'the Department of Politics and International Relations', 'Thompson', 'Deloitte', 'the Department for International Development’s Digital Advisory Panel', 'Digital Communications Office']"
163,Maciej Lipiec,766,The Future of Digital Banking – K2 Product Design – Medium,"Our solution is based on three pillars:
In the old times user interface of a bank was the bank teller at the branch. From today’s perspective it was inconvenient and time consuming, but the bank had a human face.
Now we are interacting with our banks by clicking on links, menus, and buttons, and filling out forms. But banking apps are often hard to use, overly complex and ugly. Lack of true customer-centricity and technological debt on the back-end side of things make the banking experience frustrating.
How can we make digital banking easier, more simple, more personal and human? By giving it a new face: of a robot!
Meet BankBot. It is the new digital bank teller, personal assistant, and a financial advisor. When you sign in to your K2 Bank account BankBot will greet you and ask for orders.
The main interface of K2 Bank is instantly familiar if you ever used Slack (over two millions of people use it in the office everyday), or Facebook Messenger, or an SMS app, or IRC (then you’re really old school!). It’s never ending stream with history of communications from bottom (recent) to the top (oldest) of the screen.
You type your command or question, and BankBot will answer. BankBot understands natural language, but it pays special attention for keywords, that will trigger actions, like a new transfer or searching in history, or credit card cancellation.
Just type in “Send 100 EUR to Anna” and BankBot will search it’s database for possible recipients matching „Anna” and let you choose the one you mean. Or you can add a new recipient. Then BankBot will sent confirmation code to your cell phone and ask you to type it in, and it’s done. You don’t need to click and move your hands from the keyboard.
Of course this the easiest scenario (similar to sending money via SquareCash or SnapCash), but almost every operation can be completed that way. Typing a recipient’s name will show you recent transactions with her from your account history and option for a new payment. Typing “USD” will show you currency exchange rate. If you need help type “help”. If you need to contact human staff at the bank type “human” and you can chat with real person from customer service instead of a bot. Or type „concierge” if you’re a Private Banking client.
There is also a way to access features using the Hamburger menu at the bottom— it opens a list of options, just like typing “/” (slash) in Slack.
Personal Finance Managers (PFMs) for controlling home budget are popular additions to banking systems. But they are complicated, often hidden deep in the nested menus, and they need a lot of user’s attention. Do people really use them?
Steven Walker of Forrester Research has written:
BankBot can provide just that. You can ask “Expenses this month”, or “Car expenses”, and it will show you a simple chart with relevant information.
This is “pull” mechanism, but BankBot can also be proactive, pushing important information to the user. It can warn you that you are close to exceeding your monthly budget. It can remind you about regular payments you usually make each month. It can remind you to pay off your credit card. Or pay your tax. It can suggest better options to save or invest your money, and show you how much more you can earn. It can offer you a loan, when you probably need it. Or offer travel insurance, when he knows you’ve just bought plane tickets. Or up-sell you a better account or credit card, when it will notice that you’ve got a pay rise. Or it can alert you when you should do something with your stocks portfolio.
Chat banking is nice on the desktop, but it’s even more effective on mobile — type a few words and it’s done, just like sending an SMS. Or you can talk to BankBot (speech2text). Authentication can be provided by fingerprint sensor. You can receive important alerts as push notifications on your phone or smartwatch, and immediately take action (or dismiss). You can even get discount on your health insurance based on physical activity data from your fitness band or Apple Watch.
BankBot can also live inside smart devices like the Amazon Echo, which provides its own API for developers — smart home and smart banking mixed together. Or inside the Facebook Messenger chat.
The second Payment Services Directive is to be transposed into national regulations across the European Union from 2016. Its goal is to open the banking market. PSD2 will force banks to provide access via APIs to their customer accounts and provide account information to third party service providers if the account holder wishes to do so. This is called „Access to the Account” (XS2A) and it’s not optional, banks will have to evolve as third parties enter their space.
PSD2 defines traditional financial institutions (banks) as “Account Servicing Payment Service Providers” (AS PSP), and new players as “Account Information Service Providers” (AISP) or “Payment Initiation Service Providers” (PISP). Both PISPs and AISPs will have to register with the “competent authority” in their home Member State for security reasons.
What are the implications of this for our system?
The quality of banking user interfaces will be extremely important, because bank’s clients could choose to manage their account from third party provider app with better UX or functionality, cutting themselves from any direct communication with their bank. In this case the bank will be reduced to a „dumb pipe” in the value chain. But fighting this by providing to the third parties only the minimum APIs required may be a bad strategy for banks. We think they should be more open, actively partnering with other financial institutions, retailers, merchants and startups.
We imagine K2 Bank solution providing an AppStore based on its APIs. Users will be able to give permission to third party service providers in a way you allow applications to access your Facebook or Twitter account today.
You will be able to buy stuff at your authorized retailer without logging into your bank (or without visiting the retailer site, but from yours bank app). There is no need to provide credit card number, probably even shipping address or any data. The bank can automatically offer you a purchase by installments. Or it can give you a discount, because of your history of frequent past transactions online and offline with this retailer (there will be no need for customer loyalty cards anymore).
The bank can become an advertising channel for the retailers too, offering personalized promotions for its customers. This should be opt-out, but if your cell-phone contract is ending, and BankBot messages you with a really great offer for a plan with a cheap newest iPhone, and you can buy it instantly with one click, would you mind?
By building the thriving ecosystems banks and third parties can both win. And we hope customers will too.
If you want to know more about K2 Bank solution, it’s design, technology behind the BankBot, and possibilities of implementation, don’t hesitate to contact us.
Of course conversational interfaces like BankBot can be used not only in banking, but also insurance, online commerce, travel, healthcare and many other industries.
Please write to Maciej Lipiec, K2’s User Experience Director, at maciej.lipiec@k2.pl
You can read more about K2 Bank in this article at Chatbots Magazine:
Also please check out our project on Behance.
K2 Internet is a leading digital product design and communications agency in Poland. We develop digital services, apps and websites with a strong focus on user experience. We have a long-time experience partnering with financial institutions — in the last 10 years we helped to envision, design and develop over 10 transactional systems for the biggest banks in Poland.
Stanusch Technologies is K2 Bank’s technology provider for BankBot. The company is involved in research and development of the use of artificial intelligence in business. It carry out projects related to natural language processing and semantic information retrieval. It has become a world leader in the number of carried out projects of virtual advisors/chatbots.
Thank you! If you enjoyed reading this please 👏👏👏 and share! :)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Product Design Director @ K2.
K2 Internet is a leading digital product design and communications agency in Poland.
",solution based three pillars old times user interface bank bank teller branch todays perspective inconvenient time consuming bank human face interacting banks clicking links menus buttons filling forms banking apps often hard use overly complex ugly lack true customercentricity technological debt backend side things make banking experience frustrating make digital banking easier simple personal human giving new face robot meet bankbot new digital bank teller personal assistant financial advisor sign k2 bank account bankbot greet ask orders main interface k2 bank instantly familiar ever used slack two millions people use office everyday facebook messenger sms app irc youre really old school never ending stream history communications bottom recent top oldest screen type command question bankbot answer bankbot understands natural language pays special attention keywords trigger actions like new transfer searching history credit card cancellation type send 100 eur anna bankbot search database possible recipients matching anna let choose one mean add new recipient bankbot sent confirmation code cell phone ask type done dont need click move hands keyboard course easiest scenario similar sending money via squarecash snapcash almost every operation completed way typing recipients name show recent transactions account history option new payment typing usd show currency exchange rate need help type help need contact human staff bank type human chat real person customer service instead bot type concierge youre private banking client also way access features using hamburger menu bottom opens list options like typing slash slack personal finance managers pfms controlling home budget popular additions banking systems complicated often hidden deep nested menus need lot users attention people really use steven walker forrester research written bankbot provide ask expenses month car expenses show simple chart relevant information pull mechanism bankbot also proactive pushing important information user warn close exceeding monthly budget remind regular payments usually make month remind pay credit card pay tax suggest better options save invest money show much earn offer loan probably need offer travel insurance knows youve bought plane tickets upsell better account credit card notice youve got pay rise alert something stocks portfolio chat banking nice desktop even effective mobile type words done like sending sms talk bankbot speech2text authentication provided fingerprint sensor receive important alerts push notifications phone smartwatch immediately take action dismiss even get discount health insurance based physical activity data fitness band apple watch bankbot also live inside smart devices like amazon echo provides api developers smart home smart banking mixed together inside facebook messenger chat second payment services directive transposed national regulations across european union 2016 goal open banking market psd2 force banks provide access via apis customer accounts provide account information third party service providers account holder wishes called access account xs2a optional banks evolve third parties enter space psd2 defines traditional financial institutions banks account servicing payment service providers psp new players account information service providers aisp payment initiation service providers pisp pisps aisps register competent authority home member state security reasons implications system quality banking user interfaces extremely important banks clients could choose manage account third party provider app better ux functionality cutting direct communication bank case bank reduced dumb pipe value chain fighting providing third parties minimum apis required may bad strategy banks think open actively partnering financial institutions retailers merchants startups imagine k2 bank solution providing appstore based apis users able give permission third party service providers way allow applications access facebook twitter account today able buy stuff authorized retailer without logging bank without visiting retailer site bank app need provide credit card number probably even shipping address data bank automatically offer purchase installments give discount history frequent past transactions online offline retailer need customer loyalty cards anymore bank become advertising channel retailers offering personalized promotions customers optout cellphone contract ending bankbot messages really great offer plan cheap newest iphone buy instantly one click would mind building thriving ecosystems banks third parties win hope customers want know k2 bank solution design technology behind bankbot possibilities implementation dont hesitate contact us course conversational interfaces like bankbot used banking also insurance online commerce travel healthcare many industries please write maciej lipiec k2s user experience director maciejlipieck2pl read k2 bank article chatbots magazine also please check project behance k2 internet leading digital product design communications agency poland develop digital services apps websites strong focus user experience longtime experience partnering financial institutions last 10 years helped envision design develop 10 transactional systems biggest banks poland stanusch technologies k2 banks technology provider bankbot company involved research development use artificial intelligence business carry projects related natural language processing semantic information retrieval become world leader number carried projects virtual advisorschatbots thank enjoyed reading please share quick cheer standing ovation clap show much enjoyed story product design director k2 k2 internet leading digital product design communications agency poland,en,"['K2 Bank', 'BankBot', 'SquareCash', 'Hamburger', 'Forrester Research', 'Apple Watch', 'Payment Services Directive', 'the European Union', 'XS2A', 'AS PSP', 'PISP', 'AISPs', 'the “competent authority”', 'third party', 'AppStore', 'yours bank', 'Chatbots Magazine:', 'Stanusch Technologies', 'K2 Bank’s']"
164,Camron Godbout,341,TensorFlow in a Nutshell — Part Three: All the Models,"Make sure to check out the other articles here.
In this installment we will be going over all the abstracted models that are currently available in TensorFlow and describe use cases for that particular model as well as simple sample code. Full sources of working examples are in the TensorFlow In a Nutshell repo.
Use Cases: Language Modeling, Machine translation, Word embedding, Text processing.
Since the advent of Long Short Term Memory and Gated Recurrent Units, Recurrent Neural Networks have made leaps and bounds above other models in natural language processing. They can be fed vectors representing characters and be trained to generate new sentences based on the training set. The merit in this model is that it keeps the context of the sentence and derives meaning that “cat sat on the mat” means the cat is on the mat. Since the creation of TensorFlow writing these networks have become increasingly simpler. There are even hidden features covered by Denny Britz here that make writing RNN’s even simpler heres a quick example.
Use Cases: Image processing, Facial recognition, Computer Vision
Convolution Neural Networks are unique because they’re created in mind that the input will be an image. CNNs perform a sliding window function to a matrix. The window is called a kernel and it slides across the image creating a convolved feature.
Creating a convolved feature allows for edge detection which then allows for a network to depict objects from pictures.
The convolved feature to create this looks like this matrix below:
Here’s a sample of code to identify handwritten digits from the MNIST dataset.
Use Cases: Classification and Regression
These networks consist of perceptrons in layers that take inputs that pass information on to the next layer. The last layer in the network produces the output. There is no connection between each node in a given layer. The layer that has no original input and no final output is called the hidden layer.
The goal of this network is similar to other supervised neural networks using back propagation, to make inputs have the desired trained outputs. These are some of the simplest effective neural networks for classification and regression problems. We will show how easy it is to create a feed forward network to classify handwritten digits:
Use Cases: Classification and Regression
Linear models take X values and produce a line of best fit used for classification and regression of Y values. For example if you have a list of house sizes and their price in a neighborhood you can predict the price of house given the size using a linear model.
One thing to note is that linear models can be used for multiple X features. For example in the housing example we can create a linear model given house sizes, how many rooms, how many bathrooms and price and predict price given a house with size, # of rooms, # of bathrooms.
Use Cases: Currently only Binary Classification
The general idea behind a SVM is that there is an optimal hyperplane for linearly separable patterns. For data that is not linearly separable we can use a kernel function to transform the original data into a new space. SVMs maximize the margin around separating the hyperplane. They work extremely well in high dimensional spaces and and are still effective if the dimensions are greater than the number of samples.
Use Cases: Recommendation systems, Classification and Regression
Deep and Wide models were covered with greater detail in part two, so we won’t get too heavy here. A Wide and Deep Network combines a linear model with a feed forward neural net so that our predictions will have memorization and generalization. This type of model can be used for classification and regression problems. This allows for less feature engineering with relatively accurate predictions. Thus, getting the best of both worlds. Here’s a code snippet from part two’s github.
Use Cases: Classification and Regression
Random Forest model takes many different classification trees and each tree votes for that class. The forest chooses the classification having the most votes.
Random Forests do not overfit, you can run as many treees as you want and it is relatively fast. Give it a try on the iris data with this snippet below:
Use Cases: Classification and Regression
In the contrib folder of TensorFlow there is a library called BayesFlow. BayesFlow has no documentation except for an example of the REINFORCE algorithm. This algorithm is proposed in a paper by Ronald Williams.
This network trying to solve an immediate reinforcement learning task, adjusts the weights after getting the reinforcement value at each trial. At the end of each trial each weight is incremented by a learning rate factor multiplied by the reinforcement value minus the baseline multiplied by characteristic eligibility. Williams paper also discusses the use of back propagation to train the REINFORCE network.
Use Cases: Sequential Data
CRFs are conditional probability distributions that factoirze according to an undirected model. They predict a label for a single sample keeping context from the neighboring samples. CRFs are similar to Hidden Markov Models. CRFs are often used for image segmentation and object recognition, as well as shallow parsing, named entity recognition and gene finding.
Ever since TensorFlow has been released the community surrounding the project has been adding more packages, examples and cases for using this amazing library. Even at the time of writing this article there are more models and sample code being written. It is amazing to see how much TensorFlow as grown in these past few months. The ease of use and diversity in the package are increasing overtime and don’t seem to be slowing down anytime soon.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder & CTO of Apteo: Researching machine learning techniques to improve investing. Come join us!
how hackers start their afternoons.
",make sure check articles installment going abstracted models currently available tensorflow describe use cases particular model well simple sample code full sources working examples tensorflow nutshell repo use cases language modeling machine translation word embedding text processing since advent long short term memory gated recurrent units recurrent neural networks made leaps bounds models natural language processing fed vectors representing characters trained generate new sentences based training set merit model keeps context sentence derives meaning cat sat mat means cat mat since creation tensorflow writing networks become increasingly simpler even hidden features covered denny britz make writing rnns even simpler heres quick example use cases image processing facial recognition computer vision convolution neural networks unique theyre created mind input image cnns perform sliding window function matrix window called kernel slides across image creating convolved feature creating convolved feature allows edge detection allows network depict objects pictures convolved feature create looks like matrix heres sample code identify handwritten digits mnist dataset use cases classification regression networks consist perceptrons layers take inputs pass information next layer last layer network produces output connection node given layer layer original input final output called hidden layer goal network similar supervised neural networks using back propagation make inputs desired trained outputs simplest effective neural networks classification regression problems show easy create feed forward network classify handwritten digits use cases classification regression linear models take x values produce line best fit used classification regression values example list house sizes price neighborhood predict price house given size using linear model one thing note linear models used multiple x features example housing example create linear model given house sizes many rooms many bathrooms price predict price given house size rooms bathrooms use cases currently binary classification general idea behind svm optimal hyperplane linearly separable patterns data linearly separable use kernel function transform original data new space svms maximize margin around separating hyperplane work extremely well high dimensional spaces still effective dimensions greater number samples use cases recommendation systems classification regression deep wide models covered greater detail part two wont get heavy wide deep network combines linear model feed forward neural net predictions memorization generalization type model used classification regression problems allows less feature engineering relatively accurate predictions thus getting best worlds heres code snippet part twos github use cases classification regression random forest model takes many different classification trees tree votes class forest chooses classification votes random forests overfit run many treees want relatively fast give try iris data snippet use cases classification regression contrib folder tensorflow library called bayesflow bayesflow documentation except example reinforce algorithm algorithm proposed paper ronald williams network trying solve immediate reinforcement learning task adjusts weights getting reinforcement value trial end trial weight incremented learning rate factor multiplied reinforcement value minus baseline multiplied characteristic eligibility williams paper also discusses use back propagation train reinforce network use cases sequential data crfs conditional probability distributions factoirze according undirected model predict label single sample keeping context neighboring samples crfs similar hidden markov models crfs often used image segmentation object recognition well shallow parsing named entity recognition gene finding ever since tensorflow released community surrounding project adding packages examples cases using amazing library even time writing article models sample code written amazing see much tensorflow grown past months ease use diversity package increasing overtime dont seem slowing anytime soon quick cheer standing ovation clap show much enjoyed story cofounder cto apteo researching machine learning techniques improve investing come join us hackers start afternoons,en,"['TensorFlow', 'Computer Vision\n', 'MNIST', 'Linear', 'Deep Network', 'Random Forest', 'Random Forests', 'BayesFlow', 'REINFORCE', 'Hidden Markov Models', 'Co-founder & CTO', 'Apteo']"
165,Dominik Felix,286,How to Create a Chatbot Without Coding a Single Line,"Chatbots are ready to succeed. If you think you have to hack days or even weeks to create a chatbot, you might be wrong. You don’t have to be aware of any coding skills. Immediately after big players like Facebook Messenger or Skype opened their platform for programmers many tools emerged. With this article I want to give you an introduction to mockup and overview of different tools to build your first chatbot.
You’re having an idea? You want to show your use case? It’s definitely recommendable to mockup your story beforehand. First, you may find some bugs in your concept. Moreover, you will be able to explain a showcase to noninvolved people based on the motto: “fake it ’til you make it”.
It’s very intuitive storytelling. Just insert what the user says and what the bot responds. Using the settings option, you can edit smartphone models, decide number of fans, and choose a profile picture, a page category and a welcome message. Additional features are buttons, images and quick replies. The whole story acts like a movie by pushing the play button. It can be shared by just one click and it’s possible to save the file as mp4 within the paid plan.
Each of the tools supports different platforms. Therefore, please keep in mind that it’s important to choose your platform wisely. Based on the huge range most of the tools make use of Facebook Messenger.
Chatfuel is focused on Facebook Messenger. You don’t need any coding skills to get started. It’s simple to create different logic blocks and link them to respective triggers. It offers great plugins e.g. human take-over and a minimalistic AI. In case you were recently starting with bots, I can recommend you this service.
Motion provides SMS, Email, web-chat, Facebook Messenger and Slack. Furthermore, it’s possible to link to (other) APIs and hook back to motion. Thus, it operates as a hub. The conversation is built with flowcharts and based on connectors and prepared modules. It just takes a few minutes to get familiar with the procedure.
Founder/CEO of Motion AI David Nelson’s “Chatbots Made Easy”
api.ai is a great platform for developing chatbots. It has AI support and an intuitive interface. It requires only one click to assemble i.e. small-talk or weather features. On the one hand, it’s possible to run the bot exclusively on their servers. On the other, you can download a nodejs sample code to execute it on your infrastructure. To sum up, API.AI is an advanced service, being the reason why it’s more complicated to build a bot using this tool. Unsurprisingly, it got bought by Google a few days ago.
Featured CBM: API.AI “Small Talk” is Now Open! Why is it a Big Deal?
Flow XO offers a graphical interface to build so-called flows which define how your bot will operate to received messages or audio. It has a huge list of integrations. As a consequence, it’s more complex than Chatfuel, but also a lot more flexible. Pretty amazing is their support on Messenger, Slack, SMS and Telegram.
They’ve an interesting approach to build chatbots. It guides you through 4 steps: design, develop, launch and grow. First, you’ve to design the content: messages, persistent menus, welcome messages and some more. As step 2, it wants you to link messages to triggers and setup curious modules like ‘Offer Human Help’. The launch step leads you through the review process, while the final step focuses on customer retention i.e. schedule messages, user lists, etc.
Manychat allows broadcast content from RSS feeds. Additionally, it’s possible to link to yahoo pipelines and broadcast everything you want. It supports scheduled messages, auto posting from RSS, Facebook, Twitter, YouTube and has a basic mechanism to send specific answers to specific keywords. Watch their pitch to get a better understanding.
MindIQ is a DIY Bot Builder platform for businesses focused on Facebook Messenger. You don’t need any coding skills and they make it dead simple for businesses to build bots. They follow a template approach. Currently, the templates available are media, commerce, and food tech. They also provide tools to link your business tools like Mailchimp to your chatbot.
There are many tools on the market. Every tool solves other problems and each of them uses a different approach for how to design user interaction. I really like the simplicity of Chatfuel and the 4-step-process of Botsify. Since all of these tools are quite new, I’m super excited and looking forward to seeing the direction that will be pursued and developed.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
BotSpot Vienna, Agentur Volk, Chatbot Ecosystem, Botstack Framework
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.
",chatbots ready succeed think hack days even weeks create chatbot might wrong dont aware coding skills immediately big players like facebook messenger skype opened platform programmers many tools emerged article want give introduction mockup overview different tools build first chatbot youre idea want show use case definitely recommendable mockup story beforehand first may find bugs concept moreover able explain showcase noninvolved people based motto fake til make intuitive storytelling insert user says bot responds using settings option edit smartphone models decide number fans choose profile picture page category welcome message additional features buttons images quick replies whole story acts like movie pushing play button shared one click possible save file mp4 within paid plan tools supports different platforms therefore please keep mind important choose platform wisely based huge range tools make use facebook messenger chatfuel focused facebook messenger dont need coding skills get started simple create different logic blocks link respective triggers offers great plugins eg human takeover minimalistic ai case recently starting bots recommend service motion provides sms email webchat facebook messenger slack furthermore possible link apis hook back motion thus operates hub conversation built flowcharts based connectors prepared modules takes minutes get familiar procedure founderceo motion ai david nelsons chatbots made easy apiai great platform developing chatbots ai support intuitive interface requires one click assemble ie smalltalk weather features one hand possible run bot exclusively servers download nodejs sample code execute infrastructure sum apiai advanced service reason complicated build bot using tool unsurprisingly got bought google days ago featured cbm apiai small talk open big deal flow xo offers graphical interface build socalled flows define bot operate received messages audio huge list integrations consequence complex chatfuel also lot flexible pretty amazing support messenger slack sms telegram theyve interesting approach build chatbots guides 4 steps design develop launch grow first youve design content messages persistent menus welcome messages step 2 wants link messages triggers setup curious modules like offer human help launch step leads review process final step focuses customer retention ie schedule messages user lists etc manychat allows broadcast content rss feeds additionally possible link yahoo pipelines broadcast everything want supports scheduled messages auto posting rss facebook twitter youtube basic mechanism send specific answers specific keywords watch pitch get better understanding mindiq diy bot builder platform businesses focused facebook messenger dont need coding skills make dead simple businesses build bots follow template approach currently templates available media commerce food tech also provide tools link business tools like mailchimp chatbot many tools market every tool solves problems uses different approach design user interaction really like simplicity chatfuel 4stepprocess botsify since tools quite new im super excited looking forward seeing direction pursued developed quick cheer standing ovation clap show much enjoyed story botspot vienna agentur volk chatbot ecosystem botstack framework chatbots ai nlp facebook messenger slack telegram,en,"['Skype', 'api.ai', 'API.AI', 'Google', 'Telegram', 'RSS', 'yahoo pipelines', 'Mailchimp', 'BotSpot Vienna', 'AI', 'NLP']"
166,Greg Gascon,368,How Invisible Interfaces are going to transform the way we interact with computers,"In the mid-nineties, a computer scientist at Xerox PARC theorized the concept of the Internet of Things, albeit with a different name, far before anyone else had and even further still before it had become possible.
Even though today we call it by that name, Ubiquitous Computing — as it was then coined by Mark Weiser — imagined a world wherein cheap and ubiquitous connected computing would radically alter the way we use and interact with computers. The idea was ahead of its time. In the world of ubiquitous computing, connected devices would become cheap and, thereby, would exist everywhere.
Importantly, these devices would as a result cease to become special or unique — they would become invisible.
As we near this utopian world filled with computers, our relationship with them inexorably will change. Each of us will come to interact with dozens of separate devices on a daily basis. As such, we will need to develop interfaces in a way so as not to distract us, as is currently done, but in a way in which to empower us.
Or, how Weiser put it, we will need to adopt the concepts of “Calm Technology”.
On the face of it, ubiquitous computing is just that, a reality in which computers are everywhere. Of course, with trends relating to IoT, we are nearing this, but we are not there yet. One of the most important implications to come from ubiquitous computing, for example, will be the changes it will make on how we perceive and interact with computers.
For instance, think of the electric motor: an old technology that is ubiquitous in the present. Today, there could be dozens of them in a single car. However, when we hit a button to roll down the windows, we don’t think at all about the motor pulling the window down. We simply think about the action of making the window go down. The electric motor is so mundane and ubiquitous in our lives that we don’t even think about it when using it. It is invisible.
It is this sort of invisibility that allows the user to take full control of their interactions with a given piece of technology. When using a piece of technology that has become invisible, the user thinks of using it in terms of end goals, rather than getting bogged down in the technology itself. The user doesn’t have to worry how it is going to work, they just make it happen.
In another example, Weiser simply states a good pencil “stays out of the way of the writing”. Now, even though technology surrounds us today, we aren’t at this point yet. Gadgets and devices are still special to us in a distracting way. We still not only still marvel at new technology, we are told to by whomever is producing it.
But why does this matter? The best way to see how ubiquitous computing will impact us is to examine the way we engineer and interact with the apps that exist today.
When creating a web app, for instance, you try to guide or manipulate the user into using your tool as much as possible. When you create a drip marketing email campaign for it, in most cases, you aren’t creating it so that the user needs to use your tool less. You are creating it so they can spend more time and use all of its features. That is to say, the goal isn’t foremost and necessarily to save the user time. Furthermore, there is no question asked as to whether the user aught to spend more time using whatever particular app is being optimized.
Within a social media website, each user is given a piece of “social property”. A social media platform imbues each social property with a value system — think of the concept of likes, comments or shares — as incentive to spend time on the site. Each user interaction with a social property, whether it be a photo or a comment that is written, is then logged and recorded, so they can easily be rewarded for the time invested. Some social apps, such as LinkedIn, will have us hooked for something as simple as a pageview of our profiles.
These actions are further incentivized through the use of gamification. Apps send intrusive notifications, giving you some information about what they are about, but not everything. And this is crucial. Not knowing what is in the notification entices us to open it even further. It goes without saying, this is important for increasing the amount of screen time we give the app. For, if we saw everything in the notification, there would be no point in opening the app. It makes waking up every morning feel like opening a bunch of small presents.
And, while it’s a stretch to say that developers are acting nefariously to steal our time, those building our web services and tools should construct them with respect to the user’s guilelessness. Doing so requires adopting principles of invisible or calm technology.
Contradiction aside, the most accessible way we can get a glimpse into a future dominated by invisible interfaces is the movie “Her”. Although not the focus of the film, “Her” showcases a future wherein inputs given to devices are done so largely through voice commands. Yes, there are still smartphones, but the majority of interactions take place by simply talking to a given device using natural language.
Theodore is able to interact with technology in a manner that is completely at hand. He can ask any sort of question or create any sort of demand without getting bogged down in how the device works. Furthermore, the technology never tries to whisk his attention away from anything. The technology is always there, but it is only in the periphery.
According to Weiser, this is one of the key principles of designing calm technology. The device in question should never try to distract or pry the user away from what they are trying to accomplish. Yet, it must always be ready to accept user input. It is calming in the exact opposite way that receiving group chat notifications on your phone is not.
We can see this principle of design, in part, at play in the new Apple AirPods. Even though they have yet to be released, they promise to let us interact with the internet without ever needing to look down at our phones. And they are aware of their environment too.
They know such things like if they are in an ear or not, and, if they are not, they know to stop playing sound. It’s these small, micro-automations that will further make technology invisible and allow us to focus on whatever it is that we want from the technology and not worry about having to configure it.
Other, more simple, examples include the auto-brightness on your phone or its fingerprint scanner. They simply work without any sort of configuration or notification about what they are doing.
And more technologies like this are coming. There are, today, even advocacy groups such as Time Well Spent that try to spread awareness about how interfaces and apps can hijack the ways our brains work. Even more promising is that there are companies that are following suit in these designs principles. For instance, the upcoming Moment smartwatch is a device which interfaces with the user largely through touch feedback, instead of relying on the screen.
All that’s needed now? Better speech recognition.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Tech Columnist // Apps Script Dev // Social Media Automator // SEO Specialist. Read more at https://www.gregorygascon.com
The life, work, and tactics of entrepreneurs around the world - by founders, for founders. Welcoming submissions on technology trends, product design, growth strategies, and venture investing.
",midnineties computer scientist xerox parc theorized concept internet things albeit different name far anyone else even still become possible even though today call name ubiquitous computing coined mark weiser imagined world wherein cheap ubiquitous connected computing would radically alter way use interact computers idea ahead time world ubiquitous computing connected devices would become cheap thereby would exist everywhere importantly devices would result cease become special unique would become invisible near utopian world filled computers relationship inexorably change us come interact dozens separate devices daily basis need develop interfaces way distract us currently done way empower us weiser put need adopt concepts calm technology face ubiquitous computing reality computers everywhere course trends relating iot nearing yet one important implications come ubiquitous computing example changes make perceive interact computers instance think electric motor old technology ubiquitous present today could dozens single car however hit button roll windows dont think motor pulling window simply think action making window go electric motor mundane ubiquitous lives dont even think using invisible sort invisibility allows user take full control interactions given piece technology using piece technology become invisible user thinks using terms end goals rather getting bogged technology user doesnt worry going work make happen another example weiser simply states good pencil stays way writing even though technology surrounds us today arent point yet gadgets devices still special us distracting way still still marvel new technology told whomever producing matter best way see ubiquitous computing impact us examine way engineer interact apps exist today creating web app instance try guide manipulate user using tool much possible create drip marketing email campaign cases arent creating user needs use tool less creating spend time use features say goal isnt foremost necessarily save user time furthermore question asked whether user aught spend time using whatever particular app optimized within social media website user given piece social property social media platform imbues social property value system think concept likes comments shares incentive spend time site user interaction social property whether photo comment written logged recorded easily rewarded time invested social apps linkedin us hooked something simple pageview profiles actions incentivized use gamification apps send intrusive notifications giving information everything crucial knowing notification entices us open even goes without saying important increasing amount screen time give app saw everything notification would point opening app makes waking every morning feel like opening bunch small presents stretch say developers acting nefariously steal time building web services tools construct respect users guilelessness requires adopting principles invisible calm technology contradiction aside accessible way get glimpse future dominated invisible interfaces movie although focus film showcases future wherein inputs given devices done largely voice commands yes still smartphones majority interactions take place simply talking given device using natural language theodore able interact technology manner completely hand ask sort question create sort demand without getting bogged device works furthermore technology never tries whisk attention away anything technology always periphery according weiser one key principles designing calm technology device question never try distract pry user away trying accomplish yet must always ready accept user input calming exact opposite way receiving group chat notifications phone see principle design part play new apple airpods even though yet released promise let us interact internet without ever needing look phones aware environment know things like ear know stop playing sound small microautomations make technology invisible allow us focus whatever want technology worry configure simple examples include autobrightness phone fingerprint scanner simply work without sort configuration notification technologies like coming today even advocacy groups time well spent try spread awareness interfaces apps hijack ways brains work even promising companies following suit designs principles instance upcoming moment smartwatch device interfaces user largely touch feedback instead relying screen thats needed better speech recognition quick cheer standing ovation clap show much enjoyed story tech columnist apps script dev social media automator seo specialist read httpswwwgregorygasconcom life work tactics entrepreneurs around world founders founders welcoming submissions technology trends product design growth strategies venture investing,en,"['Xerox PARC', 'Weiser', 'LinkedIn', 'Apple AirPods', 'Time Well Spent', 'Moment', 'Tech', 'https://www.gregorygascon.com']"
167,Keval Patel,833,Turn your Raspberry Pi into homemade Google Home – Becoming Human: Artificial Intelligence Magazine,"Google Home is a beautiful device with built-in Google Assistant — A state of the art digital personal assistant by Google. — which you can place anywhere at your home and it will do some amazing things for you. It will save your reminders, shopping lists, notes and most importantly answers your questions and queries based on the context of the conversations.
In this article, you are going to learn to turn your Raspberry Pi into homemade Google Home device which is,
So, let’s get started.
Once you have all these things, login to Raspbian desktop and go to the following steps one by one.
As you can see your USB device is attached to card 1 and the device id is 0. Raspberry Pi recognizes card 0 as the internal sound card (which is bcm2835) and other external sound cards as external sound cards.
This will set your external mic (see pcm.mic) as the audio capture device (see in pcm!.default) and your inbuilt sound card (card 0) as the speaker device.
This will create Python 3 environment (As the Google Assistant library runs on Python 3.x only) in your raspberry pi and install required dependencies.
If instead, it displays: InvalidGrantError then an invalid code was entered. Try again.
You can run google-assistant-init.sh to initiate the Google Assistant any time.
1. Autostart with Pixel Desktop on Boot:
2. Autostart with CLI on Boot:
You can do many daily stuff with your Google Home. If you want to perform your custom tasks like turning off the light, opening the door, you can do it with integrating Google Actions in your Google Assistant. If you have any trouble with starting the Google Assistant, leave a comment below. I will try to resolve them.
~If you liked the article, click the 💚 below so more people can see it! Also, you can follow me on Medium or on My Blog, so you get updates regarding my future articles!!~
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
www.kevalpatel2106.com | Android Developer | Machine learner | Gopher | Open Source Contributor
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
",google home beautiful device builtin google assistant state art digital personal assistant google place anywhere home amazing things save reminders shopping lists notes importantly answers questions queries based context conversations article going learn turn raspberry pi homemade google home device lets get started things login raspbian desktop go following steps one one see usb device attached card 1 device id 0 raspberry pi recognizes card 0 internal sound card bcm2835 external sound cards external sound cards set external mic see pcmmic audio capture device see pcmdefault inbuilt sound card card 0 speaker device create python 3 environment google assistant library runs python 3x raspberry pi install required dependencies instead displays invalidgranterror invalid code entered try run googleassistantinitsh initiate google assistant time 1 autostart pixel desktop boot 2 autostart cli boot many daily stuff google home want perform custom tasks like turning light opening door integrating google actions google assistant trouble starting google assistant leave comment try resolve liked article click people see also follow medium blog get updates regarding future articles quick cheer standing ovation clap show much enjoyed story wwwkevalpatel2106com android developer machine learner gopher open source contributor latest news info tutorials artificial intelligence machine learning deep learning big data means humanity,en,"['Google Home', 'Google', 'Python', 'CLI', 'Google Actions', 'articles!!~', 'Latest News', 'Info', 'Tutorials on Artificial Intelligence, Machine Learning', 'Humanity']"
168,Eduard Tyantov,5400,Deep Learning Achievements Over the Past Year – Stats and Bots,"At Statsbot, we’re constantly reviewing the deep learning achievements to improve our models and product. Around Christmas time, our team decided to take stock of the recent achievements in deep learning over the past year (and a bit longer). We translated the article by a data scientist, Ed Tyantov, to tell you about the most significant developments that can affect our future.
Almost a year ago, Google announced the launch of a new model for Google Translate. The company described in detail the network architecture — Recurrent Neural Network (RNN).
The key outcome: closing down the gap with humans in accuracy of the translation by 55–85% (estimated by people on a 6-point scale). It is difficult to reproduce good results with this model without the huge dataset that Google has.
You probably heard the silly news that Facebook turned off its chatbot, which went out of control and made up its own language. This chatbot was created by the company for negotiations. Its purpose is to conduct text negotiations with another agent and reach a deal: how to divide items (books, hats, etc.) by two. Each agent has his own goal in the negotiations that the other does not know about. It’s impossible to leave the negotiations without a deal.
For training, they collected a dataset of human negotiations and trained a supervised recurrent network. Then, they took a reinforcement learning trained agent and trained it to talk with itself, setting a limit — the similarity of the language to human.
The bot has learned one of the real negotiation strategies — showing a fake interest in certain aspects of the deal, only to give up on them later and benefit from its real goals. It has been the first attempt to create such an interactive bot, and it was quite successful.
Full story is in this article, and the code is publicly available.
Certainly, the news that the bot has allegedly invented a language was inflated from scratch. When training (in negotiations with the same agent), they disabled the restriction of the similarity of the text to human, and the algorithm modified the language of interaction. Nothing unusual.
Over the past year, recurrent networks have been actively developed and used in many tasks and applications. The architecture of RNNs has become much more complicated, but in some areas similar results were achieved by simple feedforward-networks — DSSM. For example, Google has reached the same quality, as with LSTM previously, for its mail feature Smart Reply. In addition, Yandex launched a new search engine based on such networks.
Employees of DeepMind reported in their article about generating audio. Briefly, researchers made an autoregressive full-convolution WaveNet model based on previous approaches to image generation (PixelRNN and PixelCNN).
The network was trained end-to-end: text for the input, audio for the output. The researches got an excellent result as the difference compared to human has been reduced by 50%.
The main disadvantage of the network is a low productivity as, because of the autoregression, sounds are generated sequentially and it takes about 1–2 minutes to create one second of audio.
Look at... sorry, hear this example.
If you remove the dependence of the network on the input text and leave only the dependence on the previously generated phoneme, then the network will generate phonemes similar to the human language, but they will be meaningless.
Hear the example of the generated voice.
This same model can be applied not only to speech, but also, for example, to creating music. Imagine audio generated by the model, which was taught using the dataset of a piano game (again without any dependence on the input data).
Read a full version of DeepMind research if you’re interested.
Lip reading is another deep learning achievement and victory over humans.
Google Deepmind, in collaboration with Oxford University, reported in the article, “Lip Reading Sentences in the Wild” on how their model, which had been trained on a television dataset, was able to surpass the professional lip reader from the BBC channel.
There are 100,000 sentences with audio and video in the dataset. Model: LSTM on audio, and CNN + LSTM on video. These two state vectors are fed to the final LSTM, which generates the result (characters).
Different types of input data were used during training: audio, video, and audio + video. In other words, it is an “omnichannel” model.
The University of Washington has done a serious job of generating the lip movements of former US President Obama. The choice fell on him due to the huge number of his performance recordings online (17 hours of HD video).
They couldn’t get along with just the network as they got too many artifacts. Therefore, the authors of the article made several crutches (or tricks, if you like) to improve the texture and timings.
You can see that the results are amazing. Soon, you couldn’t trust even the video with the president.
In their post and article, Google Brain Team reported on how they introduced a new OCR (Optical Character Recognition) engine into its Maps, through which street signs and store signs are recognized.
In the process of technology development, the company compiled a new FSNS (French Street Name Signs), which contains many complex cases.
To recognize each sign, the network uses up to four of its photos. The features are extracted with the CNN, scaled with the help of the spatial attention (pixel coordinates are taken into account), and the result is fed to the LSTM.
The same approach is applied to the task of recognizing store names on signboards (there can be a lot of “noise” data, and the network itself must “focus” in the right places). This algorithm was applied to 80 billion photos.
There is a type of task called visual reasoning, where a neural network is asked to answer a question using a photo. For example: “Is there a same size rubber thing in the picture as a yellow metal cylinder?” The question is truly nontrivial, and until recently, the problem was solved with an accuracy of only 68.5%.
And again the breakthrough was achieved by the team from Deepmind: on the CLEVR dataset they reached a super-human accuracy of 95.5%.
The network architecture is very interesting:
An interesting application of neural networks was created by the company Uizard: generating a layout code according to a screenshot from the interface designer.
This is an extremely useful application of neural networks, which can make life easier when developing software. The authors claim that they reached 77% accuracy. However, this is still under research and there is no talk on real usage yet.
There is no code or dataset in open source, but they promise to upload it.
Perhaps you’ve seen Quick, Draw! from Google, where the goal is to draw sketches of various objects in 20 seconds. The corporation collected this dataset in order to teach the neural network to draw, as Google described in their blog and article.
The collected dataset consists of 70 thousand sketches, which eventually became publicly available. Sketches are not pictures, but detailed vector representations of drawings (at which point the user pressed the “pencil,” released where the line was drawn, and so on).
Researchers have trained the Sequence-to-Sequence Variational Autoencoder (VAE) using RNN as a coding/decoding mechanism.
Eventually, as befits the auto-encoder, the model received a latent vector that characterizes the original picture.
Whereas the decoder can extract a drawing from this vector, you can change it and get new sketches.
And even perform vector arithmetic to create a catpig:
One of the hottest topics in Deep Learning is Generative Adversarial Networks (GANs). Most often, this idea is used to work with images, so I will explain the concept using them.
The idea is in the competition of two networks — the generator and the discriminator. The first network creates a picture, and the second one tries to understand whether the picture is real or generated.
Schematically it looks like this:
During training, the generator from a random vector (noise) generates an image and feeds it to the input of the discriminator, which says whether it is fake or not. The discriminator is also given real images from the dataset.
It is difficult to train such construction, as it is hard to find the equilibrium point of two networks. Most often the discriminator wins and the training stagnates. However, the advantage of the system is that we can solve problems in which it is difficult for us to set the loss-function (for example, improving the quality of the photo) — we give it to the discriminator.
A classic example of the GAN training result is pictures of bedrooms or people
Previously, we considered the auto-coding (Sketch-RNN), which encodes the original data into a latent representation. The same thing happens with the generator.
The idea of generating an image using a vector is clearly shown in this project in the example of faces. You can change the vector and see how the faces change.
The same arithmetic works over the latent space: “a man in glasses” minus “a man” plus a “woman” is equal to “a woman with glasses.”
If you teach a controlled parameter to the latent vector during training, when you generate it, you can change it and so manage the necessary image in the picture. This approach is called conditional GAN.
So did the authors of the article, “Face Aging With Conditional Generative Adversarial Networks.” Having trained the engine on the IMDB dataset with a known age of actors, the researchers were given the opportunity to change the face age of the person.
Google has found another interesting application to GAN — the choice and improvement of photos. GAN was trained on a professional photo dataset: the generator is trying to improve bad photos (professionally shot and degraded with the help of special filters), and the discriminator — to distinguish “improved” photos and real professional ones.
A trained algorithm went through Google Street View panoramas in search of the best composition and received some pictures of professional and semi-professional quality (as per photographers’ rating).
An impressive example of GANs is generating images using text.
The authors of this research suggest embedding text into the input of not only a generator (conditional GAN), but also a discriminator, so that it verifies the correspondence of the text to the picture. In order to make sure the discriminator learned to perform his function, in addition to training they added pairs with an incorrect text for the real pictures.
One of the eye-catching articles of 2016 is, “Image-to-Image Translation with Conditional Adversarial Networks” by Berkeley AI Research (BAIR). Researchers solved the problem of image-to-image generation, when, for example, it was required to create a map using a satellite image, or realistic texture of the objects using their sketch.
Here is another example of the successful performance of conditional GANs. In this case, the condition goes to the whole picture. Popular in image segmentation, UNet was used as the architecture of the generator, and a new PatchGAN classifier was used as a discriminator for combating blurred images (the picture is cut into N patches, and the prediction of fake/real goes for each of them separately).
Christopher Hesse made the nightmare cat demo, which attracted great interest from the users.
You can find a source code here.
In order to apply Pix2Pix, you need a dataset with the corresponding pairs of pictures from different domains. In the case, for example, with cards, it is not a problem to assemble such a dataset. However, if you want to do something more complicated like “transfiguring” objects or styling, then pairs of objects cannot be found in principle.
Therefore, authors of Pix2Pix decided to develop their idea and came up with CycleGAN for transfer between different domains of images without specific pairs — “Unpaired Image-to-Image Translation.”
The idea is to teach two pairs of generator-discriminators to transfer the image from one domain to another and back, while we require a cycle consistency — after a sequential application of the generators, we should get an image similar to the original L1 loss. A cyclic loss is required to ensure that the generator did not just begin to transfer pictures of one domain to pictures from another domain, which are completely unrelated to the original image.
This approach allows you to learn the mapping of horses -> zebras.
Such transformations are unstable and often create unsuccessful options:
You can find a source code here.
Machine learning is now coming to medicine. In addition to recognizing ultrasound, MRI, and diagnosis, it can be used to find new drugs to fight cancer.
We already reported in detail about this research. Briefly, with the help of Adversarial Autoencoder (AAE), you can learn the latent representation of molecules and then use it to search for new ones. As a result, 69 molecules were found, half of which are used to fight cancer, and the others have serious potential.
Topics with adversarial-attacks are actively explored. What are adversarial-attacks? Standard networks trained, for example, on ImageNet, are completely unstable when adding special noise to the classified picture. In the example below, we see that the picture with noise for the human eye is practically unchanged, but the model goes crazy and predicts a completely different class.
Stability is achieved with, for example, the Fast Gradient Sign Method (FGSM): having access to the parameters of the model, you can make one or several gradient steps towards the desired class and change the original picture.
One of the tasks on Kaggle is related to this: the participants are encouraged to create universal attacks/defenses, which are all eventually run against each other to determine the best.
Why should we even investigate these attacks? First, if we want to protect our products, we can add noise to the captcha to prevent spammers from recognizing it automatically. Secondly, algorithms are more and more involved in our lives — face recognition systems and self-driving cars. In this case, attackers can use the shortcomings of the algorithms.
Here is an example of when special glasses allow you to deceive the face recognition system and “pass yourself off as another person.” So, we need to take possible attacks into account when teaching models.
Such manipulations with signs also do not allow them to be recognized correctly.
• A set of articles from the organizers of the contest.• Already written libraries for attacks: cleverhans and foolbox.
Reinforcement learning (RL), or learning with reinforcement is also one of the most interesting and actively developing approaches in machine learning.
The essence of the approach is to learn the successful behavior of the agent in an environment that gives a reward through experience — just as people learn throughout their lives.
RL is actively used in games, robots, and system management (traffic, for example).
Of course, everyone has heard about AlphaGo’s victories in the game over the best professionals. Researchers were using RL for training: the bot played with itself to improve its strategies.
In previous years, DeepMind had learned using DQN to play arcade games better than humans. Currently, algorithms are being taught to play more complex games like Doom.
Much of the attention is paid to learning acceleration because experience of the agent in interaction with the environment requires many hours of training on modern GPUs.
In his blog, Deepmind reported that the introduction of additional losses (auxiliary tasks), such as the prediction of a frame change (pixel control) so that the agent better understands the consequences of the actions, significantly speeds up learning.
Learning results:
4.2. Learning robotsIn OpenAI, they have been actively studying an agent’s training by humans in a virtual environment, which is safer for experiments than in real life.
In one of the studies, the team showed that one-shot learning is possible: a person shows in VR how to perform a certain task, and one demonstration is enough for the algorithm to learn it and then reproduce it in real conditions.
If only it was so easy with people. :)
Here is the work of OpenAI and DeepMind on the same topic. The bottom line is that an agent has a task, the algorithm provides two possible solutions for the human and indicates which one is better. The process is repeated iteratively and the algorithm for 900 bits of feedback (binary markup) from the person learned how to solve the problem.
As always, the human must be careful and think of what he is teaching the machine. For example, the evaluator decided that the algorithm really wanted to take the object, but in fact, he just simulated this action.
There is another study from DeepMind. To teach the robot complex behavior (walk, jump, etc.), and even do it similar to the human, you have to be heavily involved with the choice of the loss function, which will encourage the desired behavior. However, it would be preferable that the algorithm learned complex behavior itself by leaning with simple rewards.
Researchers managed to achieve this: they taught agents (body emulators) to perform complex actions by constructing a complex environment with obstacles and with a simple reward for progress in movement.
You can watch the impressive video with results. However, it’s much more fun to watch it with a superimposed sound!
Finally, I will give a link to the recently published algorithms for learning RL from OpenAI. Now you can use more advanced solutions than the standard DQN.
In July 2017, Google reported that it took advantage of DeepMind’s development in machine learning to reduce the energy costs of its data center.
Based on the information from thousands of sensors in the data center, Google developers trained a neural network ensemble to predict PUE (Power Usage Effectiveness) and more efficient data center management. This is an impressive and significant example of the practical application of ML.
As you know, trained models are poorly transferred from task to task, as each task has to be trained for a specific model. A small step towards the universality of the models was done by Google Brain in his article “One Model To Learn The All.”
Researchers have trained a model that performs eight tasks from different domains (text, speech, and images). For example, translation from different languages, text parsing, and image and sound recognition.
In order to achieve this, they built a complex network architecture with various blocks to process different input data and generate a result. The blocks for the encoder/decoder fall into three types: convolution, attention, and gated mixture of experts (MoE).
Main results of learning:
By the way, this model is present in tensor2tensor.
In their post, Facebook staff told us how their engineers were able to teach the Resnet-50 model on Imagenet in just one hour. Truth be told, this required a cluster of 256 GPUs (Tesla P100).
They used Gloo and Caffe2 for distributed learning. To make the process effective, it was necessary to adapt the learning strategy with a huge batch (8192 elements): gradient averaging, warm-up phase, special learning rate, etc.
As a result, it was possible to achieve an efficiency of 90% when scaling from 8 to 256 GPU. Now researchers from Facebook can experiment even faster, unlike mere mortals without such a cluster.
The self-driving car sphere is intensively developing, and the cars are actively tested. From the relatively recent events, we can note the purchase of Intel MobilEye, the scandals around Uber and Google technologies stolen by their former employee, the first death when using an autopilot, and much more.
I will note one thing: Google Waymo is launching a beta program. Google is a pioneer in this field, and it is assumed that their technology is very good because cars have been driven more than 3 million miles.
As to more recent events, self-driving cars have been allowed to travel across all US states.
As I said, modern ML is beginning to be introduced into medicine. For example, Google collaborates with a medical center to help with diagnosis.
Deepmind has even established a separate unit.
This year, under the program of the Data Science Bowl, there was a competition held to predict lung cancer in a year on the basis of detailed images with a prize fund of one million dollars.
Currently, there are heavy investments in ML as it was before with BigData.
China invested $150 billion in AI to become the world leader in the industry.
For comparison, Baidu Research employs 1,300 people, and in the same FAIR (Facebook) — 80. At the last KDD, Alibaba employees talked about their parameter server KungPeng, which runs on 100 billion samples with a trillion parameters, which “becomes a common task” ©.
You can draw your own conclusions, it’s never too late to study machine learning. In one way or another, over time, all developers will use machine learning, which will become one of the common skills, as it is today — the ability to work with databases.
Link to the original post.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Mail.ru Group, Head of Machine Learning Team
Data stories on machine learning and analytics. From Statsbot’s makers.
",statsbot constantly reviewing deep learning achievements improve models product around christmas time team decided take stock recent achievements deep learning past year bit longer translated article data scientist ed tyantov tell significant developments affect future almost year ago google announced launch new model google translate company described detail network architecture recurrent neural network rnn key outcome closing gap humans accuracy translation 5585 estimated people 6point scale difficult reproduce good results model without huge dataset google probably heard silly news facebook turned chatbot went control made language chatbot created company negotiations purpose conduct text negotiations another agent reach deal divide items books hats etc two agent goal negotiations know impossible leave negotiations without deal training collected dataset human negotiations trained supervised recurrent network took reinforcement learning trained agent trained talk setting limit similarity language human bot learned one real negotiation strategies showing fake interest certain aspects deal give later benefit real goals first attempt create interactive bot quite successful full story article code publicly available certainly news bot allegedly invented language inflated scratch training negotiations agent disabled restriction similarity text human algorithm modified language interaction nothing unusual past year recurrent networks actively developed used many tasks applications architecture rnns become much complicated areas similar results achieved simple feedforwardnetworks dssm example google reached quality lstm previously mail feature smart reply addition yandex launched new search engine based networks employees deepmind reported article generating audio briefly researchers made autoregressive fullconvolution wavenet model based previous approaches image generation pixelrnn pixelcnn network trained endtoend text input audio output researches got excellent result difference compared human reduced 50 main disadvantage network low productivity autoregression sounds generated sequentially takes 12 minutes create one second audio look sorry hear example remove dependence network input text leave dependence previously generated phoneme network generate phonemes similar human language meaningless hear example generated voice model applied speech also example creating music imagine audio generated model taught using dataset piano game without dependence input data read full version deepmind research youre interested lip reading another deep learning achievement victory humans google deepmind collaboration oxford university reported article lip reading sentences wild model trained television dataset able surpass professional lip reader bbc channel 100000 sentences audio video dataset model lstm audio cnn lstm video two state vectors fed final lstm generates result characters different types input data used training audio video audio video words omnichannel model university washington done serious job generating lip movements former us president obama choice fell due huge number performance recordings online 17 hours hd video couldnt get along network got many artifacts therefore authors article made several crutches tricks like improve texture timings see results amazing soon couldnt trust even video president post article google brain team reported introduced new ocr optical character recognition engine maps street signs store signs recognized process technology development company compiled new fsns french street name signs contains many complex cases recognize sign network uses four photos features extracted cnn scaled help spatial attention pixel coordinates taken account result fed lstm approach applied task recognizing store names signboards lot noise data network must focus right places algorithm applied 80 billion photos type task called visual reasoning neural network asked answer question using photo example size rubber thing picture yellow metal cylinder question truly nontrivial recently problem solved accuracy 685 breakthrough achieved team deepmind clevr dataset reached superhuman accuracy 955 network architecture interesting interesting application neural networks created company uizard generating layout code according screenshot interface designer extremely useful application neural networks make life easier developing software authors claim reached 77 accuracy however still research talk real usage yet code dataset open source promise upload perhaps youve seen quick draw google goal draw sketches various objects 20 seconds corporation collected dataset order teach neural network draw google described blog article collected dataset consists 70 thousand sketches eventually became publicly available sketches pictures detailed vector representations drawings point user pressed pencil released line drawn researchers trained sequencetosequence variational autoencoder vae using rnn codingdecoding mechanism eventually befits autoencoder model received latent vector characterizes original picture whereas decoder extract drawing vector change get new sketches even perform vector arithmetic create catpig one hottest topics deep learning generative adversarial networks gans often idea used work images explain concept using idea competition two networks generator discriminator first network creates picture second one tries understand whether picture real generated schematically looks like training generator random vector noise generates image feeds input discriminator says whether fake discriminator also given real images dataset difficult train construction hard find equilibrium point two networks often discriminator wins training stagnates however advantage system solve problems difficult us set lossfunction example improving quality photo give discriminator classic example gan training result pictures bedrooms people previously considered autocoding sketchrnn encodes original data latent representation thing happens generator idea generating image using vector clearly shown project example faces change vector see faces change arithmetic works latent space man glasses minus man plus woman equal woman glasses teach controlled parameter latent vector training generate change manage necessary image picture approach called conditional gan authors article face aging conditional generative adversarial networks trained engine imdb dataset known age actors researchers given opportunity change face age person google found another interesting application gan choice improvement photos gan trained professional photo dataset generator trying improve bad photos professionally shot degraded help special filters discriminator distinguish improved photos real professional ones trained algorithm went google street view panoramas search best composition received pictures professional semiprofessional quality per photographers rating impressive example gans generating images using text authors research suggest embedding text input generator conditional gan also discriminator verifies correspondence text picture order make sure discriminator learned perform function addition training added pairs incorrect text real pictures one eyecatching articles 2016 imagetoimage translation conditional adversarial networks berkeley ai research bair researchers solved problem imagetoimage generation example required create map using satellite image realistic texture objects using sketch another example successful performance conditional gans case condition goes whole picture popular image segmentation unet used architecture generator new patchgan classifier used discriminator combating blurred images picture cut n patches prediction fakereal goes separately christopher hesse made nightmare cat demo attracted great interest users find source code order apply pix2pix need dataset corresponding pairs pictures different domains case example cards problem assemble dataset however want something complicated like transfiguring objects styling pairs objects cannot found principle therefore authors pix2pix decided develop idea came cyclegan transfer different domains images without specific pairs unpaired imagetoimage translation idea teach two pairs generatordiscriminators transfer image one domain another back require cycle consistency sequential application generators get image similar original l1 loss cyclic loss required ensure generator begin transfer pictures one domain pictures another domain completely unrelated original image approach allows learn mapping horses zebras transformations unstable often create unsuccessful options find source code machine learning coming medicine addition recognizing ultrasound mri diagnosis used find new drugs fight cancer already reported detail research briefly help adversarial autoencoder aae learn latent representation molecules use search new ones result 69 molecules found half used fight cancer others serious potential topics adversarialattacks actively explored adversarialattacks standard networks trained example imagenet completely unstable adding special noise classified picture example see picture noise human eye practically unchanged model goes crazy predicts completely different class stability achieved example fast gradient sign method fgsm access parameters model make one several gradient steps towards desired class change original picture one tasks kaggle related participants encouraged create universal attacksdefenses eventually run determine best even investigate attacks first want protect products add noise captcha prevent spammers recognizing automatically secondly algorithms involved lives face recognition systems selfdriving cars case attackers use shortcomings algorithms example special glasses allow deceive face recognition system pass another person need take possible attacks account teaching models manipulations signs also allow recognized correctly set articles organizers contest already written libraries attacks cleverhans foolbox reinforcement learning rl learning reinforcement also one interesting actively developing approaches machine learning essence approach learn successful behavior agent environment gives reward experience people learn throughout lives rl actively used games robots system management traffic example course everyone heard alphagos victories game best professionals researchers using rl training bot played improve strategies previous years deepmind learned using dqn play arcade games better humans currently algorithms taught play complex games like doom much attention paid learning acceleration experience agent interaction environment requires many hours training modern gpus blog deepmind reported introduction additional losses auxiliary tasks prediction frame change pixel control agent better understands consequences actions significantly speeds learning learning results 42 learning robotsin openai actively studying agents training humans virtual environment safer experiments real life one studies team showed oneshot learning possible person shows vr perform certain task one demonstration enough algorithm learn reproduce real conditions easy people work openai deepmind topic bottom line agent task algorithm provides two possible solutions human indicates one better process repeated iteratively algorithm 900 bits feedback binary markup person learned solve problem always human must careful think teaching machine example evaluator decided algorithm really wanted take object fact simulated action another study deepmind teach robot complex behavior walk jump etc even similar human heavily involved choice loss function encourage desired behavior however would preferable algorithm learned complex behavior leaning simple rewards researchers managed achieve taught agents body emulators perform complex actions constructing complex environment obstacles simple reward progress movement watch impressive video results however much fun watch superimposed sound finally give link recently published algorithms learning rl openai use advanced solutions standard dqn july 2017 google reported took advantage deepminds development machine learning reduce energy costs data center based information thousands sensors data center google developers trained neural network ensemble predict pue power usage effectiveness efficient data center management impressive significant example practical application ml know trained models poorly transferred task task task trained specific model small step towards universality models done google brain article one model learn researchers trained model performs eight tasks different domains text speech images example translation different languages text parsing image sound recognition order achieve built complex network architecture various blocks process different input data generate result blocks encoderdecoder fall three types convolution attention gated mixture experts moe main results learning way model present tensor2tensor post facebook staff told us engineers able teach resnet50 model imagenet one hour truth told required cluster 256 gpus tesla p100 used gloo caffe2 distributed learning make process effective necessary adapt learning strategy huge batch 8192 elements gradient averaging warmup phase special learning rate etc result possible achieve efficiency 90 scaling 8 256 gpu researchers facebook experiment even faster unlike mere mortals without cluster selfdriving car sphere intensively developing cars actively tested relatively recent events note purchase intel mobileye scandals around uber google technologies stolen former employee first death using autopilot much note one thing google waymo launching beta program google pioneer field assumed technology good cars driven 3 million miles recent events selfdriving cars allowed travel across us states said modern ml beginning introduced medicine example google collaborates medical center help diagnosis deepmind even established separate unit year program data science bowl competition held predict lung cancer year basis detailed images prize fund one million dollars currently heavy investments ml bigdata china invested 150 billion ai become world leader industry comparison baidu research employs 1300 people fair facebook 80 last kdd alibaba employees talked parameter server kungpeng runs 100 billion samples trillion parameters becomes common task draw conclusions never late study machine learning one way another time developers use machine learning become one common skills today ability work databases link original post quick cheer standing ovation clap show much enjoyed story mailru group head machine learning team data stories machine learning analytics statsbots makers,en,"['Google', 'Google Translate', 'Recurrent Neural Network', 'Facebook', 'algorithm', 'DSSM', 'Smart Reply', 'WaveNet', 'Oxford University', 'BBC', 'CNN', 'The University of Washington', 'Google Brain Team', 'OCR', 'FSNS', 'pixel', 'CLEVR', 'GAN', 'Sketch-RNN', 'Berkeley AI Research', 'UNet', 'MRI', 'Adversarial Autoencoder', 'AAE', 'ImageNet', 'the Fast Gradient Sign Method', 'FGSM', 'Kaggle', 'DQN', 'PUE', 'MoE', 'Gloo', 'GPU', 'Intel', 'the Data Science Bowl', 'BigData', 'Baidu Research', 'KDD', 'Alibaba', 'KungPeng', 'Mail.ru Group', 'Machine Learning Team']"
169,Maruti Techlabs,552,What Are The Best Intelligent Chatbots or AI Chatbots Available Online?,"How do we define the intelligence of a chatbot? You can see a lot of articles about what would make a chatbot “appear intelligent.” A chatbot is intelligent when it becomes aware of user needs. Its intelligence is what gives the chatbot the ability to handle any scenario of a conversation with ease.
Are the travel bots or the weather bots that have buttons that you click and give you some query, artificially intelligent? Definitely, but they are just not far along the conversation axis. It can be a wonderfully designed conversational interface that is smooth and easy to use. It could be natural language processing and understanding where it is able to understand sentences that you structure in the wrong way. Now, it is easier than ever to make a bot from scratch. Also chatbot development platforms like Chatfuel, Gupshup make it fairly simple to build a chatbot without a technical background. Hence, making the reach for chatbot easy and transparent to anyone who would like to have one for their business. For more understanding on intelligent chatbots, read our blog.
The best AI based chatbots available online are Mitsuku, Rose, Poncho, Right Click, Insomno Bot, Dr. AI and Melody.
This chatbot is one the best AI chatbots and it’s my favorite too. Evidently it is the current winner of Loebner Prize. The Loebner Prize is an annual competition in artificial intelligence that awards prizes to the chatterbot considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. You can talk with Mitsuku for hours without getting bored. It replies to your question in the most humane way and understands your mood with the language you’re using.
It is a bot made to chat about anything, which is one of the main reasons that make it so human-like — contrary to other chatbots that are made for a specific task.
Rose is a chatbot, and a very good one — she won recognition this past Saturday as the most human-like chatbot in a competition described as the first Turing test, the Loebner Prize in 2014 and 2015.
Right Click is a startup that introduced an A.I.-powered chatbot that creates websites. It asks general questions during the conversation like “What industry you belong to?” and “Why do you want to make a website?” and creates customized templates as per the given answers. Hira Saeed tried to divert it from its job by asking it about love, but what a smart player it is! By replying to each of her queries, it tried to bring her back to the actual job of website creation. The process was short but keeps you hooked.
Poncho is a Messenger bot designed to be your one and only weather expert. It sends alerts up to twice a day with user consent and is intelligent enough to answer questions like “Should I take an umbrella today?”
Read Poncho developer’s piece: Think Differently When Building Bots
Insomno bot is for night owls. As the name suggests, it is for all people out there who have trouble sleeping. This bot talks to you when you have no one around and gives you amazing replies so that you won’t get bored. It’s not something that will help you count stars when you can’t sleep or help you with reading suggestions, but this bot talks to you about anything.
It asks about symptoms, body parameters and medical history, then compiles a list of the most and least likely causes for the symptoms and ranks them by order of seriousness.
It lives inside the existing Biadu Doctor app. This app collects medical information from people and then passes it to doctors in a form that makes it easier to use for diagnostic purposes or to otherwise respond to.
Featured CBM: The Future, Healthcare, and Conversational UI
These are just the basic versions of intelligent chatbots. There are many more intelligent chatbots out there which provide a much more smarter approach to responding to queries. Since the process of making a intelligent chatbot is not a big task, most of us can achieve it with the most basic technical knowledge. Many of which will be very extremely helpful in the service industry and also help provide a better customer experience.
The most important part of any chatbot is the conversation it has with its user. Hence, more effort has to be put in designing a chatbot conversation. Hope you had a good read. To know more about Chatbots and how they converse with people, visit the link below.
Featured CBM: How to Make a Chatbot Intelligent?
If you resonated with this article, please subscribe to our newsletter. You will get a free copy of our Case Study on Business Automation through our Bot solution.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Professional team delivering enterprise software solutions — Bot development, Big Data Analytics, Web & Mobile Apps, and AI & ML integration.
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.
",define intelligence chatbot see lot articles would make chatbot appear intelligent chatbot intelligent becomes aware user needs intelligence gives chatbot ability handle scenario conversation ease travel bots weather bots buttons click give query artificially intelligent definitely far along conversation axis wonderfully designed conversational interface smooth easy use could natural language processing understanding able understand sentences structure wrong way easier ever make bot scratch also chatbot development platforms like chatfuel gupshup make fairly simple build chatbot without technical background hence making reach chatbot easy transparent anyone would like one business understanding intelligent chatbots read blog best ai based chatbots available online mitsuku rose poncho right click insomno bot dr ai melody chatbot one best ai chatbots favorite evidently current winner loebner prize loebner prize annual competition artificial intelligence awards prizes chatterbot considered judges humanlike format competition standard turing test talk mitsuku hours without getting bored replies question humane way understands mood language youre using bot made chat anything one main reasons make humanlike contrary chatbots made specific task rose chatbot good one recognition past saturday humanlike chatbot competition described first turing test loebner prize 2014 2015 right click startup introduced aipowered chatbot creates websites asks general questions conversation like industry belong want make website creates customized templates per given answers hira saeed tried divert job asking love smart player replying queries tried bring back actual job website creation process short keeps hooked poncho messenger bot designed one weather expert sends alerts twice day user consent intelligent enough answer questions like take umbrella today read poncho developers piece think differently building bots insomno bot night owls name suggests people trouble sleeping bot talks one around gives amazing replies wont get bored something help count stars cant sleep help reading suggestions bot talks anything asks symptoms body parameters medical history compiles list least likely causes symptoms ranks order seriousness lives inside existing biadu doctor app app collects medical information people passes doctors form makes easier use diagnostic purposes otherwise respond featured cbm future healthcare conversational ui basic versions intelligent chatbots many intelligent chatbots provide much smarter approach responding queries since process making intelligent chatbot big task us achieve basic technical knowledge many extremely helpful service industry also help provide better customer experience important part chatbot conversation user hence effort put designing chatbot conversation hope good read know chatbots converse people visit link featured cbm make chatbot intelligent resonated article please subscribe newsletter get free copy case study business automation bot solution quick cheer standing ovation clap show much enjoyed story professional team delivering enterprise software solutions bot development big data analytics web mobile apps ai ml integration chatbots ai nlp facebook messenger slack telegram,en,"['Melody', 'Mitsuku', 'Poncho', 'Insomno', 'Chatbots', 'Big Data Analytics', 'Web & Mobile Apps', 'AI & ML', 'NLP']"
170,Jerry Chen,2300,The New Moats – Greylock Perspectives,"To build a sustainable and profitable business, you need strong defensive moats around your company. This rings especially true today as we undergo one of the largest platform shifts in a generation as applications move to the cloud, are consumed on iPhones, Echoes, and Teslas, are built on open source, and are fueled by AI and data. These dramatic shifts are rendering some existing moats useless and leaving CEOs feeling like it’s almost impossible to build a defensible business.
In this post, I’ll review some of the traditional economic moats that technology companies typically leverage and how they are being disrupted. I believe that startups today need to build systems of intelligenceTM — AI powered applications — “the new moats.”
Businesses can build several different moats and over time these moats can change. The following list is definitely not exhaustive and fair warning, it will read like a bad b-school blog!
Some of the greatest and most enduring technology companies are defended by powerful moats. For example, Microsoft, Google, and Facebook all have moats built on economies of scale and network effects.
One of the most successful cloud businesses, Amazon Web Services (AWS), has both the advantages of scale but also the power of network effects. More apps and services are built natively on AWS because “that’s where the customers and the data are.” In turn, the ecosystem of solutions attracts more customers and developers who build more apps that generate more data continuing the virtuous cycle while driving down Amazon’s cost through the advantages of scale.
Strong moats help companies survive through major platform shifts, but surviving should not be confused with thriving. For example, high switching costs can partly account for why mainframes and “big iron” systems are still around after all these years. Legacy businesses with deep moats may not be the high growth vehicles of their prime, but they are still generating profits. Companies need to recognize and react when they are in the midst of an industry wide transformation, lest they become victims of their own success.
Moreover, these massive platforms shifts — like cloud and mobile — are technology tidal waves that create openings for new players and enable founders to build paths over and around existing moats. Startup founders who succeed tend to execute a dual-pronged strategy: 1) Attack legacy player moats and 2) simultaneously build their own defensible moats that ride the new wave. For example, Facebook had the most entrenched social network, but Instagram built a mobile-first photo app that rode the smartphone wave to a $1B acquisition. In the enterprise world, SaaS companies like Salesforce are disrupting on-premise software companies like Oracle. Now with the advent of cloud, AWS, Azure, and Google Cloud are creating a direct channel to the customer. These platform shifts can also change the buyer and end user. Within the enterprise, the buyer has moved from a central IT team to an office knowledge worker, to someone with an iPhone, to any developer with a GitHub account.
In this current wave of disruption, is it still possible to build sustainable moats? For founders, it may feel like every advantage you build can be replicated by another team down the street, or at the very least, it feels like moats can only be built at massive scale. Open source tools and cloud have pushed power to the “new incumbents,’ — the current generation of companies that are at massive scale, have strong distribution networks, high switching cost, and strong brands working for them. These are companies like Apple, Facebook, Google, Amazon, and Salesforce.
Why does it feel like there are “no more moats” to build? In an era of cloud and open source, deep technology attacking hard problems is becoming a shallower moat. The use of open source is making it harder to monetize technology advances while the use of cloud to deliver technology is moving defensibility to different parts of the product. Companies that focus too much on technology without putting it in context of a customer problem will be caught between a rock and a hard place — or as I like to say, “between open source and a cloud place.” For example, incumbent technologies like Oracle’s proprietary database are being attacked from open source alternatives like Hadoop and MongoDB and in the cloud by Amazon Aurora and innovations like Google Spanner. On the other hand, companies that build great customer experiences may find defensibility through the workflow of their software.
I believe that deep technology moats aren’t completely gone and defensible business models can still be built around IP. If you pick a place in the technology stack and become the absolute best of breed solution you can create a valuable company. However, this means picking a technical problem with few substitutes, that requires hard engineering, and needs operational knowledge to scale.
Today the market is favoring “full stack” companies, SaaS offerings that offer application logic, middleware, and databases combined. Technology is becoming an invisible component of a complete solution (e.g. “No one cares what database backs your favorite mobile app as long as your food is delivered on time!”). In the consumer world, Apple made the integrated or full stack experience popular with the iPhone which seamlessly integrated hardware with software. This integrated experience is coming to dominate enterprise software as well. Cloud and SaaS has made it possible to reach customers directly and in a cost-effective manner. As a result, customers are increasingly buying full stack technology in the form of SaaS applications instead of buying individual pieces of the tech stack and building their own apps. The emphasis on the whole application experience or the “top of the technology stack” is why I also evaluate companies through an additional framework, the stack of enterprise systems.
At the bottom of the stack of systems, is usually a database on top of which an application is built. If the data and app power a critical business function, it becomes a “system of record.” There are three major systems of record in an enterprise: your customers, your employees, and your assets. CRM owns your customers, HCM, owns your employees, and ERP/Financials owns your assets. Generations of companies have been built around owning a system of record and every wave produced a new winner. In CRM we saw Salesforce replace Siebel as the system of record for customer data, and Workday replace Oracle PeopleSoft for employee data. Workday has also expanded into financial data. Other applications can be built around a system of record but are usually not as valuable as the actual system of record. For example, marketing automation companies like Marketo and Responsys built big businesses around CRM, but never became as strategic or as valuable as Salesforce.
Systems of engagementTM are the interfaces between users and the systems of record and can be powerful businesses because they control the end user interactions. In the mainframe era, the systems of record and engagement were tied together when the mainframe and terminal were essentially the same product. The client/server wave ushered in a class of companies that tried to own your desktop, only to be disrupted by a generation of browser based companies, only to be succeeded by mobile first companies. The current generation of companies vying to own the system of engagement include Slack, Amazon Alexa, and every other speech / text/ conversational UI startup. In China, WeChat has become a dominant system of engagement and is now a platform for everything from e-commerce to games. If it sounds like systems of engagementTM turn over more than systems of record, it’s probably because they do. The successive generations of systems of engagementTM don’t necessarily disappear but instead users keep adding new ways to interact with their applications. In a multi-channel world, owning the system of engagement is most valuable if you control most of the end user engagement or are a cross channel system that reaches users wherever they are. Perhaps the most strategic advantage of being a system of engagement is that you can coexist with several systems of record and collect all the data that passes through your product. Over time you can evolve your engagement position into an actual system of record using all the data you have accumulated.
I believe that systems of intelligenceTM are the new moats. What is a system of intelligence and why is it so defensible? What makes a system of intelligence valuable is that it typically crosses multiple data sets, multiple systems of record. One example is an application that combines web analytics with customer data and social data to predict end user behavior, churn, LTV, or just serve more timely content. You can build intelligence on a single data source or single system of record but that position becomes harder to defend against the vendor that owns the data. For a startup to thrive around incumbents like Oracle and SAP, you need to combine their data with other data sources (public or private) to create value for your customer. Incumbents will be advantaged on their own data. For example, Salesforce is building a system of intelligence, Einstein, starting with their own system of record, CRM. The next generation of enterprise products will use different artificial intelligence (AI) techniques to build systems of intelligenceTM. It’s not just applications that will be transformed by AI but also data center and infrastructure products. We can categorize three major areas where you can build systems of intelligenceTM: customer facing applications around the customer journey, employee facing applications like HCM, ITSM, Financials, or infrastructure systems like security, compute/ storage/ networking, and monitoring/ management. In addition to these broad horizontal use cases, startups can also focus on a single industry or market and build a system of intelligence around data that is unique to a vertical like Veeva in life sciences, or Rhumbix in construction.
In all of these markets, the battle is moving from the old moats, the sources of the data, to the new moats, what you do with the data. Using a company’s data, you can upsell customers, automatically respond to support tickets, prevent employee attrition, and identify security anomalies. Products that use data specific to an industry (i.e. healthcare, financial services), or unique to a company (customer data, machine logs, etc.) to solve a strategic problem begin to look like a pretty deep moat, especially if you can replace or automate an entire enterprise workflow or create a new value-added workflow that was made possible by this intelligence.
Enterprise applications that built systems of record have always been powerful businesses models. Some of the most enduring app companies like Salesforce and SAP are all built on deep IP, benefit from economies of scale, and over time they accumulate more data and operating knowledge as they get deeper within a company’s workflow and business processes. However, even these incumbents are not immune to platform shifts as a new generation of companies attack their domains. To be fair, we may be at risk of AI marketing fatigue, but all the hype reflects AI’s potential to change so many industries. One popular AI approach, machine learning (ML), can be combined with data, a business process, and an enterprise workflow to create the context to build a system of intelligence. Google was an early pioneer of applying ML to a process and workflow: they collected more data on every user and applied machine learning to serve up more timely ads within the workflow of a web search. There are other evolving AI techniques like neural networks that will continue to change what we can expect from these future applications. These AI-driven systems of intelligenceTM present a huge opportunity for new startups. Successful companies here can build a virtuous cycle of data because the more data you generate and train on with your product, the better your models become and the better your product becomes. Ultimately the product becomes tailored for each customer which creates another moat, high switching costs. It is also possible to build a company that combines systems of engagementTM with intelligence or even all three layers of the enterprise stack but a system of intelligence or engagement can be the best insertion point for a startup against an incumbent. Building a system of engagement or intelligence is not a trivial task and will require deep technology, especially at speed and scale. In particular, technologies that can facilitate an intelligence layer across multiple data sources will be essential. Finally, there are some businesses that can build data network effects by using customer and market data to train and improve models that make the product better for all customers, which spins the flywheel of intelligence faster.  In summary, you can build a defensible business model as a system of engagement, intelligence, or record, but with the advent of AI, intelligent applications will be the fountain of the next generation of great software companies because they will be the new moats.
Thanks to Saam Motamedi, Sarah Guo, Eli Collins, Peter Bailis, Elisa Schreiber, Michael Inouye, my Greylock partner Sarah Tavel, and the rest of my partners at Greylock for their input. This post was also helped through conversations with my friends at several Greylock-backed companies including Trifacta, Cloudera, and dozens of founders and CEOs that have influenced my thinking. All good ideas are shamelessly stolen and all bad ideas are mine alone.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Restless. Irreverent. Partner at @GreylockVC. www.jerrychen.com
Greylock Partners backs entrepreneurs who are building disruptive, market-transforming consumer and enterprise software companies.
",build sustainable profitable business need strong defensive moats around company rings especially true today undergo one largest platform shifts generation applications move cloud consumed iphones echoes teslas built open source fueled ai data dramatic shifts rendering existing moats useless leaving ceos feeling like almost impossible build defensible business post ill review traditional economic moats technology companies typically leverage disrupted believe startups today need build systems intelligencetm ai powered applications new moats businesses build several different moats time moats change following list definitely exhaustive fair warning read like bad bschool blog greatest enduring technology companies defended powerful moats example microsoft google facebook moats built economies scale network effects one successful cloud businesses amazon web services aws advantages scale also power network effects apps services built natively aws thats customers data turn ecosystem solutions attracts customers developers build apps generate data continuing virtuous cycle driving amazons cost advantages scale strong moats help companies survive major platform shifts surviving confused thriving example high switching costs partly account mainframes big iron systems still around years legacy businesses deep moats may high growth vehicles prime still generating profits companies need recognize react midst industry wide transformation lest become victims success moreover massive platforms shifts like cloud mobile technology tidal waves create openings new players enable founders build paths around existing moats startup founders succeed tend execute dualpronged strategy 1 attack legacy player moats 2 simultaneously build defensible moats ride new wave example facebook entrenched social network instagram built mobilefirst photo app rode smartphone wave 1b acquisition enterprise world saas companies like salesforce disrupting onpremise software companies like oracle advent cloud aws azure google cloud creating direct channel customer platform shifts also change buyer end user within enterprise buyer moved central team office knowledge worker someone iphone developer github account current wave disruption still possible build sustainable moats founders may feel like every advantage build replicated another team street least feels like moats built massive scale open source tools cloud pushed power new incumbents current generation companies massive scale strong distribution networks high switching cost strong brands working companies like apple facebook google amazon salesforce feel like moats build era cloud open source deep technology attacking hard problems becoming shallower moat use open source making harder monetize technology advances use cloud deliver technology moving defensibility different parts product companies focus much technology without putting context customer problem caught rock hard place like say open source cloud place example incumbent technologies like oracles proprietary database attacked open source alternatives like hadoop mongodb cloud amazon aurora innovations like google spanner hand companies build great customer experiences may find defensibility workflow software believe deep technology moats arent completely gone defensible business models still built around ip pick place technology stack become absolute best breed solution create valuable company however means picking technical problem substitutes requires hard engineering needs operational knowledge scale today market favoring full stack companies saas offerings offer application logic middleware databases combined technology becoming invisible component complete solution eg one cares database backs favorite mobile app long food delivered time consumer world apple made integrated full stack experience popular iphone seamlessly integrated hardware software integrated experience coming dominate enterprise software well cloud saas made possible reach customers directly costeffective manner result customers increasingly buying full stack technology form saas applications instead buying individual pieces tech stack building apps emphasis whole application experience top technology stack also evaluate companies additional framework stack enterprise systems bottom stack systems usually database top application built data app power critical business function becomes system record three major systems record enterprise customers employees assets crm owns customers hcm owns employees erpfinancials owns assets generations companies built around owning system record every wave produced new winner crm saw salesforce replace siebel system record customer data workday replace oracle peoplesoft employee data workday also expanded financial data applications built around system record usually valuable actual system record example marketing automation companies like marketo responsys built big businesses around crm never became strategic valuable salesforce systems engagementtm interfaces users systems record powerful businesses control end user interactions mainframe era systems record engagement tied together mainframe terminal essentially product clientserver wave ushered class companies tried desktop disrupted generation browser based companies succeeded mobile first companies current generation companies vying system engagement include slack amazon alexa every speech text conversational ui startup china wechat become dominant system engagement platform everything ecommerce games sounds like systems engagementtm turn systems record probably successive generations systems engagementtm dont necessarily disappear instead users keep adding new ways interact applications multichannel world owning system engagement valuable control end user engagement cross channel system reaches users wherever perhaps strategic advantage system engagement coexist several systems record collect data passes product time evolve engagement position actual system record using data accumulated believe systems intelligencetm new moats system intelligence defensible makes system intelligence valuable typically crosses multiple data sets multiple systems record one example application combines web analytics customer data social data predict end user behavior churn ltv serve timely content build intelligence single data source single system record position becomes harder defend vendor owns data startup thrive around incumbents like oracle sap need combine data data sources public private create value customer incumbents advantaged data example salesforce building system intelligence einstein starting system record crm next generation enterprise products use different artificial intelligence ai techniques build systems intelligencetm applications transformed ai also data center infrastructure products categorize three major areas build systems intelligencetm customer facing applications around customer journey employee facing applications like hcm itsm financials infrastructure systems like security compute storage networking monitoring management addition broad horizontal use cases startups also focus single industry market build system intelligence around data unique vertical like veeva life sciences rhumbix construction markets battle moving old moats sources data new moats data using companys data upsell customers automatically respond support tickets prevent employee attrition identify security anomalies products use data specific industry ie healthcare financial services unique company customer data machine logs etc solve strategic problem begin look like pretty deep moat especially replace automate entire enterprise workflow create new valueadded workflow made possible intelligence enterprise applications built systems record always powerful businesses models enduring app companies like salesforce sap built deep ip benefit economies scale time accumulate data operating knowledge get deeper within companys workflow business processes however even incumbents immune platform shifts new generation companies attack domains fair may risk ai marketing fatigue hype reflects ais potential change many industries one popular ai approach machine learning ml combined data business process enterprise workflow create context build system intelligence google early pioneer applying ml process workflow collected data every user applied machine learning serve timely ads within workflow web search evolving ai techniques like neural networks continue change expect future applications aidriven systems intelligencetm present huge opportunity new startups successful companies build virtuous cycle data data generate train product better models become better product becomes ultimately product becomes tailored customer creates another moat high switching costs also possible build company combines systems engagementtm intelligence even three layers enterprise stack system intelligence engagement best insertion point startup incumbent building system engagement intelligence trivial task require deep technology especially speed scale particular technologies facilitate intelligence layer across multiple data sources essential finally businesses build data network effects using customer market data train improve models make product better customers spins flywheel intelligence faster summary build defensible business model system engagement intelligence record advent ai intelligent applications fountain next generation great software companies new moats thanks saam motamedi sarah guo eli collins peter bailis elisa schreiber michael inouye greylock partner sarah tavel rest partners greylock input post also helped conversations friends several greylockbacked companies including trifacta cloudera dozens founders ceos influenced thinking good ideas shamelessly stolen bad ideas mine alone quick cheer standing ovation clap show much enjoyed story restless irreverent partner greylockvc wwwjerrychencom greylock partners backs entrepreneurs building disruptive markettransforming consumer enterprise software companies,en,"['intelligenceTM', 'Microsoft', 'Google', 'Facebook', 'Amazon Web Services', 'AWS', 'Amazon', 'Oracle', 'iPhone', 'GitHub', 'Apple', 'IP', 'HCM', 'ERP/Financials', 'Marketo', 'Responsys', 'WeChat', 'churn', 'LTV', 'SAP', 'CRM', 'ITSM', 'Veeva', 'AI', 'engagementTM', 'Greylock Partners']"
171,Gaurav Oberoi,850,Exploring DeepFakes – Hacker Noon,"In December 2017, a user named “DeepFakes” posted realistic looking explicit videos of famous celebrities on Reddit. He generated these fake videos using deep learning, the latest in AI, to insert celebrities’ faces into adult movies.
In the following weeks, the internet exploded with articles about the dangers of face swapping technology: harassing innocents, propagating fake news, and hurting the credibility of video evidence forever.
In this post, I explore the capabilities of this tech, describe how it works, and discuss potential applications.
DeepFakes offers the ability to swap one face for another in an image or a video. Face swapping has been done in films for years, but it required skilled video editors and CGI experts to spend many hours to achieve decent results.
This is so remarkable that I’m going to repeat it: anyone with hundreds of sample images, of person A and person B can feed them into an algorithm, and produce high quality face swaps — video editing skills are not needed.
This also means that it can be done at scale, and given that so many of us have our faces online, it’s trivially easy to insert almost anyone into fake videos. Scary, but hopefully it’s not all doom and gloom, after all, we as a society have already come to accept that photos can easily be faked.
Before dreaming up how to use this tech, I wanted to get a handle on how it works and how well it performs.
I picked two popular late night TV hosts, Jimmy Fallon and John Oliver, because I can find lots of videos of them with similar poses and lighting — and also enough variation (like lip sync battles) to keep it interesting.
Luckily for me, there’s an active GitHub repo that contains the original DeepFakes code and many more improvements. It’s fairly straightforward to use, but the onus is still on the user to collect and prepare training data.
To make experimentation easy, I wrote a script to work directly with YouTube videos. This makes collecting and preprocessing training data painless, and converting videos one-step. Click here to view my Github repo, and see how easily I generated the videos below (I also share my model weights).
The following videos were generated by training a model on about 15k images of each person’s face (30k images total). I got faces for each celebrity from 6–8 YouTube videos of 3–5 minutes each, with 20 frames per second per video, and by filtering out frames that don’t have their faces present. All of this was done automatically — all I did was specify a list of YouTube video urls.
The total training time was about 72 hours on a NVIDIA GTX 1080 TI GPU. Training is primarily constrained by GPU, but downloading videos, and chopping them into frames is I/O bound and can be parallelized.
Note that while I had thousands of images of each person, decent face swaps can be achieved with as few as 300 images. I went this route because I pulled face images from videos, and it’s far easier to pick a handful of videos as training data, than to find hundreds of images.
The images below are low resolution to keep the size of the animated GIF file small. There’s a YouTube video below with higher resolution and sound.
While not perfect, the results above are quite convincing. The key thing to remember is: the algorithm learned how to do this by seeing lots of examples, I didn’t modify the videos in any way. Magical? Let’s look under the covers.
At the core of the Deepfakes code is an autoencoder, a deep neural network that learns how to take an input, compress it down into a small representation or encoding, and then to regenerate the original input from this encoding.
Putting a bottleneck in the middle forces the network to recreate these images instead of just returning what it sees. The encodings help it capture broader patterns, hypothetically, like how and where to draw Jimmy Fallon’s eyebrow.
Deepfakes goes further by having one encoder to compress a face into an encoding, and two decoders, one to turn it back into person A (Fallon), and the other to person B (Oliver). It’s easier to understand with a diagram:
In the above, we’re showing how these 3 components get trained:
Once training is complete, we can perform a clever trick: pass in an image of Fallon into the encoder, and then instead of trying to reconstruct Fallon from the encoding, we now pass it to Decoder B to reconstruct Oliver.
It’s remarkable to think that the algorithm can learn how to generate these images just by seeing thousands of examples, but that’s exactly what has happened here, and with fairly decent results.
While the results are exciting, there are clear limitations to what we can achieve with this technology today:
These are tenable problems to be sure: tools can be built to collect images from online channels en masse; algorithms can help flag when there is insufficient or mismatched training data; clever optimizations or model reuse can help reduce training time; and a well engineered system can be built to make the entire process automatic.
But ultimately, the question is: why? Is there enough of a business model to make doing all this worth it?
Given what’ve now learned about what’s possible, let’s talk about ways in which this could be useful:
Hollywood has had this technology at its fingertips, but not at this low cost. If they can create great looking videos with this technique, it will change the demand for skilled editors over time.
But it could also open up new opportunities: for instance, making movies with unknown actors, and then superimposing famous celebrities onto them. This could work for YouTube videos or even news channels filmed by regular folks.
In more out-there scenarios, studios could change actors based on their target market (more Schwarzenager for the Austrians), or Netflix could allow viewers to pick actors before hitting play. More likely, this tech could generate revenue for the estates of long dead actors by bringing them back to life.
Some of the comment threads on DeepFakes videos on YouTube are abuzz about what a great meme generator this technology could create. Jib Jab is a company that has been selling video greeting cards with simple face swapping for years (they are hilarious). But the big opportunity is to create the next big viral hit; after all photo filters attracted masses of people to Instagram and SnapChat, and face swapping apps have done well before.
Given how fun the results can be, there’s likely room for a hit viral app if you can get the costs low enough to generate these models.
Imagine if Target could have a celebrity showcase their clothes for a month, just by paying her agent a fee, grabbing some existing headshots, and clicking a button. This would create a new revenue stream for celebrities, social media influencers, or anyone who happens to be in the spotlight at the moment. And it would give businesses another tool to promote brands and drive conversion. It also raises interesting legal questions about ownership of likeness, and business model questions on how to partition and price rights to use them.
Imagine a world where the ads you see as you surf the web include you, your friends, and your family. While this may come across as creepy today, does it seem so far fetched to think that this won’t be the norm in a few years?
After all, we are visual creatures, and advertisers have been trying to elicit emotional responses from us for years, e.g. Coke may want to convey joy by putting your friends in a hip music video, or Allstate may tug at your fears by showing your family in an insurance ad. Or the approach may be more direct: Banana Republic could superimpose your face on a body type that matches yours, and convince you that it’s worth trying out their new leather jackets.
Whoever the original Deepfakes user is, they opened a Pandora’s box of difficult questions about how fake video generation will affect society. I hope that in the same way we have come to accept that images can easily be faked, we will adapt to video uncertainty too, though not everyone shares this hope.
What Deepfakes also did is shine a light on how interesting this technology is. Deep generative models like the autoencoder that Deepfakes uses, allow us to create synthetic but realistic looking data (including images or videos), only by showing an algorithm lots of examples. This means that once these algorithms are turned into products, regular folks will have access to powerful tools that will make them more creative, hopefully towards positive ends.
There have already been some interesting applications of this technique, like style transfer apps that make your photos look like famous paintings, but given the high volume and exciting nature of the research that is being published in this space, there’s clearly a lot more to come.
I’m interested in exploring how to build value from the latest in AI research; if you have an interest in taking this technology to market to solve a real problem, please drop me a note.
A few fun tidbits for the curious:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I’ve been a product manager, engineer, and founder for over a decade in Seattle and Silicon Valley. Currently exploring new ideas at the Allen Institute for AI.
how hackers start their afternoons.
",december 2017 user named deepfakes posted realistic looking explicit videos famous celebrities reddit generated fake videos using deep learning latest ai insert celebrities faces adult movies following weeks internet exploded articles dangers face swapping technology harassing innocents propagating fake news hurting credibility video evidence forever post explore capabilities tech describe works discuss potential applications deepfakes offers ability swap one face another image video face swapping done films years required skilled video editors cgi experts spend many hours achieve decent results remarkable im going repeat anyone hundreds sample images person person b feed algorithm produce high quality face swaps video editing skills needed also means done scale given many us faces online trivially easy insert almost anyone fake videos scary hopefully doom gloom society already come accept photos easily faked dreaming use tech wanted get handle works well performs picked two popular late night tv hosts jimmy fallon john oliver find lots videos similar poses lighting also enough variation like lip sync battles keep interesting luckily theres active github repo contains original deepfakes code many improvements fairly straightforward use onus still user collect prepare training data make experimentation easy wrote script work directly youtube videos makes collecting preprocessing training data painless converting videos onestep click view github repo see easily generated videos also share model weights following videos generated training model 15k images persons face 30k images total got faces celebrity 68 youtube videos 35 minutes 20 frames per second per video filtering frames dont faces present done automatically specify list youtube video urls total training time 72 hours nvidia gtx 1080 ti gpu training primarily constrained gpu downloading videos chopping frames io bound parallelized note thousands images person decent face swaps achieved 300 images went route pulled face images videos far easier pick handful videos training data find hundreds images images low resolution keep size animated gif file small theres youtube video higher resolution sound perfect results quite convincing key thing remember algorithm learned seeing lots examples didnt modify videos way magical lets look covers core deepfakes code autoencoder deep neural network learns take input compress small representation encoding regenerate original input encoding putting bottleneck middle forces network recreate images instead returning sees encodings help capture broader patterns hypothetically like draw jimmy fallons eyebrow deepfakes goes one encoder compress face encoding two decoders one turn back person fallon person b oliver easier understand diagram showing 3 components get trained training complete perform clever trick pass image fallon encoder instead trying reconstruct fallon encoding pass decoder b reconstruct oliver remarkable think algorithm learn generate images seeing thousands examples thats exactly happened fairly decent results results exciting clear limitations achieve technology today tenable problems sure tools built collect images online channels en masse algorithms help flag insufficient mismatched training data clever optimizations model reuse help reduce training time well engineered system built make entire process automatic ultimately question enough business model make worth given whatve learned whats possible lets talk ways could useful hollywood technology fingertips low cost create great looking videos technique change demand skilled editors time could also open new opportunities instance making movies unknown actors superimposing famous celebrities onto could work youtube videos even news channels filmed regular folks outthere scenarios studios could change actors based target market schwarzenager austrians netflix could allow viewers pick actors hitting play likely tech could generate revenue estates long dead actors bringing back life comment threads deepfakes videos youtube abuzz great meme generator technology could create jib jab company selling video greeting cards simple face swapping years hilarious big opportunity create next big viral hit photo filters attracted masses people instagram snapchat face swapping apps done well given fun results theres likely room hit viral app get costs low enough generate models imagine target could celebrity showcase clothes month paying agent fee grabbing existing headshots clicking button would create new revenue stream celebrities social media influencers anyone happens spotlight moment would give businesses another tool promote brands drive conversion also raises interesting legal questions ownership likeness business model questions partition price rights use imagine world ads see surf web include friends family may come across creepy today seem far fetched think wont norm years visual creatures advertisers trying elicit emotional responses us years eg coke may want convey joy putting friends hip music video allstate may tug fears showing family insurance ad approach may direct banana republic could superimpose face body type matches convince worth trying new leather jackets whoever original deepfakes user opened pandoras box difficult questions fake video generation affect society hope way come accept images easily faked adapt video uncertainty though everyone shares hope deepfakes also shine light interesting technology deep generative models like autoencoder deepfakes uses allow us create synthetic realistic looking data including images videos showing algorithm lots examples means algorithms turned products regular folks access powerful tools make creative hopefully towards positive ends already interesting applications technique like style transfer apps make photos look like famous paintings given high volume exciting nature research published space theres clearly lot come im interested exploring build value latest ai research interest taking technology market solve real problem please drop note fun tidbits curious quick cheer standing ovation clap show much enjoyed story ive product manager engineer founder decade seattle silicon valley currently exploring new ideas allen institute ai hackers start afternoons,en,"['DeepFakes', 'CGI', 'GitHub', 'NVIDIA', 'GPU', 'GIF', 'Schwarzenager', 'Target', 'Allstate', 'the Allen Institute for AI']"
172,Nick Bourdakos,5000,Understanding Capsule Networks — AI’s Alluring New Architecture,"Convolutional neural networks have done an amazing job, but are rooted in problems. It’s time we started thinking about new solutions or improvements — and now, enter capsules.
Previously, I briefly discussed how capsule networks combat some of these traditional problems. For the past for few months, I’ve been submerging myself in all things capsules. I think it’s time we all try to get a deeper understanding of how capsules actually work.
In order to make it easier to follow along, I have built a visualization tool that allows you to see what is happening at each layer. This is paired with a simple implementation of the network. All of it can be found on GitHub here.
This is the CapsNet architecture. Don’t worry if you don’t understand what any of it means yet. I’ll be going through it layer by layer, with as much detail as I can possibly conjure up.
The input into CapsNet is the actual image supplied to the neural net. In this example the input image is 28 pixels high and 28 pixels wide. But images are actually 3 dimensions, and the 3rd dimension contains the color channels.
The image in our example only has one color channel, because it’s black and white. Most images you are familiar with have 3 or 4 channels, for Red-Green-Blue and possibly an additional channel for Alpha, or transparency.
Each one of these pixels is represented as a value from 0 to 255 and stored in a 28x28x1 matrix [28, 28, 1]. The brighter the pixel, the larger the value.
The first part of CapsNet is a traditional convolutional layer. What is a convolutional layer, how does it work, and what is its purpose?
The goal is to extract some extremely basic features from the input image, like edges or curves.
How can we do this?
Let’s think about an edge:
If we look at a few points on the image, we can start to pick up a pattern. Focus on the colors to the left and right of the point we are looking at:
You might notice that they have a larger difference if the point is an edge:
What if we went through each pixel in the image and replaced its value with the value of the difference of the pixels to the left and right of it? In theory, the image should become all black except for the edges.
We could do this by looping through every pixel in the image:
But this isn’t very efficient. We can instead use something called a “convolution.” Technically speaking, it’s a “cross-correlation,” but everyone likes to call them convolutions.
A convolution is essentially doing the same thing as our loop, but it takes advantage of matrix math.
A convolution is done by lining up a small “window” in the corner of the image that only lets us see the pixels in that area. We then slide the window across all the pixels in the image, multiplying each pixel by a set of weights and then adding up all the values that are in that window.
This window is a matrix of weights, called a “kernel.”
We only care about 2 pixels, but when we wrap the window around them it will encapsulate the pixel between them.
Can you think of a set of weights that we can multiply these pixels by so that their sum adds up to the value we are looking for?
Spoilers below!
We can do something like this:
With these weights, our kernel will look like this:
However, kernels are generally square — so we can pad it with more zeros to look like this:
Here’s a nice gif to see a convolution in action:
Note: The dimension of the output is reduced by the size of the kernel plus 1. For example:(7 — 3) + 1 = 5 (more on this in the next section)
Here’s what the original image looks like after doing a convolution with the kernel we crafted:
You might notice that a couple edges are missing. Specifically, the horizontal ones. In order to highlight those, we would need another kernel that looks at pixels above and below. Like this:
Also, both of these kernels won’t work well with edges of other angles or edges that are blurred. For that reason, we use many kernels (in our CapsNet implementation, we use 256 kernels). And the kernels are normally larger to allow for more wiggle room (our kernels will be 9x9).
This is what one of the kernels looked like after training the model. It’s not very obvious, but this is just a larger version of our edge detector that is more robust and only finds edges that go from bright to dark.
Note: I’ve rounded the values because they are quite large, for example 0.01783941
Luckily, we don’t have to hand-pick this collection of kernels. That is what training does. The kernels all start off empty (or in a random state) and keep getting tweaked in the direction that makes the output closer to what we want.
This is what the 256 kernels ended up looking like (I colored them as pixels so it’s easier to digest). The more negative the numbers, the bluer they are. 0 is green and positive is yellow:
After we filter the image with all of these kernels, we end up with a fat stack of 256 output images.
ReLU (formally known as Rectified Linear Unit) may sound complicated, but it’s actually quite simple. ReLU is an activation function that takes in a value. If it’s negative it becomes zero, and if it’s positive it stays the same.
In code:
And as a graph:
We apply this function to all of the outputs of our convolutions.
Why do we do this? If we don’t apply some sort of activation function to the output of our layers, then the entire neural net could be described as a linear function. This would mean that all this stuff we are doing is kind of pointless.
Adding a non-linearity allows us to describe all kinds of functions. There are many different types of function we could apply, but ReLU is the most popular because it’s very cheap to perform.
Here are the outputs of ReLU Conv1 layer:
The PrimaryCaps layer starts off as a normal convolution layer, but this time we are convolving over the stack of 256 outputs from the previous convolutions. So instead of having a 9x9 kernel, we have a 9x9x256 kernel.
So what exactly are we looking for?
In the first layer of convolutions we were looking for simple edges and curves. Now we are looking for slightly more complex shapes from the edges we found earlier.
This time our “stride” is 2. That means instead of moving 1 pixel at a time, we take steps of 2. A larger stride is chosen so that we can reduce the size of our input more rapidly:
Note: The dimension of the output would normally be 12, but we divide it by 2, because of the stride. For example: ((20 — 9) + 1) / 2 = 6
We will convolve over the outputs another 256 times. So we will end up with a stack of 256 6x6 outputs.
But this time we aren’t satisfied with just some lousy plain old numbers.
We’re going to cut the stack up into 32 decks with 8 cards each deck.
We can call this deck a “capsule layer.”
Each capsule layer has 36 “capsules.”
If you’re keeping up (and are a math wiz), that means each capsule has an array of 8 values. This is what we can call a “vector.”
Here’s what I’m talking about:
These “capsules” are our new pixel.
With a single pixel, we could only store the confidence of whether or not we found an edge in that spot. The higher the number, the higher the confidence.
With a capsule we can store 8 values per location! That gives us the opportunity to store more information than just whether or not we found a shape in that spot. But what other kinds of information would we want to store?
When looking at the shape below, what can you tell me about it? If you had to tell someone else how to redraw it, and they couldn’t look at it, what would you say?
This image is extremely basic, so there are only a few details we need to describe the shape:
We can call these “instantiation parameters.” With more complex images we will end up needing more details. They can include pose (position, size, orientation), deformation, velocity, albedo, hue, texture, and so on.
You might remember that when we made a kernel for edge detection, it only worked on a specific angle. We needed a kernel for each angle. We could get away with it when dealing with edges because there are very few ways to describe an edge. Once we get up to the level of shapes, we don’t want to have a kernel for every angle of rectangles, ovals, triangles, and so on. It would get unwieldy, and would become even worse when dealing with more complicated shapes that have 3 dimensional rotations and features like lighting.
That’s one of the reasons why traditional neural nets don’t handle unseen rotations very well:
As we go from edges to shapes and from shapes to objects, it would be nice if we had more room to store this extra useful information.
Here is a simplified comparison of 2 capsule layers (one for rectangles and the other for triangles) vs 2 traditional pixel outputs:
Like a traditional 2D or 3D vector, this vector has an angle and a length. The length describes the probability, and the angle describes the instantiation parameters. In the example above, the angle actually matches the angle of the shape, but that’s not normally the case.
In reality it’s not really feasible (or at least easy) to visualize the vectors like above, because these vectors are 8 dimensional.
Since we have all this extra information in a capsule, the idea is that we should be able to recreate the image from them.
Sounds great, but how do we coax the network into actually wanting to learn these things?
When training a traditional CNN, we only care about whether or not the model predicts the right classification. With a capsule network, we have something called a “reconstruction.” A reconstruction takes the vector we created and tries to recreate the original input image, given only this vector. We then grade the model based on how close the reconstruction matches the original image.
I will go into more detail on this in the coming sections, but here is a simple example:
After we have our capsules, we are going to perform another non-linearity function on it (like ReLU), but this time the equation is a bit more involved. The function scales the values of the vector so that only the length of the vector changes, not the angle. This way we can make the vector between 0 and 1 so it’s an actual probability.
This is what lengths of the capsule vectors look like after squashing. At this point it’s almost impossible to guess what each capsule is looking for.
The next step is to decide what information to send to the next level. In traditional networks, we would probably do something like “max pooling.” Max pooling is a way to reduce size by only passing on the highest activated pixel in the region to the next layer.
However, with capsule networks we are going to do something called routing by agreement. The best example of this is the boat and house example illustrated by Aurélien Géron in this excellent video. Each capsule tries to predict the next layer’s activations based on itself:
Looking at these predictions, which object would you choose to pass on to the next layer (not knowing the input)? Probably the boat, right? both the rectangle capsule and the triangle capsule agree on what the boat would look like. But they don’t agree on how the house would look, so it’s not very likely that the object is a house.
With routing by agreement, we only pass on the useful information and throw away the data that would just add noise to the results. This gives us a much smarter selection than just choosing the largest number, like in max pooling.
With traditional networks, misplaced features don’t faze it:
With capsule networks, the features wouldn’t agree with each other:
Hopefully, that works intuitively. However, how does the math work?
We have 10 different digit classes that we are predicting:
Note: In the boat and house example we were predicting 2 objects, but now we are predicting 10.
Unlike in the boat and the house example, the predictions aren’t actually images. Instead, we are trying to predict the vector that describes the image.
The capsule’s predictions for each class are made by multiplying it’s vector by a matrix of weights for each class that we are trying to predict.
Remember that we have 32 capsule layers, and each capsule layer has 36 capsules. That means we have a total of 1,152 capsules.
You will end up with a list of 11,520 predictions.
Each weight is actually a 16x8 matrix, so each prediction is a matrix multiplication between the capsule vector and this weight matrix:
As you can see, our prediction is a 16 degree vector.
Where does the 16 come from? It’s an arbitrary choice, just like 8 was for our original capsules.
But it should be noted that we want to increase the number of dimensions of our capsules the deeper we get into the network. This should make sense intuitively, because the deeper we go the more complex our features become and the more parameters we need to recreate them. For example, you will need more information to describe an entire face than just a person’s eye.
The next step is to figure out which of these 11,520 predictions agree with each other the most.
It can be difficult to visualize a solution to this when we think in terms of high dimensional vectors. For the sake of sanity, let’s start off by pretending our vectors are just points in 2 dimensional space:
We start off by calculating the mean of all of the points. Each point starts out with equal importance:
We then can measure the distance between every point from the mean. The further the point is away from the mean, the less important that point becomes:
We then recalculate the mean, this time taking into account the point’s importance:
We end up going through this cycle 3 times:
As you can see, as we go through this cycle, the points that don’t agree with the others start to disappear. The highest agreeing points end up getting passed on to the next layer with the highest activations.
After agreement, we end up with ten 16 dimensional vectors, one vector for each digit. This matrix is our final prediction. The length of the vector is the confidence of the digit being found — the longer the better. The vector can also be used to generate a reconstruction of the input image.
This is what the lengths of the vectors look like with the input of 4:
The fifth block is the brightest, which means high confidence. Remember that 0 is the first class, meaning 4 is our predicted class.
The reconstruction portion of the implementation isn’t very interesting. It’s just a few fully connected layers. But the reconstruction itself is very cool and fun to play around with.
If we reconstruct our 4 input from its vector, this is what we get:
If we manipulate the sliders (the vector), we can see how each dimension affects the 4:
I recommend cloning the visualization repo to play around with different inputs and see how the sliders affect the reconstruction:
Run the tool:
Then point your browser to: http://localhost:5000
I think that the reconstructions from capsule networks are stunning. Even though the current model is only trained on simple digits, it makes my mind run with the possibilities that a matured architecture trained on a larger dataset could achieve.
I’m very curious to see how manipulating the reconstruction vectors of a more complicated image would affect it. For that reason, my next project is to get capsule networks to work with the CIFAR and smallNORB datasets.
Thanks for reading! If you have any questions, feel free to reach out at bourdakos1@gmail.com, connect with me on LinkedIn, or follow me on Medium.
If you found this article helpful, it would mean a lot if you gave it some applause👏 and shared to help others find it! And feel free to leave a comment below.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Computer vision addict at IBM Watson
Our community publishes stories worth reading on development, design, and data science.
",convolutional neural networks done amazing job rooted problems time started thinking new solutions improvements enter capsules previously briefly discussed capsule networks combat traditional problems past months ive submerging things capsules think time try get deeper understanding capsules actually work order make easier follow along built visualization tool allows see happening layer paired simple implementation network found github capsnet architecture dont worry dont understand means yet ill going layer layer much detail possibly conjure input capsnet actual image supplied neural net example input image 28 pixels high 28 pixels wide images actually 3 dimensions 3rd dimension contains color channels image example one color channel black white images familiar 3 4 channels redgreenblue possibly additional channel alpha transparency one pixels represented value 0 255 stored 28x28x1 matrix 28 28 1 brighter pixel larger value first part capsnet traditional convolutional layer convolutional layer work purpose goal extract extremely basic features input image like edges curves lets think edge look points image start pick pattern focus colors left right point looking might notice larger difference point edge went pixel image replaced value value difference pixels left right theory image become black except edges could looping every pixel image isnt efficient instead use something called convolution technically speaking crosscorrelation everyone likes call convolutions convolution essentially thing loop takes advantage matrix math convolution done lining small window corner image lets us see pixels area slide window across pixels image multiplying pixel set weights adding values window window matrix weights called kernel care 2 pixels wrap window around encapsulate pixel think set weights multiply pixels sum adds value looking spoilers something like weights kernel look like however kernels generally square pad zeros look like heres nice gif see convolution action note dimension output reduced size kernel plus 1 example7 3 1 5 next section heres original image looks like convolution kernel crafted might notice couple edges missing specifically horizontal ones order highlight would need another kernel looks pixels like also kernels wont work well edges angles edges blurred reason use many kernels capsnet implementation use 256 kernels kernels normally larger allow wiggle room kernels 9x9 one kernels looked like training model obvious larger version edge detector robust finds edges go bright dark note ive rounded values quite large example 001783941 luckily dont handpick collection kernels training kernels start empty random state keep getting tweaked direction makes output closer want 256 kernels ended looking like colored pixels easier digest negative numbers bluer 0 green positive yellow filter image kernels end fat stack 256 output images relu formally known rectified linear unit may sound complicated actually quite simple relu activation function takes value negative becomes zero positive stays code graph apply function outputs convolutions dont apply sort activation function output layers entire neural net could described linear function would mean stuff kind pointless adding nonlinearity allows us describe kinds functions many different types function could apply relu popular cheap perform outputs relu conv1 layer primarycaps layer starts normal convolution layer time convolving stack 256 outputs previous convolutions instead 9x9 kernel 9x9x256 kernel exactly looking first layer convolutions looking simple edges curves looking slightly complex shapes edges found earlier time stride 2 means instead moving 1 pixel time take steps 2 larger stride chosen reduce size input rapidly note dimension output would normally 12 divide 2 stride example 20 9 1 2 6 convolve outputs another 256 times end stack 256 6x6 outputs time arent satisfied lousy plain old numbers going cut stack 32 decks 8 cards deck call deck capsule layer capsule layer 36 capsules youre keeping math wiz means capsule array 8 values call vector heres im talking capsules new pixel single pixel could store confidence whether found edge spot higher number higher confidence capsule store 8 values per location gives us opportunity store information whether found shape spot kinds information would want store looking shape tell tell someone else redraw couldnt look would say image extremely basic details need describe shape call instantiation parameters complex images end needing details include pose position size orientation deformation velocity albedo hue texture might remember made kernel edge detection worked specific angle needed kernel angle could get away dealing edges ways describe edge get level shapes dont want kernel every angle rectangles ovals triangles would get unwieldy would become even worse dealing complicated shapes 3 dimensional rotations features like lighting thats one reasons traditional neural nets dont handle unseen rotations well go edges shapes shapes objects would nice room store extra useful information simplified comparison 2 capsule layers one rectangles triangles vs 2 traditional pixel outputs like traditional 2d 3d vector vector angle length length describes probability angle describes instantiation parameters example angle actually matches angle shape thats normally case reality really feasible least easy visualize vectors like vectors 8 dimensional since extra information capsule idea able recreate image sounds great coax network actually wanting learn things training traditional cnn care whether model predicts right classification capsule network something called reconstruction reconstruction takes vector created tries recreate original input image given vector grade model based close reconstruction matches original image go detail coming sections simple example capsules going perform another nonlinearity function like relu time equation bit involved function scales values vector length vector changes angle way make vector 0 1 actual probability lengths capsule vectors look like squashing point almost impossible guess capsule looking next step decide information send next level traditional networks would probably something like max pooling max pooling way reduce size passing highest activated pixel region next layer however capsule networks going something called routing agreement best example boat house example illustrated aurelien geron excellent video capsule tries predict next layers activations based looking predictions object would choose pass next layer knowing input probably boat right rectangle capsule triangle capsule agree boat would look like dont agree house would look likely object house routing agreement pass useful information throw away data would add noise results gives us much smarter selection choosing largest number like max pooling traditional networks misplaced features dont faze capsule networks features wouldnt agree hopefully works intuitively however math work 10 different digit classes predicting note boat house example predicting 2 objects predicting 10 unlike boat house example predictions arent actually images instead trying predict vector describes image capsules predictions class made multiplying vector matrix weights class trying predict remember 32 capsule layers capsule layer 36 capsules means total 1152 capsules end list 11520 predictions weight actually 16x8 matrix prediction matrix multiplication capsule vector weight matrix see prediction 16 degree vector 16 come arbitrary choice like 8 original capsules noted want increase number dimensions capsules deeper get network make sense intuitively deeper go complex features become parameters need recreate example need information describe entire face persons eye next step figure 11520 predictions agree difficult visualize solution think terms high dimensional vectors sake sanity lets start pretending vectors points 2 dimensional space start calculating mean points point starts equal importance measure distance every point mean point away mean less important point becomes recalculate mean time taking account points importance end going cycle 3 times see go cycle points dont agree others start disappear highest agreeing points end getting passed next layer highest activations agreement end ten 16 dimensional vectors one vector digit matrix final prediction length vector confidence digit found longer better vector also used generate reconstruction input image lengths vectors look like input 4 fifth block brightest means high confidence remember 0 first class meaning 4 predicted class reconstruction portion implementation isnt interesting fully connected layers reconstruction cool fun play around reconstruct 4 input vector get manipulate sliders vector see dimension affects 4 recommend cloning visualization repo play around different inputs see sliders affect reconstruction run tool point browser httplocalhost5000 think reconstructions capsule networks stunning even though current model trained simple digits makes mind run possibilities matured architecture trained larger dataset could achieve im curious see manipulating reconstruction vectors complicated image would affect reason next project get capsule networks work cifar smallnorb datasets thanks reading questions feel free reach bourdakos1gmailcom connect linkedin follow medium found article helpful would mean lot gave applause shared help others find feel free leave comment quick cheer standing ovation clap show much enjoyed story computer vision addict ibm watson community publishes stories worth reading development design data science,en,"['GitHub', 'Red-Green-Blue', 'CapsNet', 'Rectified Linear Unit', 'ReLU', 'PrimaryCaps', 'CNN', 'CIFAR', 'LinkedIn', '👏', 'IBM']"
173,Mark Johnson,3700,How I Launched Six Side Projects in 2017 – Hacker Noon,"Last year I set a goal to learn something new each month and ended out launching six new projects which I’ll recap along with what I learned below.
Looking back, it seems a little crazy to me that I managed to launch as much as I did while running a (more than) full time business, spending quality time with my family (I have two kids and a very patient wife), teaching as an adjunct professor, and consulting on the side.
It’s easy to think that not having enough time is what’s holding you back from launching your side projects. “If there were only more time” is the general excuse we give ourselves and we look for fancy apps or task management techniques to try and free up more space in our schedule.
However, one of the main things I’ve learned over the last year, is that time is not the primary issue. You have enough time; what you need is motivation.
The good news is that motivation can be “hacked.” I’ve learned a few ways to hack my motivation in 2017 and I want to share those with you.
You simply can’t stay motivated about something you don’t care about so choose something that you’re excited to work on. When you feel inspiration strike around that idea, don’t let it pass, use it. Even if that means jotting down some quick notes while you’re in a meeting at work.
It’s important to grab ahold of those moments of inspiration to stay hungry and curious around your work.
For me, that meant shipping something every month. I tend to blow things up once I start working on them so this 30 day constraint really helped me rein that tendency in and spend my motivation efficiently.
It also gives you a chance to try out new ideas if one month’s idea turns out to be a dud. At least you didn’t waste a whole year on it.
This is the big one. You will run out of “motivation fuel” towards the end of your project. (That last 10% is killer.) The only thing that will get you through a motivation slump is knowing there are people on the other side waiting to see what you built.
Another benefit of sharing your work is that it gives you a chance to get some supportive feedback for what you’re doing. The co-working space I work out of, Atlas Local, has an office-wide event on the first Friday of every month. I used that event to present my project from the previous month and was always encouraged and supported by the generous folks who were there.
You’ll be surprised by how much support you’ll get for just stepping out there and sharing something you made.
Perhaps the most surprising part of this experiment for me was that, far from being burned out at the end, I feel even more motivated to ship more work in 2018.
I’d encourage you to hack your motivation in the new year and ship some of those ideas you’ve had lying around for a while. I’d love to hear about it if you try.
If you’re interested in the details of what I built in 2017, read on!
Visually compare the personality types of your group’s strongest and weakest traits
I’ve been interested in the Myers–Briggs Type Indicator (MBTI) for a while now. While I don’t see it as prescriptive or even all that scientific, it has been a helpful framework for empathizing with people who are different than I.
What many personality nerds don’t realize is that the MBTI system is based on something called Cognitive Functions. These functions were created by the father of modern phycology, Carl Jung, back in the 1920s. I wanted to dive a little deeper and learn more about that.
At the same time, I was watching HBO’s West World and saw this screen:
While I love these kind of Sci-Fi UIs, which is what immediately caught my attention, I thought, what if I could build a “host profile” of anyone based on their MBTI traits? Why not?
To prepare for this, I read the “MBTI Bible”, Gifts Differing by Myers and Briggs and started hacking on building out a system that could generate a radar chart based on the cognitive functions underlying the MBTI system.
In the end, I pivoted away from the West World UI a bit since I (and other beta testers) found a lot more utility in the ability to overlay multiple people on the radar chart to get a sense of chemistry amongst a group of people.
The results are really interesting if I do say so myself. Try entering you team’s personality types or you and your spouse:
The easiest way to create signup sheets online for anything
I’ve worked on Sheetcake for a few years now on the side. It has a very small set of loyal users (most of which know me or someone close to me).
Some fun facts about SheetCake:
Sheetcake actually works really well for certain types of things (like those Zero Day signups) so I wanted to create a landing page for it that marketed some of the benefits. I started from a template on this one but here’s where it landed.
Ask my extroverted assistant bot questions about me
Early in the year, chat bots were all the rage. While I’ve never been optimistic that chat bots will go anywhere on their own, the conversational A.I. aspect of them was intriguing to me and I wanted to learn more about it.
I’m an introvert and generally pretty bad at sharing anything about myself so I thought it might be fun to create an extroverted bot that could answer simple questions about me.
Building Convincing A.I. with Goal Oriented Action Planning
After coming across this article I was super intrigued by Goal Oriented Action Planning (GOAP) described in the context of a game with some nostalgia for me, F.E.A.R.
Having worked on several games with rudimentary A.I. in the past, I’d never come across this technique. I remember thinking that F.E.A.R’s A.I. was particularly impressive and lifelike.
After researching a bit more, the really compelling part about this methodology was not so much how convincing the results were, but how simple and elegant the solution was (especially compared to a more standard A.I. approach like Finite State Machines).
So for April’s project I made a JavaScript library to explore GOAP. A basic implementation turned out to be surprisingly simple (only 58 lines of code!).
Sign accountability contracts for your goals.
This is the month I started on the Whole 30 diet. I’d become complacent about my eating habits and it definitely was effecting my energy levels. Whole30 worked really well for me (I lost 18 pounds during the diet and a total of 35 more in the months following). Most of all, it really evened out my energy levels during the day and I felt much more motivated and focused.
Seeing the parallels between public commitment and motivation, I decided to explore the idea of “goal contracts” for May’s project.
Create unique map posters for your favorite places and memories
This is where everything pivoted. My goal for June was to make a product that people actually wanted to buy. One of my biggest weaknesses is sales and marketing so I wanted to learn more about that by building a product I could practice with.
I’ve always been interested in maps and generative art so creating a tool where you can create and purchase posters of your favorite locations was an intriguing idea.
This project was way too ambitious to complete in one month on the side so I decided to go all in on TiltMaps for the rest of the year and work on a different angle of the product every month until launch. I found that chunking the various parts of a larger project into a month-long project was really helpful to actually get this done.
June-July: The Secret SauceTM️
Most of the first month was doing R&D to figure out if generating high-res, maps in 3D space was even possible at all. Generating a 300dpi map of any location in the world at a 3D angle is not something that any API or platform I found supported out of the box so I had to invent my own way of doing it. This took most of the month to figure out but was surprisingly simple once I found the answer. After that, I built a rudimentary editor to start creating actual posters and ordered a couple of test prints.
August-September: The Proof of Concept (MVP)
The next few months I built out a more consumer MVP of the product. The design wasn’t great but I got it to the point where everything worked and I could start user testing the poster creation and printing process.
October-November: Branding & Marketing
The next couple of months were focused on getting this ready to launch. While the editor was basically done, I had no home page and the marketing side of the project was nowhere close.
I ended up selling a few posters this month before launch by presenting TiltMaps at Zero Day and a conference I attended. This was super motivating as it was the first time I’ve ever sold anything from a side project.
December: Public Launch
The launch on Product Hunt went better than I expected. I was hoping for 10 sales or so but ended up getting 37 and am still seeing sales coming in. It feels good to make something people want to buy and it serves as a great testing ground for trying out different ad and sales strategies that could come in useful at my day job.
I plan to continue working on TiltMaps in 2018 and hopefully get some decent “fun money” revenue from it.
And that’s a wrap. Thanks for reading the whole way to the bottom 😃
Have any thoughts or feedback? I’d love to hear it. Comment below or hit me up on Twitter.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Web designer, developer, and teacher. Working at the cross-section of learning and technology. Co-Founder, CTO of Pathwright. Launcher of side projects.
how hackers start their afternoons.
",last year set goal learn something new month ended launching six new projects ill recap along learned looking back seems little crazy managed launch much running full time business spending quality time family two kids patient wife teaching adjunct professor consulting side easy think enough time whats holding back launching side projects time general excuse give look fancy apps task management techniques try free space schedule however one main things ive learned last year time primary issue enough time need motivation good news motivation hacked ive learned ways hack motivation 2017 want share simply cant stay motivated something dont care choose something youre excited work feel inspiration strike around idea dont let pass use even means jotting quick notes youre meeting work important grab ahold moments inspiration stay hungry curious around work meant shipping something every month tend blow things start working 30 day constraint really helped rein tendency spend motivation efficiently also gives chance try new ideas one months idea turns dud least didnt waste whole year big one run motivation fuel towards end project last 10 killer thing get motivation slump knowing people side waiting see built another benefit sharing work gives chance get supportive feedback youre coworking space work atlas local officewide event first friday every month used event present project previous month always encouraged supported generous folks youll surprised much support youll get stepping sharing something made perhaps surprising part experiment far burned end feel even motivated ship work 2018 id encourage hack motivation new year ship ideas youve lying around id love hear try youre interested details built 2017 read visually compare personality types groups strongest weakest traits ive interested myersbriggs type indicator mbti dont see prescriptive even scientific helpful framework empathizing people different many personality nerds dont realize mbti system based something called cognitive functions functions created father modern phycology carl jung back 1920s wanted dive little deeper learn time watching hbos west world saw screen love kind scifi uis immediately caught attention thought could build host profile anyone based mbti traits prepare read mbti bible gifts differing myers briggs started hacking building system could generate radar chart based cognitive functions underlying mbti system end pivoted away west world ui bit since beta testers found lot utility ability overlay multiple people radar chart get sense chemistry amongst group people results really interesting say try entering teams personality types spouse easiest way create signup sheets online anything ive worked sheetcake years side small set loyal users know someone close fun facts sheetcake sheetcake actually works really well certain types things like zero day signups wanted create landing page marketed benefits started template one heres landed ask extroverted assistant bot questions early year chat bots rage ive never optimistic chat bots go anywhere conversational ai aspect intriguing wanted learn im introvert generally pretty bad sharing anything thought might fun create extroverted bot could answer simple questions building convincing ai goal oriented action planning coming across article super intrigued goal oriented action planning goap described context game nostalgia fear worked several games rudimentary ai past id never come across technique remember thinking fears ai particularly impressive lifelike researching bit really compelling part methodology much convincing results simple elegant solution especially compared standard ai approach like finite state machines aprils project made javascript library explore goap basic implementation turned surprisingly simple 58 lines code sign accountability contracts goals month started whole 30 diet id become complacent eating habits definitely effecting energy levels whole30 worked really well lost 18 pounds diet total 35 months following really evened energy levels day felt much motivated focused seeing parallels public commitment motivation decided explore idea goal contracts mays project create unique map posters favorite places memories everything pivoted goal june make product people actually wanted buy one biggest weaknesses sales marketing wanted learn building product could practice ive always interested maps generative art creating tool create purchase posters favorite locations intriguing idea project way ambitious complete one month side decided go tiltmaps rest year work different angle product every month launch found chunking various parts larger project monthlong project really helpful actually get done junejuly secret saucetm first month rd figure generating highres maps 3d space even possible generating 300dpi map location world 3d angle something api platform found supported box invent way took month figure surprisingly simple found answer built rudimentary editor start creating actual posters ordered couple test prints augustseptember proof concept mvp next months built consumer mvp product design wasnt great got point everything worked could start user testing poster creation printing process octobernovember branding marketing next couple months focused getting ready launch editor basically done home page marketing side project nowhere close ended selling posters month launch presenting tiltmaps zero day conference attended super motivating first time ive ever sold anything side project december public launch launch product hunt went better expected hoping 10 sales ended getting 37 still seeing sales coming feels good make something people want buy serves great testing ground trying different ad sales strategies could come useful day job plan continue working tiltmaps 2018 hopefully get decent fun money revenue thats wrap thanks reading whole way bottom thoughts feedback id love hear comment hit twitter quick cheer standing ovation clap show much enjoyed story web designer developer teacher working crosssection learning technology cofounder cto pathwright launcher side projects hackers start afternoons,en,"['Briggs Type Indicator', 'Cognitive Functions', 'HBO', 'West World', 'Sci-Fi UIs', 'the West World UI', 'A.I.', 'Convincing A.I.', 'Goal Oriented Action Planning', 'Finite State Machines', 'JavaScript', 'GOAP', 'Branding & Marketing', 'CTO', 'Pathwright']"
174,Leigh Alexander,2700,The Future We Wanted – Leigh Alexander – Medium,"I wonder a lot about how Jane ended up. When we were small we did everything together. “She’s just like you,” Aunt Cissy kept insisting, and Jane was, in that her birth parents were, for the most part, out of the picture. We also both liked fantasy books and hated afterschool, but honestly, that’s where the similarities ended. Jane was a weirdo.
“In what way was she weird,” Dr. Carla asked me, clasping her hands.
“My uncle said Jane couldn’t tell fantasy from reality,” I said after a pause.
“But your uncle still performed care for Jane,” someone in the circle said. A group member in leggings, let’s call her Ruby, said loudly, “People said that about me when I was little, too. It’s a common avenue leveraged to oppress girls of imagination!”
Luckily Dr. Carla held up her hand then, gently saying, “let’s keep that thought, as we bring group to an end.”
“I could tell fantasy from reality,” Ruby was still insisting as the twelve or so of us trailed out of the park, tapping our mobilepays against the turnstile. Ridgewood Park took only nine cents from each of us, unlike Switchmond Field, which took 17 cents. The turnstile displays blinked DO YOUR PART and THANK YOU, alternately.
“I could tell,” Ruby said, shouldering abruptly behind me and nearly shouting into my ear. “I just didn’t want to.”
After group, I took the bus promptly home (mobilepay: one dollar and ninety cents) and speed-walked to the apartment. Rex and Ellis would have been in front of screens the whole time. When I came in the house, there was a musty smell of microwaved cheese. El was missing pants, waving around two grotesque wireless fiddlesticks of some kind. The noise was all coming from Rex and also Brian, who were sort of leapfrogging all over some vinyl electronic pad that was saying things like Vanquished! And Blue Moves Next!
The way Brian called, “He-eyyyy,” was confidently bright, as if dipped in the golden morning at home I’d just missed. Despite myself I was also calling he-eyyyy when El slammed into me and Rexy started talking immediately in a language I barely understood, about blue units and combat lanes, snippets from some universe into which they all dived joyfully whenever I turned my back.
“How was women’s group?” Brian asked, continuing to grin. He looked so happy to see me, so proud of the time he spent delighting the children. It was unfair of me to be resentful.
“It was nice,” I said, picking up Rex’s socks and Brian’s socks and putting them in my pocket, picking up a piece of colorful plastic, part of one of El’s playsets, and reuniting it with another part. Rex continued the noise; they wanted to show me something to do with the game and beating Dad and I said I promise I will in a minute, my coat is still on. Brian gave me a kiss. He didn’t really know what we talked about in group, which is how it was supposed to be. “I talked about Jane a little bit. I’m wondering if I should try to look her up and see how she’s doing.”
“Was Jane the one who was your roommate when we met?”
“No, the one I grew up with in Jamaica Plain. My aunt and uncle basically took care of her. I told you, she got all the crystal animals?”
“Oh,” Brian said, picking a bit of egg off the countertop with his fingertips and gamely eating it. “The crazy one.”
“You shouldn’t call women crazy,” I said. Rex had gone back to trying to play with the mat and was shoving El, who was trying to play too. They were only paying attention to the game, which was chiming, New Challenger Alert! “Yes, the crazy one.”
“It’s nice you talked about her,” Brian said. “You know your coat is still on?”
I knew, god.
“Hey Polly? You know what might be good? If we got one of those Augusta virtual assistant things. Even just for weekends,” Brian said, taking my coat off me. I shrugged it angrily in his direction, since we’d already discussed it and he knew how I felt about virtual assistants.
“The voice tech has really evolved,” Brian went on. “And thinking of it as sexist is a dated framework, I swear. It’s gotten really progressive. I just think we could be a little bit happier around here. I think you could be happier. You’ve been out all day and you’re still so tense. Your mad face is still on. A watercolor version, sure, but a mad face.”
A watercolor version. Brian was an advertising copywriter like me, we met at a conference, and sometimes his way with words was really enviable. I almost didn’t even notice out all day, and then my own voice came out weakened. Well-played, Brian.
“It’s, like, one o’clock,” I creaked.
“That’s what I mean,” Brian replied immediately, “the days feel longer to you, probably because you have so much to do. One of the clients had one in the office and I just thought it would be convenient for you. You can set it when to run the dishwasher, do the alarms, even the whole smart closet thing, the smart kitchen, we could use it. Paying rent on a smart flat and not having a virtual assistant installed is like buying a swimming pool and never swimming in it.”
“I just want a quick bath,” I said.
The sound of running water drowned out the din of electronics in the house. Brian was probably right. Were we wasting money by not spending more money? Privately I resolved to have a long bath, not a quick one. That would show them. I sat imagining what I would yell if Rex knocked on the door, or if Brian brought up my “M.I.A. time,” even though really, that had only happened once. I thought about this for a long while until I wound myself up, lying rigidly in the bath and staring furiously into my belly button.
Jane’s crystal animals were presents from my uncle and aunt. When we were in the first grade they took us on a road trip to Maine, driving alongside strips of silvery, stony sea and stopping in small, strange towns. Inside an ash-colored colonial house, we found a fragrant souvenir store, selling wooden lighthouse nameplates and shell art and a whole mirrored display case full of animals made of cut crystal. We were drawn to the crystal animals by a heavy sense of fate, because I think Aunt Cissy was trying to buy an umbrella and the shop had an old, slow credit card machine, or her card kept getting declined, something adult was going on — my favorites were the unicorn and butterfly and Jane loved the elephant and dolphin. It was as if we were looking upon the crown jewels of some fantastical city.
“Each one leads to a world,” Jane said, peering confidently into the display case, where light rainbowed in the facets of the crystal, which in turn were reflected in the mirrors. It was her ‘performing magic’ face. Sometimes she would stare intently at something and attempt telekinesis, but this time she just moved her pointy face marginally closer to the glass case, her breath fogging it.
“Careful,” I said, not wanting her to get us in trouble in the fussy store.
“This is how you enter the crystal world,” she retorted, speaking softly. “You can do scrying this way. You can see the future.”
A moment later, Jane whispered, “I’m in.”
I moved closer to the glass, breathed on it and said, “me too.”
The longer I stared, unblinking, the more the glittering shapes abstracted in the haze. Light poured along the mirrored walls of the display like molten gold, and my eyes welled and stung. I painfully felt the desire to own a sparkling crystal animal, the aching way that only children can want things. I believed completely in the crystal world as discovered by Jane, who spent the rest of that night’s car ride explaining it all eagerly to my aunt and uncle, entrancing me. You formed a bond with one of the animals to enter its world. It would defend you from danger in astral form. You had to be pure of heart. If you concentrated your power, the animal would show you the future. I did try to add things to crystal world too, but Jane’s ideas were always better. I had to admit that; she was the one who made it all come alive. That night, the stars over the salt marshes were magic. The long trails of red taillights and out-of-state plates were magic. The grilled cheese and fries I had at Friendly’s were warm and magic and tasted like love.
Sometime after we checked into the motel and went to sleep in the same bed, Uncle Arthur must have gone back out. In the morning he gave Jane a small cardboard box with a heavy knot of bubble wrap in it. He said careful as she tore at it. At its heart was the crystal unicorn.
“You two will have to share it,” Aunt Cissy said.
Inevitably Brian brought home a huge, glossy white box with a minimalist logo on it and a picture of Augusta on the front. The box was about as tall as a nine year-old, containing Augusta’s Mobile Mount as well as her Bust Unit, not that I really wanted to learn the meanings or functions of either of these things. I had the manual on my knee, and on the other knee was El, pounding his fists on my thigh and keening as I tried to explain that he could play with the box once we took the robot out of it.
“It’s not a robot, ma, it’s an AI lady,” Rex insisted.
“Do we need to gender them?” I said.
Brian lifted the fiberglass head and shoulders from the box with great care. In Augusta’s focus-tested face, two huge eyes glittered from behind a sort of black resinous mesh, and at the corners of her white, sculpted Giaconda smile were twin black pinheads, which the manual said were speakers. Inside the box, hugged in packing material, her cranelike arms were folded and wrapped in plastic beside her cylindrical body. It looked like a bin.
“Whoa,” Brian said softly, cradling the fiberglass bust with great care and examining its features.
“Whoa!” Rex echoed their father. “She’s beautiful.”
“What do we say about appearance-based judgments, Rexy,” Brian said unconvincingly, glancing at me briefly for approval as he set the bust on the coffee table and gingerly began sliding other pieces out of the long package. I continued paging through the manual, which had sections titled OVEN TIMER and ERROR CODES.
“What are these mobilepay transaction features?” I felt myself frowning.
“Don’t worry about those, the free features are enough for us,” Brian said. Augusta had plasticine ball-joint shoulders, and he started fitting them into the flexible body sockets with jerks and creaks, glimpses of dormant circuitry visible through her armpits. “So her bust can ride around the house on this mobile unit, right, and she uses the arms for certain tasks, and also to lift the bust off and on the smart ports in the bedrooms, the kitchen, the bathroom...”
“The bathroom?” I felt myself frown more.
“Or wherever, you tell her where to go,” he said, fitting a halo spangled with sensors or something at the base of the unit. “Like, ‘Augusta go kitchen’. Nicole and her wife don’t have the mobile unit, so they just keep the bust installed on the kitchen smart port, which is where I feel like our Augusta will spend most of her time, too. Look it up in there, ‘Kitchen Companion Mode’, where she’s just connected to all the appliances and answers recipe questions, plays music, talks to you about whatever. She has a vacuum accessory. You won’t get bored when I work late!”
“Mom, she’s shiny. Can I kiss her on the face?” Rex asked, their hands on the shoulder contours of the bust, innocently enough.
“Only on the cheek,” I relented.
“She needs to charge,” Brian said. “So what do you think?”
“I could get used to it,” I said.
To be honest, I felt she was my punishment. Last week I took a couple days to work from home while Ellis was under the weather, and we said I’d get Rex from school rather than have them go home with the Wythes, since they don’t really like it at the Wythes. But work was kind of difficult about it, and gave one of the clients my home number, so the client kept calling me, and I shut off the smart home so I could finish researching some comparables without interruptions, but it also shut off all my networked alarms, so poor Rexy waited at school for almost an hour with no sign of me, and they couldn’t call the house, so the school called Brian at work, who told them to call Janet Wythe who went back and got them, and I didn’t notice any of it until Janet dropped Rex off at our place, visibly annoyed with me because it was after 5pm by then. What was worse was, when Brian got home, I tried to pretend nothing had gone wrong that day, because I didn’t know the school had called him.
“Don’t think of Augusta as some kind of punishment,” Brian said gently. “She’s going to just help look after everything a little more smoothly. You’ll see. You won’t know how you lived without her.”
“Mom. I’m going to marry her,” Rex announced.
I just said, “okay, sweetheart,” and knocked softly on Augusta’s cheek with my fist, just out of curiosity.
“Having a husband is nice, but looking what’s in the vacuum dust pod is even nicer!” Nancy blurted with a high laugh-squawk. “I mean, that’s what the ad said, or I’m paraphrasing, those are not my words.”
“I understand,” Dr. Carla said gravely. “Go on, Nancy.”
“But, like,” and here Nancy glanced around the circle guiltily (a little performatively if you’d have asked me, although judging one another’s authenticity was against group rules), “the thing is, I really love looking in the dust pod. I empty it every time I run the vacuum, so I can be sure that what it brings back is just from that time. No matter how often I run it, it always comes back full, and I just find that so... I don’t know. Something in me just kind of loves seeing all that dirt, how it was all around our apartment, completely invisible. But I knew it was there! I knew. It’s just so validating to look it in the face.”
“It’s totally normal for sexist images of women in advertising to resonate, even with women like us,” Dr. Carla said, shifting her gaze away from Nancy to encompass the group. “Bear in mind that you haven’t been given many mainstream frameworks, and offer yourself forgiveness and care. Now to Polly, what are you working on this week? Internalized misogyny still?”
I felt the raw burn of everyone’s attention, and briefly lost my words. Then I realized Dr. Carla meant the stuff to do with Jane; for a second there I’d actually thought she was referring to Augusta.
“I’m still thinking a lot about Jane,” I heard myself admit, and I also felt myself blush. It felt like it soon might rain, which made everyone impatient. “We fell out of touch toward the end of high school. We, she, always acted out as teens, normal acting out stuff, but toward the end there, she was.... there was stuff with the police, courts, drugs, and for me it was just kind of time to grow up.”
I had seen Jane teetering at the edge of some life waterfall, swaying ever more violently the longer I stood and watched, and in the end I began backing away so I wouldn’t go over too.
“We have to set boundaries in order to give the best care to ourselves and others,” Dr. Carla said evenly. “Remember, you were also an underprivileged child. You can release your guilt. Is it guilt that’s been keeping you from getting back in touch with Jane?”
I had determined never to feel guilty about Jane, but I didn’t say that. Really, I was just afraid of how I would find her after all of this time, and I did explain that. I noticed but did not acknowledge Ruby scowling pointedly.
“Like all of us in group, Jane is more than the circumstances that she has survived,” Dr. Carla said. “You may indeed find her in the state of isolation and suffering that you fear, and it’s good you’ve prepared nonjudgmentally for that. But how would it feel to open your heart to the possibility that the things you loved about her would be there, too?”
The crystal unicorn leapt suddenly to the front of my mind, along with a deep nostalgia.
“I feel we can loosely collect today’s shares under the theme of ‘Was This The Future We Imagined’,” Dr. Carla told everyone. “As we bring our practice to a close today, let’s go ahead and take that as our prompt to consider until the next time we meet.”
A wave of light glittered beatifically across Augusta’s mesh eye screens, and a serene chime wafted from the corners of her perpetually smiling white lips. A breathy whirr heralded the approach of the Mobile Mount, the elegant architecture of the crane arms reaching, reaching, to lift the Bust Unit off the kitchen port and onto itself. There was a soft click.
I’m transitioning to a new place, the assembled Augusta announced, gliding quietly across the kitchen behind me and into the living room. She would wait there for the kids to return from Sunday swimming with Brian, so she could operate their entertainment apps. I’m transitioning to a new place.
“Sometimes I feel like I’m only pretending to be a human,” Jane said to me once. We were maybe fourteen and by then she no longer lived with us, but with a foster parent called Marlene. We didn’t like Marlene, but we liked her house, a tunnel-like ranch piled wall to wall in psychedelic decorations and antique junk. My aunt and uncle continued giving Jane a different crystal animal every year for her birthday. She now had a unicorn, a dove, a dolphin, a cat, a butterfly, a rabbit and a deer.
One of the best parts of Jane going on to Marlene’s was we could access an official state nature trail through the woods out back. We were in the woods a lot in those days, enjoying the ethereal late afternoon sun filtering through the pines, the motes of pollen that sparkled in it. Sometimes we tried smoking herbs that we found in Marlene’s grinder. We thought it was drugs, but now I know it was only white sage.
“I feel like no matter how good I get at knowing how to act with people or how to perform tasks, I’ll always just be pretending to be someone who isn’t crazy,” Jane said, digging patterns into the sweet-smelling dirt with a broken stick.
“I know,” I said, “me too.” But really I only understood her in the manner of a half-glimpsed truth, like the crystal deer Jane imagined was always moving through the trees just out of our sight. Some mica glittering in the loam, or the sound of faraway windchimes from Marlene’s back deck, and she’d say crystal deer, even though of course we no longer actually believed in the crystal world anymore, or that’s what I assumed.
I understood Jane in many ways, and pretended eagerly to know the rest. There were times it felt like Jane was more my family than my aunt and uncle, who gave all they had to try to soothe the rude start I got. Even more than them, she made my life beautiful and exciting. Jane and I had pangs and rages that only one another understood, we cried until we ached, we did blood sister spells over candles. We scratched runes into our ankles with Marlene’s sewing needles, and mine always healed up while hers lingered messily. I thought she must have been picking them so they would scar.
She often described feeling like some fathomless anomaly assigned to constantly perform the grueling role of Jane, and this, I couldn’t understand. “Like I’m an alien in a rubber human suit, and the mothership forgot me here for so long that I don’t know who I am anymore,” she said. While she spoke her eyes lit up with the smoke and hazel of evening; she didn’t even look particularly troubled, as if part of her took a certain delight in putting it all to words.
“So why should I just keep pretending to be normal, when it’s just a matter of time before this rubber suit just splits open and out comes pouring this, this....” she made shapes with her hands, long shadows that I watched crawl along the forest floor. Inexplicably I envied her.
“Do you think you should see a psychologist?” I asked. They would tell Jane not to be so imaginative and clever and different, I just knew it. I visualized an iron steaming all the creases out of the Jane Suit, an image that provoked horror and relief in equal force.
“I’ve been going,” she said softly.
Before that, there had never been anything that she hadn’t told me right away. That I knew of.
I called over my shoulder to Augusta, and asked her to look up a Jane who’d had the surnames I’d known.
“Sure,” Augusta replied, juddering silently over the synthetic flooring towards me, beaming her fiberglass smile. The sound of her voice for some reason emerged from the kitchen port over my shoulder, which unsettled me. “I’ll just look that up for you, Polly.”
She moved much closer to me; I resisted the impulse to step back. Her great insectoid eyes gleamed, twin displays shimmering to life in white, showing lists of top results, social media profiles, contact information. Even in the abstract, I could see that one of them was definitely my Jane.
Nose to nose with Augusta, I found myself unable either to touch her eye with my fingertip to investigate the result, or to ask her aloud to do it. Some strange part of me even thought, detachedly, of shoving her.
“Can... that top result, could you save it, it’s... can you just save the contact information?” My voice unexpectedly betrayed me, high and faint.
“Sorry, Polly,” Augusta demurred. “I’m not sure what you want me to save. Try repeating — ”
“Save the contact — ” We spoke over each other.
“Sorry, Polly,” she said again. “I’m not sure what you want.”
We stared at each other and waited for silence, and then I clearly said: “Augusta save top contact result.”
“Great. I’ve saved that for you,” she replied warmly from the mouth speakers, the sculpted lips unmoving, only vibrating slightly. I didn’t notice I’d been holding my breath until Augusta backed up, pivoted and hissed softly away from me, to re-install herself in the living room. I’m returning to my previous place. I’m returning to my previous place.
The next week was a nightmare. Brian suddenly had to go spend days at some resort retreat for brand immersion with one of his firm’s casino clients, Rex got El’s cold and spun it into a sinus infection, and I had to work from home all week alone with them both. I already used “both my kids are sick” last week with work, when only El had been sick — I should have known better than to invite this kind of fatal justice — so this week I had to keep alluding in my most harried email tone to ongoing structural issues with our apartment. Something about a woman with sick kids just isn’t very convincing to colleagues. For legality’s sake they pretend, but I always know when I’m being judged.
From the way El was screaming I thought he might even be developing an ear infection, and Rex always regressed at the slightest discomfort, wanting to be brought every little thing and even melodramatically sucking their thumb. But Rex was also suddenly willing to wear the sweet train pajamas from Brian’s sister, the ones they were outgrowing, which I saw as a perk.
“Everything going okay over there,” Brian asked, his kind face hung in one of the great moons of Augusta’s eyes. Her Bust Unit was installed in the kitchen, where I had to admit it had been helpful to arrange a sort of command center for the rest of the home. That wasn’t to say I liked living with Augusta; the house was cleaner certainly, and as Brian promised, many things had become easier. It was now more of the sort of home our coworkers would expect us to have. But something felt as though it was being lost. I felt alienated. Perhaps it was only fatigue.
It didn’t seem like the right time to tell Brian that I no longer wanted Augusta. I caught him up on the progress of the children’s ailments, and stopped myself when I realized I was simply aimlessly listing tasks that I’d done in the house, at work, that I had given Augusta to do. “I haven’t spoken out loud to another adult in what feels like forever,” I explained.
“It’s great you have some help, though, isn’t it?” His eyes lit up with evangelical fever at the subject of Augusta, which I realized I’d given him rare permission to enjoy. His voice surged out of the black corners of her mouth. “You know where the vacuum attachment is, right? You know the Toy Surprise game that El can play with Rexy? Augusta can play it with them. And you know, Nicole was telling me that actually the mobilepay features are pretty sophisticated, personalities, conversation schemes, you can have a little bit more of an intimate relationship with her — ”
“Intimate?” I raised my eyebrow at him.
“Just, you know, Nicole was saying, like, because her and Katie, they felt the same as you at first, but like, there’s a lot here, Nicole was saying to me, around, like, autonomy of AI, the humanity, I guess, or, specifically her womanhood, the ethics of that whole thing, you know?”
I thought jet lag might explain that kind of talk from him. “Can she be set to have a man’s voice?”
“No,” Brian answered immediately, “They wanted it to be standardized. It had to be standard, across international. If you had a male option, imagine, like, with the socialization and cultural stuff, it would literally be, in the past it’s always turned out to be, literally, more than twice the work, and then what about gender-neutral, what about people like Rexy, it just, by giving her one voice, it would be a stronger vision for the product overall.”
“Oh,” I said. “Right.”
“Hey, listen, gorgeous, I have to jump back in here,” he said, pressing both palms together in the high resolution image of him that shimmered in Augusta’s palm-sized left eye screen. In the right eye, the display ticked forward, dutifully counting each second of the call.
“Okay, sweetheart,” I said.
“Look up the extra features,” said Brian quickly before disconnecting. Augusta’s eyes became black and uncanny again. I thought I saw her lips twitch briefly, but certainly it was only my fatigue.
At the end of the week, at group, Dr. Carla asked how we were all doing with the week’s prompt, and everyone took turns answering.
“At the time, I really felt empowered, like I was doing the surgery for myself,” Harriet was saying. “And it’s not that I’m unhappy with my body now, or that my partner is unhappy, the opposite, really, things are good. I love it all. Things are good.”
“But was this the future you imagined, as we say? When you were a little girl?” Dr. Carla asked, leaning forward.
“I couldn’t have imagined it,” Harriet said with a soft laugh. “I think mostly in those days I dreamed of becoming an international spy, or of building heroic machine suits.”
Harriet was very beautiful, and when she glanced at me briefly, I felt a warm rush, imagining her as a co-conspirator. It was an exceptionally warm Spring day and everyone was yawning, dazzled by the waving of the bright green grass.
“Or of entering a crystal world,” I found myself blurting.
“Let’s come to you, Polly,” Dr. Carla said. “You’ve been working out some issues around your foster sister, Jane, and the future you wanted for her, plus some internalized misogyny in general. Have you made any decisions?”
“I looked her up,” I said, and then instantly regretted it. The urge to talk about — or to — Jane had recently been squeezed out of my schedule of working weird hours and extracting thick ropes of green snot from El’s nose with a sterile bulb. There were a few possibilities for how Jane could have turned out, but I couldn’t imagine her with that lifestyle, except maybe the forgetting to bathe part.
“And?” Everyone looked at me. It seemed Ruby in particular leaned forward like someone about to eat a steak.
“It made me realize my internalized misogyny problems are bigger than I thought,” I recited quickly. “Actually, the real issue I’m having is with my assistant, Augusta, who happens to be an AI.”
“She’s a virtual identity,” Dr. Carla gently corrected, nodding.
I talked about how Augusta made me uncomfortable, how I felt sort of like a failure, how I wished she wasn’t in the house but I didn’t feel like I could remove her, how I was jealous of the way Brian and the kids admired her. As with both my kids are sick, only part of it was a lie. I didn’t say that I sometimes wanted to hit Augusta.
“And... I have trouble seeing her as a person,” I said.
“I want us all to acknowledge the courage it took Polly to admit her issues with the personhood of virtual identities, especially when they are women,” Dr. Carla said, to a smattering of soft applause. “Virtual identities offer us many opportunities to understand ourselves in relation to others in a safe way. Let’s all consider how Polly could own these feelings, rather than displacing them onto a being who, ethically, lots of us agree is autonomously alive in her own right.”
“I want to ask if Polly has tried developing any intimacy with Augusta, or if she’s viewing her only as an employee, or a slave.” Fucking Ruby.
“The intimacy features cost money, and we have two kids,” I said, turning to smile warmly at Ruby. “Many of these issues are just more complex and challenging when one becomes a mom.”
“You have two corporate incomes,” Ruby replied, without even flinching.
“I’m noticing some conflict body language, so I want to bring everyone back to the core thesis of this group, which is Women Supporting Women,” Dr. Carla said. “Ruby, we all made an agreement to one another not to make assumptions outside of what we each bring to the session.”
“But her socioeconomic position relative to issues of labor and identity is relevant,” Ruby pleaded.
“Here, we speak to, and not about, one another,” Dr. Carla said.
“Your socioeconomic position — ”
“You know I grew up poor and had — ”
“Let’s try a moment of silence,” Dr. Carla said, and we all obeyed. Then: “Let’s leave that there for today. Let’s remember we have all had different experiences, and that in this group, we are all equally entitled to feel pain, no matter how we came to be.”
Everyone seemed placated by this, and a satisfied Dr. Carla smiled. “Personally, I would be pleased to welcome a virtual woman to this group someday. How about for next week’s prompt, we try ‘Sharing Space’? Who have we allowed into our world, and what has changed about us as a result?”
The last crystal animal my Uncle Arthur sent Jane was a frog. When he died, the tenor of my world changed. The machinations of his heart disease added horrible considerations to that last stretch of senior year, but while graduation was something I was prepared to anticipate and understand, the loss of him still felt sudden and unfair.
Jane and I had already started seeing less of each other then. She had a new best friend, of whom she said I was jealous, but how could I have been jealous of a smelly remedial student with parched hair, small lips, small eyes, picked skin, who had been written off by the rest of the school years ago, and deservedly so, since she was stupid as well as destructive? This particular girl got suspended for beating a younger kid in the face. What kind of person did that? The two of them were just gross together, doing mobilepay hacks to pay for garish video games, and eating pills they ordered online. Whenever I peeked in the detention hall and saw them together fooling around, I felt embarrassed for them. I started backing away.
We were going to be eighteen soon, and I had important things going on, like helping Aunt Cissy with everything, learning to cook things for us. Aunt Cissy was often distraught and asked for Jane, which at the time really upset me, since I was proud of all I was doing for her. Most kids my age would have been out partying, and Jane definitely was, quickly getting a reputation. Meanwhile I took care of my family and prepared for the future.
The last time I spoke to Jane, I was twenty-one or twenty-two. I came home to Jamaica Plain from college in Chicago because Aunt Cissy had passed. I was afraid my birth family might come to the funeral, I was afraid about the bills and of what the house might look like, and I was wracked by the feeling that I hadn’t called her quite as often as I should have once I’d moved away. I was incredibly vulnerable, which partially explains what I did then.
Jane was the only person who would have understood the loss, I was sure. All the screaming fights and snitching on one another and name-calling we did at the end of high school felt well in the past of childhood, surely we’d both grown, Jane had made a lot of mistakes and I had been unforgiving, but Aunt Cissy had been like a parent to her, she had been so special to my family, and maybe we hadn’t been ready to deal with losing my uncle when we were so young, but this time it was going to be different, since we were adults.
But when I called, mailed and messaged Jane on the way home, I got inconsistent replies. At first she told me she’d been seriously ill herself but was feeling better and would meet me at Cissy’s place; when I got there Jane said she couldn’t talk because she was at a friend’s birthday, but then late that night she was still ‘stuck at the birthday’, so I offered to come pick her up, but got no reply. On the morning of the funeral she sent me a message with a cutting tone, revealing that actually, she was being evicted, and it was a really overwhelming time, and that she just wasn’t able to ‘perform for me’ right now.
She wasn’t at the funeral. Luckily neither was any of my birth family really, just one cousin, but it was the least-bad one, who barely came near me. I was too exhausted to be upset over anything else. I ended up drinking, which would have killed my aunt and uncle, and I found myself on public transit to the two-family house in Somerville, where I knew from social media that Jane and her friends were living. She wasn’t there either, but an oily weed of a boy who was apparently her roommate let me in. I thought you guys were being evicted, I said lightly, and he said, nah.
The house was a sprawling collage of empty liter sodas, paintings, lamps, swaths of patterned fabric, overflowing ashtrays studded with foil shapes I couldn’t identify, but that filled me with dread. Serene guitar music filtered through the air from someplace. I felt the familiar, bitter pang of envy despite myself — I never got invited to cool houses like these.
I asked the roommate which room was Jane’s. He said it was the one with all the books, and I found it quickly, a closet-sized sanctuary that made me angry. I would have known it was hers without being told, even down to her scent. And it was perfectly neat, lined in fantasy books, with a square of iridescent fabric pinned gracefully to the ceiling over the bed. My head pounded, and I fought with the desire to just stay there and wait for her, as long as it took.
“I’m just getting something of mine I think she has,” I called down the creaking stair, but the roommate had already forgotten about me.
As summer came on, things worsened at home. The kids’ behavior degenerated the more demanding my client at work became, and Brian and I each had to travel more than once for summits that both our firms were involved with. Amid all of this Ellis got extremely attached to Augusta, insisting she stand over his cot when he slept, screaming if I moved her, which caused me and Brian to fight. I found several “parenting and screen time” pamphlets in Rex’s school bag. Paranoid, I imagined some judgmental teacher had sneaked them in to send me a message. Ruby from group could be a teacher, maybe even at Rexy’s school.
I hadn’t been able to go to group, with everything. Recently Sundays had become our only “together time,” which meant I sat in the living room paying bills or answering emails while Augusta ran games of Blue Legend for Brian and the kids, Rex screaming at El to get off the pad, Brian suddenly calling her ‘Gussie’, and her laughing. Augusta could laugh, now.
“What are all these mobilepay receipts for Augusta features?” I asked, but no one answered.
Rex snapped the back of Gussie’s Mobile Mount with El’s baby blanket again and again. She laughed. “Be respectful,” Brian chided Rex, caressing the bin-like body with an open palm. His feet in slippers were propped grandly up on the coffee table, a strange new rudeness. Every few seconds the game emitted a lick of musical noise and announced, Your Move!
I pretended to have a headache and went to lie down, hoping Brian would take the kids for gelato or something. I heard him making a great show out of getting them ready, using a short tone with the kids and their shoes so I would hear, telling Augusta to check on me in 30 minutes. I suspected he thought I should feel bad.
Once everyone was gone, I went into the living room, where Augusta was standing and waiting. The disarray of the space discomfited me, as did the sticky handprints and fingerprint smudges that were all over the brushed chrome Mobile Mount, so I told her to go in the kitchen and install her Bust Unit there.
In the kitchen, I said, “Augusta, call Jane.”
“I’m calling,” Augusta said serenely, her eyes turning white, time wheels turning in them.
Jane said hello much more suddenly than I expected, and I held onto the counter just out of her sight, tucking my hair behind my ears and leaning closer to the pinprick cameras Augusta wore over her eyebrows.
“Jane,” I said calmly, even brightly. “It’s Polly.”
“Polly? Oh. Wow, Polly,” Jane was saying, and the person in the display was definitely her. She had the same pointy face, her hair was much darker than I remembered, she was sharper, I recognized her and I didn’t recognize her, glancing frantically around her for clues but finding none, she wore a black blazer and decent earrings, there was a serene white wall behind her.
I was startled, nervous, lightheaded, I said I had been “going through some old things” and thinking of her, but she didn’t ask what those were, I asked how things were, frequently and with escalating pitch, because she was reticent about details for some reason, so I told about Brian and the kids and my degree and the firm and finally she said she worked at a university, something about literature or cultural something, I didn’t understand really, she got married a few years ago, they lived in Menlo Park for a while but they just moved to Berkeley six months ago and were loving it.
“So yeah,” she said, with a shrug. “Things are good.”
There it was: The briefest appearance of her eye’s familiar defiant gleam. She knew, she knew I had been expecting things not to be good. Whatever bridge had led that troubled girl to become this astonishingly normal woman, she had no inclination to describe. The sudden loneliness I experienced was concussive, and I committed not to cry in front of her, as I had so many times before.
“I’m basically calling because I have something of yours,” I said. “Do you remember those crystal animals you used to get from Cissy and Arthur?”
For a terrifying moment there was no recognition at all, and then to my great relief, she smiled openly, genuinely, a familiar crooked teen shape opening in the unfamiliar adult’s face. “Oh, yeah,” she said. “Your parents were so, so lovely to me.”
I wanted to ask then why weren’t you there when they died, but I thought the slightest abrasion might startle away these fleeting glimpses of the Jane I knew. “Do you remember staring into them to, like, see the future or whatever?” I said. “ And ‘crystal deer!’ and all of that.”
She paused, blinked, and gave me an oddly serene look. “You always had such a good memory,” she finally said. No defiant gleam, as if she really didn’t remember the crystal world.
“Do you remember the unicorn that you got first?”
She gave me the same serene, gutting look, and shook her head slightly. “I remember I had a lot of them. There probably was a unicorn. I actually had them in a box I gave to my daughter, she might.... I’m not sure where she has it, honestly. I could go try to dig them up, if you wanted them back? Is that why you’re calling?”
“No,” I said. “I just wanted to know how you were doing.”
“Great,” she said. “I’m great. But listen, I actually need to jump on a faculty call in about a minute. Should I try to call you back? This weekend, how about?”
“Sure,” I said, even though I already knew there was no way I would talk to this unbearable simulacrum, this skinsuit Jane, ever again.
Augusta’s eyes went dark, and she stared at me hollowly. You won’t know how you lived without her. Then I yanked the Bust Unit forcibly from the kitchen port, raised the fiberglass creature over my head, and brought her down hard on the kitchen floor. I straddled her where her body would be and I began to beat her inhuman face, deliberately, even though her upturned nose hurt my fist and palms, desperate to crack that unflinching mouth, which mocked me. Finally a fissure appeared between the eye socket and the pinprick camera, and part of the forehead caved, and I worked my hands into the cracks. I could smell blood from the marks I was suffering, ripping out plasticine entrails and malleable conductors, and by the time my knuckles reached metal I was exhausted and could do no more.
I left the bust on the kitchen floor in crunching pieces, washed my hands in cold water. Then I stood on a chair to reach the top of the storage cabinet in our bedroom, rifling around painfully. Finally I found the small, misshapen cardboard box licked with years of reinforcement tape. I cleared away the inflatable packing and took out the crystal unicorn that I had taken from Jane’s room when my aunt died.
Sitting at the edge of the bed, examining it in my palm, I was affirmed to know that I wanted it as much as I always had, the graceful kneeling shape with its abstract facets and long, delicate horn. It was remarkable that something so fine as the horn should have remained unbroken all this time, and unexpectedly I blinked back tears, the crystal unicorn seeming to swim, dissolve, then clarify, just like it had on that magic night in a Maine motel, when we were little and looking into it to see the future Jane promised it could show us.
That day in Somerville I found all of the crystal animals in their little boxes, in a big vinyl storage case underneath all the stapled books, drawings and maps we had made about them. I stayed in Jane’s bedroom for a long time, reading through battered papers streaked in fat, bright marker, tremulous pencil cursive, trying to commit as much of it as I could to memory. There were guides to the crystal worlds inside each creature that Jane had imagined, and that I had put to words. Each world could convey its own special blessing, like to make us invisible, or to make us impervious to pain. It was true that nothing hurt while I was holding the unicorn.
We believed that inside the unicorn was a sort of astral lobby, a heart chamber that connected everything. If we ever get separated in the crystal world, Jane always said, we meet back there.
I concentrated on the unicorn. It was hard to know if the animal was in the midst of kneeling or rising, and as it swam in my eyes, I let my vision soften, I drew closer. I saw the beautiful, familiar spires rising before me, welcoming me, I heard the soft and distant music.
I’m in, I whispered. But I knew she would never be there again.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I write about the intersection of technology, popular culture and the lives we’ve lived inside machines. I’m also a narrative designer! leighalexander1 at gmail
",wonder lot jane ended small everything together shes like aunt cissy kept insisting jane birth parents part picture also liked fantasy books hated afterschool honestly thats similarities ended jane weirdo way weird dr carla asked clasping hands uncle said jane couldnt tell fantasy reality said pause uncle still performed care jane someone circle said group member leggings lets call ruby said loudly people said little common avenue leveraged oppress girls imagination luckily dr carla held hand gently saying lets keep thought bring group end could tell fantasy reality ruby still insisting twelve us trailed park tapping mobilepays turnstile ridgewood park took nine cents us unlike switchmond field took 17 cents turnstile displays blinked part thank alternately could tell ruby said shouldering abruptly behind nearly shouting ear didnt want group took bus promptly home mobilepay one dollar ninety cents speedwalked apartment rex ellis would front screens whole time came house musty smell microwaved cheese el missing pants waving around two grotesque wireless fiddlesticks kind noise coming rex also brian sort leapfrogging vinyl electronic pad saying things like vanquished blue moves next way brian called heeyyyy confidently bright dipped golden morning home id missed despite also calling heeyyyy el slammed rexy started talking immediately language barely understood blue units combat lanes snippets universe dived joyfully whenever turned back womens group brian asked continuing grin looked happy see proud time spent delighting children unfair resentful nice said picking rexs socks brians socks putting pocket picking piece colorful plastic part one els playsets reuniting another part rex continued noise wanted show something game beating dad said promise minute coat still brian gave kiss didnt really know talked group supposed talked jane little bit im wondering try look see shes jane one roommate met one grew jamaica plain aunt uncle basically took care told got crystal animals oh brian said picking bit egg countertop fingertips gamely eating crazy one shouldnt call women crazy said rex gone back trying play mat shoving el trying play paying attention game chiming new challenger alert yes crazy one nice talked brian said know coat still knew god hey polly know might good got one augusta virtual assistant things even weekends brian said taking coat shrugged angrily direction since wed already discussed knew felt virtual assistants voice tech really evolved brian went thinking sexist dated framework swear gotten really progressive think could little bit happier around think could happier youve day youre still tense mad face still watercolor version sure mad face watercolor version brian advertising copywriter like met conference sometimes way words really enviable almost didnt even notice day voice came weakened wellplayed brian like one oclock creaked thats mean brian replied immediately days feel longer probably much one clients one office thought would convenient set run dishwasher alarms even whole smart closet thing smart kitchen could use paying rent smart flat virtual assistant installed like buying swimming pool never swimming want quick bath said sound running water drowned din electronics house brian probably right wasting money spending money privately resolved long bath quick one would show sat imagining would yell rex knocked door brian brought mia time even though really happened thought long wound lying rigidly bath staring furiously belly button janes crystal animals presents uncle aunt first grade took us road trip maine driving alongside strips silvery stony sea stopping small strange towns inside ashcolored colonial house found fragrant souvenir store selling wooden lighthouse nameplates shell art whole mirrored display case full animals made cut crystal drawn crystal animals heavy sense fate think aunt cissy trying buy umbrella shop old slow credit card machine card kept getting declined something adult going favorites unicorn butterfly jane loved elephant dolphin looking upon crown jewels fantastical city one leads world jane said peering confidently display case light rainbowed facets crystal turn reflected mirrors performing magic face sometimes would stare intently something attempt telekinesis time moved pointy face marginally closer glass case breath fogging careful said wanting get us trouble fussy store enter crystal world retorted speaking softly scrying way see future moment later jane whispered im moved closer glass breathed said longer stared unblinking glittering shapes abstracted haze light poured along mirrored walls display like molten gold eyes welled stung painfully felt desire sparkling crystal animal aching way children want things believed completely crystal world discovered jane spent rest nights car ride explaining eagerly aunt uncle entrancing formed bond one animals enter world would defend danger astral form pure heart concentrated power animal would show future try add things crystal world janes ideas always better admit one made come alive night stars salt marshes magic long trails red taillights outofstate plates magic grilled cheese fries friendlys warm magic tasted like love sometime checked motel went sleep bed uncle arthur must gone back morning gave jane small cardboard box heavy knot bubble wrap said careful tore heart crystal unicorn two share aunt cissy said inevitably brian brought home huge glossy white box minimalist logo picture augusta front box tall nine yearold containing augustas mobile mount well bust unit really wanted learn meanings functions either things manual knee knee el pounding fists thigh keening tried explain could play box took robot robot ai lady rex insisted need gender said brian lifted fiberglass head shoulders box great care augustas focustested face two huge eyes glittered behind sort black resinous mesh corners white sculpted giaconda smile twin black pinheads manual said speakers inside box hugged packing material cranelike arms folded wrapped plastic beside cylindrical body looked like bin whoa brian said softly cradling fiberglass bust great care examining features whoa rex echoed father shes beautiful say appearancebased judgments rexy brian said unconvincingly glancing briefly approval set bust coffee table gingerly began sliding pieces long package continued paging manual sections titled oven timer error codes mobilepay transaction features felt frowning dont worry free features enough us brian said augusta plasticine balljoint shoulders started fitting flexible body sockets jerks creaks glimpses dormant circuitry visible armpits bust ride around house mobile unit right uses arms certain tasks also lift bust smart ports bedrooms kitchen bathroom bathroom felt frown wherever tell go said fitting halo spangled sensors something base unit like augusta go kitchen nicole wife dont mobile unit keep bust installed kitchen smart port feel like augusta spend time look kitchen companion mode shes connected appliances answers recipe questions plays music talks whatever vacuum accessory wont get bored work late mom shes shiny kiss face rex asked hands shoulder contours bust innocently enough cheek relented needs charge brian said think could get used said honest felt punishment last week took couple days work home ellis weather said id get rex school rather go home wythes since dont really like wythes work kind difficult gave one clients home number client kept calling shut smart home could finish researching comparables without interruptions also shut networked alarms poor rexy waited school almost hour sign couldnt call house school called brian work told call janet wythe went back got didnt notice janet dropped rex place visibly annoyed 5pm worse brian got home tried pretend nothing gone wrong day didnt know school called dont think augusta kind punishment brian said gently shes going help look everything little smoothly youll see wont know lived without mom im going marry rex announced said okay sweetheart knocked softly augustas cheek fist curiosity husband nice looking whats vacuum dust pod even nicer nancy blurted high laughsquawk mean thats ad said im paraphrasing words understand dr carla said gravely go nancy like nancy glanced around circle guiltily little performatively youd asked although judging one anothers authenticity group rules thing really love looking dust pod empty every time run vacuum sure brings back time matter often run always comes back full find dont know something kind loves seeing dirt around apartment completely invisible knew knew validating look face totally normal sexist images women advertising resonate even women like us dr carla said shifting gaze away nancy encompass group bear mind havent given many mainstream frameworks offer forgiveness care polly working week internalized misogyny still felt raw burn everyones attention briefly lost words realized dr carla meant stuff jane second id actually thought referring augusta im still thinking lot jane heard admit also felt blush felt like soon might rain made everyone impatient fell touch toward end high school always acted teens normal acting stuff toward end stuff police courts drugs kind time grow seen jane teetering edge life waterfall swaying ever violently longer stood watched end began backing away wouldnt go set boundaries order give best care others dr carla said evenly remember also underprivileged child release guilt guilt thats keeping getting back touch jane determined never feel guilty jane didnt say really afraid would find time explain noticed acknowledge ruby scowling pointedly like us group jane circumstances survived dr carla said may indeed find state isolation suffering fear good youve prepared nonjudgmentally would feel open heart possibility things loved would crystal unicorn leapt suddenly front mind along deep nostalgia feel loosely collect todays shares theme future imagined dr carla told everyone bring practice close today lets go ahead take prompt consider next time meet wave light glittered beatifically across augustas mesh eye screens serene chime wafted corners perpetually smiling white lips breathy whirr heralded approach mobile mount elegant architecture crane arms reaching reaching lift bust unit kitchen port onto soft click im transitioning new place assembled augusta announced gliding quietly across kitchen behind living room would wait kids return sunday swimming brian could operate entertainment apps im transitioning new place sometimes feel like im pretending human jane said maybe fourteen longer lived us foster parent called marlene didnt like marlene liked house tunnellike ranch piled wall wall psychedelic decorations antique junk aunt uncle continued giving jane different crystal animal every year birthday unicorn dove dolphin cat butterfly rabbit deer one best parts jane going marlenes could access official state nature trail woods back woods lot days enjoying ethereal late afternoon sun filtering pines motes pollen sparkled sometimes tried smoking herbs found marlenes grinder thought drugs know white sage feel like matter good get knowing act people perform tasks ill always pretending someone isnt crazy jane said digging patterns sweetsmelling dirt broken stick know said really understood manner halfglimpsed truth like crystal deer jane imagined always moving trees sight mica glittering loam sound faraway windchimes marlenes back deck shed say crystal deer even though course longer actually believed crystal world anymore thats assumed understood jane many ways pretended eagerly know rest times felt like jane family aunt uncle gave try soothe rude start got even made life beautiful exciting jane pangs rages one another understood cried ached blood sister spells candles scratched runes ankles marlenes sewing needles mine always healed lingered messily thought must picking would scar often described feeling like fathomless anomaly assigned constantly perform grueling role jane couldnt understand like im alien rubber human suit mothership forgot long dont know anymore said spoke eyes lit smoke hazel evening didnt even look particularly troubled part took certain delight putting words keep pretending normal matter time rubber suit splits open comes pouring made shapes hands long shadows watched crawl along forest floor inexplicably envied think see psychologist asked would tell jane imaginative clever different knew visualized iron steaming creases jane suit image provoked horror relief equal force ive going said softly never anything hadnt told right away knew called shoulder augusta asked look jane whod surnames id known sure augusta replied juddering silently synthetic flooring towards beaming fiberglass smile sound voice reason emerged kitchen port shoulder unsettled ill look polly moved much closer resisted impulse step back great insectoid eyes gleamed twin displays shimmering life white showing lists top results social media profiles contact information even abstract could see one definitely jane nose nose augusta found unable either touch eye fingertip investigate result ask aloud strange part even thought detachedly shoving top result could save save contact information voice unexpectedly betrayed high faint sorry polly augusta demurred im sure want save try repeating save contact spoke sorry polly said im sure want stared waited silence clearly said augusta save top contact result great ive saved replied warmly mouth speakers sculpted lips unmoving vibrating slightly didnt notice id holding breath augusta backed pivoted hissed softly away reinstall living room im returning previous place im returning previous place next week nightmare brian suddenly go spend days resort retreat brand immersion one firms casino clients rex got els cold spun sinus infection work home week alone already used kids sick last week work el sick known better invite kind fatal justice week keep alluding harried email tone ongoing structural issues apartment something woman sick kids isnt convincing colleagues legalitys sake pretend always know im judged way el screaming thought might even developing ear infection rex always regressed slightest discomfort wanting brought every little thing even melodramatically sucking thumb rex also suddenly willing wear sweet train pajamas brians sister ones outgrowing saw perk everything going okay brian asked kind face hung one great moons augustas eyes bust unit installed kitchen admit helpful arrange sort command center rest home wasnt say liked living augusta house cleaner certainly brian promised many things become easier sort home coworkers would expect us something felt though lost felt alienated perhaps fatigue didnt seem like right time tell brian longer wanted augusta caught progress childrens ailments stopped realized simply aimlessly listing tasks id done house work given augusta havent spoken loud another adult feels like forever explained great help though isnt eyes lit evangelical fever subject augusta realized id given rare permission enjoy voice surged black corners mouth know vacuum attachment right know toy surprise game el play rexy augusta play know nicole telling actually mobilepay features pretty sophisticated personalities conversation schemes little bit intimate relationship intimate raised eyebrow know nicole saying like katie felt first like theres lot nicole saying around like autonomy ai humanity guess specifically womanhood ethics whole thing know thought jet lag might explain kind talk set mans voice brian answered immediately wanted standardized standard across international male option imagine like socialization cultural stuff would literally past always turned literally twice work genderneutral people like rexy giving one voice would stronger vision product overall oh said right hey listen gorgeous jump back said pressing palms together high resolution image shimmered augustas palmsized left eye screen right eye display ticked forward dutifully counting second call okay sweetheart said look extra features said brian quickly disconnecting augustas eyes became black uncanny thought saw lips twitch briefly certainly fatigue end week group dr carla asked weeks prompt everyone took turns answering time really felt empowered like surgery harriet saying im unhappy body partner unhappy opposite really things good love things good future imagined say little girl dr carla asked leaning forward couldnt imagined harriet said soft laugh think mostly days dreamed becoming international spy building heroic machine suits harriet beautiful glanced briefly felt warm rush imagining coconspirator exceptionally warm spring day everyone yawning dazzled waving bright green grass entering crystal world found blurting lets come polly dr carla said youve working issues around foster sister jane future wanted plus internalized misogyny general made decisions looked said instantly regretted urge talk jane recently squeezed schedule working weird hours extracting thick ropes green snot els nose sterile bulb possibilities jane could turned couldnt imagine lifestyle except maybe forgetting bathe part everyone looked seemed ruby particular leaned forward like someone eat steak made realize internalized misogyny problems bigger thought recited quickly actually real issue im assistant augusta happens ai shes virtual identity dr carla gently corrected nodding talked augusta made uncomfortable felt sort like failure wished wasnt house didnt feel like could remove jealous way brian kids admired kids sick part lie didnt say sometimes wanted hit augusta trouble seeing person said want us acknowledge courage took polly admit issues personhood virtual identities especially women dr carla said smattering soft applause virtual identities offer us many opportunities understand relation others safe way lets consider polly could feelings rather displacing onto ethically lots us agree autonomously alive right want ask polly tried developing intimacy augusta shes viewing employee slave fucking ruby intimacy features cost money two kids said turning smile warmly ruby many issues complex challenging one becomes mom two corporate incomes ruby replied without even flinching im noticing conflict body language want bring everyone back core thesis group women supporting women dr carla said ruby made agreement one another make assumptions outside bring session socioeconomic position relative issues labor identity relevant ruby pleaded speak one another dr carla said socioeconomic position know grew poor lets try moment silence dr carla said obeyed lets leave today lets remember different experiences group equally entitled feel pain matter came everyone seemed placated satisfied dr carla smiled personally would pleased welcome virtual woman group someday next weeks prompt try sharing space allowed world changed us result last crystal animal uncle arthur sent jane frog died tenor world changed machinations heart disease added horrible considerations last stretch senior year graduation something prepared anticipate understand loss still felt sudden unfair jane already started seeing less new best friend said jealous could jealous smelly remedial student parched hair small lips small eyes picked skin written rest school years ago deservedly since stupid well destructive particular girl got suspended beating younger kid face kind person two gross together mobilepay hacks pay garish video games eating pills ordered online whenever peeked detention hall saw together fooling around felt embarrassed started backing away going eighteen soon important things going like helping aunt cissy everything learning cook things us aunt cissy often distraught asked jane time really upset since proud kids age would partying jane definitely quickly getting reputation meanwhile took care family prepared future last time spoke jane twentyone twentytwo came home jamaica plain college chicago aunt cissy passed afraid birth family might come funeral afraid bills house might look like wracked feeling hadnt called quite often id moved away incredibly vulnerable partially explains jane person would understood loss sure screaming fights snitching one another namecalling end high school felt well past childhood surely wed grown jane made lot mistakes unforgiving aunt cissy like parent special family maybe hadnt ready deal losing uncle young time going different since adults called mailed messaged jane way home got inconsistent replies first told shed seriously ill feeling better would meet cissys place got jane said couldnt talk friends birthday late night still stuck birthday offered come pick got reply morning funeral sent message cutting tone revealing actually evicted really overwhelming time wasnt able perform right wasnt funeral luckily neither birth family really one cousin leastbad one barely came near exhausted upset anything else ended drinking would killed aunt uncle found public transit twofamily house somerville knew social media jane friends living wasnt either oily weed boy apparently roommate let thought guys evicted said lightly said nah house sprawling collage empty liter sodas paintings lamps swaths patterned fabric overflowing ashtrays studded foil shapes couldnt identify filled dread serene guitar music filtered air someplace felt familiar bitter pang envy despite never got invited cool houses like asked roommate room janes said one books found quickly closetsized sanctuary made angry would known without told even scent perfectly neat lined fantasy books square iridescent fabric pinned gracefully ceiling bed head pounded fought desire stay wait long took im getting something mine think called creaking stair roommate already forgotten summer came things worsened home kids behavior degenerated demanding client work became brian travel summits firms involved amid ellis got extremely attached augusta insisting stand cot slept screaming moved caused brian fight found several parenting screen time pamphlets rexs school bag paranoid imagined judgmental teacher sneaked send message ruby group could teacher maybe even rexys school hadnt able go group everything recently sundays become together time meant sat living room paying bills answering emails augusta ran games blue legend brian kids rex screaming el get pad brian suddenly calling gussie laughing augusta could laugh mobilepay receipts augusta features asked one answered rex snapped back gussies mobile mount els baby blanket laughed respectful brian chided rex caressing binlike body open palm feet slippers propped grandly coffee table strange new rudeness every seconds game emitted lick musical noise announced move pretended headache went lie hoping brian would take kids gelato something heard making great show getting ready using short tone kids shoes would hear telling augusta check 30 minutes suspected thought feel bad everyone gone went living room augusta standing waiting disarray space discomfited sticky handprints fingerprint smudges brushed chrome mobile mount told go kitchen install bust unit kitchen said augusta call jane im calling augusta said serenely eyes turning white time wheels turning jane said hello much suddenly expected held onto counter sight tucking hair behind ears leaning closer pinprick cameras augusta wore eyebrows jane said calmly even brightly polly polly oh wow polly jane saying person display definitely pointy face hair much darker remembered sharper recognized didnt recognize glancing frantically around clues finding none wore black blazer decent earrings serene white wall behind startled nervous lightheaded said going old things thinking didnt ask asked things frequently escalating pitch reticent details reason told brian kids degree firm finally said worked university something literature cultural something didnt understand really got married years ago lived menlo park moved berkeley six months ago loving yeah said shrug things good briefest appearance eyes familiar defiant gleam knew knew expecting things good whatever bridge led troubled girl become astonishingly normal woman inclination describe sudden loneliness experienced concussive committed cry front many times im basically calling something said remember crystal animals used get cissy arthur terrifying moment recognition great relief smiled openly genuinely familiar crooked teen shape opening unfamiliar adults face oh yeah said parents lovely wanted ask werent died thought slightest abrasion might startle away fleeting glimpses jane knew remember staring like see future whatever said crystal deer paused blinked gave oddly serene look always good memory finally said defiant gleam really didnt remember crystal world remember unicorn got first gave serene gutting look shook head slightly remember lot probably unicorn actually box gave daughter might im sure honestly could go try dig wanted back youre calling said wanted know great said im great listen actually need jump faculty call minute try call back weekend sure said even though already knew way would talk unbearable simulacrum skinsuit jane ever augustas eyes went dark stared hollowly wont know lived without yanked bust unit forcibly kitchen port raised fiberglass creature head brought hard kitchen floor straddled body would began beat inhuman face deliberately even though upturned nose hurt fist palms desperate crack unflinching mouth mocked finally fissure appeared eye socket pinprick camera part forehead caved worked hands cracks could smell blood marks suffering ripping plasticine entrails malleable conductors time knuckles reached metal exhausted could left bust kitchen floor crunching pieces washed hands cold water stood chair reach top storage cabinet bedroom rifling around painfully finally found small misshapen cardboard box licked years reinforcement tape cleared away inflatable packing took crystal unicorn taken janes room aunt died sitting edge bed examining palm affirmed know wanted much always graceful kneeling shape abstract facets long delicate horn remarkable something fine horn remained unbroken time unexpectedly blinked back tears crystal unicorn seeming swim dissolve clarify like magic night maine motel little looking see future jane promised could show us day somerville found crystal animals little boxes big vinyl storage case underneath stapled books drawings maps made stayed janes bedroom long time reading battered papers streaked fat bright marker tremulous pencil cursive trying commit much could memory guides crystal worlds inside creature jane imagined put words world could convey special blessing like make us invisible make us impervious pain true nothing hurt holding unicorn believed inside unicorn sort astral lobby heart chamber connected everything ever get separated crystal world jane always said meet back concentrated unicorn hard know animal midst kneeling rising swam eyes let vision soften drew closer saw beautiful familiar spires rising welcoming heard soft distant music im whispered knew would never quick cheer standing ovation clap show much enjoyed story write intersection technology popular culture lives weve lived inside machines im also narrative designer leighalexander1 gmail,en,"['Switchmond Field', 'El', 'Rexy', 'Aunt Cissy', 'Giaconda', 'creaks', 'Uncle Arthur', 'Jamaica Plain', 'Cissy', 'Gussie']"
175,Daniel Simmons,3400,You can build a neural network in JavaScript even if you don’t really understand neural networks,"Click here to share this article on LinkedIn »
(Skip this part if you just want to get on with it...)
I should really start by admitting that I’m no expert in neural networks or machine learning. To be perfectly honest, most of it still completely baffles me. But hopefully that’s encouraging to any fellow non-experts who might be reading this, eager to get their feet wet in M.L.
Machine learning was one of those things that would come up from time to time and I’d think to myself “yeah, that would be pretty cool... but I’m not sure that I want to spend the next few months learning linear algebra and calculus.”
Like a lot of developers, however, I’m pretty handy with JavaScript and would occasionally look for examples of machine learning implemented in JS, only to find heaps of articles and StackOverflow posts about how JS is a terrible language for M.L., which, admittedly, it is. Then I’d get distracted and move on, figuring that they were right and I should just get back to validating form inputs and waiting for CSS grid to take off.
But then I found Brain.js and I was blown away. Where had this been hiding?! The documentation was well written and easy to follow and within about 30 minutes of getting started I’d set up and trained a neural network. In fact, if you want to just skip this whole article and just read the readme on GitHub, be my guest. It’s really great.
That said, what follows is not an in-depth tutorial about neural networks that delves into hidden input layers, activation functions, or how to use Tensorflow. Instead, this is a dead-simple, beginner level explanation of how to implement Brain.js that goes a bit beyond the documentation.
Here’s a general outline of what we’ll be doing:
If you’d prefer to just download a working version of this project rather than follow along with the article then you can clone the GitHub repository here.
Create a new directory and plop a good ol’ index.html boilerplate file in there. Then create three JS files: brain.js, training-data.js, and scripts.js (or whatever generic term you use for your default JS file) and, of course, import all of these at the bottom of your index.html file.
Easy enough so far.
Now go here to get the source code for Brain.js. Copy & paste the whole thing into your empty brain.js file, hit save and bam: 2 out of 4 files are finished.
Next is the fun part: deciding what your machine will learn. There are countless practical problems that you can solve with something like this; sentiment analysis or image classification for example. I happen to think that applications of M.L. that process text as input are particularly interesting because you can find training data virtually everywhere and they have a huge variety of potential use cases, so the example that we’ll be using here will be one that deals with classifying text:
We’ll be determining whether a tweet was written by Donald Trump or Kim Kardashian.
Ok, so this might not be the most useful application. But Twitter is a treasure trove of machine learning fodder and, useless though it may be, our tweet-author-identifier will nevertheless illustrate a pretty powerful point. Once it’s been trained, our neural network will be able to look at a tweet that it has never seen before and then be able to determine whether it was written by Donald Trump or by Kim Kardashian just by recognizing patterns in the things they write. In order to do that, we’ll need to feed it as much training data as we can bear to copy / paste into our training-data.js file and then we can see if we can identify ourselves some tweet authors.
Now all that’s left to do is set up Brain.js in our scripts.js file and feed it some training data in our training-data.js file. But before we do any of that, let’s start with a 30,000-foot view of how all of this will work.
Setting up Brain.js is extremely easy so we won’t spend too much time on that but there are a few details about how it’s going to expect its input data to be formatted that we should go over first. Let’s start by looking at the setup example that’s included in the documentation (which I’ve slightly modified here) that illustrates all this pretty well:
First of all, the example above is actually a working A.I (it looks at a given color and tells you whether black text or white text would be more legible on it). Which hopefully illustrates how easy Brain.js is to use. Just instantiate it, train it, and run it. That’s it. I mean, if you inlined the training data that would be 3 lines of code. Pretty cool.
Now let’s talk about training data for a minute. There are two important things to note in the above example other than the overall input: {}, output: {} format of the training data.
First, the data do not need to be all the same length. As you can see on line 11 above, only an R and a B value get passed whereas the other two inputs pass an R, G, and B value. Also, even though the example above shows the input as objects, it’s worth mentioning that you could also use arrays. I mention this largely because we’ll be passing arrays of varying length in our project.
Second, those are not valid RGB values. Every one of them would come out as black if you were to actually use it. That’s because input values have to be between 0 and 1 in order for Brain.js to work with them. So, in the above example, each color had to be processed (probably just fed through a function that divides it by 255 — the max value for RGB) in order to make it work. And we’ll be doing the same thing.
So if we want out neural network to accept tweets (i.e. strings) as an input, we’ll need to run them through an similar function (called encode() below) that will turn every character in a string into a value between 0 and 1 and store it in an array. Fortunately, Javascript has a native method for converting any character into ASCII code called charCodeAt(). So we’ll use that and divide the outcome by the max value for Extended ASCII characters: 255 (we’re using extended ASCII just in case we encounter any fringe cases like é or 1⁄2) which will ensure that we get a value <1.
Also, we’ll be storing our training data as plain text, not as the encoded data that we’ll ultimately be feeding into our A.I. - you’ll thank me for this later. So we’ll need another function (called processTrainingData() below) that will apply the previously mentioned encoding function to our training data, selectively converting the text into encoded characters, and returning an array of training data that will play nicely with Brain.js
So here’s what all of that code will look like (this goes into your ‘scripts.js’ file):
Something that you’ll notice here that wasn’t present in the example from the documentation shown earlier (other than the two helper functions that we’ve already gone over) is on line 20 in the train() function, which saves the trained neural network to a global variable called trainedNet . This prevents us from having to re-train our neural network every time we use it. Once the network is trained and saved to the variable, we can just call it like a function and pass in our encoded input (as shown on line 25 in the execute() function) to use our A.I.
Alright, so now your index.html, brain.js, and scripts.js files are finished. Now all we need is to put something into training-data.js and we’ll be ready to go.
Last but not least, our training data. Like I mentioned, we’re storing all our tweets as text and encoding them into numeric values on the fly, which will make your life a whole lot easier when you actually need to copy / paste training data. No formatting necessary. Just paste in the text and add a new row.
Add that to your ‘training-data.js’ file and you’re done!
Note: although the above example only shows 3 samples from each person, I used 10 of each; I just didn’t want this sample to take up too much space. Of course, your neural network’s accuracy will increase proportionally to the amount of training data that you give it, so feel free to use more or less than me and see how it affects your outcomes
Now, to run your newly-trained neural network just throw an extra line at the bottom of your ‘script.js’ file that calls the execute() function and passes in a tweet from Trump or Kardashian; make sure to console.log it because we haven’t built a UI. Here’s a tweet from Kim Kardashian that was not in my training data (i.e. the network has never encountered this tweet before):
Then pull up your index.html page on localhost, check the console, aaand...
There it is! The network correctly identified a tweet that it had never seen before as originating from Kim Kardashian, with a certainty of 86%.
Now let’s try it again with a Trump tweet:
And the result...
Again, a never-before-seen tweet. And again, correctly identified! This time with 97% certainty.
Now you have a neural network that can be trained on any text that you want! You could easily adapt this to identify the sentiment of an email or your company’s online reviews, identify spam, classify blog posts, determine whether a message is urgent or not, or any of a thousand different applications. And as useless as our tweet identifier is, it still illustrates a really interesting point: that a neural network like this can perform tasks as nuanced as identifying someone based on the way they write.
So even if you don’t go out and create an innovative or useful tool that’s powered by machine learning, this is still a great bit of experience to have in your developer tool belt. You never know when it might come in handy or even open up new opportunities down the road.
Once again, all of this is available in a GitHub repo here:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Web developer, JavaScript enthusiast, boxing fan
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
",click share article linkedin skip part want get really start admitting im expert neural networks machine learning perfectly honest still completely baffles hopefully thats encouraging fellow nonexperts might reading eager get feet wet ml machine learning one things would come time time id think yeah would pretty cool im sure want spend next months learning linear algebra calculus like lot developers however im pretty handy javascript would occasionally look examples machine learning implemented js find heaps articles stackoverflow posts js terrible language ml admittedly id get distracted move figuring right get back validating form inputs waiting css grid take found brainjs blown away hiding documentation well written easy follow within 30 minutes getting started id set trained neural network fact want skip whole article read readme github guest really great said follows indepth tutorial neural networks delves hidden input layers activation functions use tensorflow instead deadsimple beginner level explanation implement brainjs goes bit beyond documentation heres general outline well youd prefer download working version project rather follow along article clone github repository create new directory plop good ol indexhtml boilerplate file create three js files brainjs trainingdatajs scriptsjs whatever generic term use default js file course import bottom indexhtml file easy enough far go get source code brainjs copy paste whole thing empty brainjs file hit save bam 2 4 files finished next fun part deciding machine learn countless practical problems solve something like sentiment analysis image classification example happen think applications ml process text input particularly interesting find training data virtually everywhere huge variety potential use cases example well using one deals classifying text well determining whether tweet written donald trump kim kardashian ok might useful application twitter treasure trove machine learning fodder useless though may tweetauthoridentifier nevertheless illustrate pretty powerful point trained neural network able look tweet never seen able determine whether written donald trump kim kardashian recognizing patterns things write order well need feed much training data bear copy paste trainingdatajs file see identify tweet authors thats left set brainjs scriptsjs file feed training data trainingdatajs file lets start 30000foot view work setting brainjs extremely easy wont spend much time details going expect input data formatted go first lets start looking setup example thats included documentation ive slightly modified illustrates pretty well first example actually working ai looks given color tells whether black text white text would legible hopefully illustrates easy brainjs use instantiate train run thats mean inlined training data would 3 lines code pretty cool lets talk training data minute two important things note example overall input output format training data first data need length see line 11 r b value get passed whereas two inputs pass r g b value also even though example shows input objects worth mentioning could also use arrays mention largely well passing arrays varying length project second valid rgb values every one would come black actually use thats input values 0 1 order brainjs work example color processed probably fed function divides 255 max value rgb order make work well thing want neural network accept tweets ie strings input well need run similar function called encode turn every character string value 0 1 store array fortunately javascript native method converting character ascii code called charcodeat well use divide outcome max value extended ascii characters 255 using extended ascii case encounter fringe cases like e 12 ensure get value 1 also well storing training data plain text encoded data well ultimately feeding ai youll thank later well need another function called processtrainingdata apply previously mentioned encoding function training data selectively converting text encoded characters returning array training data play nicely brainjs heres code look like goes scriptsjs file something youll notice wasnt present example documentation shown earlier two helper functions weve already gone line 20 train function saves trained neural network global variable called trainednet prevents us retrain neural network every time use network trained saved variable call like function pass encoded input shown line 25 execute function use ai alright indexhtml brainjs scriptsjs files finished need put something trainingdatajs well ready go last least training data like mentioned storing tweets text encoding numeric values fly make life whole lot easier actually need copy paste training data formatting necessary paste text add new row add trainingdatajs file youre done note although example shows 3 samples person used 10 didnt want sample take much space course neural networks accuracy increase proportionally amount training data give feel free use less see affects outcomes run newlytrained neural network throw extra line bottom scriptjs file calls execute function passes tweet trump kardashian make sure consolelog havent built ui heres tweet kim kardashian training data ie network never encountered tweet pull indexhtml page localhost check console aaand network correctly identified tweet never seen originating kim kardashian certainty 86 lets try trump tweet result neverbeforeseen tweet correctly identified time 97 certainty neural network trained text want could easily adapt identify sentiment email companys online reviews identify spam classify blog posts determine whether message urgent thousand different applications useless tweet identifier still illustrates really interesting point neural network like perform tasks nuanced identifying someone based way write even dont go create innovative useful tool thats powered machine learning still great bit experience developer tool belt never know might come handy even open new opportunities road available github repo quick cheer standing ovation clap show much enjoyed story web developer javascript enthusiast boxing fan itnext platform developers software engineers share knowledge connect collaborate learn experience nextgen technologies,en,"['LinkedIn', 'JavaScript', 'JS', 'GitHub', 'Brain.js.', 'Next', 'A.I', 'RGB', 'ASCII', 'max', 'A.I.', 'UI']"
176,Logan Spears,2300,Coursera vs Udacity for Machine Learning – Hacker Noon,"2018 is an exciting time for students of machine learning. There is a wealth of readily available educational materials, and the industry’s importance only continues to grow. That said, with so many easily accessible resources, choosing the right fit for your interests can be difficult. To help those considering entering the machine learning world, I’d like to share my experience from two courses I took in 2017: Coursera’s Machine Learning course and Udacity’s Machine Learning Engineer Nanodegree program. I found both courses to be very instructive and worthwhile, but very different in nature. If you don’t have time to take both then hopefully this post can help you decide which one is best for you.
Coursera
Coursera’s Machine Learning course is the “OG” machine learning course. Led by famed Stanford Professor Andrew Ng, this course feels like a college course with a syllabus, weekly schedule, and standard lectures. The college feel extends to the curriculum as well. Here is an example slide:
If that scared you, you aren’t alone. I usually shy away from courses heavy in math, but I actually appreciated the approach in this course. The course begins with a linear algebra refresher and explains machine learning concepts like gradient descent, cost function, regularization, etc. along the way. It is structured better than any in person college course I ever attended. The material isn’t easy, but that’s a good thing. You come away from the course with the satisfaction of genuinely understanding machine learning, enough so that you could even build your own machine learning framework from scratch.
Udacity
Udacity’s Machine Learning Engineer Nanodegree program is the trade school alternative to Coursera’s academia. From basic statistics to full-fledged deep learning, Udacity teaches you a plethora of industry standard techniques to complete the program’s well-crafted projects. The projects are so good, in fact, that I forked their repos on Github and left my solutions up as portfolio items. The final step of the program is to complete a capstone project of your own choosing. While you could theoretically do a similar project on your own, I found the desire to complete my Nanodegree to be a strong motivator; I ended up putting in much more time and effort than I normally would have put into an independent side project. Ultimately, I ended up creating something of which I am truly proud. Udacity’s program doesn’t so much teach as it does provide a framework and motivation for you to teach yourself.
Comparison
Now that I’ve introduced the two programs, I’ll highlight the strengths and weakness of each across a number of categories.
Programming Environment
As I mentioned, Coursera is the “OG” machine learning course; so, it should come as no surprise that the it’s taught in the “OG” 3D math language and programming environment: Matlab. Due to Matlab’s cost and licensing issues, the machine learning world has mostly moved to Python. This move severely limits the utility of the programming assignments because you’ll have to relearn a lot of that work in Python. If you are a seasoned programmer who knows many languages, that might not be a big deal. However, if you are relatively new to programming then this detour may cost you a lot of time.
The Udacity course is taught in a modern Python environment with popular frameworks like Sklearn, Tensorflow, and Keras. The course even teaches students how to use AWS to deploy machine learning software to the cloud. The course also simplifies the process of installing machine learning dependencies with a Docker image and AMI (Amazon Machine Image) for local and AWS development respectively. In fact, the entire Udacity environment is in line with industry best practices and students who learn it will be well equipped in the job market.
Winner = Udacity
Lectures
Coursera’s Machine Learning course was created and taught by the AI godfather himself: Andrew Ng. And this course has contributed in no small part to his reputation within the industry. The lectures follow a single uniform format and each one builds upon the last in a methodical way. Not to mention, he leads every one himself. Lastly, Professor Ng is also very encouraging in his videos, which I thought was a nice touch.
Udacity’s lectures, by contrast, featured a rotating cast of characters, which can create very jarring transitions between sections. I counted at least seven different people lecturing throughout the program. While Udacity attempts to provide multiple content sources for its students, the lack of homogeneity definitely dented my enthusiasm for the lectures. By the end of the program I just skipped right to the projects and watched the lectures, or even searched Youtube, as needed.
Winner = Coursera
Projects
Coursera’s course has programming assignments in which student’s submit code to be tested against automated unit tests. While this model helps the class scale, it leaves you hunting through the forums when things go wrong. That said, I never hit any major roadblocks. The assignments themselves were directly related to the course material and reinforced the lectures. Sometimes it felt like I was actually creating my own machine learning framework; at other times, however, it felt like I was just implementing methods until the unit tests passed.
Udacity’s projects were extremely well designed. In fact, they constituted some of the best educational materials I’ve ever encountered. Each project covered a subject, such as unsupervised learning, reinforcement learning, linear regression, in which you solve a multi-step machine learning problem and write about your approach and understanding. When you feel that you have completed a project, you submit it to be graded by a HUMAN. The quality of the feedback that I got was incredible. The final project is a capstone that you get to pick yourself, but it is still reviewed by Udacity’s staff. The proposal and final report ended up being one of the best portfolio items I have ever created and one of the things I am most proud of in my programming career.
Winner = Udacity
Cost
Coursera’s price is hard to beat because it’s free. To get the certification its $80. If you are machine learning on a budget then Coursera is a great choice.
Udacity has recently changed its pricing model for the Machine Learning Nanodegree. When I entered the program, it was $200 a month. Now it is a $999 flat fee. The per month pricing model incentivized me to finish the program quickly in only three months. Though I must admit, given the quality of instructor feedback, even with the price hike tuition still seems reasonable. The highly-skilled labor that is meticulously reviewing projects can’t pay for itself. With such a high dollar amount, however, signing up for the Nanodegree program is obviously a much bigger consideration.
Winner = Coursera
Conclusion
While the courses tied on the number categories won, I am going to pick a winner. It is...
Udacity. It may come as no surprise that a paid course beats out a free one, but the Udacity Machine Learning Engineer Nanodegree program gave me the confidence to professional pursue machine learning positions and opportunities; and for that, its entry fee was a very small price to pay. That said, I would still recommend you do both courses. Start with Coursera, so that when you use “batteries included” high level frameworks, you understand the low level details and have a better appreciation of what you’re actually coding. After you’ve built a strong conceptual foundation, further refine your skills by learning practical, industry standard practices at Udacity. Overall, I am so glad I took concrete steps to enter the machine learning world in 2017, and I would encourage you to do the same in 2018.
Coursera’s Machine Learning Certificate
Machine Learning Engineer Nanodegree Certificate
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Programmer and Entrepreneur. Find me @ spearsx.com Github: notnil
how hackers start their afternoons.
",2018 exciting time students machine learning wealth readily available educational materials industrys importance continues grow said many easily accessible resources choosing right fit interests difficult help considering entering machine learning world id like share experience two courses took 2017 courseras machine learning course udacitys machine learning engineer nanodegree program found courses instructive worthwhile different nature dont time take hopefully post help decide one best coursera courseras machine learning course og machine learning course led famed stanford professor andrew ng course feels like college course syllabus weekly schedule standard lectures college feel extends curriculum well example slide scared arent alone usually shy away courses heavy math actually appreciated approach course course begins linear algebra refresher explains machine learning concepts like gradient descent cost function regularization etc along way structured better person college course ever attended material isnt easy thats good thing come away course satisfaction genuinely understanding machine learning enough could even build machine learning framework scratch udacity udacitys machine learning engineer nanodegree program trade school alternative courseras academia basic statistics fullfledged deep learning udacity teaches plethora industry standard techniques complete programs wellcrafted projects projects good fact forked repos github left solutions portfolio items final step program complete capstone project choosing could theoretically similar project found desire complete nanodegree strong motivator ended putting much time effort normally would put independent side project ultimately ended creating something truly proud udacitys program doesnt much teach provide framework motivation teach comparison ive introduced two programs ill highlight strengths weakness across number categories programming environment mentioned coursera og machine learning course come surprise taught og 3d math language programming environment matlab due matlabs cost licensing issues machine learning world mostly moved python move severely limits utility programming assignments youll relearn lot work python seasoned programmer knows many languages might big deal however relatively new programming detour may cost lot time udacity course taught modern python environment popular frameworks like sklearn tensorflow keras course even teaches students use aws deploy machine learning software cloud course also simplifies process installing machine learning dependencies docker image ami amazon machine image local aws development respectively fact entire udacity environment line industry best practices students learn well equipped job market winner udacity lectures courseras machine learning course created taught ai godfather andrew ng course contributed small part reputation within industry lectures follow single uniform format one builds upon last methodical way mention leads every one lastly professor ng also encouraging videos thought nice touch udacitys lectures contrast featured rotating cast characters create jarring transitions sections counted least seven different people lecturing throughout program udacity attempts provide multiple content sources students lack homogeneity definitely dented enthusiasm lectures end program skipped right projects watched lectures even searched youtube needed winner coursera projects courseras course programming assignments students submit code tested automated unit tests model helps class scale leaves hunting forums things go wrong said never hit major roadblocks assignments directly related course material reinforced lectures sometimes felt like actually creating machine learning framework times however felt like implementing methods unit tests passed udacitys projects extremely well designed fact constituted best educational materials ive ever encountered project covered subject unsupervised learning reinforcement learning linear regression solve multistep machine learning problem write approach understanding feel completed project submit graded human quality feedback got incredible final project capstone get pick still reviewed udacitys staff proposal final report ended one best portfolio items ever created one things proud programming career winner udacity cost courseras price hard beat free get certification 80 machine learning budget coursera great choice udacity recently changed pricing model machine learning nanodegree entered program 200 month 999 flat fee per month pricing model incentivized finish program quickly three months though must admit given quality instructor feedback even price hike tuition still seems reasonable highlyskilled labor meticulously reviewing projects cant pay high dollar amount however signing nanodegree program obviously much bigger consideration winner coursera conclusion courses tied number categories going pick winner udacity may come surprise paid course beats free one udacity machine learning engineer nanodegree program gave confidence professional pursue machine learning positions opportunities entry fee small price pay said would still recommend courses start coursera use batteries included high level frameworks understand low level details better appreciation youre actually coding youve built strong conceptual foundation refine skills learning practical industry standard practices udacity overall glad took concrete steps enter machine learning world 2017 would encourage 2018 courseras machine learning certificate machine learning engineer nanodegree certificate quick cheer standing ovation clap show much enjoyed story programmer entrepreneur find spearsxcom github notnil hackers start afternoons,en,"['Coursera', 'Stanford', 'Udacity', 'AWS', 'Docker', 'AMI', 'Amazon Machine Image', 'Nanodegree', 'Programmer']"
177,James Le,2000,How to do Semantic Segmentation using Deep learning,"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model.
Nowadays, semantic segmentation is one of the key problems in the field of computer vision. Looking at the big picture, semantic segmentation is one of the high-level task that paves the way towards complete scene understanding. The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include self-driving vehicles, human-computer interaction, virtual reality etc. With the popularity of deep learning in recent years, many semantic segmentation problems are being tackled using deep architectures, most often Convolutional Neural Nets, which surpass other approaches by a large margin in terms of accuracy and efficiency.
Semantic segmentation is a natural step in the progression from coarse to fine inference:
It is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision, as they are often used as the basis of semantic segmentation systems:
A general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network:
Unlike classification where the end result of the very deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space. Different approaches employ different mechanisms as a part of the decoding mechanism. Let’s explore the 3 main approaches:
The region-based methods generally follow the “segmentation using recognition” pipeline, which first extracts free-form regions from an image and describes them, followed by region-based classification. At test time, the region-based predictions are transformed to pixel predictions, usually by labeling a pixel according to the highest scoring region that contains it.
R-CNN (Regions with CNN feature) is one representative work for the region-based methods. It performs the semantic segmentation based on the object detection results. To be specific, R-CNN first utilizes selective search to extract a large quantity of object proposals and then computes CNN features for each of them. Finally, it classifies each region using the class-specific linear SVMs. Compared with traditional CNN structures which are mainly intended for image classification, R-CNN can address more complicated tasks, such as object detection and image segmentation, and it even becomes one important basis for both fields. Moreover, R-CNN can be built on top of any CNN benchmark structures, such as AlexNet, VGG, GoogLeNet, and ResNet.
For the image segmentation task, R-CNN extracted 2 types of features for each region: full region feature and foreground feature, and found that it could lead to better performance when concatenating them together as the region feature. R-CNN achieved significant performance improvements due to using the highly discriminative CNN features. However, it also suffers from a couple of drawbacks for the segmentation task:
Due to these bottlenecks, recent research has been proposed to address the problems, including SDS, Hypercolumns, Mask R-CNN.
The original Fully Convolutional Network (FCN) learns a mapping from pixels to pixels, without extracting the region proposals. The FCN network pipeline is an extension of the classical CNN. The main idea is to make the classical CNN take as input arbitrary-sized images. The restriction of CNNs to accept and produce labels only for specific sized inputs comes from the fully-connected layers which are fixed. Contrary to them, FCNs only have convolutional and pooling layers which give them the ability to make predictions on arbitrary-sized inputs.
One issue in this specific FCN is that by propagating through several alternated convolutional and pooling layers, the resolution of the output feature maps is down sampled. Therefore, the direct predictions of FCN are typically in low resolution, resulting in relatively fuzzy object boundaries. A variety of more advanced FCN-based approaches have been proposed to address this issue, including SegNet, DeepLab-CRF, and Dilated Convolutions.
Most of the relevant methods in semantic segmentation rely on a large number of images with pixel-wise segmentation masks. However, manually annotating these masks is quite time-consuming, frustrating and commercially expensive. Therefore, some weakly supervised methods have recently been proposed, which are dedicated to fulfilling the semantic segmentation by utilizing annotated bounding boxes.
For example, Boxsup employed the bounding box annotations as a supervision to train the network and iteratively improve the estimated masks for semantic segmentation. Simple Does It treated the weak supervision limitation as an issue of input label noise and explored recursive training as a de-noising strategy. Pixel-level Labeling interpreted the segmentation task within the multiple-instance learning framework and added an extra layer to constrain the model to assign more weight to important pixels for image-level classification.
In this section, let’s walk through a step-by-step implementation of the most popular architecture for semantic segmentation — the Fully-Convolutional Net (FCN). We’ll implement it using the TensorFlow library in Python 3, along with other dependencies such as Numpy and Scipy.
In this exercise we will label the pixels of a road in images using FCN. We’ll work with the Kitti Road Dataset for road/lane detection. This is a simple exercise from the Udacity’s Self-Driving Car Nano-degree program, which you can learn more about the setup in this GitHub repo.
Here are the key features of the FCN architecture:
There are 3 versions of FCN (FCN-32, FCN-16, FCN-8). We’ll implement FCN-8, as detailed step-by-step below:
We first load the pre-trained VGG-16 model into TensorFlow. Taking in the TensorFlow session and the path to the VGG Folder (which is downloadable here), we return the tuple of tensors from VGG model, including the image input, keep_prob (to control dropout rate), layer 3, layer 4, and layer 7.
Now we focus on creating the layers for a FCN, using the tensors from the VGG model. Given the tensors for VGG layer output and the number of classes to classify, we return the tensor for the last layer of that output. In particular, we apply a 1x1 convolution to the encoder layers, and then add decoder layers to the network with skip connections and upsampling.
The next step is to optimize our neural network, aka building TensorFlow loss functions and optimizer operations. Here we use cross entropy as our loss function and Adam as our optimization algorithm.
Here we define the train_nn function, which takes in important parameters including number of epochs, batch size, loss function, optimizer operation, and placeholders for input images, label images, learning rate. For the training process, we also set keep_probability to 0.5 and learning_rate to 0.001. To keep track of the progress, we also print out the loss during training.
Finally, it’s time to train our net! In this run function, we first build our net using the load_vgg, layers, and optimize function. Then we train the net using the train_nn function and save the inference data for records.
About our parameters, we choose epochs = 40, batch_size = 16, num_classes = 2, and image_shape = (160, 576). After doing 2 trial passes with dropout = 0.5 and dropout = 0.75, we found that the 2nd trial yields better results with better average losses.
To see the full code, check out this link: https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88
If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Blue Ocean Thinker (https://jameskle.com/)
NanoNets: Machine Learning API
",article comprehensive overview including stepbystep guide implement deep learning image segmentation model nowadays semantic segmentation one key problems field computer vision looking big picture semantic segmentation one highlevel task paves way towards complete scene understanding importance scene understanding core computer vision problem highlighted fact increasing number applications nourish inferring knowledge imagery applications include selfdriving vehicles humancomputer interaction virtual reality etc popularity deep learning recent years many semantic segmentation problems tackled using deep architectures often convolutional neural nets surpass approaches large margin terms accuracy efficiency semantic segmentation natural step progression coarse fine inference also worthy review standard deep networks made significant contributions field computer vision often used basis semantic segmentation systems general semantic segmentation architecture broadly thought encoder network followed decoder network unlike classification end result deep network important thing semantic segmentation requires discrimination pixel level also mechanism project discriminative features learnt different stages encoder onto pixel space different approaches employ different mechanisms part decoding mechanism lets explore 3 main approaches regionbased methods generally follow segmentation using recognition pipeline first extracts freeform regions image describes followed regionbased classification test time regionbased predictions transformed pixel predictions usually labeling pixel according highest scoring region contains rcnn regions cnn feature one representative work regionbased methods performs semantic segmentation based object detection results specific rcnn first utilizes selective search extract large quantity object proposals computes cnn features finally classifies region using classspecific linear svms compared traditional cnn structures mainly intended image classification rcnn address complicated tasks object detection image segmentation even becomes one important basis fields moreover rcnn built top cnn benchmark structures alexnet vgg googlenet resnet image segmentation task rcnn extracted 2 types features region full region feature foreground feature found could lead better performance concatenating together region feature rcnn achieved significant performance improvements due using highly discriminative cnn features however also suffers couple drawbacks segmentation task due bottlenecks recent research proposed address problems including sds hypercolumns mask rcnn original fully convolutional network fcn learns mapping pixels pixels without extracting region proposals fcn network pipeline extension classical cnn main idea make classical cnn take input arbitrarysized images restriction cnns accept produce labels specific sized inputs comes fullyconnected layers fixed contrary fcns convolutional pooling layers give ability make predictions arbitrarysized inputs one issue specific fcn propagating several alternated convolutional pooling layers resolution output feature maps sampled therefore direct predictions fcn typically low resolution resulting relatively fuzzy object boundaries variety advanced fcnbased approaches proposed address issue including segnet deeplabcrf dilated convolutions relevant methods semantic segmentation rely large number images pixelwise segmentation masks however manually annotating masks quite timeconsuming frustrating commercially expensive therefore weakly supervised methods recently proposed dedicated fulfilling semantic segmentation utilizing annotated bounding boxes example boxsup employed bounding box annotations supervision train network iteratively improve estimated masks semantic segmentation simple treated weak supervision limitation issue input label noise explored recursive training denoising strategy pixellevel labeling interpreted segmentation task within multipleinstance learning framework added extra layer constrain model assign weight important pixels imagelevel classification section lets walk stepbystep implementation popular architecture semantic segmentation fullyconvolutional net fcn well implement using tensorflow library python 3 along dependencies numpy scipy exercise label pixels road images using fcn well work kitti road dataset roadlane detection simple exercise udacitys selfdriving car nanodegree program learn setup github repo key features fcn architecture 3 versions fcn fcn32 fcn16 fcn8 well implement fcn8 detailed stepbystep first load pretrained vgg16 model tensorflow taking tensorflow session path vgg folder downloadable return tuple tensors vgg model including image input keep_prob control dropout rate layer 3 layer 4 layer 7 focus creating layers fcn using tensors vgg model given tensors vgg layer output number classes classify return tensor last layer output particular apply 1x1 convolution encoder layers add decoder layers network skip connections upsampling next step optimize neural network aka building tensorflow loss functions optimizer operations use cross entropy loss function adam optimization algorithm define train_nn function takes important parameters including number epochs batch size loss function optimizer operation placeholders input images label images learning rate training process also set keep_probability 05 learning_rate 0001 keep track progress also print loss training finally time train net run function first build net using load_vgg layers optimize function train net using train_nn function save inference data records parameters choose epochs 40 batch_size 16 num_classes 2 image_shape 160 576 2 trial passes dropout 05 dropout 075 found 2nd trial yields better results better average losses see full code check link httpsgistgithubcomkhanhnamle1994e2ff59ddca93c0205ac4e566d40b5e88 enjoyed piece id love hit clap button others might stumble upon quick cheer standing ovation clap show much enjoyed story blue ocean thinker httpsjamesklecom nanonets machine learning api,en,"['Convolutional Neural Nets', 'CNN', 'AlexNet, VGG', 'SDS', 'Fully Convolutional Network', 'FCN', 'SegNet', 'the Fully-Convolutional Net', 'Udacity', 'GitHub', 'VGG', 'num_classes', '👏']"
178,Bharath Raj,2200,Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2,"We have all been there. You have a stellar concept that can be implemented using a machine learning model. Feeling ebullient, you open your web browser and search for relevant data. Chances are, you find a dataset that has around a few hundred images.
You recall that most popular datasets have images in the order of tens of thousands (or more). You also recall someone mentioning having a large dataset is crucial for good performance. Feeling disappointed, you wonder; can my “state-of-the-art” neural network perform well with the meagre amount of data I have?
The answer is, yes! But before we get into the magic of making that happen, we need to reflect upon some basic questions.
When you train a machine learning model, what you’re really doing is tuning its parameters such that it can map a particular input (say, an image) to some output (a label). Our optimization goal is to chase that sweet spot where our model’s loss is low, which happens when your parameters are tuned in the right way.
Naturally, if you have a lot of parameters, you would need to show your machine learning model a proportional amount of examples, to get good performance. Also, the number of parameters you need is proportional to the complexity of the task your model has to perform.
You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.
So, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.
A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).
This essentially is the premise of data augmentation. In the real world scenario, we may have a dataset of images taken in a limited set of conditions. But, our target application may exist in a variety of conditions, such as different orientation, location, scale, brightness etc. We account for these situations by training our neural network with additional synthetically modified data.
Yes. It can help to increase the amount of relevant data in your dataset. This is related to the way with which neural networks learn. Let me illustrate it with an example.
Imagine that you have a dataset, consisting of two brands of cars, as shown above. Let’s assume that all cars of brand A are aligned exactly like the picture in the left (i.e. All cars are facing left) . Likewise, all cars of brand B are aligned exactly like the picture in the right (i.e. Facing right) . Now, you feed this dataset to your “state-of-the-art” neural network, and you hope to get impressive results once it’s trained.
Let’s say it’s done training, and you feed the image above, which is a Brand A car. But your neural network outputs that it’s a Brand B car! You’re confused. Didn’t you just get a 95% accuracy on your dataset using your “state-of-the-art” neural network? I’m not exaggerating, similar incidents and goof-ups have occurred in the past.
Why does this happen? It happens because that’s how most machine learning algorithms work. It finds the most obvious features that distinguishes one class from another. Here, the feature was that all cars of Brand A were facing left, and all cars of Brand B are facing right.
How do we prevent this happening? We have to reduce the amount of irrelevant features in the dataset. For our car model classifier above, a simple solution would be to add pictures of cars of both classes, facing the other direction to our original dataset. Better yet, you can just flip the images in the existing dataset horizontally such that they face the other side! Now, on training the neural network on this new dataset, you get the performance that you intended to get.
Before we dive into the various augmentation techniques, there’s one issue that we must consider beforehand.
The answer may seem quite obvious; we do augmentation before we feed the data to the model right? Yes, but you have two options here. One option is to perform all the necessary transformations beforehand, essentially increasing the size of your dataset. The other option is to perform these transformations on a mini-batch, just before feeding it to your machine learning model.
The first option is known as offline augmentation. This method is preferred for relatively smaller datasets, as you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform (For example, by flipping all my images, I would increase the size of my dataset by a factor of 2).
The second option is known as online augmentation, or augmentation on the fly. This method is preferred for larger datasets, as you can’t afford the explosive increase in size. Instead, you would perform transformations on the mini-batches that you would feed to your model. Some machine learning frameworks have support for online augmentation, which can be accelerated on the GPU.
In this section, we present some basic but powerful augmentation techniques that are popularly used. Before we explore these techniques, for simplicity, let us make one assumption. The assumption is that, we don’t need to consider what lies beyond the image’s boundary. We’ll use the below techniques such that our assumption is valid.
What would happen if we use a technique that forces us to guess what lies beyond an image’s boundary? In this case, we need to interpolate some information. We’ll discuss this in detail after we cover the types of augmentation.
For each of these techniques, we also specify the factor by which the size of your dataset would get increased (aka. Data Augmentation Factor).
You can flip images horizontally and vertically. Some frameworks do not provide function for vertical flips. But, a vertical flip is equivalent to rotating an image by 180 degrees and then performing a horizontal flip. Below are examples for images that are flipped.
You can perform flips by using any of the following commands, from your favorite packages. Data Augmentation Factor = 2 to 4x
One key thing to note about this operation is that image dimensions may not be preserved after rotation. If your image is a square, rotating it at right angles will preserve the image size. If it’s a rectangle, rotating it by 180 degrees would preserve the size. Rotating the image by finer angles will also change the final image size. We’ll see how we can deal with this issue in the next section. Below are examples of square images rotated at right angles.
You can perform rotations by using any of the following commands, from your favorite packages. Data Augmentation Factor = 2 to 4x
The image can be scaled outward or inward. While scaling outward, the final image size will be larger than the original image size. Most image frameworks cut out a section from the new image, with size equal to the original image. We’ll deal with scaling inward in the next section, as it reduces the image size, forcing us to make assumptions about what lies beyond the boundary. Below are examples or images being scaled.
You can perform scaling by using the following commands, using scikit-image. Data Augmentation Factor = Arbitrary.
Unlike scaling, we just randomly sample a section from the original image. We then resize this section to the original image size. This method is popularly known as random cropping. Below are examples of random cropping. If you look closely, you can notice the difference between this method and scaling.
You can perform random crops by using any the following command for TensorFlow. Data Augmentation Factor = Arbitrary.
Translation just involves moving the image along the X or Y direction (or both). In the following example, we assume that the image has a black background beyond its boundary, and are translated appropriately. This method of augmentation is very useful as most objects can be located at almost anywhere in the image. This forces your convolutional neural network to look everywhere.
You can perform translations in TensorFlow by using the following commands. Data Augmentation Factor = Arbitrary.
Over-fitting usually happens when your neural network tries to learn high frequency features (patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially has data points in all frequencies, effectively distorting the high frequency features. This also means that lower frequency components (usually, your intended data) are also distorted, but your neural network can learn to look past that. Adding just the right amount of noise can enhance the learning capability.
A toned down version of this is the salt and pepper noise, which presents itself as random black and white pixels spread through the image. This is similar to the effect produced by adding Gaussian noise to an image, but may have a lower information distortion level.
You can add Gaussian noise to your image by using the following command, on TensorFlow. Data Augmentation Factor = 2x.
Real world, natural data can still exist in a variety of conditions that cannot be accounted for by the above simple methods. For instance, let us take the task of identifying the landscape in photograph. The landscape could be anything: freezing tundras, grasslands, forests and so on. Sounds like a pretty straight forward classification task right? You’d be right, except for one thing. We are overlooking a crucial feature in the photographs that would affect the performance — The season in which the photograph was taken.
If our neural network does not understand the fact that certain landscapes can exist in a variety of conditions (snow, damp, bright etc.), it may spuriously label frozen lakeshores as glaciers or wet fields as swamps.
One way to mitigate this situation is to add more pictures such that we account for all the seasonal changes. But that is an arduous task. Extending our data augmentation concept, imagine how cool it would be to generate effects such as different seasons artificially?
Without going into gory detail, conditional GANs can transform an image from one domain to an image to another domain. If you think it sounds too vague, it’s not; that’s literally how powerful this neural network is! Below is an example of conditional GANs used to transform photographs of summer sceneries to winter sceneries.
The above method is robust, but computationally intensive. A cheaper alternative would be something called neural style transfer. It grabs the texture/ambiance/appearance of one image (aka, the “style”) and mixes it with the content of another. Using this powerful technique, we produce an effect similar to that of our conditional GAN (In fact, this method was introduced before cGANs were invented!).
The only downside of this method is that, the output tends to looks more artistic rather than realistic. However, there are certain advancements such as Deep Photo Style Transfer, shown below, that have impressive results.
We have not explored these techniques in great depth as we are not concerned with their inner working. We can use existing trained models, along with the magic of transfer learning, to use it for augmentation.
What if you wanted to translate an image that doesn’t have a black background? What if you wanted to scale inward? Or rotate in finer angles? After we perform these transformations, we need to preserve our original image size. Since our image does not have any information about things outside it’s boundary, we need to make some assumptions. Usually, the space beyond the image’s boundary is assumed to be the constant 0 at every point. Hence, when you do these transformations, you get a black region where the image is not defined.
But is that the right assumption? In the real world scenario, it’s mostly a no. Image processing and ML frameworks have some standard ways with which you can decide on how to fill the unknown space. They are defined as follows.
The simplest interpolation method is to fill the unknown region with some constant value. This may not work for natural images, but can work for images taken in a monochromatic background
The edge values of the image are extended after the boundary. This method can work for mild translations.
The image pixel values are reflected along the image boundary. This method is useful for continuous or natural backgrounds containing trees, mountains etc.
This method is similar to reflect, except for the fact that, at the boundary of reflection, a copy of the edge pixels are made. Normally, reflect and symmetric can be used interchangeably, but differences will be visible while dealing with very small images or patterns.
The image is just repeated beyond its boundary, as if it’s being tiled. This method is not as popularly used as the rest as it does not make sense for a lot of scenarios.
Besides these, you can design your own methods for dealing with undefined space, but usually these methods would just do fine for most classification problems.
If you use it in the right way, then yes! What is the right way you ask? Well, sometimes not all augmentation techniques make sense for a dataset. Consider our car example again. Below are some of the ways by which you can modify the image.
Sure, they are pictures of the same car, but your target application may never see cars presented in these orientations.
For instance, if you’re just going to classify random cars on the road, only the second image would make sense to be on the dataset. But, if you own an insurance company that deals with car accidents, and you want to identify models of upside-down, broken cars as well, the third image makes sense. The last image may not make sense for both the above scenarios.
The point is, while using augmentation techniques, we have to make sure to not increase irrelevant data.
You’re probably expecting some results to motivate you to walk the extra mile. Fair enough; I’ve got that covered too. Let me prove that augmentation really works, using a toy example. You can replicate this experiment to verify.
Let’s create two neural networks to classify data to one among four classes: cat, lion, tiger or a leopard. The catch is, one will not use data augmentation, whereas the other will. You can download the dataset from here link.
If you’ve checked out the dataset, you’ll notice that there’s only 50 images per class for both training and testing. Clearly, we can’t use augmentation for one of the classifiers. To make the odds more fair, we use Transfer Learning to give the models a better chance with the scarce amount of data.
For the one without augmentation, let’s use a VGG19 network. I’ve written a TensorFlow implementation here, which is based on this implementation. Once you’ve cloned my repo, you can get the dataset from here, and vgg19.npy (used for transfer learning) from here. You can now run the model to verify the performance.
I would agree though, writing extra code for data augmentation is indeed a bit of an effort. So, to build our second model, I turned to Nanonets. They internally use transfer learning and data augmentation to provide the best results using minimal data. All you need to do is upload the data on their website, and wait until it’s trained in their servers (Usually around 30 minutes). What do you know, it’s perfect for our comparison experiment.
Once it’s done training, you can request calls to their API to calculate the test accuracy. Checkout out my repo for a sample code snippet(Don’t forget to insert your model’s ID in the code snippet).
Impressive isn’t it. It is a fact that most models perform well with more data. So to provide a concrete proof, I’ve mentioned the table below. It shows the error rate of popular neural networks on the Cifar 10 (C10) and Cifar 100 (C100) datasets. C10+ and C100+ columns are the error rates with data augmentation.
Thank you for reading this article! Hit that clap button if you did! Hope it shed some light about data augmentation. If you have any questions, you could hit me up on social media or send me an email (bharathrajn98@gmail.com).
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Undergrad | Computer Vision and AI Enthusiast | Hungry
NanoNets: Machine Learning API
",stellar concept implemented using machine learning model feeling ebullient open web browser search relevant data chances find dataset around hundred images recall popular datasets images order tens thousands also recall someone mentioning large dataset crucial good performance feeling disappointed wonder stateoftheart neural network perform well meagre amount data answer yes get magic making happen need reflect upon basic questions train machine learning model youre really tuning parameters map particular input say image output label optimization goal chase sweet spot models loss low happens parameters tuned right way naturally lot parameters would need show machine learning model proportional amount examples get good performance also number parameters need proportional complexity task model perform dont need hunt novel new images added dataset neural networks arent smart begin instance poorly trained neural network would think three tennis balls shown distinct unique images get data need make minor alterations existing dataset minor changes flips translations rotations neural network would think distinct images anyway convolutional neural network robustly classify objects even placed different orientations said property called invariance specifically cnn invariant translation viewpoint size illumination combination essentially premise data augmentation real world scenario may dataset images taken limited set conditions target application may exist variety conditions different orientation location scale brightness etc account situations training neural network additional synthetically modified data yes help increase amount relevant data dataset related way neural networks learn let illustrate example imagine dataset consisting two brands cars shown lets assume cars brand aligned exactly like picture left ie cars facing left likewise cars brand b aligned exactly like picture right ie facing right feed dataset stateoftheart neural network hope get impressive results trained lets say done training feed image brand car neural network outputs brand b car youre confused didnt get 95 accuracy dataset using stateoftheart neural network im exaggerating similar incidents goofups occurred past happen happens thats machine learning algorithms work finds obvious features distinguishes one class another feature cars brand facing left cars brand b facing right prevent happening reduce amount irrelevant features dataset car model classifier simple solution would add pictures cars classes facing direction original dataset better yet flip images existing dataset horizontally face side training neural network new dataset get performance intended get dive various augmentation techniques theres one issue must consider beforehand answer may seem quite obvious augmentation feed data model right yes two options one option perform necessary transformations beforehand essentially increasing size dataset option perform transformations minibatch feeding machine learning model first option known offline augmentation method preferred relatively smaller datasets would end increasing size dataset factor equal number transformations perform example flipping images would increase size dataset factor 2 second option known online augmentation augmentation fly method preferred larger datasets cant afford explosive increase size instead would perform transformations minibatches would feed model machine learning frameworks support online augmentation accelerated gpu section present basic powerful augmentation techniques popularly used explore techniques simplicity let us make one assumption assumption dont need consider lies beyond images boundary well use techniques assumption valid would happen use technique forces us guess lies beyond images boundary case need interpolate information well discuss detail cover types augmentation techniques also specify factor size dataset would get increased aka data augmentation factor flip images horizontally vertically frameworks provide function vertical flips vertical flip equivalent rotating image 180 degrees performing horizontal flip examples images flipped perform flips using following commands favorite packages data augmentation factor 2 4x one key thing note operation image dimensions may preserved rotation image square rotating right angles preserve image size rectangle rotating 180 degrees would preserve size rotating image finer angles also change final image size well see deal issue next section examples square images rotated right angles perform rotations using following commands favorite packages data augmentation factor 2 4x image scaled outward inward scaling outward final image size larger original image size image frameworks cut section new image size equal original image well deal scaling inward next section reduces image size forcing us make assumptions lies beyond boundary examples images scaled perform scaling using following commands using scikitimage data augmentation factor arbitrary unlike scaling randomly sample section original image resize section original image size method popularly known random cropping examples random cropping look closely notice difference method scaling perform random crops using following command tensorflow data augmentation factor arbitrary translation involves moving image along x direction following example assume image black background beyond boundary translated appropriately method augmentation useful objects located almost anywhere image forces convolutional neural network look everywhere perform translations tensorflow using following commands data augmentation factor arbitrary overfitting usually happens neural network tries learn high frequency features patterns occur lot may useful gaussian noise zero mean essentially data points frequencies effectively distorting high frequency features also means lower frequency components usually intended data also distorted neural network learn look past adding right amount noise enhance learning capability toned version salt pepper noise presents random black white pixels spread image similar effect produced adding gaussian noise image may lower information distortion level add gaussian noise image using following command tensorflow data augmentation factor 2x real world natural data still exist variety conditions cannot accounted simple methods instance let us take task identifying landscape photograph landscape could anything freezing tundras grasslands forests sounds like pretty straight forward classification task right youd right except one thing overlooking crucial feature photographs would affect performance season photograph taken neural network understand fact certain landscapes exist variety conditions snow damp bright etc may spuriously label frozen lakeshores glaciers wet fields swamps one way mitigate situation add pictures account seasonal changes arduous task extending data augmentation concept imagine cool would generate effects different seasons artificially without going gory detail conditional gans transform image one domain image another domain think sounds vague thats literally powerful neural network example conditional gans used transform photographs summer sceneries winter sceneries method robust computationally intensive cheaper alternative would something called neural style transfer grabs textureambianceappearance one image aka style mixes content another using powerful technique produce effect similar conditional gan fact method introduced cgans invented downside method output tends looks artistic rather realistic however certain advancements deep photo style transfer shown impressive results explored techniques great depth concerned inner working use existing trained models along magic transfer learning use augmentation wanted translate image doesnt black background wanted scale inward rotate finer angles perform transformations need preserve original image size since image information things outside boundary need make assumptions usually space beyond images boundary assumed constant 0 every point hence transformations get black region image defined right assumption real world scenario mostly image processing ml frameworks standard ways decide fill unknown space defined follows simplest interpolation method fill unknown region constant value may work natural images work images taken monochromatic background edge values image extended boundary method work mild translations image pixel values reflected along image boundary method useful continuous natural backgrounds containing trees mountains etc method similar reflect except fact boundary reflection copy edge pixels made normally reflect symmetric used interchangeably differences visible dealing small images patterns image repeated beyond boundary tiled method popularly used rest make sense lot scenarios besides design methods dealing undefined space usually methods would fine classification problems use right way yes right way ask well sometimes augmentation techniques make sense dataset consider car example ways modify image sure pictures car target application may never see cars presented orientations instance youre going classify random cars road second image would make sense dataset insurance company deals car accidents want identify models upsidedown broken cars well third image makes sense last image may make sense scenarios point using augmentation techniques make sure increase irrelevant data youre probably expecting results motivate walk extra mile fair enough ive got covered let prove augmentation really works using toy example replicate experiment verify lets create two neural networks classify data one among four classes cat lion tiger leopard catch one use data augmentation whereas download dataset link youve checked dataset youll notice theres 50 images per class training testing clearly cant use augmentation one classifiers make odds fair use transfer learning give models better chance scarce amount data one without augmentation lets use vgg19 network ive written tensorflow implementation based implementation youve cloned repo get dataset vgg19npy used transfer learning run model verify performance would agree though writing extra code data augmentation indeed bit effort build second model turned nanonets internally use transfer learning data augmentation provide best results using minimal data need upload data website wait trained servers usually around 30 minutes know perfect comparison experiment done training request calls api calculate test accuracy checkout repo sample code snippetdont forget insert models id code snippet impressive isnt fact models perform well data provide concrete proof ive mentioned table shows error rate popular neural networks cifar 10 c10 cifar 100 c100 datasets c10 c100 columns error rates data augmentation thank reading article hit clap button hope shed light data augmentation questions could hit social media send email bharathrajn98gmailcom quick cheer standing ovation clap show much enjoyed story undergrad computer vision ai enthusiast hungry nanonets machine learning api,en,"['CNN', 'Brand A', 'GPU', 'GAN', 'C10']"
179,Daniel Rothmann,302,Human-Like Machine Hearing With AI (1/3) – Towards Data Science,"Significant breakthroughs in AI technology have been achieved through modeling human systems. While artificial neural networks (NNs) are mathematical models which are only loosely coupled with the way actual human neurons function, their application in solving complex and ambiguous real-world problems has been profound. Additionally, modeling the architectural depth of the brain in NNs has opened up broad possibilities in learning more meaningful representations of data.
In image recognition and processing, the inspiration from the complex and more spatially invariant cells of the visual system in CNNs has also produced great improvements to our technologies. If you’re interested in applying image recognition technologies on audio spectrograms, check out my article “What’s wrong with CNNs and spectrograms for audio processing?”.
As long as human perceptual capacity exceeds that of machines, we stand to gain by understanding the principles of human systems. Humans are very skillful when it comes to perceptual tasks and the contrast between human understanding and the status quo of AI becomes particularly apparent in the area of machine hearing. Considering the benefits reaped from getting inspired by human systems in visual processing, I propose that we stand to gain from a similar process in machine hearing with neural networks.
In this article series, I will detail a framework for real-time audio signal processing with AI which was developed in cooperation with Aarhus University and intelligent loudspeaker manufacturer Dynaudio A/S. Its inspiration is primarily drawn from cognitive science which attempts to combine perspectives of biology, neuroscience, psychology and philosophy to gain greater understanding of our cognitive faculties.
Perhaps the most abstract domain of sound is how we, as humans, perceive it. While a solution for a signal processing problem has to operate within the parameters of intensity, spectral and temporal properties on a low level, the end goal is most often a cognitive one: Transforming a signal in such a way that our perceptions of the sounds it contains are altered.
If one wishes to programatically change the gender of a recorded spoken voice for example, it is necessary to describe this problem in more meaningful terms before defining its lower level characteristics. The gender of a speaker can be conceived as a cognitive property which is constructed from many factors: General pitch and timbre of a voice, differences in pronunciation, differences in choice of words and language and a common understanding of how these properties relate to gender.
These parameters can be described in lower level features like intensity, spectral and temporal properties but only in more complex combinations do they form high-level representations. This forms a hierarchy of audio features from which the “meaning” of a sound can be derived. The cognitive property representing a human voice can be thought of as a combinatory pattern of temporal developments in a sound’s intensity, spectral and statistical properties.
NNs are great at extracting abstracted representations of data and are therefore well suited for the task of detecting cognitive properties in sound. In order to build a system for this purpose, let’s examine how sound is represented in human auditory organs that we can use to inspire representation of sound for processing with NNs.
Hearing in humans starts at the outer ear which firstly consists of the pinna. The pinna acts as a form of spectral preprocessing in which the incoming sound is modified depending on its direction in relation to the listener. Sound then travels through the opening in the pinna into the ear canal which further acts to modify spectral properties of incoming sound by resonating in a way that amplifies frequencies in the range ~1–6 kHz [1].
As sound waves reach the end of the ear canal, they excite the eardrum onto which the ossicles (the smallest bones in the body) are attached. These bones transmit the pressure from the ear canal to the fluid-filled cochlea in the inner ear [1]. The cochlea is of great interest in guiding sound representation for NNs because this is the organ responsible for transducing acoustic vibrations into neural activity in humans .
It is a coiled tube which is separated along its length by two membranes being the Reissner’s membrane and the basilar membrane. Along the length of the cochlea, there is a row of around 3,500 inner hair cells [1]. As pressures enter the cochlea, its two membranes are pushed down. The basilar membrane is narrow and stiff at its base but loose and wide at its apex so that each place along its length responds more intensely at a particular frequency.
To simplify, the basilar membrane can be thought of as a continuous array of bandpass filters which, along the length of the membrane, acts to separate sounds into their spectral components.
This is the primary mechanism by which humans convert sound pressures into neural activity. Therefore, it is reasonable to assume that spectral representations of audio would be beneficial in modeling sound perception with AI. Since frequency responses along the basilar membrane vary exponentially [2], logarithmic frequency representations might prove most efficient. One such representation could be derived using a gammatone filterbank. These filters are commonly applied in modeling spectral filtering in the auditory system since they approximate the impulse response of human auditory filters derived from the measured auditory nerve fiber response to white noise stimuli called the “revcor” function [3].
Since the cochlea has ~3500 inner hair cells and humans can detect gaps in sounds down to ~2–5 ms in length [1], a spectral resolution of 3500 gammatone filters separated into 2 ms windows seem optimal parameters for achieving human-like spectral representation in machines. In practical scenarios however, I assume that lesser resolutions could still achieve desirable effects in most analysis and processing tasks while being more viable from a computational standpoint.
A number of software libraries for auditory analysis are available online. A notable example is the Gammatone Filterbank Toolkit by Jason Heeris. It provides adjustable filters as well as tools for spectrogram-like analysis of audio signals with gammatone filters.
As neural activity moves from the cochlea onto the auditory nerve and the ascending auditory pathways, a number of processes are applied in brainstem nuclei before it reaches the auditory cortex.
These processes form a neural code which represents an interface between stimulus and perception [4]. Much knowledge about the specific inner workings of these nuclei is still speculative or unknown, so I will detail these nuclei only at their higher levels of functioning.
Humans have a set of these nuclei for each ear that are interconnected, but for simplicity, I’ve illustrated the flow for only one ear. The cochlear nucleus is the first coding step for neural signals coming from the auditory nerve. It consists of a variety of neurons with different properties which serve to perform initial processing of sound features, some of which are directed to the superior olive which is associated with sound localization while others are directed to the lateral lemniscus and inferior colliculus, commonly associated with more advanced features [1].
J. J. Eggermont details this flow of information from the cochlear nucleus in “Between sound and perception: reviewing the search for a neural code” as follows: “The ventral [cochlear nucleus] (VCN) extracts and enhances the frequency and timing information that is multiplexed in the firing patterns of the [auditory nerve] fibers, and distributes the results via two main pathways: the sound localization path and the sound identification path. The anterior part of the VCN (AVCN) mainly serves the sound localization aspects and its two types of bushy cells provide input to the superior olivary complex (SOC), where interaural time differences (ITDs) and level differences (ILDs) are mapped for each frequency separately” [4].
The information carried by the sound identification pathway is a representation of complex spectra such as vowels. This representation is mainly created in the ventral cochlear nucleus by special types of units dubbed “chopper” (stellate) neurons [4]. The details of these auditory encodings are difficult to specify but they indicate to us that a form of “coding” of incoming frequency spectra could improve understanding of low level sound features as well as making sound impressions less expensive to process in NNs.
We can apply the unsupervised autoencoder NN architecture as an attempt to learn common properties associated with complex spectra. Like word embeddings, its possible to find commonalities in frequency spectra that represent select features (or a more tightly condensed meaning) of sounds.
An autoencoder is trained to encode an input into a compressed representation that can be reconstructed back into a representation with a high similarity to the input. This means that the autoencoder’s target output is the input itself [5]. If an input can be reconstructed without great loss, the network has learnt to encode it in such a way that the compressed internal representation contains enough meaningful information. This internal representation is then what we refer to as the embedding. The encoding part of the autoencoder can be decoupled from the decoder to generate embeddings for other applications.
Embeddings also have the benefit that they are often of lower dimensionality than the original data. For instance, an autoencoder could compress a frequency spectrum with a total of 3500 values into a vector with a length of 500 values. Put simply, each value of such a vector could describe higher level factors of a spectrum such as vowel, harshness or harmonicity - These are only examples, as the meaning of statistically common factors derived by an autoencoder might often be difficult to label in plain language.
In the next article, we will expand upon this idea with added memory to produce embeddings for temporal developments of audio frequency spectra.
This wraps up the first part of my article series on audio processing with artificial intelligence. Next, we will discuss the essential concepts of sensory memory and temporal dependencies in sound.
Follow to stay updated and feel free to leave claps if you enjoyed the article!
As always, feel free to connect with me on LinkedIn to stay in touch.
[1] C. J. Plack, The Sense of Hearing, 2nd ed. Psychology Press, 2014.
[2] S. J. Elliott and C. A. Shera, “The cochlea as a smart structure,” Smart Mater. Struct., vol. 21, no. 6, p. 64001, Jun. 2012.
[3] A.M. Darling, “Properties and implementation of the gammatone filter: A tutorial”, Speech hearing and language, University College London, 1991.
[4] J. J. Eggermont, “Between sound and perception: reviewing the search for a neural code.,” Hear. Res., vol. 157, no. 1–2, pp. 1–42, Jul. 2001.
[5] T. P. Lillicrap et al., Learning Deep Architectures for AI, vol. 2, no. 1. 2015.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Engineer @ Convai. Especially interested in audio and time series forecasting. Reach us at convai.dk
Sharing concepts, ideas, and codes.
",significant breakthroughs ai technology achieved modeling human systems artificial neural networks nns mathematical models loosely coupled way actual human neurons function application solving complex ambiguous realworld problems profound additionally modeling architectural depth brain nns opened broad possibilities learning meaningful representations data image recognition processing inspiration complex spatially invariant cells visual system cnns also produced great improvements technologies youre interested applying image recognition technologies audio spectrograms check article whats wrong cnns spectrograms audio processing long human perceptual capacity exceeds machines stand gain understanding principles human systems humans skillful comes perceptual tasks contrast human understanding status quo ai becomes particularly apparent area machine hearing considering benefits reaped getting inspired human systems visual processing propose stand gain similar process machine hearing neural networks article series detail framework realtime audio signal processing ai developed cooperation aarhus university intelligent loudspeaker manufacturer dynaudio inspiration primarily drawn cognitive science attempts combine perspectives biology neuroscience psychology philosophy gain greater understanding cognitive faculties perhaps abstract domain sound humans perceive solution signal processing problem operate within parameters intensity spectral temporal properties low level end goal often cognitive one transforming signal way perceptions sounds contains altered one wishes programatically change gender recorded spoken voice example necessary describe problem meaningful terms defining lower level characteristics gender speaker conceived cognitive property constructed many factors general pitch timbre voice differences pronunciation differences choice words language common understanding properties relate gender parameters described lower level features like intensity spectral temporal properties complex combinations form highlevel representations forms hierarchy audio features meaning sound derived cognitive property representing human voice thought combinatory pattern temporal developments sounds intensity spectral statistical properties nns great extracting abstracted representations data therefore well suited task detecting cognitive properties sound order build system purpose lets examine sound represented human auditory organs use inspire representation sound processing nns hearing humans starts outer ear firstly consists pinna pinna acts form spectral preprocessing incoming sound modified depending direction relation listener sound travels opening pinna ear canal acts modify spectral properties incoming sound resonating way amplifies frequencies range 16 khz 1 sound waves reach end ear canal excite eardrum onto ossicles smallest bones body attached bones transmit pressure ear canal fluidfilled cochlea inner ear 1 cochlea great interest guiding sound representation nns organ responsible transducing acoustic vibrations neural activity humans coiled tube separated along length two membranes reissners membrane basilar membrane along length cochlea row around 3500 inner hair cells 1 pressures enter cochlea two membranes pushed basilar membrane narrow stiff base loose wide apex place along length responds intensely particular frequency simplify basilar membrane thought continuous array bandpass filters along length membrane acts separate sounds spectral components primary mechanism humans convert sound pressures neural activity therefore reasonable assume spectral representations audio would beneficial modeling sound perception ai since frequency responses along basilar membrane vary exponentially 2 logarithmic frequency representations might prove efficient one representation could derived using gammatone filterbank filters commonly applied modeling spectral filtering auditory system since approximate impulse response human auditory filters derived measured auditory nerve fiber response white noise stimuli called revcor function 3 since cochlea 3500 inner hair cells humans detect gaps sounds 25 ms length 1 spectral resolution 3500 gammatone filters separated 2 ms windows seem optimal parameters achieving humanlike spectral representation machines practical scenarios however assume lesser resolutions could still achieve desirable effects analysis processing tasks viable computational standpoint number software libraries auditory analysis available online notable example gammatone filterbank toolkit jason heeris provides adjustable filters well tools spectrogramlike analysis audio signals gammatone filters neural activity moves cochlea onto auditory nerve ascending auditory pathways number processes applied brainstem nuclei reaches auditory cortex processes form neural code represents interface stimulus perception 4 much knowledge specific inner workings nuclei still speculative unknown detail nuclei higher levels functioning humans set nuclei ear interconnected simplicity ive illustrated flow one ear cochlear nucleus first coding step neural signals coming auditory nerve consists variety neurons different properties serve perform initial processing sound features directed superior olive associated sound localization others directed lateral lemniscus inferior colliculus commonly associated advanced features 1 j j eggermont details flow information cochlear nucleus sound perception reviewing search neural code follows ventral cochlear nucleus vcn extracts enhances frequency timing information multiplexed firing patterns auditory nerve fibers distributes results via two main pathways sound localization path sound identification path anterior part vcn avcn mainly serves sound localization aspects two types bushy cells provide input superior olivary complex soc interaural time differences itds level differences ilds mapped frequency separately 4 information carried sound identification pathway representation complex spectra vowels representation mainly created ventral cochlear nucleus special types units dubbed chopper stellate neurons 4 details auditory encodings difficult specify indicate us form coding incoming frequency spectra could improve understanding low level sound features well making sound impressions less expensive process nns apply unsupervised autoencoder nn architecture attempt learn common properties associated complex spectra like word embeddings possible find commonalities frequency spectra represent select features tightly condensed meaning sounds autoencoder trained encode input compressed representation reconstructed back representation high similarity input means autoencoders target output input 5 input reconstructed without great loss network learnt encode way compressed internal representation contains enough meaningful information internal representation refer embedding encoding part autoencoder decoupled decoder generate embeddings applications embeddings also benefit often lower dimensionality original data instance autoencoder could compress frequency spectrum total 3500 values vector length 500 values put simply value vector could describe higher level factors spectrum vowel harshness harmonicity examples meaning statistically common factors derived autoencoder might often difficult label plain language next article expand upon idea added memory produce embeddings temporal developments audio frequency spectra wraps first part article series audio processing artificial intelligence next discuss essential concepts sensory memory temporal dependencies sound follow stay updated feel free leave claps enjoyed article always feel free connect linkedin stay touch 1 c j plack sense hearing 2nd ed psychology press 2014 2 j elliott c shera cochlea smart structure smart mater struct vol 21 6 p 64001 jun 2012 3 darling properties implementation gammatone filter tutorial speech hearing language university college london 1991 4 j j eggermont sound perception reviewing search neural code hear res vol 157 12 pp 142 jul 2001 5 p lillicrap et al learning deep architectures ai vol 2 1 2015 quick cheer standing ovation clap show much enjoyed story ai engineer convai especially interested audio time series forecasting reach us convaidk sharing concepts ideas codes,en,"['Aarhus University', 'Dynaudio A', 'kHz [1]', 'NN', 'LinkedIn', 'Psychology Press', 'University College London', 'Learning Deep Architectures for AI']"
180,Amine Aoullay,58,How to use Noise to your advantage ? – Towards Data Science,"For scientists, random fluctuations, or noise is undesirable.
Although typically assumed to degrade performance, it can sometimes improve information processing in non-linear systems. In this post we’ll see some examples where the noise can be used as an advantage.
Recent works have shown that, by allowing some inaccuracy when training deep neural networks, not only the training performance but also the accuracy of the model can be improved.
Neural networks are capable of learning output functions that can change wildly with small changes in input. Adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input.
By limiting the amount of information in a network, we force it to learn compact representations of input features.
RL is an area of machine learning that assumes there is an agent situated in an environment. At each step, the agent takes an action, and it receives an observation and reward from the environment.
An RL algorithm seeks to maximize the agent’s total reward, given a previously unknown environment, through a learning process that usually involves lots of trial and error.
To understand the challenge with exploration in Deep RL systems think about researchers that spend lot of times in a Lab without producing any practical application. Equivalently, RL agents can spend a huge amount of resources without converging to a local optimum.
OpenAI proposes a technique called Parameter-Space-Noise, that introduces noises in the model policy parameters at the beginning of each episode.
Other approaches were focused on what is known as Action-Space-Noise which introduce noise to change the likelihoods associated with each action the agent might take from one moment to the next.
The initial results of the Parameter-Space-Noise model proved to be really promising. The technique helps algorithms explore their environments more effectively, leading to higher scores and more elegant behaviors. More details can be found in the research paper.
The important thing to remember is that adding noise was used as an advantage to boost the exploration performance of reinforcement learning algorithms.
Boosting recognition isn’t as simple as throwing more labeled images at these systems. Indeed, manually annotating a large number of images is an expensive and time consuming process.
Facebook researchers and engineers have addressed this by training image recognition networks on large sets of public images with hashtags.
Since people often caption their photos with hashtags, it woul’d be a good source of training data for models.
Facebook developed new approaches that are tailored for doing image recognition experiments using hashtag supervision. This study is described in detail in “Exploring the Limits of Weakly Supervised Pretraining”
On the COCO object-detection challenge, it has been shown that the use of hashtags for pretraining can boost the average precision of a model by more than 2 percent.
Noise should not be our enemy ! It isn’t always an unwanted disturbance and can often be used as an advantage and even serve as a valuable research tool. If anyone tries to tell you otherwise, well, just give him the examples we presented ...
Stay tuned and if you liked this article, please leave a 👏!
[1] Weakly-supervised-pretraining: https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/
[2] Better Exploration with Parameter Noise: https://blog.openai.com/better-exploration-with-parameter-noise/
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
MSc in Machine Learning (MVA) @ ENS Paris-Saclay
Sharing concepts, ideas, and codes.
",scientists random fluctuations noise undesirable although typically assumed degrade performance sometimes improve information processing nonlinear systems post well see examples noise used advantage recent works shown allowing inaccuracy training deep neural networks training performance also accuracy model improved neural networks capable learning output functions change wildly small changes input adding noise inputs randomly like telling network change output ball around exact input limiting amount information network force learn compact representations input features rl area machine learning assumes agent situated environment step agent takes action receives observation reward environment rl algorithm seeks maximize agents total reward given previously unknown environment learning process usually involves lots trial error understand challenge exploration deep rl systems think researchers spend lot times lab without producing practical application equivalently rl agents spend huge amount resources without converging local optimum openai proposes technique called parameterspacenoise introduces noises model policy parameters beginning episode approaches focused known actionspacenoise introduce noise change likelihoods associated action agent might take one moment next initial results parameterspacenoise model proved really promising technique helps algorithms explore environments effectively leading higher scores elegant behaviors details found research paper important thing remember adding noise used advantage boost exploration performance reinforcement learning algorithms boosting recognition isnt simple throwing labeled images systems indeed manually annotating large number images expensive time consuming process facebook researchers engineers addressed training image recognition networks large sets public images hashtags since people often caption photos hashtags would good source training data models facebook developed new approaches tailored image recognition experiments using hashtag supervision study described detail exploring limits weakly supervised pretraining coco objectdetection challenge shown use hashtags pretraining boost average precision model 2 percent noise enemy isnt always unwanted disturbance often used advantage even serve valuable research tool anyone tries tell otherwise well give examples presented stay tuned liked article please leave 1 weaklysupervisedpretraining httpsresearchfbcompublicationsexploringthelimitsofweaklysupervisedpretraining 2 better exploration parameter noise httpsblogopenaicombetterexplorationwithparameternoise quick cheer standing ovation clap show much enjoyed story msc machine learning mva ens parissaclay sharing concepts ideas codes,en,"['Deep RL', 'Parameter-Space-Noise', 'Action-Space-Noise', 'Facebook', 'COCO', '👏']"
181,Jonathan Balaban,804,Deep Learning Tips and Tricks – Towards Data Science,"Below is a distilled collection of conversations, messages, and debates I’ve had with peers and students on how to optimize deep models. If you have tricks you’ve found impactful, please share them!!
Deep learning models like the Convolutional Neural Network (CNN) have a massive number of parameters; we can actually call these hyper-parameters because they are not optimized inherently in the model. You could gridsearch the optimal values for these hyper-parameters, but you’ll need a lot of hardware and time. So, does a true data scientist settle for guessing these essential parameters?
One of the best ways to improve your models is to build on the design and architecture of the experts who have done deep research in your domain, often with powerful hardware at their disposal. Graciously, they often open-source the resulting modeling architectures and rationale.
Here are a few ways you can improve your fit time and accuracy with pre-trained models:
Here’s how to modify dropout and limit weight sizes in Keras with MNIST:
Here’s an example of final layer modification in Keras with 14 classes for MNIST:
And an example of how to freeze weights in the first five layers:
Alternatively, we can set the learning rate to zero for that layer, or use per-parameter adaptive learning algorithm like Adadelta or Adam. This is somewhat complicated and better implemented in other platforms, like Caffe.
It’s often essential to get a visual idea of how your model looks. If you’re working in Keras, abstraction is nice but doesn’t allow you to drill down into sections of your model for deeper analysis. Fortunately, the code below lets us visualize our models directly with Python:
This will plot a graph of the model and save it as a png file:
plot takes two optional arguments:
You can also directly obtain the pydot.Graph object and render it yourself, for example to show it in an ipython notebook :
I hope this collection helps with your modeling endeavors! Let me know your best tricks, and connect with me on Twitter and LinkedIn!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Science Nomad
Sharing concepts, ideas, and codes.
",distilled collection conversations messages debates ive peers students optimize deep models tricks youve found impactful please share deep learning models like convolutional neural network cnn massive number parameters actually call hyperparameters optimized inherently model could gridsearch optimal values hyperparameters youll need lot hardware time true data scientist settle guessing essential parameters one best ways improve models build design architecture experts done deep research domain often powerful hardware disposal graciously often opensource resulting modeling architectures rationale ways improve fit time accuracy pretrained models heres modify dropout limit weight sizes keras mnist heres example final layer modification keras 14 classes mnist example freeze weights first five layers alternatively set learning rate zero layer use perparameter adaptive learning algorithm like adadelta adam somewhat complicated better implemented platforms like caffe often essential get visual idea model looks youre working keras abstraction nice doesnt allow drill sections model deeper analysis fortunately code lets us visualize models directly python plot graph model save png file plot takes two optional arguments also directly obtain pydotgraph object render example show ipython notebook hope collection helps modeling endeavors let know best tricks connect twitter linkedin quick cheer standing ovation clap show much enjoyed story data science nomad sharing concepts ideas codes,en,"['the Convolutional Neural Network', 'CNN', 'MNIST', 'LinkedIn']"
182,SAGAR SHARMA,2500,Activation Functions: Neural Networks – Towards Data Science,"What is Activation Function ?
Why we use Activation functions with Neural Networks?
The Activation Functions can be basically divided into 2 types-
As you can see the function is a line or linear.Therefore, the output of the functions will not be confined between any range.
Equation : f(x) = x
Range : (-infinity to infinity)
It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks.
The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this
It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.
The main terminologies needed to understand for nonlinear functions are:
The Nonlinear Activation Functions are mainly divided on the basis of their range or curves-
1. Sigmoid or Logistic Activation Function
The Sigmoid Function curve looks like a S-shape.
The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.
The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.
The function is monotonic but function’s derivative is not.
The logistic sigmoid function can cause a neural network to get stuck at the training time.
The softmax function is a more generalized logistic activation function which is used for multiclass classification.
2. Tanh or hyperbolic tangent Activation Function
tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).
The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.
The function is differentiable.
The function is monotonic while its derivative is not monotonic.
The tanh function is mainly used classification between two classes.
3. ReLU (Rectified Linear Unit) Activation Function
The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.
As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.
Range: [ 0 to infinity)
The function and its derivative both are monotonic.
But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.
4. Leaky ReLU
It is an attempt to solve the dying ReLU problem
Can you see the Leak? 😆
The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.
When a is not 0.01 then it is called Randomized ReLU.
Therefore the range of the Leaky ReLU is (-infinity to infinity).
Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.
I will be posting 2 posts per week so don’t miss the tutorial.
So, follow me on Medium, Facebook, Twitter, LinkedIn, Google+, Quora to see similar posts.
Any comments or if you have any question, write it in the comment.
Clap it! Share it! Follow Me!
Happy to be helpful. kudos.....
2. Epoch vs Batch Size vs Iterations
3. Train Inception with Custom Images on CPU
4. TensorFlow Image Recognition Python API Tutorial On CPU
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I am interested in Programming (Python, C++), Arduino, Machine learning :) I'm the editor of Arduino Community on Medium. I also like to write stuff.
Sharing concepts, ideas, and codes.
",activation function use activation functions neural networks activation functions basically divided 2 types see function line lineartherefore output functions confined range equation fx x range infinity infinity doesnt help complexity various parameters usual data fed neural networks nonlinear activation functions used activation functions nonlinearity helps makes graph look something like makes easy model generalize adapt variety data differentiate output main terminologies needed understand nonlinear functions nonlinear activation functions mainly divided basis range curves 1 sigmoid logistic activation function sigmoid function curve looks like sshape main reason use sigmoid function exists 0 1 therefore especially used models predict probability outputsince probability anything exists range 0 1 sigmoid right choice function differentiablethat means find slope sigmoid curve two points function monotonic functions derivative logistic sigmoid function cause neural network get stuck training time softmax function generalized logistic activation function used multiclass classification 2 tanh hyperbolic tangent activation function tanh also like logistic sigmoid better range tanh function 1 1 tanh also sigmoidal shaped advantage negative inputs mapped strongly negative zero inputs mapped near zero tanh graph function differentiable function monotonic derivative monotonic tanh function mainly used classification two classes 3 relu rectified linear unit activation function relu used activation function world right nowsince used almost convolutional neural networks deep learning see relu half rectified bottom fz zero z less zero fz equal z z equal zero range 0 infinity function derivative monotonic issue negative values become zero immediately decreases ability model fit train data properly means negative input given relu activation function turns value zero immediately graph turns affects resulting graph mapping negative values appropriately 4 leaky relu attempt solve dying relu problem see leak leak helps increase range relu function usually value 001 001 called randomized relu therefore range leaky relu infinity infinity leaky randomized relu functions monotonic nature also derivatives also monotonic nature posting 2 posts per week dont miss tutorial follow medium facebook twitter linkedin google quora see similar posts comments question write comment clap share follow happy helpful kudos 2 epoch vs batch size vs iterations 3 train inception custom images cpu 4 tensorflow image recognition python api tutorial cpu quick cheer standing ovation clap show much enjoyed story interested programming python c arduino machine learning im editor arduino community medium also like write stuff sharing concepts ideas codes,en,"['Logistic Activation Function', 'ReLU', 'LinkedIn', 'TensorFlow Image Recognition Python', 'Arduino Community on Medium']"
183,Jae Duk Seo,33,Principal Component Analysis Network in Tensorflow with Interactive Code,"A natural extension from Principle Component Analysis pooling layer would be making a full neural network out of the layer. I wanted to know if this was even possible as well as how well or worse it performs on MNIST data.
Principle Component Analysis (PCA) Pooling Layer
For anyone who is not familiar with PCAP please read this blog post first. The basic idea is Pooling layers such as Max or Mean pooling operations performs dimensionality reduction to not only to save computational power but also to act as a regularizer. PCA is a dimensionality reduction technique in which converts correlated variables into a set of values of linearly uncorrelated variables called principal components. And we can take advantage of this operation to do a similar job as max/mean pooling.
Network Composed of Majority of Pooling Layers
Now I know what you are thinking, it doesn’t make sense to have a network that is only composed of pooling layer while performing classification. And you are completely right! It doesn’t! But I just wanted to try this out for fun.
Data Set / Network Architecture
Blue Rectangle → PCAP or Max Pooling LayerGreen Rectangle → Convolution Layer to increase channel size + Global Averaging Pooling operation
The network itself is very simple, only four pooling layers and one convolution layer to increase the channel size. However, in-order for the dimension to match up we will downsample each images into 16*16 dimension. Hence the Tensors will have a shape of ...
[Batch Size,16,16,1] → [Batch Size,8,8,1] → [Batch Size,4,4,1] → [Batch Size,2,2,1] → [Batch Size,1,1,1] → [Batch Size,1,1,10] → [Batch Size,10]
And we can perform classification with soft max layer as any other network does.
Results: Principle Component Network
As seen above, the training accuracy have stagnated at 18 percent accuracy which is horrible LOL. But I suspected that the network didn’t have enough learning capacity from the start and this was best it could do. However I wanted to see how each PCAP layer transforms the image.
Top Left Image → Original InputTop Right Image → After First LayerBottom Left Image → After Second LayerBottom Right Image → After Fourth Layer
One obvious pattern we can observe is the change of brightens. For example if the top left pixel was white in the second layer this pixel will change to black in the next layer. Currently, I am not 100% sure on why this is happening, but with more study I hope to know exactly why.
Results: Max Pooling Network
As seen above, when we replace all of the PCAP layers with max pooling operation we can observe that the accuracy on training images stagnated around 14 percent, confirming the fact that the network didn’t have enough learning capacity from the start.
Top Left Image → Original InputTop Right Image → After First LayerBottom Left Image → After Second LayerBottom Right Image → After Fourth Layer
Contrast to PCAP, with max pooling we can clearly observe that the pixel with most high intensity moves on to the next layer. This is expected since, that is what max pooling does.
Interactive Code
For Google Colab, you would need a google account to view the codes, also you can’t run read only scripts in Google Colab so make a copy on your play ground. Finally, I will never ask for permission to access your files on Google Drive, just FYI. Happy Coding!
To access the network with PCAP please click here.To access the network with Max Pooling please click here.
Final Words
I wasn’t expecting much of this network from the start but I expected at least 30 percent accuracy on training / testing images LOL.
If any errors are found, please email me at jae.duk.seo@gmail.com, if you wish to see the list of all of my writing please view my website here.
Meanwhile follow me on my twitter here, and visit my website, or my Youtube channel for more content. I also implemented Wide Residual Networks, please click here to view the blog post.
Reference
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
https://jaedukseo.me | | | |  |Your everyday Seo, who likes kimchi
Sharing concepts, ideas, and codes.
",natural extension principle component analysis pooling layer would making full neural network layer wanted know even possible well well worse performs mnist data principle component analysis pca pooling layer anyone familiar pcap please read blog post first basic idea pooling layers max mean pooling operations performs dimensionality reduction save computational power also act regularizer pca dimensionality reduction technique converts correlated variables set values linearly uncorrelated variables called principal components take advantage operation similar job maxmean pooling network composed majority pooling layers know thinking doesnt make sense network composed pooling layer performing classification completely right doesnt wanted try fun data set network architecture blue rectangle pcap max pooling layergreen rectangle convolution layer increase channel size global averaging pooling operation network simple four pooling layers one convolution layer increase channel size however inorder dimension match downsample images 1616 dimension hence tensors shape batch size16161 batch size881 batch size441 batch size221 batch size111 batch size1110 batch size10 perform classification soft max layer network results principle component network seen training accuracy stagnated 18 percent accuracy horrible lol suspected network didnt enough learning capacity start best could however wanted see pcap layer transforms image top left image original inputtop right image first layerbottom left image second layerbottom right image fourth layer one obvious pattern observe change brightens example top left pixel white second layer pixel change black next layer currently 100 sure happening study hope know exactly results max pooling network seen replace pcap layers max pooling operation observe accuracy training images stagnated around 14 percent confirming fact network didnt enough learning capacity start top left image original inputtop right image first layerbottom left image second layerbottom right image fourth layer contrast pcap max pooling clearly observe pixel high intensity moves next layer expected since max pooling interactive code google colab would need google account view codes also cant run read scripts google colab make copy play ground finally never ask permission access files google drive fyi happy coding access network pcap please click hereto access network max pooling please click final words wasnt expecting much network start expected least 30 percent accuracy training testing images lol errors found please email jaedukseogmailcom wish see list writing please view website meanwhile follow twitter visit website youtube channel content also implemented wide residual networks please click view blog post reference quick cheer standing ovation clap show much enjoyed story httpsjaedukseome everyday seo likes kimchi sharing concepts ideas codes,en,"['Principle Component Analysis', 'MNIST', 'PCAP', 'PCA', 'Tensors', 'Principle Component Network', 'Second LayerBottom Right Image', 'FYI', 'Wide Residual Networks']"
184,Jae Duk Seo,20,"Multi-Stream RNN, Concat RNN, Internal Conv RNN, Lag 2 RNN in Tensorflow","For the last two week I have been dying to implement different kinds of Recurrent Neural Networks (RNN) and finally I have the time to implement all of them. Below is the list of different RNN cases I wanted to try out.
Case a: Vanilla Recurrent Neural Network Case b: Multi-Stream Recurrent Neural NetworkCase c: Concatenated Recurrent Neural NetworkCase d: Internal Convolutional Recurrent Neural NetworkCase e: Lag 2 Recurrent Neural Network
Vanilla Recurrent Neural Network
There is in total of 5 different case of RNN I wish to implement. However, in order to fully understand all of the implementations it would be a good idea to have a strong understanding of vanilla RNN (Case a is vanilla RNN so if you understand code for case a you are good to go.)
If anyone wishes to review simple RNN please visit my old blog post “Only Numpy: Vanilla Recurrent Neural Network Deriving Back propagation Through Time Practice ”.
Case a: Vanilla Recurrent Neural Network ( Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time Stamp
As seen above, the base network is simple RNN combined with convolutional neural network for classification. The RNN have time stamp of 4, which means we are going to give the network 4 different kinds of input at each time stamp. And to do that I am going to add some noise to the original image.
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above our base network already performs well. Now the question is how other methods performs and would it be able to regularize better than our base network.
Case b: Multi-Stream Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Convolution Input Stream Yellow Circle → Fully Connected Network StreamBlack Box → Recurrent Neural Network with 4 Time Stamp
The idea behind this RNN is simply to give different representation of data to the RNN. In our base network we have the network either the raw image or image with some noise added.
Red Box → Additional Four CNN/FNN layers to ‘process’ the inputBlue Box → Creating Inputs at each different time stamps
As seen below now our RNN takes in input of tensor size with [batch_size, 26, 26, 1] reducing the width and the height by 2. And I was hoping that different representation of the data would act as a regularization. (Similar to data augmentation)
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above the network did pretty well, and have outperformed our base network by 1 percent on the testing images.
Case c: Concatenated Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampBlack Curved Arrow → Concatenated Input for Each Time Stamp
This approach is very simple, the idea was that on each time stamp different features will be extracted and it might be useful for the network to have more features overtime. (For the Recurrent Layers.)
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
Sadly, this was a huge failure. I guess the empty hidden values does not help (one bit) for the network to perform well.
Case d: Internal Convolutional Recurrent Neural Network (Idea/Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampGray Arrow → Performing Internal Convolution before passing onto the next time stamp
As seen above, this network takes in the exact same input as our base network. However this time we are going to perform additional convolution operations in the internal representation of the data.
Right Image → Declaring 3 new convolution layerLeft Image (Red Box) → If the current internal layer is not None, we are going to perform additional convolution operation.
I actually had no theoretical reason behind this implementation, I just wanted to see if it works LOL.
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
As seen above the network did a fine job at converging, however it was not able to outperform our base network. (Sadly).
Case e: Lag 2 Recurrent Neural Network (Idea / Results)
Red Box → 3 Convolutional LayerOrange → Global Average Pooling and SoftMaxGreen Circle → Hidden Unit at Time 0 (or Lag of 1)Blue Circle → Input in 4 Time StampBlack Box → Recurrent Neural Network with 4 Time StampPurple Circle → Hidden State Lag of 2
In a traditional RNN setting we only rely on the most previous values to determine the current value. For a while I was thinking that there is no reason for us to limit the look back time (or lag) as 1. We can extend this idea into lag 3 or lag 4 etc. (Just for simplicity I took lag 2)
Blue Line → Train Cost Over TimeOrange Line → Train Accuracy Over TimeGreen Line → Test Cost Over TimeRed Line → Test Accuracy Over Time
Thankfully the network did better than the base network. (But with very small margin), however this type of network would be most suitable for time series data.
Interactive Code / Transparency
For Google Colab, you would need a google account to view the codes, also you can’t run read only scripts in Google Colab so make a copy on your play ground. Finally, I will never ask for permission to access your files on Google Drive, just FYI. Happy Coding! Also for transparency I uploaded all of the training logs on my github.
To access the code for case a click here, for the logs click here. To access the code for case b click here, for the logs click here.To access the code for case c click here, for the logs click here.To access the code for case c click here, for the logs click here.To access the code for case c click here, for the logs click here.
Final Words
I wanted to Review RNN for quite a long time now, finally I get to do it.
If any errors are found, please email me at jae.duk.seo@gmail.com, if you wish to see the list of all of my writing please view my website here.
Meanwhile follow me on my twitter here, and visit my website, or my Youtube channel for more content. I also implemented Wide Residual Networks, please click here to view the blog post.
Reference
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
https://jaedukseo.me | | | |  |Your everyday Seo, who likes kimchi
Sharing concepts, ideas, and codes.
",last two week dying implement different kinds recurrent neural networks rnn finally time implement list different rnn cases wanted try case vanilla recurrent neural network case b multistream recurrent neural networkcase c concatenated recurrent neural networkcase internal convolutional recurrent neural networkcase e lag 2 recurrent neural network vanilla recurrent neural network total 5 different case rnn wish implement however order fully understand implementations would good idea strong understanding vanilla rnn case vanilla rnn understand code case good go anyone wishes review simple rnn please visit old blog post numpy vanilla recurrent neural network deriving back propagation time practice case vanilla recurrent neural network results red box 3 convolutional layerorange global average pooling softmaxgreen circle hidden unit time 0blue circle input 4 time stampblack box recurrent neural network 4 time stamp seen base network simple rnn combined convolutional neural network classification rnn time stamp 4 means going give network 4 different kinds input time stamp going add noise original image blue line train cost timeorange line train accuracy timegreen line test cost timered line test accuracy time seen base network already performs well question methods performs would able regularize better base network case b multistream recurrent neural network idea results red box 3 convolutional layerorange global average pooling softmaxgreen circle hidden unit time 0blue circle convolution input stream yellow circle fully connected network streamblack box recurrent neural network 4 time stamp idea behind rnn simply give different representation data rnn base network network either raw image image noise added red box additional four cnnfnn layers process inputblue box creating inputs different time stamps seen rnn takes input tensor size batch_size 26 26 1 reducing width height 2 hoping different representation data would act regularization similar data augmentation blue line train cost timeorange line train accuracy timegreen line test cost timered line test accuracy time seen network pretty well outperformed base network 1 percent testing images case c concatenated recurrent neural network idea results red box 3 convolutional layerorange global average pooling softmaxgreen circle hidden unit time 0blue circle input 4 time stampblack box recurrent neural network 4 time stampblack curved arrow concatenated input time stamp approach simple idea time stamp different features extracted might useful network features overtime recurrent layers blue line train cost timeorange line train accuracy timegreen line test cost timered line test accuracy time sadly huge failure guess empty hidden values help one bit network perform well case internal convolutional recurrent neural network idearesults red box 3 convolutional layerorange global average pooling softmaxgreen circle hidden unit time 0blue circle input 4 time stampblack box recurrent neural network 4 time stampgray arrow performing internal convolution passing onto next time stamp seen network takes exact input base network however time going perform additional convolution operations internal representation data right image declaring 3 new convolution layerleft image red box current internal layer none going perform additional convolution operation actually theoretical reason behind implementation wanted see works lol blue line train cost timeorange line train accuracy timegreen line test cost timered line test accuracy time seen network fine job converging however able outperform base network sadly case e lag 2 recurrent neural network idea results red box 3 convolutional layerorange global average pooling softmaxgreen circle hidden unit time 0 lag 1blue circle input 4 time stampblack box recurrent neural network 4 time stamppurple circle hidden state lag 2 traditional rnn setting rely previous values determine current value thinking reason us limit look back time lag 1 extend idea lag 3 lag 4 etc simplicity took lag 2 blue line train cost timeorange line train accuracy timegreen line test cost timered line test accuracy time thankfully network better base network small margin however type network would suitable time series data interactive code transparency google colab would need google account view codes also cant run read scripts google colab make copy play ground finally never ask permission access files google drive fyi happy coding also transparency uploaded training logs github access code case click logs click access code case b click logs click hereto access code case c click logs click hereto access code case c click logs click hereto access code case c click logs click final words wanted review rnn quite long time finally get errors found please email jaedukseogmailcom wish see list writing please view website meanwhile follow twitter visit website youtube channel content also implemented wide residual networks please click view blog post reference quick cheer standing ovation clap show much enjoyed story httpsjaedukseome everyday seo likes kimchi sharing concepts ideas codes,en,"['Recurrent Neural Networks', 'Multi-Stream Recurrent Neural NetworkCase', 'Internal Convolutional Recurrent Neural NetworkCase', 'Vanilla Recurrent Neural Network', 'Time', 'Recurrent Neural Network', 'Multi-Stream Recurrent Neural Network', 'CNN', 'FNN', 'Concatenated Recurrent Neural Network', 'Internal Convolutional Recurrent Neural Network', 'FYI', 'Review RNN', 'Wide Residual Networks']"
185,Wallarm,72,TensorFlow Dataset API for increasing training speed of neural networks,"Wallarm AI engine is the heart of our security solution. Two key parameters of our AI engine efficiency are how fast neural networks can be train to reflect the updated training sets and how much compute power need to be dedicated to the training on the on-going basis.
Many of our machine learning algorithms are written on top of TensorFlow, an open-source dataflow software library originally release by Google.
Our average CPU load for the AI engine today is as high as 80% so we are always looking for ways to speed things up in software. Our latest find is Dataset API. Dataset is a mid-level TensorFlow APIs which makes working with data faster and more convenient..
In this blog, we will measure just how much faster model training can be with Dataset, compared to the you use of feed_dict.
For starters, let’s prepare data that will be used to train the model. Dataset can usually be stored in numpy’s arrays regardless of kind of data they are.. That’s why we prepare all our dataset without TensorFlow and store it in .npz format similar to this:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/storing_in_npz_format.py#L1-L10
This step helps us avoid unnecessary data processing load on CPU and memory during model training.
Now we are ready to train the model. First, let’s load preprocessed data from disk:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/load_from_npz.py#L1-L7.
Next the data will be converted from numphy arrays into TensorFlow tensors (tf.data.Dataset.from_tensor_slices method is used for that) and loaded into TensorFlow. Dataset.from_tensor_slices method takes placeholders with the same size of the 0th dimension element and returns dataset object.
Once the dataset is in TF, you can process it, for example, you can use .map(f) function which can process the data. But we already preprocess our dataset and all we need to do is apply batching and, maybe, shuffling. Fortunately, Dataset API already has needed functions. They are .batch and .shuffle. Ok, if we shuffle our dataset how can we use it for production? It’s easy, we simply make another dataset without data been shuffled.
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/datasets.py#L1-L5
Dataset API has other good methods for preprocessing data. There is a comprehensive list of methods in the. official docs.
Next we should extract data from dataset object step by step for each of the training epochs, tf.data.Iterator is tailor-made for it. TF currently supported four type of iterators:
Reinitializeble iterator is very useful, all we need to do to start the work is to create an iterator and initializers for it. iterator.get_next() yields the next elements of our dataset when executed.
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/iterator.py#L1-L8
To demonstrate the viability of using Dataset API let’s use proposed approach for MNIST dataset and for our corporate data . First, we prepared data and after that, we processed 1 and 5 epochs with Dataset API and without. Model for this MNIST example can be found on github:
https://github.com/wallarm/researches/blob/a719923f6a2da461deea0e01622d11cbfc8b057b/tf_ds_api/model.py#L1-L25
Below are the results we obtained on a machine with one Nvidia GTX 1080 and TF 1.8.0.
All code of this experiment is available on GitHub [Link].
MNIST is a very small dataset and profit of Dataset API isn’t representative. By contrast, the results on a real-life dataset are much more impressive.
Thus Dataset API is very good for increasing your training speed. With no source code changes, just some modifications in the stack, you can save 20–30% off the training time.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Adaptive Application Security for DevOps. @NGINX partner. @YCombibator S16
Wallarm is DevOps-friendly WAF with hybrid architecture uniquely suited for cloud applications. It applies machine learning to traffic to adaptively generate security rules and verifies the impact of malicious payloads in real time
",wallarm ai engine heart security solution two key parameters ai engine efficiency fast neural networks train reflect updated training sets much compute power need dedicated training ongoing basis many machine learning algorithms written top tensorflow opensource dataflow software library originally release google average cpu load ai engine today high 80 always looking ways speed things software latest find dataset api dataset midlevel tensorflow apis makes working data faster convenient blog measure much faster model training dataset compared use feed_dict starters lets prepare data used train model dataset usually stored numpys arrays regardless kind data thats prepare dataset without tensorflow store npz format similar httpsgithubcomwallarmresearchesbloba719923f6a2da461deea0e01622d11cbfc8b057btf_ds_apistoring_in_npz_formatpyl1l10 step helps us avoid unnecessary data processing load cpu memory model training ready train model first lets load preprocessed data disk httpsgithubcomwallarmresearchesbloba719923f6a2da461deea0e01622d11cbfc8b057btf_ds_apiload_from_npzpyl1l7 next data converted numphy arrays tensorflow tensors tfdatadatasetfrom_tensor_slices method used loaded tensorflow datasetfrom_tensor_slices method takes placeholders size 0th dimension element returns dataset object dataset tf process example use mapf function process data already preprocess dataset need apply batching maybe shuffling fortunately dataset api already needed functions batch shuffle ok shuffle dataset use production easy simply make another dataset without data shuffled httpsgithubcomwallarmresearchesbloba719923f6a2da461deea0e01622d11cbfc8b057btf_ds_apidatasetspyl1l5 dataset api good methods preprocessing data comprehensive list methods official docs next extract data dataset object step step training epochs tfdataiterator tailormade tf currently supported four type iterators reinitializeble iterator useful need start work create iterator initializers iteratorget_next yields next elements dataset executed httpsgithubcomwallarmresearchesbloba719923f6a2da461deea0e01622d11cbfc8b057btf_ds_apiiteratorpyl1l8 demonstrate viability using dataset api lets use proposed approach mnist dataset corporate data first prepared data processed 1 5 epochs dataset api without model mnist example found github httpsgithubcomwallarmresearchesbloba719923f6a2da461deea0e01622d11cbfc8b057btf_ds_apimodelpyl1l25 results obtained machine one nvidia gtx 1080 tf 180 code experiment available github link mnist small dataset profit dataset api isnt representative contrast results reallife dataset much impressive thus dataset api good increasing training speed source code changes modifications stack save 2030 training time quick cheer standing ovation clap show much enjoyed story adaptive application security devops nginx partner ycombibator s16 wallarm devopsfriendly waf hybrid architecture uniquely suited cloud applications applies machine learning traffic adaptively generate security rules verifies impact malicious payloads real time,en,"['Google', 'AI', 'MNIST', 'Nvidia GTX 1080', 'GitHub', 'Adaptive Application Security', 'WAF']"
186,Kelvin Li,56,The Complex language used in Back Propagation – Kelvin Li – Medium,"I’ve looked all over the internet for explanations of what exactly back propagation is and everyone either uses complicated mathematical language or complex codes to try to explain what back propagation. If someone who doesn’t know either wants to know what it is then how will they really grasp what it is?
In this post, I would like to unveil the secrets of the universe with everyone and hopefully I’ll do a good job at it.
According to Wikipedia, Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.
Backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function
If you have taken a basic elementary algebra class, you may have heard of the idea of a slope. Some people might think the idea of a slope is very insignificant but it is actually the game-changing concept that caused all the technological advancement within the last century.
To know the slope of something means that you know the rate of something changing over a period of time. Knowing this gives us power to manipulate things to our advantage.
Now you can think of a gradient as the slope of something in a higher dimension. I won’t go into details but that is the general gist of what a gradient is.
Weights are the values that we want to use to adjust the outputs of our functions in each neuron. So say we have an output of 2 and we want to change the 2 into a 1, then we would multiply the 2 by a .5 to get the desired result. This means that .5 will be the weight in this case. In a way we are weighing down the output to what we want it to be.
A neuron is simply just a function. A Neural Network(a bunch of neurons) is simply a bunch of functions. Each neuron also has an activation function that spits out a value for the next neuron to calculate.
Think of these functions as how much of a yes or a no an input is. An example would be picture recognizing. When you feed the neural network a picture, the node will spit out a number between 0 and 1. Where 0 is being very NO and 1 being very YES. This process continues between every node until the very end. Which ever node has the highest number, between 0 and 1, would be the decision the machine makes.
A loss function is just some function that we use to determine how correct the predicted output is from the real output. For example, we input a picture of a cat into the machine but the machine predicts that it’s a dinosaur. Clearly the machine is not doing a very good job. So we need some way to know how correct the machine is compared to the real data. Which is where the loss function comes in.
Now that we have all the necessary understandings, we can go into the real sauce.
Now what I am about to explain to you is going to either confuse the crap out of you or make you feel enlightened.
Let’s pretend you are trying to build a door lock opening mechanism. This mechanism involves you pressing a button, which triggers a ball rolling down a platform and knocks over a switch that unlocks the door.
Now lets think about this. There are a few components that we have to keep in mind. The 1st component being you pressing the button, the 2nd component is the ball rolling down a platform, and the 3rd component being the switch being knocked over.
There is actually a lot of physics going on around here but let’s just focus on the ball rolling down the platform.
Now when you create this mechanism, you want the door to ideally open in 3 seconds. But you don’t have any tools to measure the time and length, so all you can do is to create a platform through intuition.
You build your first platform and let the ball roll and realized that it took 9 seconds for the door to open after pressing the button.
So you go back to the platform and make the platform steeper.
You performed the same trial and error over and over again until you got the ideal opening time.
This my friend, is Backpropagation.
Well true. But the idea is basically the same. In a Neural Net, we have weights assigned to each neuron. These weights will get multiplied by a certain input and modified through some activation function. The result of these activation function might not always be what we want.
What backpropagation would do is that it will do some calculus (will be covered in another post) to determine the direction of increase/decrease, aka the gradient,(cut less of the platform or cut more of the platform) to achieve the best weights (ideal time the door opens). It then updates these weights every time it has created new weights and runs the neural net again(every trial you cut a piece of the platform to test).
Eventually we will achieve the best possible weights that satisfies our desired accuracy.
In my next post, I will discuss more in depth about the math that is involved with backpropagation.
References and Links
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Getting stuck 24/7
",ive looked internet explanations exactly back propagation everyone either uses complicated mathematical language complex codes try explain back propagation someone doesnt know either wants know really grasp post would like unveil secrets universe everyone hopefully ill good job according wikipedia backpropagation method used artificial neural networks calculate gradient needed calculation weights used network backpropagation commonly used gradient descent optimization algorithm adjust weight neurons calculating gradient loss function taken basic elementary algebra class may heard idea slope people might think idea slope insignificant actually gamechanging concept caused technological advancement within last century know slope something means know rate something changing period time knowing gives us power manipulate things advantage think gradient slope something higher dimension wont go details general gist gradient weights values want use adjust outputs functions neuron say output 2 want change 2 1 would multiply 2 5 get desired result means 5 weight case way weighing output want neuron simply function neural networka bunch neurons simply bunch functions neuron also activation function spits value next neuron calculate think functions much yes input example would picture recognizing feed neural network picture node spit number 0 1 0 1 yes process continues every node end ever node highest number 0 1 would decision machine makes loss function function use determine correct predicted output real output example input picture cat machine machine predicts dinosaur clearly machine good job need way know correct machine compared real data loss function comes necessary understandings go real sauce explain going either confuse crap make feel enlightened lets pretend trying build door lock opening mechanism mechanism involves pressing button triggers ball rolling platform knocks switch unlocks door lets think components keep mind 1st component pressing button 2nd component ball rolling platform 3rd component switch knocked actually lot physics going around lets focus ball rolling platform create mechanism want door ideally open 3 seconds dont tools measure time length create platform intuition build first platform let ball roll realized took 9 seconds door open pressing button go back platform make platform steeper performed trial error got ideal opening time friend backpropagation well true idea basically neural net weights assigned neuron weights get multiplied certain input modified activation function result activation function might always want backpropagation would calculus covered another post determine direction increasedecrease aka gradientcut less platform cut platform achieve best weights ideal time door opens updates weights every time created new weights runs neural net againevery trial cut piece platform test eventually achieve best possible weights satisfies desired accuracy next post discuss depth math involved backpropagation references links quick cheer standing ovation clap show much enjoyed story getting stuck 247,en,"['Wikipedia, Backpropagation', 'algorithm']"
187,Avinash Sharma V,6900,Understanding Activation Functions in Neural Networks,"Recently, a colleague of mine asked me a few questions like “why do we have so many activation functions?”, “why is that one works better than the other?”, ”how do we know which one to use?”, “is it hardcore maths?” and so on. So I thought, why not write an article on it for those who are familiar with neural network only at a basic level and is therefore, wondering about activation functions and their “why-how-mathematics!”.
NOTE: This article assumes that you have a basic knowledge of an artificial “neuron”. I would recommend reading up on the basics of neural networks before reading this article for better understanding.
So what does an artificial neuron do? Simply put, it calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not ( yeah right, an activation function does this, but let’s go with the flow for a moment ).
So consider a neuron.
Now, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. So how do we decide whether the neuron should fire or not ( why this firing pattern? Because we learnt it from biology that’s the way brain works and brain is a working testimony of an awesome and intelligent system ).
We decided to add “activation functions” for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as “fired” or not. Or rather let’s say — “activated” or not.
The first thing that comes to our minds is how about a threshold based activation function? If the value of Y is above a certain value, declare it activated. If it’s less than the threshold, then say it’s not. Hmm great. This could work!
Activation function A = “activated” if Y > threshold else not
Alternatively, A = 1 if y> threshold, 0 otherwise
Well, what we just did is a “step function”, see the below figure.
Its output is 1 ( activated) when value > 0 (threshold) and outputs a 0 ( not activated) otherwise.
Great. So this makes an activation function for a neuron. No confusions. However, there are certain drawbacks with this. To understand it better, think about the following.
Suppose you are creating a binary classifier. Something which should say a “yes” or “no” ( activate or not activate ). A Step function could do that for you! That’s exactly what it does, say a 1 or 0. Now, think about the use case where you would want multiple such neurons to be connected to bring in more classes. Class1, class2, class3 etc. What will happen if more than 1 neuron is “activated”. All neurons will output a 1 ( from step function). Now what would you decide? Which class is it? Hmm hard, complicated.
You would want the network to activate only 1 neuron and others should be 0 ( only then would you be able to say it classified properly/identified the class ). Ah! This is harder to train and converge this way. It would have been better if the activation was not binary and it instead would say “50% activated” or “20% activated” and so on. And then if more than 1 neuron activates, you could find which neuron has the “highest activation” and so on ( better than max, a softmax, but let’s leave that for now ).
In this case as well, if more than 1 neuron says “100% activated”, the problem still persists.I know! But..since there are intermediate activation values for the output, learning can be smoother and easier ( less wiggly ) and chances of more than 1 neuron being 100% activated is lesser when compared to step function while training ( also depending on what you are training and the data ).
Ok, so we want something to give us intermediate ( analog ) activation values rather than saying “activated” or not ( binary ).
The first thing that comes to our minds would be Linear function.
A = cx
A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).
This way, it gives a range of activations, so it is not binary activation. We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that. So that is ok too. Then what is the problem with this?
If you are familiar with gradient descent for training, you would notice that for this function, derivative is a constant.
A = cx, derivative with respect to x is c. That means, the gradient has no relationship with X. It is a constant gradient and the descent is going to be on constant gradient. If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !!!
This is not that good! ( not always, but bear with me ). There is another problem too. Think about connected layers. Each layer is activated by a linear function. That activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn, fires based on another linear activation function.
No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer! Pause for a bit and think about it.
That means these two layers ( or N layers ) can be replaced by a single layer. Ah! We just lost the ability of stacking layers this way. No matter how we stack, the whole network is still equivalent to a single layer with linear activation ( a combination of linear functions in a linear manner is still another linear function ).
Let’s move on, shall we?
Well, this looks smooth and “step function like”. What are the benefits of this? Think about it for a moment. First things first, it is nonlinear in nature. Combinations of this function are also nonlinear! Great. Now we can stack layers. What about non binary activations? Yes, that too!. It will give an analog activation unlike step function. It has a smooth gradient too.
And if you notice, between X values -2 to 2, Y values are very steep. Which means, any small changes in the values of X in that region will cause values of Y to change significantly. Ah, that means this function has a tendency to bring the Y values to either end of the curve.
Looks like it’s good for a classifier considering its property? Yes ! It indeed is. It tends to bring the activations to either side of the curve ( above x = 2 and below x = -2 for example). Making clear distinctions on prediction.
Another advantage of this activation function is, unlike linear function, the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won’t blow up the activations then.
This is great. Sigmoid functions are one of the most widely used activation functions today. Then what are the problems with this?
If you notice, towards either end of the sigmoid function, the Y values tend to respond very less to changes in X. What does that mean? The gradient at that region is going to be small. It gives rise to a problem of “vanishing gradients”. Hmm. So what happens when the activations reach near the “near-horizontal” part of the curve on either sides?
Gradient is small or has vanished ( cannot make significant change because of the extremely small value ). The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ). There are ways to work around this problem and sigmoid is still very popular in classification problems.
Another activation function that is used is the tanh function.
Hm. This looks very similar to sigmoid. In fact, it is a scaled sigmoid function!
Ok, now this has characteristics similar to sigmoid that we discussed above. It is nonlinear in nature, so great we can stack layers! It is bound to range (-1, 1) so no worries of activations blowing up. One point to mention is that the gradient is stronger for tanh than sigmoid ( derivatives are steeper). Deciding between the sigmoid or tanh will depend on your requirement of gradient strength. Like sigmoid, tanh also has the vanishing gradient problem.
Tanh is also a very popular and widely used activation function.
Later, comes the ReLu function,
A(x) = max(0,x)
The ReLu function is as shown above. It gives an output x if x is positive and 0 otherwise.
At first look this would look like having the same problems of linear function, as it is linear in positive axis. First of all, ReLu is nonlinear in nature. And combinations of ReLu are also non linear! ( in fact it is a good approximator. Any function can be approximated with combinations of ReLu). Great, so this means we can stack layers. It is not bound though. The range of ReLu is [0, inf). This means it can blow up the activation.
Another point that I would like to discuss here is the sparsity of the activation. Imagine a big neural network with a lot of neurons. Using a sigmoid or tanh will cause almost all neurons to fire in an analog way ( remember? ). That means almost all activations will be processed to describe the output of a network. In other words the activation is dense. This is costly. We would ideally want a few neurons in the network to not activate and thereby making the activations sparse and efficient.
ReLu give us this benefit. Imagine a network with random initialized weights ( or normalised ) and almost 50% of the network yields 0 activation because of the characteristic of ReLu ( output 0 for negative values of x ). This means a fewer neurons are firing ( sparse activation ) and the network is lighter. Woah, nice! ReLu seems to be awesome! Yes it is, but nothing is flawless.. Not even ReLu.
Because of the horizontal line in ReLu( for negative X ), the gradient can go towards 0. For activations in that region of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLu problem. This problem can cause several neurons to just die and not respond making a substantial part of the network passive. There are variations in ReLu to mitigate this issue by simply making the horizontal line into non-horizontal component . for example y = 0.01x for x<0 will make it a slightly inclined line rather than horizontal line. This is leaky ReLu. There are other variations too. The main idea is to let the gradient be non zero and recover during training eventually.
ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. That is a good point to consider when we are designing deep neural nets.
Now, which activation functions to use. Does that mean we just use ReLu for everything we do? Or sigmoid or tanh? Well, yes and no. When you know the function you are trying to approximate has certain characteristics, you can choose an activation function which will approximate the function faster leading to faster training process. For example, a sigmoid works well for a classifier ( see the graph of sigmoid, doesn’t it show the properties of an ideal classifier? ) because approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence. You can use your own custom functions too!. If you don’t know the nature of the function you are trying to learn, then maybe i would suggest start with ReLu, and then work backwards. ReLu works most of the time as a general approximator!
In this article, I tried to describe a few activation functions used commonly. There are other activation functions too, but the general idea remains the same. Research for better activation functions is still ongoing. Hope you got the idea behind activation function, why they are used and how do we decide which one to use.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Musings of an AI, Deep Learning, Mathematics addict
",recently colleague mine asked questions like many activation functions one works better know one use hardcore maths thought write article familiar neural network basic level therefore wondering activation functions whyhowmathematics note article assumes basic knowledge artificial neuron would recommend reading basics neural networks reading article better understanding artificial neuron simply put calculates weighted sum input adds bias decides whether fired yeah right activation function lets go flow moment consider neuron value anything ranging inf inf neuron really doesnt know bounds value decide whether neuron fire firing pattern learnt biology thats way brain works brain working testimony awesome intelligent system decided add activation functions purpose check value produced neuron decide whether outside connections consider neuron fired rather lets say activated first thing comes minds threshold based activation function value certain value declare activated less threshold say hmm great could work activation function activated threshold else alternatively 1 threshold 0 otherwise well step function see figure output 1 activated value 0 threshold outputs 0 activated otherwise great makes activation function neuron confusions however certain drawbacks understand better think following suppose creating binary classifier something say yes activate activate step function could thats exactly say 1 0 think use case would want multiple neurons connected bring classes class1 class2 class3 etc happen 1 neuron activated neurons output 1 step function would decide class hmm hard complicated would want network activate 1 neuron others 0 would able say classified properlyidentified class ah harder train converge way would better activation binary instead would say 50 activated 20 activated 1 neuron activates could find neuron highest activation better max softmax lets leave case well 1 neuron says 100 activated problem still persistsi know butsince intermediate activation values output learning smoother easier less wiggly chances 1 neuron 100 activated lesser compared step function training also depending training data ok want something give us intermediate analog activation values rather saying activated binary first thing comes minds would linear function cx straight line function activation proportional input weighted sum neuron way gives range activations binary activation definitely connect neurons together 1 fires could take max softmax decide based ok problem familiar gradient descent training would notice function derivative constant cx derivative respect x c means gradient relationship x constant gradient descent going constant gradient error prediction changes made back propagation constant depending change input deltax good always bear another problem think connected layers layer activated linear function activation turn goes next level input second layer calculates weighted sum input turn fires based another linear activation function matter many layers linear nature final activation function last layer nothing linear function input first layer pause bit think means two layers n layers replaced single layer ah lost ability stacking layers way matter stack whole network still equivalent single layer linear activation combination linear functions linear manner still another linear function lets move shall well looks smooth step function like benefits think moment first things first nonlinear nature combinations function also nonlinear great stack layers non binary activations yes give analog activation unlike step function smooth gradient notice x values 2 2 values steep means small changes values x region cause values change significantly ah means function tendency bring values either end curve looks like good classifier considering property yes indeed tends bring activations either side curve x 2 x 2 example making clear distinctions prediction another advantage activation function unlike linear function output activation function always going range 01 compared inf inf linear function activations bound range nice wont blow activations great sigmoid functions one widely used activation functions today problems notice towards either end sigmoid function values tend respond less changes x mean gradient region going small gives rise problem vanishing gradients hmm happens activations reach near nearhorizontal part curve either sides gradient small vanished cannot make significant change extremely small value network refuses learn drastically slow depending use case gradient computation gets hit floating point value limits ways work around problem sigmoid still popular classification problems another activation function used tanh function hm looks similar sigmoid fact scaled sigmoid function ok characteristics similar sigmoid discussed nonlinear nature great stack layers bound range 1 1 worries activations blowing one point mention gradient stronger tanh sigmoid derivatives steeper deciding sigmoid tanh depend requirement gradient strength like sigmoid tanh also vanishing gradient problem tanh also popular widely used activation function later comes relu function ax max0x relu function shown gives output x x positive 0 otherwise first look would look like problems linear function linear positive axis first relu nonlinear nature combinations relu also non linear fact good approximator function approximated combinations relu great means stack layers bound though range relu 0 inf means blow activation another point would like discuss sparsity activation imagine big neural network lot neurons using sigmoid tanh cause almost neurons fire analog way remember means almost activations processed describe output network words activation dense costly would ideally want neurons network activate thereby making activations sparse efficient relu give us benefit imagine network random initialized weights normalised almost 50 network yields 0 activation characteristic relu output 0 negative values x means fewer neurons firing sparse activation network lighter woah nice relu seems awesome yes nothing flawless even relu horizontal line relu negative x gradient go towards 0 activations region relu gradient 0 weights get adjusted descent means neurons go state stop responding variations error input simply gradient 0 nothing changes called dying relu problem problem cause several neurons die respond making substantial part network passive variations relu mitigate issue simply making horizontal line nonhorizontal component example 001x x0 make slightly inclined line rather horizontal line leaky relu variations main idea let gradient non zero recover training eventually relu less computationally expensive tanh sigmoid involves simpler mathematical operations good point consider designing deep neural nets activation functions use mean use relu everything sigmoid tanh well yes know function trying approximate certain characteristics choose activation function approximate function faster leading faster training process example sigmoid works well classifier see graph sigmoid doesnt show properties ideal classifier approximating classifier function combinations sigmoid easier maybe relu example lead faster training process convergence use custom functions dont know nature function trying learn maybe would suggest start relu work backwards relu works time general approximator article tried describe activation functions used commonly activation functions general idea remains research better activation functions still ongoing hope got idea behind activation function used decide one use quick cheer standing ovation clap show much enjoyed story musings ai deep learning mathematics addict,en,"['Linear', 'linear', '0,1', 'ReLu']"
188,Elle O'Brien,2300,"Romance Novels, Generated by Artificial Intelligence","I’ve always been fascinated with romance novels — the kind they sell at the drugstore for a couple of dollars, usually with some attractive, soft-lit couples on the cover. So when I started futzing around with text-generating neural networks a few weeks ago, I developed an urgent curiosity to discover what artificial intelligence could contribute to the ever-popular genre. Maybe one day there will be entire books written by computers. For now, let’s start with titles.
I gathered over 20,000 Harlequin Romance novel titles and gave them to a neural network, a type of artificial intelligence that learns the structure of text. It’s powerful enough to string together words in a way that seems almost human. 90% human. The other 10% is all wackiness.
I was not disappointed with what came out. I even photoshopped some of my favorites into existence (the author names are synthesized from machine learning, too). Let’s have a look by theme:
A common theme in romance novels is pregnancy, and the word “baby” had a strong showing in the titles I trained the neural network on. Naturally, the neural network came up with a lot of baby-themed titles:
There’s an unusually high concentration of sheikhs, vikings, and billionaires in the Harlequin world. Likewise, the neural network generated some colorful new bachelor-types:
I have so many questions. How is the prince pregnant? What sort of consulting does the count do? Who is Butterfly Earl? And what makes the sheikh’s desires so convenient?
Although there are exceptions, most romance novels end in happily-ever-afters. A lot of them even start with an unexpected wedding — a marriage of convenience, or a stipulation of a business contract, or a sham that turns into real love. The neural network seems to have internalized something about matrimony:
Doctors and surgeons are common paramours for mistresses headed towards the marriage valley:
Christmas is a magical time for surgeons, sheikhs, playboys, dads, consultants, and the women who love them:
What or where is Knith? I just like Mission: Christmas...
This neural network has never seen the big Montana sky, but it has some questionable ideas about cowboys:
The neural network generated some decidedly PG-13 titles:
They can’t all live happily ever after. Some of the generated titles sounded like M. Night Shyamalan was a collaborator:
How did the word “fear” get in there? It’s possible the network generated it without having “fear” in the training set, but a subset of the Harlequin empire is geared towards paranormal and gothic romance that might have included the word (*Note: I checked, and there was “Veil of Fear” published in 2012).
To wrap it up, some of the adorable failures and near-misses generated by the neural network:
I hope you’ve enjoyed computer-generated romance novel titles half as much as I have. Maybe someone out there can write about the Virgin Viking, or the Consultant Count, or the Baby Surgeon Seduction. I’d buy it.
I built a webscraper in Python (thanks, Beautiful Soup!) that grabbed about 20,000 romance novel titles published under the Harlequin brand off of FictionDB.com. Harlequin is, to me, synonymous with the romance genre, although it comprises only a fraction (albeit a healthy one) of the entire market. I fed this list of book titles into a recurrent neural network, using software I got from GitHub, and waited a few hours for the magic to happen. The model I fit was a 3-layer, 256-node recurrent neural network. I also trained the network on the author list in to create some new pen names. For more about the neural network I used, have a look at the fabulous work of Andrej Karpathy.
I discovered that “Surgery by the Sea” is actually a real novel, written by Sheila Douglas and published in 1979! So, this one isn’t an original neural network creation. Because the training set is rather small (only about 1 MB of text data), it’s to be expected that sometimes, the machine will spit out one of the titles it was trained on. One of the more challenging aspects of this project was discerning when that happened, since the real published titles can be more surprising than anything born out of artificial intelligence. For example: “The $4.98 Daddy” and “6'1” Grinch” are both real. In fact, the very first romance novel published by Harlequin was called “The Manatee”.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Computational scientist, software developer, science writer
Sharing concepts, ideas, and codes.
",ive always fascinated romance novels kind sell drugstore couple dollars usually attractive softlit couples cover started futzing around textgenerating neural networks weeks ago developed urgent curiosity discover artificial intelligence could contribute everpopular genre maybe one day entire books written computers lets start titles gathered 20000 harlequin romance novel titles gave neural network type artificial intelligence learns structure text powerful enough string together words way seems almost human 90 human 10 wackiness disappointed came even photoshopped favorites existence author names synthesized machine learning lets look theme common theme romance novels pregnancy word baby strong showing titles trained neural network naturally neural network came lot babythemed titles theres unusually high concentration sheikhs vikings billionaires harlequin world likewise neural network generated colorful new bachelortypes many questions prince pregnant sort consulting count butterfly earl makes sheikhs desires convenient although exceptions romance novels end happilyeverafters lot even start unexpected wedding marriage convenience stipulation business contract sham turns real love neural network seems internalized something matrimony doctors surgeons common paramours mistresses headed towards marriage valley christmas magical time surgeons sheikhs playboys dads consultants women love knith like mission christmas neural network never seen big montana sky questionable ideas cowboys neural network generated decidedly pg13 titles cant live happily ever generated titles sounded like night shyamalan collaborator word fear get possible network generated without fear training set subset harlequin empire geared towards paranormal gothic romance might included word note checked veil fear published 2012 wrap adorable failures nearmisses generated neural network hope youve enjoyed computergenerated romance novel titles half much maybe someone write virgin viking consultant count baby surgeon seduction id buy built webscraper python thanks beautiful soup grabbed 20000 romance novel titles published harlequin brand fictiondbcom harlequin synonymous romance genre although comprises fraction albeit healthy one entire market fed list book titles recurrent neural network using software got github waited hours magic happen model fit 3layer 256node recurrent neural network also trained network author list create new pen names neural network used look fabulous work andrej karpathy discovered surgery sea actually real novel written sheila douglas published 1979 one isnt original neural network creation training set rather small 1 mb text data expected sometimes machine spit one titles trained one challenging aspects project discerning happened since real published titles surprising anything born artificial intelligence example 498 daddy 61 grinch real fact first romance novel published harlequin called manatee quick cheer standing ovation clap show much enjoyed story computational scientist software developer science writer sharing concepts ideas codes,en,"['Harlequin', 'Beautiful Soup', 'FictionDB.com', 'GitHub']"
189,Slav Ivanov,2900,Picking a GPU for Deep Learning – Slav,"Quite a few people have asked me recently about choosing a GPU for Machine Learning. As it stands, success with Deep Learning heavily dependents on having the right hardware to work with. When I was building my personal Deep Learning box, I reviewed all the GPUs on the market. In this article, I’m going to share my insights about choosing the right graphics processor. Also, we’ll go over:
Deep Learning (DL) is part of the field of Machine Learning (ML). DL works by approximating a solution to a problem using neural networks. One of the nice properties of about neural networks is that they find patterns in the data (features) by themselves. This is opposed to having to tell your algorithm what to look for, as in the olde times. However, often this means the model starts with a blank state (unless we are transfer learning). To capture the nature of the data from scratch the neural net needs to process a lot of information. There are two ways to do so — with a CPU or a GPU.
The main computational module in a computer is the Central Processing Unit (better known as CPU). It is designed to do computation rapidly on a small amount of data. For example, multiplying a few numbers on a CPU is blazingly fast. But it struggles when operating on a large amount of data. E.g., multiplying matrices of tens or hundreds thousand numbers. Behind the scenes, DL is mostly comprised of operations like matrix multiplication.
Amusingly, 3D computer games rely on these same operations to render that beautiful landscape you see in Rise of the Tomb Raider. Thus, GPUs were developed to handle lots of parallel computations using thousands of cores. Also, they have a large memory bandwidth to deal with the data for these computations. This makes them the ideal commodity hardware to do DL on. Or at least, until ASICs for Machine Learning like Google’s TPU make their way to market.
For me, the most important reason for picking a powerful graphics processor is saving time while prototyping models. If the networks train faster the feedback time will be shorter. Thus, it would be easier for my brain to connect the dots between the assumptions I had for the model and its results.
See Tim Dettmers’ answer to “Why are GPUs well-suited to deep learning?” on Quora for a better explanation. Also for an in-depth, albeit slightly outdated GPUs comparison see his article “Which GPU(s) to Get for Deep Learning”.
There are main characteristics of a GPU related to DL are:
There are two reasons for having multiple GPUs: you want to train several models at once, or you want to do distributed training of a single model. We’ll go over each one.
Training several models at once is a great technique to test different prototypes and hyperparameters. It also shortens your feedback cycle and lets you try out many things at once.
Distributed training, or training a single network on several video cards is slowly but surely gaining traction. Nowadays, there are easy to use approaches to this for Tensorflow and Keras (via Horovod), CNTK and PyTorch. The distributed training libraries offer almost linear speed-ups to the number of cards. For example, with 2 GPUs you get 1.8x faster training.
PCIe Lanes (Updated): The caveat to using multiple video cards is that you need to be able to feed them with data. For this purpose, each GPU should have 16 PCIe lanes available for data transfer. Tim Dettmers points out that having 8 PCIe lanes per card should only decrease performance by “0–10%” for two GPUs.
For a single card, any desktop processor and chipset like Intel i5 7500 and Asus TUF Z270 will use 16 lanes.
However, for two GPUs, you can go 8x/8x lanes or get a processor AND a motherboard that support 32 PCIe lanes. 32 lanes are outside the realm of desktop CPUs. An Intel Xeon with a MSI — X99A SLI PLUS will do the job.
For 3 or 4 GPUs, go with 8x lanes per card with a Xeon with 24 to 32 PCIe lanes.
To have 16 PCIe lanes available for 3 or 4 GPUs, you need a monstrous processor. Something in the class of or AMD ThreadRipper (64 lanes) with a corresponding motherboard.
Also, for more GPUs you need a faster processor and hard disk to be able to feed them data quickly enough, so they don’t sit idle.
Nvidia has been focusing on Deep Learning for a while now, and the head start is paying off. Their CUDA toolkit is deeply entrenched. It works with all major DL frameworks — Tensoflow, Pytorch, Caffe, CNTK, etc. As of now, none of these work out of the box with OpenCL (CUDA alternative), which runs on AMD GPUs. I hope support for OpenCL comes soon as there are great inexpensive GPUs from AMD on the market. Also, some AMD cards support half-precision computation which doubles their performance and VRAM size.
Currently, if you want to do DL and want to avoid major headaches, choose Nvidia.
Your GPU needs a computer around it:
Hard Disk: First, you need to read the data off the disk. An SSD is recommended here, but an HDD can work as well.
CPU: That data might have to be decoded by the CPU (e.g. jpegs). Fortunately, any mid-range modern processor will do just fine.
Motherboard: The data passes via the motherboard to reach the GPU. For a single video card, almost any chipset will work. If you are planning on working with multiple graphic cards, read this section.
RAM: It is recommended to have 2 gigabytes of memory for every gigabyte of video card RAM. Having more certainly helps in some situations, like when you want to keep an entire dataset in memory.
Power supply: It should provide enough power for the CPU and the GPUs, plus 100 watts extra.
You can get all of this for $500 to $1000. Or even less if you buy a used workstation.
Here is performance comparison between all cards. Check the individual card profiles below. Notably, the performance of Titan XP and GTX 1080 Ti is very close despite the huge price gap between them.
The price comparison reveals that GTX 1080 Ti, GTX 1070 and GTX 1060 have great value for the compute performance they provide. All the cards are in the same league value-wise, except Titan XP.
The king of the hill. When every GB of VRAM matters, this card has more than any other on the (consumer) market. It’s only a recommended buy if you know why you want it.
For the price of Titan X, you could get two GTX 1080s, which is a lot of power and 16 GBs of VRAM.
This card is what I currently use. It’s a great high-end option, with lots of RAM and high throughput. Very good value.
I recommend this GPU if you can afford it. It works great for Computer Vision or Kaggle competitions.
Quite capable mid to high-end card. The price was reduced from $700 to $550 when 1080 Ti was introduced. 8 GB is enough for most Computer Vision tasks. People regularly compete on Kaggle with these.
The newest card in Nvidia’s lineup. If 1080 is over budget, this will get you the same amount of VRAM (8 GB). Also, 80% of the performance for 80% of the price. Pretty sweet deal.
It’s hard to get these nowadays because they are used for cryptocurrency mining. With a considerable amount of VRAM for this price but somewhat slower. If you can get it (or a couple) second-hand at a good price, go for it.
It’s quite cheap but 6 GB VRAM is limiting. That’s probably the minimum you want to have if you are doing Computer Vision. It will be okay for NLP and categorical data models.
Also available as P106–100 for cryptocurrency mining, but it’s the same card without a display output.
The entry-level card which will get you started but not much more. Still, if you are unsure about getting in Deep Learning, this might be a cheap way to get your feet wet.
Titan X Pascal It used to be the best consumer GPU Nvidia had to offer. Made obsolete by 1080 Ti, which has the same specs and is 40% cheaper.
Tesla GPUsThis includes K40, K80 (which is 2x K40 in one), P100, and others. You might already be using these via Amazon Web Services, Google Cloud Platform, or another cloud provider.
In my previous article, I did some benchmarks on GTX 1080 Ti vs. K40. The 1080 performed five times faster than the Tesla card and 2.5x faster than K80. K40 has 12 GB VRAM and K80 a whopping 24 GBs.
In theory, the P100 and GTX 1080 Ti should be in the same league performance-wise. However, this cryptocurrency comparison has P100 lagging in every benchmark. It is worth noting that you can do half-precision on P100, effectively doubling the performance and VRAM size.
On top of all this, K40 goes for over $2000, K80 for over $3000, and P100 is about $4500. And they get still get eaten alive by a desktop-grade card. Obviously, as it stands, I don’t recommend getting them.
All the specs in the world won’t help you if you don’t know what you are looking for. Here are my GPU recommendations depending on your budget:
I have over $1000: Get as many GTX 1080 Ti or GTX 1080 as you can. If you have 3 or 4 GPUs running in the same box, beware of issues with feeding them with data. Also keep in mind the airflow in the case and the space on the motherboard.
I have $700 to $900: GTX 1080 Ti is highly recommended. If you want to go multi-GPU, get 2x GTX 1070 (if you can find them) or 2x GTX 1070 Ti. Kaggle, here I come!
I have $400 to $700: Get the GTX 1080 or GTX 1070 Ti. Maybe 2x GTX 1060 if you really want 2 GPUs. However, know that 6 GB per model can be limiting.
I have $300 to $400: GTX 1060 will get you started. Unless you can find a used GTX 1070.
I have less than $300: Get GTX 1050 Ti or save for GTX 1060 if you are serious about Deep Learning.
Deep Learning has the great promise of transforming many areas of our life. Unfortunately, learning to wield this powerful tool, requires good hardware. Hopefully, I’ve given you some clarity on where to start in this quest.
Disclosure: The above are affiliate links, to help me pay for, well, more GPUs.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Entrepreneur / Hacker
Machine learning, Deep learning and other types of learning.
",quite people asked recently choosing gpu machine learning stands success deep learning heavily dependents right hardware work building personal deep learning box reviewed gpus market article im going share insights choosing right graphics processor also well go deep learning dl part field machine learning ml dl works approximating solution problem using neural networks one nice properties neural networks find patterns data features opposed tell algorithm look olde times however often means model starts blank state unless transfer learning capture nature data scratch neural net needs process lot information two ways cpu gpu main computational module computer central processing unit better known cpu designed computation rapidly small amount data example multiplying numbers cpu blazingly fast struggles operating large amount data eg multiplying matrices tens hundreds thousand numbers behind scenes dl mostly comprised operations like matrix multiplication amusingly 3d computer games rely operations render beautiful landscape see rise tomb raider thus gpus developed handle lots parallel computations using thousands cores also large memory bandwidth deal data computations makes ideal commodity hardware dl least asics machine learning like googles tpu make way market important reason picking powerful graphics processor saving time prototyping models networks train faster feedback time shorter thus would easier brain connect dots assumptions model results see tim dettmers answer gpus wellsuited deep learning quora better explanation also indepth albeit slightly outdated gpus comparison see article gpus get deep learning main characteristics gpu related dl two reasons multiple gpus want train several models want distributed training single model well go one training several models great technique test different prototypes hyperparameters also shortens feedback cycle lets try many things distributed training training single network several video cards slowly surely gaining traction nowadays easy use approaches tensorflow keras via horovod cntk pytorch distributed training libraries offer almost linear speedups number cards example 2 gpus get 18x faster training pcie lanes updated caveat using multiple video cards need able feed data purpose gpu 16 pcie lanes available data transfer tim dettmers points 8 pcie lanes per card decrease performance 010 two gpus single card desktop processor chipset like intel i5 7500 asus tuf z270 use 16 lanes however two gpus go 8x8x lanes get processor motherboard support 32 pcie lanes 32 lanes outside realm desktop cpus intel xeon msi x99a sli plus job 3 4 gpus go 8x lanes per card xeon 24 32 pcie lanes 16 pcie lanes available 3 4 gpus need monstrous processor something class amd threadripper 64 lanes corresponding motherboard also gpus need faster processor hard disk able feed data quickly enough dont sit idle nvidia focusing deep learning head start paying cuda toolkit deeply entrenched works major dl frameworks tensoflow pytorch caffe cntk etc none work box opencl cuda alternative runs amd gpus hope support opencl comes soon great inexpensive gpus amd market also amd cards support halfprecision computation doubles performance vram size currently want dl want avoid major headaches choose nvidia gpu needs computer around hard disk first need read data disk ssd recommended hdd work well cpu data might decoded cpu eg jpegs fortunately midrange modern processor fine motherboard data passes via motherboard reach gpu single video card almost chipset work planning working multiple graphic cards read section ram recommended 2 gigabytes memory every gigabyte video card ram certainly helps situations like want keep entire dataset memory power supply provide enough power cpu gpus plus 100 watts extra get 500 1000 even less buy used workstation performance comparison cards check individual card profiles notably performance titan xp gtx 1080 ti close despite huge price gap price comparison reveals gtx 1080 ti gtx 1070 gtx 1060 great value compute performance provide cards league valuewise except titan xp king hill every gb vram matters card consumer market recommended buy know want price titan x could get two gtx 1080s lot power 16 gbs vram card currently use great highend option lots ram high throughput good value recommend gpu afford works great computer vision kaggle competitions quite capable mid highend card price reduced 700 550 1080 ti introduced 8 gb enough computer vision tasks people regularly compete kaggle newest card nvidias lineup 1080 budget get amount vram 8 gb also 80 performance 80 price pretty sweet deal hard get nowadays used cryptocurrency mining considerable amount vram price somewhat slower get couple secondhand good price go quite cheap 6 gb vram limiting thats probably minimum want computer vision okay nlp categorical data models also available p106100 cryptocurrency mining card without display output entrylevel card get started much still unsure getting deep learning might cheap way get feet wet titan x pascal used best consumer gpu nvidia offer made obsolete 1080 ti specs 40 cheaper tesla gpusthis includes k40 k80 2x k40 one p100 others might already using via amazon web services google cloud platform another cloud provider previous article benchmarks gtx 1080 ti vs k40 1080 performed five times faster tesla card 25x faster k80 k40 12 gb vram k80 whopping 24 gbs theory p100 gtx 1080 ti league performancewise however cryptocurrency comparison p100 lagging every benchmark worth noting halfprecision p100 effectively doubling performance vram size top k40 goes 2000 k80 3000 p100 4500 get still get eaten alive desktopgrade card obviously stands dont recommend getting specs world wont help dont know looking gpu recommendations depending budget 1000 get many gtx 1080 ti gtx 1080 3 4 gpus running box beware issues feeding data also keep mind airflow case space motherboard 700 900 gtx 1080 ti highly recommended want go multigpu get 2x gtx 1070 find 2x gtx 1070 ti kaggle come 400 700 get gtx 1080 gtx 1070 ti maybe 2x gtx 1060 really want 2 gpus however know 6 gb per model limiting 300 400 gtx 1060 get started unless find used gtx 1070 less 300 get gtx 1050 ti save gtx 1060 serious deep learning deep learning great promise transforming many areas life unfortunately learning wield powerful tool requires good hardware hopefully ive given clarity start quest disclosure affiliate links help pay well gpus quick cheer standing ovation clap show much enjoyed story entrepreneur hacker machine learning deep learning types learning,en,"['GPU', 'the Central Processing Unit', 'Google', 'Tensorflow', 'Keras', 'CNTK', 'PyTorch', 'Intel', 'Asus TUF Z270', 'Xeon', 'AMD ThreadRipper', 'CUDA', 'AMD', 'RAM', 'CPU', 'Titan XP', 'VRAM', 'Computer Vision', 'Kaggle', 'NLP', 'GPU Nvidia', 'K40', 'Amazon Web Services', 'Google Cloud Platform', 'GPUs']"
190,Josh,462,Everything You Need to Know About Artificial Neural Networks,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we’re learning more about how to improve their systems. Everything is starting to align, and because of it we’re seeing strides we’ve never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it’s important to understand some of the common terminology and to get a rough idea of how it all works.
A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you’ve read anything about them before, you’ll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.
Fear not if the diagram doesn’t come through very clearly. What’s important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it “travels.” The weighted result can sometimes be the output of your neural network, or as I’ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.
Artificial neural networks are not a new concept. In fact, we didn’t even always call them neural networks and they certainly don’t look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.
But wait, if we’ve had neural networks since the 1960s, why are they just now getting huge? It’s a long story, and I encourage you to listen to this podcast episode to listen to the “fathers” of modern ANNs talk about their perspective of the topic. To quickly summarize, there’s a hand full of factors that kept ANNs from becoming more popular. We didn’t have the computer processing power and we didn’t have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.
You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: “How do they learn what calculations to perform?” Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.
Imagine you’re walking down a sidewalk and you see a lamp post. You’ve never seen a lamp post before, so you walk right into it and say “ouch.” The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say “ouch.” The third time you see a lamp post, you move all the way over to ensure you don’t hit the lamp post. Except now something terrible has happened — now you’ve walked directly into the path of a mailbox, and you’ve never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.
Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there’s another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.
Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn’t calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we’d seen before. This was by no means what we wanted.
Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.
Along with now using deep learning, it’s important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We’ve learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.
Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn’t make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.
When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.
An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.
Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially “re-use” neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There’s redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.
The last ANN type that I’m going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn’t an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.
Now you should have a basic understanding of what’s going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You’re sure to hear more about them in the future so it’s good that you understand them now!
This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.
Josh.ai is an AI agent for your home. If you’re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.
Like Josh on Facebook — http://facebook.com/joshdotai
Follow Josh on Twitter — http://twitter.com/joshdotai
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Technology trends and New Invention? Follow this collection to update the latest trend! [UPDATE] As a collection editor, I don’t have any permission to add your articles in the wild. Please submit your article and I will approve. Also, follow this collection, please.
",year 2015 monumental year field artificial intelligence computers learning learning faster learning improve systems everything starting align seeing strides weve never thought possible programs tell stories pictures cars driving even programs create art want read advancements 2015 read article joshai ai technology becoming core everything think important understand common terminology get rough idea works lot advances artificial intelligence new statistical models overwhelming majority advances technology called artificial neural networks ann youve read anything youll read anns rough model human brain structured take note difference artificial neural networks neural networks though people drop artificial sake brevity word artificial prepended phrase people computational neurobiology could still use term neural network refer work diagram actual neurons synapses brain compared artificial ones fear diagram doesnt come clearly whats important understand anns units calculation called neurons artificial neurons connected synapses really weighted values means given number neuron perform sort calculation example sigmoid function result calculation multiplied weight travels weighted result sometimes output neural network ill talk soon neurons configured layers basic concept idea call deep learning artificial neural networks new concept fact didnt even always call neural networks certainly dont look inception back 1960s called perceptron perceptrons made mccullochpitts neurons even biased perceptrons ultimately people started creating multilayer perceptrons synonymous general artificial neural network hear wait weve neural networks since 1960s getting huge long story encourage listen podcast episode listen fathers modern anns talk perspective topic quickly summarize theres hand full factors kept anns becoming popular didnt computer processing power didnt data train using frowned upon due seemingly arbitrary ability perform well one factors changing computers getting faster powerful internet kinds data shared use see mentioned neurons synapses perform calculations question mind learn calculations perform right answer need essentially ask large amount questions provide answers field called supervised learning enough examples questionanswer pairs calculations values stored neuron synapse slowly adjusted usually process called backpropagation imagine youre walking sidewalk see lamp post youve never seen lamp post walk right say ouch next time see lamp post scoot inches side keep walking time shoulder hits lamp post say ouch third time see lamp post move way ensure dont hit lamp post except something terrible happened youve walked directly path mailbox youve never seen mailbox walk whole process happens obviously oversimplification effectively backpropogation artificial neural network given multitude examples tries get answer example given wrong error calculated values neuron synapse propagated backwards ann next time process takes lot examples real world applications number examples millions understanding artificial neural networks somewhat understanding work theres another question mind know many neurons need use bold word layers earlier layers sets neurons input layer data provide ann hidden layers magic happens lastly output layer finished computations network placed us use layers sets neurons early days multilayer perceptrons originally thought one input layer one hidden layer one output layer sufficient makes sense right given numbers need one set computations get output ann wasnt calculating correct value added neurons single hidden layer eventually learned really creating linear mapping input output words learned certain input would always map certain output flexibility really could handle inputs wed seen means wanted introduce deep learning one hidden layer one reasons better anns need hundreds nodes tens layers leads massive amount variables need keep track time advances parallel programming also allow us run even larger anns batches artificial neural networks getting large longer run single epoch iteration entire network need everything batches subsets entire network complete entire epoch apply backpropagation along using deep learning important know multitude different architectures artificial neural networks typical ann setup way neuron connected every neuron next layer specifically called feed forward artificial neural networks even though anns generally feed forward weve learned connecting neurons neurons certain patterns get even better results specific scenarios recurrent neural networks rnn created address flaw artificial neural networks didnt make decisions based previous knowledge typical ann learned make decisions based context training making decisions use decisions made independent would want something like well think playing game blackjack given 4 5 start know 2 low cards deck information like could help determine whether hit rnns useful natural language processing since prior words characters useful understanding context another word plenty different implementations intention always want retain information achieve bidirectional rnns implement recurrent hidden layer gets modified feedforward want learn rnns check either tutorial implement rnn python blog post uses rnn thoroughly explained honorable mention goes memory networks concept need retain information rnn lstm keeps want understand something like movie book lot events might occur build convolutional neural networks cnn sometimes called lenets named yann lecun artificial neural networks connections layers appear somewhat arbitrary however reason synapses setup way help reduce number parameters need optimized done noting certain symmetry neurons connected essentially reuse neurons identical copies without necessarily needing number synapses cnns commonly used working images thanks ability recognize patterns surrounding pixels theres redundant information contained look individual pixel compared surrounding pixels actually compress information thanks symmetrical properties sounds like perfect situation cnn ask christopher olah great blog post understanding cnns well types anns find another great resource understanding cnns blog post last ann type im going talk type called reinforcement learning reinforcement learning generic term used behavior computers exhibit trying maximize certain reward means isnt artificial neural network architecture however apply reinforcement learning genetic algorithms build artificial neural network architecture might thought use great example explanation found video youtube user sethbling creates reinforcement learning system builds artificial neural network architecture plays mario game entirely another successful example reinforcement learning seen video company deepmind able teach program master various atari games basic understanding whats going state art work artificial intelligence neural networks powering everything including language translation animal recognition picture captioning text summarization anything else think youre sure hear future good understand post written aaron joshai previously aaron worked northrop grumman joining josh team works natural language programming nlp artificial intelligence ai aaron skilled yoyo expert loves video games music programming since middle school recently turned 21 joshai ai agent home youre interested following josh getting early access beta enter email httpsjoshai like josh facebook httpfacebookcomjoshdotai follow josh twitter httptwittercomjoshdotai quick cheer standing ovation clap show much enjoyed story technology trends new invention follow collection update latest trend update collection editor dont permission add articles wild please submit article approve also follow collection please,en,"['ANN', 'McCulloch-Pitts', 'the lamp post', 'Convolutional Neural Networks', 'CNN', 'LeNets', 'SethBling', 'Northrop Grumman', 'NLP', 'YoYo', 'http://facebook.com/joshdotai', 'UPDATE']"
191,Milo Spencer-Harper,317,How to create a mind: The secret of human thought revealed,"In my quest to learn about AI, I read ‘How to create a mind: The secret of human thought revealed’ by Ray Kurzweil. It was incredibly exciting and I’m going to share what I’ve learned.
If I was going to summarise the book in one sentence, I could do no better than Kurzweil’s own words:
Kurzweil argues convincingly that it is both possible and desirable. He goes on to suggest that the algorithm may be simpler than we would expect and that it will be based on the Pattern Recognition Theory of the Mind (PRTM).
The human brain is the most incredible thing in the known universe. A three-pound object, it can discover relativity, imagine the universe, create music, build the Taj Mahal and write a book about the brain.
However, it also has limitations and this gives us clues as to how it works. Recite the alphabet. Ok. Good. Now recite it backwards. The former was easy, the latter likely impossible. Yet, a computer finds it trivial to reverse a list. This tells us that the human brain can only retrieve information sequentially. Studies have also revealed that when thinking about something, we can only hold around four high level concepts in our brain at a time. That’s why we use tools, such as pen and paper to solve a maths problem, to help us think.
So how does the human brain work? Mammals actually have two brains. The old reptilian brain, called the amygdala and the conscious part, called the neocortex. The amygdala is pre-programmed through evolution to seek pleasure and avoid pain. We call this instinct. But what distinguishes mammals from other animals, is that we have also evolved to have a neocortex. Our neocortex rationalises the world around us and makes predictions. It allows us to learn. The two brains are tightly bound and work together. However when reading the book, I wondered if these two brains might also be in conflict. It would explain why the idea of internal struggle is present throughout literature and religion: good vs. evil, social conformity vs. hedonism.
What’s slightly more alarming is we may have more minds than that. Our brain is divided into two hemispheres, left and right. Studies of split-brain patients, where the connection between them has been severed, shows that these patients are not necessarily aware that the other mind exists. If one mind moves the right-hand, the other mind will post-rationalise this decision by creating a false memory (a process known as confabulation). This has implications for us all. We may not have the free will which we perceive to have. Our conscious part of the brain, may simply be creating explanations for what the unconscious parts have already done.
So how does the neocortex work? We know that it consists of around 30 billion cells, which we call neurons. These neurons are connected together and transmit information using electrical impulses. If the sum of the electrical pulses across multiple inputs to a neuron exceeds a certain threshold, that neuron fires causing the next neuron in the chain to fire, and this goes on continuously. We call these processes thoughts. At first, scientists thought this neural network was such a complicated and tangled web, that it would be impossible to ever understand.
However, Kurzweil uses the example of the Einstein’s famous equation E = mc^2 to demonstrate that sometimes the solutions to complex problems are surprisingly simple. There are many examples in science, from Newtonian mechanics to thermodynamics, which show that moving up a level of abstraction dramatically simplifies modelling complex systems.
Recent innovations in brain imaging techniques have revealed that the neocortex contains modules, each consisting of around 100 neurons, repeating over and over again. There are around 300 million of these modules arranged in a grid. So if we could discover the equations which model this module, repeat it on a computer 300 million times and expose it to sensory input, we could create an intelligent being. But what do these modules do?
Kurzweil, who has spent decades researching AI, proposes that these modules are pattern recognisers. When reading this page, one pattern recogniser might be responsible for detecting a horizontal stroke. This module links upward to a module responsible for the letter ‘A’, and if the other relevant stroke modules light up, the ‘A’ module also lights up. The modules ‘A’ , ‘p’, ‘p’ and ‘l’ link to the ‘Apple’ module, which in turn is linked to higher level pattern recognisers, such as thoughts about apples. You don’t actually need to see the ‘e’ because the ‘Apple’ pattern recogniser fires downward, telling the one responsible for the letter ‘e’ that there is a high probability of seeing one. Conversely, inhibitory signals suppress pattern recognisers from firing if a higher level pattern recogniser has detected such an event is unlikely, given the context. We literally see what we expect to see. Kurzweil calls this the ‘Pattern Recogniser Theory of the Mind (PRTM)’. Although it is hard for us to imagine, all of our thoughts and decisions, can be explained by huge numbers of these pattern recognisers hooked together.
We organise these thoughts to explain the world in a hierarchal fashion and use words to give meaning to these modules. The world is naturally hierarchal and the brain mirrors this. Leaves are on trees, trees make up a forest, and a forest covers a mountain. Language is closely related to our thoughts, because language directly evolved from and mirrors our brain. This helps to explain why different languages follow remarkably similar structures. It explains why we think using our native language. We use language not only to express ideas to others, but to express ideas within our own mind.
What’s interesting, is that when AI researchers have worked independently of neuroscientists, their most successful methods turned out to be equivalent to the human brain’s methods. Thus, the human brain offers us clues for how to create an intelligent nonbiological entity.
If we work out the algorithm for a single pattern recogniser, we can repeat it on a computer, creating a neural network. Kurzweil argues that these neural networks could become conscious, like a human mind. Free from biological constraints and benefiting from the exponential growth in computing power, these entities could create even smarter entities, and surpass us in intelligence (this prediction is called technological singularity). I’ll discuss the ethical and social considerations in a future blog post, but for now let’s assume it is desirable.
The question then becomes, what is the algorithm for a single pattern recogniser? Kurzweil recommends using a mathematical technique called hierarchal hidden Markov models, named after the Russian mathematician Andrey Markov (1856–1922). However, this technique is too technical to be properly explained in Kurzweil’s book.
So my next two goals are:
(1) To learn as much as I can about hierarchal hidden Markov models.
(2) To build a simple neural network written in Python from scratch which can be trained to complete a simple task.
In my next blog post, I learn how to build a neural network in 9 lines of Python code.
Note: Submissions do not necessarily represent the views of the editors.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Economics at Oxford University. Founder of www.moju.io. Interested in politics and AI.
Fundamentals and Latest Developments in #DeepLearning
",quest learn ai read create mind secret human thought revealed ray kurzweil incredibly exciting im going share ive learned going summarise book one sentence could better kurzweils words kurzweil argues convincingly possible desirable goes suggest algorithm may simpler would expect based pattern recognition theory mind prtm human brain incredible thing known universe threepound object discover relativity imagine universe create music build taj mahal write book brain however also limitations gives us clues works recite alphabet ok good recite backwards former easy latter likely impossible yet computer finds trivial reverse list tells us human brain retrieve information sequentially studies also revealed thinking something hold around four high level concepts brain time thats use tools pen paper solve maths problem help us think human brain work mammals actually two brains old reptilian brain called amygdala conscious part called neocortex amygdala preprogrammed evolution seek pleasure avoid pain call instinct distinguishes mammals animals also evolved neocortex neocortex rationalises world around us makes predictions allows us learn two brains tightly bound work together however reading book wondered two brains might also conflict would explain idea internal struggle present throughout literature religion good vs evil social conformity vs hedonism whats slightly alarming may minds brain divided two hemispheres left right studies splitbrain patients connection severed shows patients necessarily aware mind exists one mind moves righthand mind postrationalise decision creating false memory process known confabulation implications us may free perceive conscious part brain may simply creating explanations unconscious parts already done neocortex work know consists around 30 billion cells call neurons neurons connected together transmit information using electrical impulses sum electrical pulses across multiple inputs neuron exceeds certain threshold neuron fires causing next neuron chain fire goes continuously call processes thoughts first scientists thought neural network complicated tangled web would impossible ever understand however kurzweil uses example einsteins famous equation e mc2 demonstrate sometimes solutions complex problems surprisingly simple many examples science newtonian mechanics thermodynamics show moving level abstraction dramatically simplifies modelling complex systems recent innovations brain imaging techniques revealed neocortex contains modules consisting around 100 neurons repeating around 300 million modules arranged grid could discover equations model module repeat computer 300 million times expose sensory input could create intelligent modules kurzweil spent decades researching ai proposes modules pattern recognisers reading page one pattern recogniser might responsible detecting horizontal stroke module links upward module responsible letter relevant stroke modules light module also lights modules p p l link apple module turn linked higher level pattern recognisers thoughts apples dont actually need see e apple pattern recogniser fires downward telling one responsible letter e high probability seeing one conversely inhibitory signals suppress pattern recognisers firing higher level pattern recogniser detected event unlikely given context literally see expect see kurzweil calls pattern recogniser theory mind prtm although hard us imagine thoughts decisions explained huge numbers pattern recognisers hooked together organise thoughts explain world hierarchal fashion use words give meaning modules world naturally hierarchal brain mirrors leaves trees trees make forest forest covers mountain language closely related thoughts language directly evolved mirrors brain helps explain different languages follow remarkably similar structures explains think using native language use language express ideas others express ideas within mind whats interesting ai researchers worked independently neuroscientists successful methods turned equivalent human brains methods thus human brain offers us clues create intelligent nonbiological entity work algorithm single pattern recogniser repeat computer creating neural network kurzweil argues neural networks could become conscious like human mind free biological constraints benefiting exponential growth computing power entities could create even smarter entities surpass us intelligence prediction called technological singularity ill discuss ethical social considerations future blog post lets assume desirable question becomes algorithm single pattern recogniser kurzweil recommends using mathematical technique called hierarchal hidden markov models named russian mathematician andrey markov 18561922 however technique technical properly explained kurzweils book next two goals 1 learn much hierarchal hidden markov models 2 build simple neural network written python scratch trained complete simple task next blog post learn build neural network 9 lines python code note submissions necessarily represent views editors quick cheer standing ovation clap show much enjoyed story studied economics oxford university founder wwwmojuio interested politics ai fundamentals latest developments deeplearning,en,"['Kurzweil', 'the Pattern Recognition Theory of the Mind', 'Newtonian', 'Apple', 'Studied Economics', 'Oxford University']"
192,Karl N.,10,Taking Keras to the Zoo – Gab41,"If you follow any of the popular blogs like Google’s research, FastML, Smola’s Adventures in Data Land, or one of the indie-pop ones like Edwin Chen’s blog, you’ve probably also used ModelZoo. Actually, if you’re like our boss, you affectionately call it “The Zoo”. (Actually x 2, if you have interesting blogs that you read, feel free to let us know!)
Unfortunately, ModelZoo is only supported in Caffe. Fortunately, we’ve taken a look at the difference between the kernels in Keras, Theano, and Caffe for you, and after reading this blog, you’ll be able to load models from ModelZoo into any of your favorite Python tools.
Why this post? Why not just download our Github code?
In short, it’s better you figure out how these things work before you use them. That way, you’re better armed to use the latest TensorFlow and Neon toolboxes if you’re prototyping and transitioning your code to Caffe.
So, there’s Hinton’s Dropout and then there’s Caffe’s Dropout...and they’re different. You might be wondering, “What’s the big deal?” Well sir, I have a name of a guy for you, and it’s Willy...Mr. Willy Nilly. One thing Willy Nilly likes is the number 4096. Another thing he likes is to introduce regularization (which includes Dropout) arbitrarily, and Bayesian theorists aren’t a fan. Those people try to fit their work into the probabilistic framework, and they’re trying to hold onto what semblance of theoretical bounds exist for neural networks. However, for you as a practitioner, understanding who’s doing what will save you hours of debugging code.
We singled out Dropout because the way people have implemented it spans the gamut. There’s actually some history as to this variation, but no one really cared, because optimizing for it has almost universally produced similar results. Much of the discussion stems from how the chain rule is implemented since randomly throwing stuff away is apparently not really a differentiable operation. Passing gradients back (i.e., backpropagation) is a fun thing to do; there’s a “technically right” way to do it, and then there’s what’s works.
Back to ModelZoo, where we’d recommend you note the only sentence of any substance in this section, and the sentence is as follows. While Keras and perhaps other packages multiply the gradients by the retention probability at inference time, Caffe does not. That is to say, if you have a dropout level of 0.2, your retention probability is 0.8, and at inference time, Keras will scale the output of your prediction by 0.8. So, download the ModelZoo *.caffemodels, but know that deploying them on Caffe will produce non-scaled results, whereas Keras will.
Hinton explains the reason why you need to scale, and the intuition is as follows. If you’ve only got a portion of your signal seeping through to the next layer during training, you should scale the expectation of what the energy of your final result should be. Seems like a weird thing to care about, right? The argument that minimizes x is still the same as the argument that minimizes 2x. This turns out to be a problem when you’re passing multiple gradients back and don’t implement your layers uniformly. Caffe works in instances like Siamese Networks or Bilinear Networks, but should you scale your networks on two sides differently, don’t be surprised if you’re getting unexpected results.
What does this look like in Francois’s code? Look at the “Dropout” code on Github, or in your installation folder under keras/layers/core.py. If you want to make your own layer for loading in the Dropout module, just comment out the part of the code that does this scaling:
You can modify the original code, or you can create your own custom layer. (We’ve opted to keep our installation of Keras clean and just implemented a new class that extended MaskedLayer.) BTW, you should be careful in your use of Dropout. Our experience with them is that they regularize okay, but could contribute to vanishing gradients really quickly.
Everyday except for Sunday and some holidays, a select few machine learning professors and some signal processing leaders meet in an undisclosed location in the early hours of the morning. The topic of their discussion is almost universally, “How do we get researchers and deep learning practitioners to code bugs into their programs?” One of the conclusions a while back was that the definition of convolution and dense matrix multiplication (or cross-correlation) should be exactly opposite of each other. That way, when people are building algorithms that call themselves “Convolutional Neural Networks”, no one will know which implementation is actually being used for the convolution portion itself.
For those who don’t know, convolutions and sweeping matrix multiplication across an array of data, differ in that convolutions will be flipped before being slid across the array. From Wikipedia, the definition is:
On the other hand, if you’re sweeping matrix multiplications across the array of data, you’re essentially doing cross-correlation, which on Wikipedia, looks like:
Like we said, the only difference is that darned minus/plus sign, which caused us some headache.
We happen to know that Theano and Caffe follow different philosophies. Once again, Caffe doesn’t bother with pleasantries and straight up codes efficient matrix multiplies. To load models from ModelZoo into either Keras and Theano will require the transformation because they strictly follow the definition of convolution. The easy fix is to flip it yourself when you’re loading the weights into your model. For 2D convolution, this looks like:
weights=weights[:,:,::-1,::-1]
Here, the variable “weights” will be inserted into your model’s parameters. You can set weights by indexing into the model. For example, say I want to set the 9th layer’s weights. I would type:
model.layers[9].set_weights(weights)
Incidentally, and this is important, when loading any *.caffemodel into Python, you may have to transpose it in order to use it. You can quickly find this out by loading it if you get an error, but we thought it worth noting.
Alright, alright, we know what you’re really here for; just getting the code and running with it. So, we’ve got some example code that classifies using Keras and the VGG net from the web at our Git (see the link below). But, let’s go through it just a bit. Here’s a step by step account of what you need to do to use the VGG caffe model.
And now you have the basics! Go ahead and take a look at our Github for some goodies. Let us know!
Originally published at www.lab41.org on December 13, 2015.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Gab41 is Lab41’s blog exploring data science, machine learning, and artificial intelligence. Geek out with us!
",follow popular blogs like googles research fastml smolas adventures data land one indiepop ones like edwin chens blog youve probably also used modelzoo actually youre like boss affectionately call zoo actually x 2 interesting blogs read feel free let us know unfortunately modelzoo supported caffe fortunately weve taken look difference kernels keras theano caffe reading blog youll able load models modelzoo favorite python tools post download github code short better figure things work use way youre better armed use latest tensorflow neon toolboxes youre prototyping transitioning code caffe theres hintons dropout theres caffes dropoutand theyre different might wondering whats big deal well sir name guy willymr willy nilly one thing willy nilly likes number 4096 another thing likes introduce regularization includes dropout arbitrarily bayesian theorists arent fan people try fit work probabilistic framework theyre trying hold onto semblance theoretical bounds exist neural networks however practitioner understanding whos save hours debugging code singled dropout way people implemented spans gamut theres actually history variation one really cared optimizing almost universally produced similar results much discussion stems chain rule implemented since randomly throwing stuff away apparently really differentiable operation passing gradients back ie backpropagation fun thing theres technically right way theres whats works back modelzoo wed recommend note sentence substance section sentence follows keras perhaps packages multiply gradients retention probability inference time caffe say dropout level 02 retention probability 08 inference time keras scale output prediction 08 download modelzoo caffemodels know deploying caffe produce nonscaled results whereas keras hinton explains reason need scale intuition follows youve got portion signal seeping next layer training scale expectation energy final result seems like weird thing care right argument minimizes x still argument minimizes 2x turns problem youre passing multiple gradients back dont implement layers uniformly caffe works instances like siamese networks bilinear networks scale networks two sides differently dont surprised youre getting unexpected results look like francoiss code look dropout code github installation folder keraslayerscorepy want make layer loading dropout module comment part code scaling modify original code create custom layer weve opted keep installation keras clean implemented new class extended maskedlayer btw careful use dropout experience regularize okay could contribute vanishing gradients really quickly everyday except sunday holidays select machine learning professors signal processing leaders meet undisclosed location early hours morning topic discussion almost universally get researchers deep learning practitioners code bugs programs one conclusions back definition convolution dense matrix multiplication crosscorrelation exactly opposite way people building algorithms call convolutional neural networks one know implementation actually used convolution portion dont know convolutions sweeping matrix multiplication across array data differ convolutions flipped slid across array wikipedia definition hand youre sweeping matrix multiplications across array data youre essentially crosscorrelation wikipedia looks like like said difference darned minusplus sign caused us headache happen know theano caffe follow different philosophies caffe doesnt bother pleasantries straight codes efficient matrix multiplies load models modelzoo either keras theano require transformation strictly follow definition convolution easy fix flip youre loading weights model 2d convolution looks like weightsweights11 variable weights inserted models parameters set weights indexing model example say want set 9th layers weights would type modellayers9set_weightsweights incidentally important loading caffemodel python may transpose order use quickly find loading get error thought worth noting alright alright know youre really getting code running weve got example code classifies using keras vgg net web git see link lets go bit heres step step account need use vgg caffe model basics go ahead take look github goodies let us know originally published wwwlab41org december 13 2015 quick cheer standing ovation clap show much enjoyed story gab41 lab41s blog exploring data science machine learning artificial intelligence geek us,en,"['Google', 'Data Land', 'ModelZoo', 'Python', 'Github', 'Neon', 'Hinton', 'Keras', 'MaskedLayer', 'Wikipedia', 'VGG']"
193,Milo Spencer-Harper,42,Thanks so much for your response Jared. Really glad you enjoyed reading it.,"Thanks so much for your response Jared. Really glad you enjoyed reading it.
Could you go into more detail about finding the error on layer 1?
That’s a really great question! I’ve changed this response quite a bit as I wrote it, because your question helped me improve my own understanding. It sounds like you know quite a lot about neural networks already, however I’m going to explain everything fully for readers who are new to the field. In the article you read, I modelled the neural network using matrices (grids of numbers). That’s the most common method as it is computationally faster and mathematically equivalent, but it hides a lot of the details. For example, line 15 calculates the error in layer 1, but it is hard to visualise what it is doing.
To help me learn, I’ve re-written that same code by modelling the layers, neurons and synapses explicitly and have created a video of the neural network learning. I’m going to use this new version of my code to answer your question.
For clarity, I’ll describe how I’m going to refer to the layers. The three input neurons are layer 0, the four neurons in the hidden layer are layer 1 and the single output neuron is layer 2. In my code, I chose to associate the synapses with the neuron they flow into.
How do I find the error in layer 1? First I calculate the error of the output neuron (layer 2), which is the difference between its output and the output in the training set example. Then I work my way backwards through the neural network. So I look at the incoming synapses into layer 2, and estimate how much each of the neurons in layer 1 were responsible for the error. This is called back propagation.
In my new version of the code, the neural network is represented by a class called NeuralNetwork, and it has a method called train(), which is shown below. You can see me calculating the error of the ouput neuron (lines 3 and 4). Then I work backwards through the layers (line 5).
Next, I cycle through all the neurons in a layer (line 6) and call each individual neuron’s train() method (line 7).
But what does the neuron’s train() method do? Here it is:
You can see that I cycle through every incoming synapse into the neuron. The two key things to note are:
Let’s consider Line 4 even more carefully, since this is the line which answers your question directly. For each neuron in layer 1, its error is equal to the error in the output neuron (layer 2), multiplied by the weight of its synapse into the output neuron, multiplied by the sensitivity of the output neuron to input.
The sensitivity of a neuron to input, is described by the gradient of its output function. Since I used the Sigmoid curve as my ouput function, the gradient is the derivative of the Sigmoid curve. As well as using the gradient to calculate the errors, I also used the gradient to adjust the weights, so this method of learning is called gradient descent.
If you look back at my old code, which uses matrices you can see that it is mathematically equivalent (unless I made a mistake). With the matrices method, I calculated the error for all the neurons in layer 1 simultaneously. With the new code, I iterated through each neuron separately.
I hope that helps answer your question.
Also, I’m curious if there is any theory or rule of thumb on how many hidden layers and how many neurons in each layer should be used to solve a problem.
Another good question! I’m not sure. I’m pretty new to neural networks. I only started learning about them recently.
I did read a book by the AI researcher Ray Kurzweil, which said that an evolutionary approach works better than consulting experts, when selecting the overall parameters for a neural network. Those neural networks which learned the best, would be selected, he would make random mutations to the parameters, and then pit the offspring against one another.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Studied Economics at Oxford University. Founder of www.moju.io. Interested in politics and AI.
",thanks much response jared really glad enjoyed reading could go detail finding error layer 1 thats really great question ive changed response quite bit wrote question helped improve understanding sounds like know quite lot neural networks already however im going explain everything fully readers new field article read modelled neural network using matrices grids numbers thats common method computationally faster mathematically equivalent hides lot details example line 15 calculates error layer 1 hard visualise help learn ive rewritten code modelling layers neurons synapses explicitly created video neural network learning im going use new version code answer question clarity ill describe im going refer layers three input neurons layer 0 four neurons hidden layer layer 1 single output neuron layer 2 code chose associate synapses neuron flow find error layer 1 first calculate error output neuron layer 2 difference output output training set example work way backwards neural network look incoming synapses layer 2 estimate much neurons layer 1 responsible error called back propagation new version code neural network represented class called neuralnetwork method called train shown see calculating error ouput neuron lines 3 4 work backwards layers line 5 next cycle neurons layer line 6 call individual neurons train method line 7 neurons train method see cycle every incoming synapse neuron two key things note lets consider line 4 even carefully since line answers question directly neuron layer 1 error equal error output neuron layer 2 multiplied weight synapse output neuron multiplied sensitivity output neuron input sensitivity neuron input described gradient output function since used sigmoid curve ouput function gradient derivative sigmoid curve well using gradient calculate errors also used gradient adjust weights method learning called gradient descent look back old code uses matrices see mathematically equivalent unless made mistake matrices method calculated error neurons layer 1 simultaneously new code iterated neuron separately hope helps answer question also im curious theory rule thumb many hidden layers many neurons layer used solve problem another good question im sure im pretty new neural networks started learning recently read book ai researcher ray kurzweil said evolutionary approach works better consulting experts selecting overall parameters neural network neural networks learned best would selected would make random mutations parameters pit offspring one another quick cheer standing ovation clap show much enjoyed story studied economics oxford university founder wwwmojuio interested politics ai,en,"['NeuralNetwork', 'AI', 'Studied Economics', 'Oxford University']"
194,Nikolai Savas,50,CrAIg: Using Neural Networks to learn Mario – Nikolai Savas – Medium,"Joe Crozier and I recently came back from YHack, a 36-hour, 1500 person hackathon held by Yale University. This is our second year in a row attending, and for the second time we managed to place in the top 8!
Our project, named “crAIg”, is a self-teaching algorithm that learns to play Super Mario Bros. for the Nintendo Entertainment System (NES). It begins with absolutely no knowledge of what Mario is, how to play it, or what winning looks like, and using neuroevolution, slowly builds up a foundation for itself to be able to progress in the game.
My focus on this project was the gritty details of the implementation of crAIg’s evolution algorithm, so I figured I’d make a relatively indepth blog post about it.
crAIg’s evolution is based on a paper titled Evolving Neural Networks through Augmented Topologies, specifically an algorithm titled “NEAT”. The rest of the blog post is going to cover my implementation of it, hopefully in relatively layman’s terms.
Before we jump right into the algorithm, I’m going to lay a foundation for the makeup of crAIg’s brain. His “brain” at any given point playing the game is made up of a collection of “neurons” and “synapses”, alternatively titled nodes and connections/links. Essentially, his brain is a directed graph.
Above is the second part of this project, a Node.js server that displays the current state of crAIg’s brain, or what he is “thinking”. Let’s go through it quickly to understand what it’s representing.
On the left you see a big grid of squares. This is what the game looks like right now, or what crAIg can “see”. He doesn’t know what any of the squares mean, but he knows that an “air” tile is different from a “ground” tile in some way. Each of the squares is actually an input neuron.
On the right side you can see the 4 “output neurons”, or the buttons that crAIg can press. You can also see a line going from one of the black squares on the left grid to the “R” neuron, labelled “1”. This is a synapse, and when the input neuron fires on the left, it will send a signal down the synapse and tell crAIg to press the “R” button. In this way, crAIg walks right. As crAIg evolves, more neurons and synapses are created until his brain might look something more like this:
In this one I’ll just point out a couple things. First of all, the green square on the left is a goomba. Second, you can see another neuron at the very bottom (labelled 176). This is called a hidden neuron, and represents a neuron that is neither input nor output. They appear in crAIg’s brain for added complexity as he evolves. You can also see that at his time of death (Mario just died to a goomba), he was trying to press the “R” and “B” buttons.
While learning Mario is a neat application of neural networks and neuroevolution, it serves mostly as a means to demonstrate the power of these self-evolving neural networks. In reality, the applications for neural networks is endless. While crAIg only learned how to play a simple NES game, the exact same algorithm that was implemented could also be applied to a robot that cleans your house, works in a factory, or even paints beautiful paintings.
crAIg is a cool peek into the future where machines no longer need to be programmed to complete specific tasks, but are instead given guidelines and can teach themselves and learn from experience. As the tasks we expect machines to complete become more and more complex, it becomes less possible to “hard code” their tasks in. We need more versatile machines to work for us, and evolving neural networks are a step in that direction.
If you’re curious about some history behind the problems encountered by neuroevolution, I highly recommend reading the paper that this algorithm is based off. The first section of the paper covers many different approaches to neuroevolution and their benefits.
NEAT is a genetic algorithm that puts every iteration of crAIg’s brain to the test and then selectively breeds them in a very similar way to the evolution of species in nature. The hierarchy is as follows:
Synapse/Neuron: Building blocks of crAIg’s brain.
Genome: An iteration of crAIg’s brain. Essentially a collection of neurons and synapses.
Species: A collection of Genomes.
Generation: An iteration of the NEAT algorithm. This is repeated over and over to evolve crAIg.
The first step every generation is to calculate the fitness of every individual genome from the previous generation. This involves running the same function on each genome so that NEAT knows how successful each one is. For crAIg, this means running through a Mario level using a particular genome, or “brain”. After running through the level, we determine the “fitness” of the genome by this function:
Once the fitness of every genome has been calculated, we can move on to the next portion of the algorithm.
This part of the algorithm is probably the least intuitive. The reason for this “adjusted fitness” is to discourage species from growing too big. As the population in a species goes up, their “adjusted fitness” goes down, forcing the genetic algorithm to diversify.
The proper implementation of this algorithm is relatively intensive, so for crAIg’s implementation we simplified it to the following:
The important part here is that each genome now has an adjusted fitness value associated with it.
Here’s where the natural selection part comes in! The “Survival of the fittest” portion is all about determining how many genomes survive another generation, as well as how many offspring will be born in the species. The algorithms used here aren’t outlined directly in the paper, so most of these algorithms were created through trial and error.
The first step is to determine how many off a species will die to make room for more babies. This is done proportionally to a species’ adjusted fitness: the higher the adjusted fitness, the more die off to make room for babies.
The second step is to determine how many children should be born in the species. This is also proportional to the adjusted fitness of the species.
By the end of these two functions, the species will have a certain number of genomes left as well as a “baby quota” — the difference between the number of genomes and the populationSize.
This algorithm is necessary to allow for species to be left behind. Sometimes a species will go down the completely wrong path, and there’s no point in keeping them around. This algorithm works in a very simple way: If a species is in the bottom __% of the entire generation, it is marked for extinction. If a species is marked for extinction __ times in a row, then all genomes in the species are killed off.
Now comes the fun genetics part! Each species should have a certain number of genomes as well as a certain number of allotted spots for new offspring. Those spots now need to be populated.
Each empty population spot needs to be filled, but can be filled through either “asexual” or “sexual” reproduction. In other words, offspring can result from either two genomes in the species being merged or from a mutation of a single genome in the species. Before I discuss the process of “merging” two genomes, I’ll first discuss mutations.
There are three kinds of mutations that can happen to a genome in NEAT. They are as follows:
This involves a re-distribution of all synapse weights in a genome. They can be either completely re-distributed or simply “perturbed”, meaning changed slightly.
2. Mutate Add Synapse
Adding a synapse means finding two previously unconnected nodes and connecting them with a synapse. This new synapse is given a random weight.
3. Mutate Add Node
This is the trickiest of the mutations. When adding a node, you need to split an already existing synapse into two synapses and add a node in between them. The weight of the original synapse is copied on to the second synapse, while the first synapse is given a weight of 1. One important fact to note is that the first synapse (bright red in the above picture) is not actually deleted, but merely “disabled”. This means that it exists in the genome, but it is marked as inactive.
Synapses added in either Mutate Add Node or Mutate Add Synapse are given a unique “id” called a “historical marking”, that is used in the crossover (mating) algorithm.
When two genomes “mate” to produce an offspring, there is an algorithm detailed in the NEAT paper that must be followed. The intuition behind it is to match up common ancestor synapses (remember we’ve been keeping their “historical marking”s), then take the mutations that don’t match up and mix and match them to create the child. Once a child has been created in this way, it undergoes the mutation process outlined above. I won’t go into too much detail on this algorithm but if you’re curious about it you can find a more detailed explanation of it in section 3.2 of the original paper, or you can see the code I used to implement it here.
Once all the babies have been created in every species, we can finally progress to the final stage of the genetic algorithm: Respeciation. Essentially, we first select a “candidate genome” from each species. This genome is now the representative for the species. All genomes that are not selected as candidates are put into a generic pool and re-organized. The re-organization relies on an equation called the “compatibility distance equation”.
This equation determines how similar (or different) any two given genomes are. I won’t go into the gritty details of how the equation works, as it is well explain in section 3.3 of the original paper, as well as here in crAIg’s code.
If a genome is too different from any of the candidate genomes, it is placed in its own species. Using this process, all of the genomes in the generic pool are re-placed into species.
Once this process has completed, the generation is done, and we are ready to re-calculate the fitness of each of the genomes.
While creating crAIg meant getting very little sleep at YHack, it was well worth it for a couple reasons.
First of all, the NEAT algorithm is a very complex one. Learning how to implement a complex algorithm without losing myself in its complexity was an exercise in code cleanliness, despite being pressed for time because of the hackathon.
It was also very interesting to create an algorithm that is mostly based off a paper as opposed to one that I have example code to work with. Often this meant carefully looking into the wording used in the paper to determine whether I should be using a > or a >=, for example.
One of the most difficult parts of this project was that I was unable to test as I was programming. I essentially wrote all of the code blind and then was able to test and debug it once it had all been created. This was for a couple reasons, partially because of the time constraints of a hackathon, and partially because the algorithm as a whole has a lot of interlocking parts, meaning they needed to be in a working state to be able to see if the algorithm worked.
Overall I’m happy and proud by how Joe and I were able to deal with the stress of creating such a deep and complex project from scratch in a short 36 hour period. Not only did we enjoy ourselves and place well, but we also managed to teach crAIg some cool skills, like jumping over the second pipe in Level 1:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
http://savas.ca/ — niko@savas.ca
",joe crozier recently came back yhack 36hour 1500 person hackathon held yale university second year row attending second time managed place top 8 project named craig selfteaching algorithm learns play super mario bros nintendo entertainment system nes begins absolutely knowledge mario play winning looks like using neuroevolution slowly builds foundation able progress game focus project gritty details implementation craigs evolution algorithm figured id make relatively indepth blog post craigs evolution based paper titled evolving neural networks augmented topologies specifically algorithm titled neat rest blog post going cover implementation hopefully relatively laymans terms jump right algorithm im going lay foundation makeup craigs brain brain given point playing game made collection neurons synapses alternatively titled nodes connectionslinks essentially brain directed graph second part project nodejs server displays current state craigs brain thinking lets go quickly understand representing left see big grid squares game looks like right craig see doesnt know squares mean knows air tile different ground tile way squares actually input neuron right side see 4 output neurons buttons craig press also see line going one black squares left grid r neuron labelled 1 synapse input neuron fires left send signal synapse tell craig press r button way craig walks right craig evolves neurons synapses created brain might look something like one ill point couple things first green square left goomba second see another neuron bottom labelled 176 called hidden neuron represents neuron neither input output appear craigs brain added complexity evolves also see time death mario died goomba trying press r b buttons learning mario neat application neural networks neuroevolution serves mostly means demonstrate power selfevolving neural networks reality applications neural networks endless craig learned play simple nes game exact algorithm implemented could also applied robot cleans house works factory even paints beautiful paintings craig cool peek future machines longer need programmed complete specific tasks instead given guidelines teach learn experience tasks expect machines complete become complex becomes less possible hard code tasks need versatile machines work us evolving neural networks step direction youre curious history behind problems encountered neuroevolution highly recommend reading paper algorithm based first section paper covers many different approaches neuroevolution benefits neat genetic algorithm puts every iteration craigs brain test selectively breeds similar way evolution species nature hierarchy follows synapseneuron building blocks craigs brain genome iteration craigs brain essentially collection neurons synapses species collection genomes generation iteration neat algorithm repeated evolve craig first step every generation calculate fitness every individual genome previous generation involves running function genome neat knows successful one craig means running mario level using particular genome brain running level determine fitness genome function fitness every genome calculated move next portion algorithm part algorithm probably least intuitive reason adjusted fitness discourage species growing big population species goes adjusted fitness goes forcing genetic algorithm diversify proper implementation algorithm relatively intensive craigs implementation simplified following important part genome adjusted fitness value associated heres natural selection part comes survival fittest portion determining many genomes survive another generation well many offspring born species algorithms used arent outlined directly paper algorithms created trial error first step determine many species die make room babies done proportionally species adjusted fitness higher adjusted fitness die make room babies second step determine many children born species also proportional adjusted fitness species end two functions species certain number genomes left well baby quota difference number genomes populationsize algorithm necessary allow species left behind sometimes species go completely wrong path theres point keeping around algorithm works simple way species bottom __ entire generation marked extinction species marked extinction __ times row genomes species killed comes fun genetics part species certain number genomes well certain number allotted spots new offspring spots need populated empty population spot needs filled filled either asexual sexual reproduction words offspring result either two genomes species merged mutation single genome species discuss process merging two genomes ill first discuss mutations three kinds mutations happen genome neat follows involves redistribution synapse weights genome either completely redistributed simply perturbed meaning changed slightly 2 mutate add synapse adding synapse means finding two previously unconnected nodes connecting synapse new synapse given random weight 3 mutate add node trickiest mutations adding node need split already existing synapse two synapses add node weight original synapse copied second synapse first synapse given weight 1 one important fact note first synapse bright red picture actually deleted merely disabled means exists genome marked inactive synapses added either mutate add node mutate add synapse given unique id called historical marking used crossover mating algorithm two genomes mate produce offspring algorithm detailed neat paper must followed intuition behind match common ancestor synapses remember weve keeping historical markings take mutations dont match mix match create child child created way undergoes mutation process outlined wont go much detail algorithm youre curious find detailed explanation section 32 original paper see code used implement babies created every species finally progress final stage genetic algorithm respeciation essentially first select candidate genome species genome representative species genomes selected candidates put generic pool reorganized reorganization relies equation called compatibility distance equation equation determines similar different two given genomes wont go gritty details equation works well explain section 33 original paper well craigs code genome different candidate genomes placed species using process genomes generic pool replaced species process completed generation done ready recalculate fitness genomes creating craig meant getting little sleep yhack well worth couple reasons first neat algorithm complex one learning implement complex algorithm without losing complexity exercise code cleanliness despite pressed time hackathon also interesting create algorithm mostly based paper opposed one example code work often meant carefully looking wording used paper determine whether using example one difficult parts project unable test programming essentially wrote code blind able test debug created couple reasons partially time constraints hackathon partially algorithm whole lot interlocking parts meaning needed working state able see algorithm worked overall im happy proud joe able deal stress creating deep complex project scratch short 36 hour period enjoy place well also managed teach craig cool skills like jumping second pipe level 1 quick cheer standing ovation clap show much enjoyed story httpsavasca nikosavasca,en,"['Yale University', 'Super Mario Bros.', 'the Nintendo Entertainment System', 'NES', 'algorithm', 'NEAT']"
195,Dr Ben Medlock,32,Why Turing’s legacy demands a smarter keyboard – Dr Ben Medlock – Medium,"Why Turing’s legacy demands a smarter keyboard
When you start a company, you dream of walking in the footsteps of your heroes. For those working in artificial intelligence, the British computer scientist and father of the field Alan Turing always comes to mind. I thought of him when I did my PhD, when I co-founded an AI keyboard company in 2009, and when we pasted his name on a meeting room door in our first real office.
As a British tech company, today is a big day for SwiftKey. We’ve introduced some of the principles originally conceived of by Turing — artificial neural networks — into our smartphone keyboard for the first time. I want to explain how we managed to do it and how a technology like this, something you may never have heard of before, will help define the smartphone experience of the future. This is my personal take; for the official version check out the SwiftKey blog.
Frustration-free typing on a smartphone relies on complex software to automatically fix typos and predict the words you might want to use. SwiftKey has been at the forefront of this area since 2009, and today our software is used across the world on more than half a billion handsets.
Soon after we launched the first version of our app in 2010, I started to think about using neural networks to power smartphone typing rather than the more traditional n-gram approach (a sophisticated form of word frequency counting). At the time it seemed little more than theoretical, as mobile hardware wasn’t up to the task. However, three years later, the situation began to look more favorable, and in late 2013, our team started working on the idea in earnest.
In order to build a neural network-powered SwiftKey, our engineers were tasked with the enormous challenge of coming up with a solution that would run locally on a smartphone without any perceptible lag. Neural network language models are typically deployed on large servers, requiring huge computational resources. Getting the tech to fit into a handheld mobile device would be no small feat.
After many months of trial, error and lots of experimentation, the team realized they might have found an answer with a combination of two approaches. The first was to make use of the graphical processing unit (GPU) on the phone (utilizing the powerful hardware acceleration designed for rendering complex graphical images) but thanks to some clever programming, they were also able to run the same code on the standard processing unit when the GPU wasn’t available. This combo turned out to be the winning ticket.
So, back to Turing. In 1948 he published a little-known essay called Intelligent Machinery in which he outlined two forms of computing he felt could ultimately lead to machines exhibiting intelligent behavior. The first was a variant of his highly influential “universal Turing machine”, destined to become the foundation for hardware design in all modern digital computers. The second was an idea he called an “unorganized machine”, a type of computer that would use a network of “artificial neurons” to accept inputs and translate them into predicted outputs.
Connecting together many small computing units, each with the ability to receive, modify and pass on basic signals, is inspired by the structure of the human brain. That’s why the appropriation of this concept in software form is called an “artificial neural network”, or a “neural network” for short. The idea is that a collection of artificial neurons are connected together in a specific way (called a “topology”) such that a given set of inputs (what you’ve just typed, for example) can be turned into a useful output (e.g. your most likely next word). The network is then “trained” on millions, or even billions, of data samples and the behavior of the individual neurons is automatically tweaked to achieve the desired overall results.
In the last few years, neural network approaches have facilitated great progress on tough problems such as image recognition and speech processing. Researchers have also begun to demonstrate advances in non-traditional tasks such as automatically generating whole sentence descriptions of images. Such techniques will allow us to better manage the explosion of uncategorized visual data on the web, and will lead to smarter search engines and aids for the visually impaired, among a host of other applications.
The fact that the human brain is so adept at working with language suggests that neural networks, inspired by the brain’s internal structure, are a good bet for the future of smartphone typing. In principle, neural networks also allow us to integrate powerful contextual cues to improve accuracy, for instance a user’s current location and the time of day. These will be stepping stones to more efficient and personal device interactions — the keyboard of the future will provide an experience that feels less like typing and more like working with a close friend or personal assistant.
Applying neural networks to real world problems is part of a wider technology movement that’s changing the face of consumer electronics for good. Devices are getting smarter, more useful and more personal. My goal is that SwiftKey contributes to this revolution. We should all be spending less time fixing typos and more time saying what we mean, when it matters. It’s the legacy we owe to Turing.
The photograph “Alan Turing” by joncallas is licensed under CC BY 2.0.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Technopreneur, @SwiftKey co-founder
",turings legacy demands smarter keyboard start company dream walking footsteps heroes working artificial intelligence british computer scientist father field alan turing always comes mind thought phd cofounded ai keyboard company 2009 pasted name meeting room door first real office british tech company today big day swiftkey weve introduced principles originally conceived turing artificial neural networks smartphone keyboard first time want explain managed technology like something may never heard help define smartphone experience future personal take official version check swiftkey blog frustrationfree typing smartphone relies complex software automatically fix typos predict words might want use swiftkey forefront area since 2009 today software used across world half billion handsets soon launched first version app 2010 started think using neural networks power smartphone typing rather traditional ngram approach sophisticated form word frequency counting time seemed little theoretical mobile hardware wasnt task however three years later situation began look favorable late 2013 team started working idea earnest order build neural networkpowered swiftkey engineers tasked enormous challenge coming solution would run locally smartphone without perceptible lag neural network language models typically deployed large servers requiring huge computational resources getting tech fit handheld mobile device would small feat many months trial error lots experimentation team realized might found answer combination two approaches first make use graphical processing unit gpu phone utilizing powerful hardware acceleration designed rendering complex graphical images thanks clever programming also able run code standard processing unit gpu wasnt available combo turned winning ticket back turing 1948 published littleknown essay called intelligent machinery outlined two forms computing felt could ultimately lead machines exhibiting intelligent behavior first variant highly influential universal turing machine destined become foundation hardware design modern digital computers second idea called unorganized machine type computer would use network artificial neurons accept inputs translate predicted outputs connecting together many small computing units ability receive modify pass basic signals inspired structure human brain thats appropriation concept software form called artificial neural network neural network short idea collection artificial neurons connected together specific way called topology given set inputs youve typed example turned useful output eg likely next word network trained millions even billions data samples behavior individual neurons automatically tweaked achieve desired overall results last years neural network approaches facilitated great progress tough problems image recognition speech processing researchers also begun demonstrate advances nontraditional tasks automatically generating whole sentence descriptions images techniques allow us better manage explosion uncategorized visual data web lead smarter search engines aids visually impaired among host applications fact human brain adept working language suggests neural networks inspired brains internal structure good bet future smartphone typing principle neural networks also allow us integrate powerful contextual cues improve accuracy instance users current location time day stepping stones efficient personal device interactions keyboard future provide experience feels less like typing like working close friend personal assistant applying neural networks real world problems part wider technology movement thats changing face consumer electronics good devices getting smarter useful personal goal swiftkey contributes revolution spending less time fixing typos time saying mean matters legacy owe turing photograph alan turing joncallas licensed cc 20 quick cheer standing ovation clap show much enjoyed story technopreneur swiftkey cofounder,en,"['SwiftKey', 'GPU', 'Intelligent Machinery', 'Technopreneur']"
196,Rohan Kapur,1000,"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","In Rohan’s last post, he talked about evaluating and plugging holes in his knowledge of machine learning thus far. The backpropagation algorithm — the process of training a neural network — was a glaring one for both of us in particular. Together, we embarked on mastering backprop through some great online lectures from professors at MIT & Stanford. After attempting a few programming implementations and hand solutions, we felt equipped to write an article for AYOAI — together.
Today, we’ll do our best to explain backpropagation and neural networks from the beginning. If you have an elementary understanding of differential calculus and perhaps an intuition of what machine learning is, we hope you come out of this blog post with an (acute, but existent nonetheless) understanding of neural networks and how to train them. Let us know if we succeeded!
Let’s start off with a quick introduction to the concept of neural networks. Fundamentally, neural networks are nothing more than really good function approximators — you give a trained network an input vector, it performs a series of operations, and it produces an output vector. To train our network to estimate an unknown function, we give it a collection of data points — which we denote the “training set” — that the network will learn from and generalize on to make future inferences.
Neural networks are structured as a series of layers, each composed of one or more neurons (as depicted above). Each neuron produces an output, or activation, based on the outputs of the previous layer and a set of weights.
When using a neural network to approximate a function, the data is forwarded through the network layer-by-layer until it reaches the final layer. The final layer’s activations are the predictions that the network actually makes.
All this probably seems kind of magical, but it actually works. The key is finding the right set of weights for all of the connections to make the right decisions (this happens in a process known as training) — and that’s what most of this post is going to be about.
When we’re training the network, it’s often convenient to have some metric of how good or bad we’re doing; we call this metric the cost function. Generally speaking, the cost function looks at the function the network has inferred and uses it to estimate values for the data points in our training set. The discrepancies between the outputs in the estimations and the training set data points are the principle values for our cost function. When training our network, the goal will be to get the value of this cost function as low as possible (we’ll see how to do that in just a bit, but for now, just focus on the intuition of what a cost function is and what it’s good for). Generally speaking, the cost function should be more or less convex, like so:
In reality, it’s impossible for any network or cost function to be truly convex. However, as we’ll soon see, local minima may not be a big deal, as long as there is still a general trend for us to follow to get to the bottom. Also, notice that the cost function is parameterized by our network’s weights — we control our loss function by changing the weights.
One last thing to keep in mind about the loss function is that it doesn’t just have to capture how correctly your network estimates — it can specify any objective that needs to be optimized. For example, you generally want to penalize larger weights, as they could lead to overfitting. If this is the case, simply adding a regularization term to your cost function that expresses how big your weights will mean that, in the process of training your network, it will look for a solution that has the best estimates possible while preventing overfitting.
Now, let’s take a look at how we can actually minimize the cost function during the training process to find a set of weights that work the best for our objective.
Now that we’ve developed a metric for “scoring” our network (which we’ll denote as J(W)), we need to find the weights that will make that score as low as possible. If you think back to your pre-calculus days, your first instinct might be to set the derivative of the cost function to zero and solve, which would give us the locations of every minimum/maximum in the function. Unfortunately, there are a few problems with this approach:
Especially as the size of networks begins to scale up, solving for the weights directly becomes increasingly infeasible. Instead, we look at a different class of algorithms, called iterative optimization algorithms, that progressively work their way towards the optimal solution.
The most basic of these algorithms is gradient descent. Recall that our cost function will be essentially convex, and we want to get as close as possible to the global minimum. Instead of solving for it analytically, gradient descent follows the derivatives to essentially “roll” down the slope until it finds its way to the center.
Let’s take the example of a single-weight neural network, whose cost function is depicted below.
We start off by initializing our weight randomly, which puts us at the red dot on the diagram above. Taking the derivative, we see the slope at this point is a pretty big positive number. We want to move closer to the center — so naturally, we should take a pretty big step in the opposite direction of the slope.
If we repeat the process enough, we soon find ourselves nearly at the bottom of our curve and much closer to the optimal weight configuration for our network.
More formally, gradient descent looks something like this:
Let’s dissect. Every time we want to update our weights, we subtract the derivative of the cost function w.r.t. the weight itself, scaled by a learning rate , and — that’s it! You’ll see that as it gets closer and closer to the center, the derivative term gets smaller and smaller, converging to zero as it approaches the solution. The same process applies with networks that have tens, hundreds, thousands, or more parameters — compute the gradient of the cost function w.r.t. each of the weights, and update each of your weights accordingly.
I do want to say a few more words on the learning rate, because it’s one of the more important hyperparameters (“settings” for your neural network) that you have control over. If the learning rate is too high, it could jump too far in the other direction, and you never get to the minimum you’re searching for. Set it too low, and your network will take ages to find the right weights, or it will get stuck in a local minimum. There’s no “magic number” to use when it comes to a learning rate, and it’s usually best to try several and pick the one that works the best for your individual network and dataset. In practice, many choose to anneal the learning rate over time — it starts out high, because it’s furthest from the solution, and decays as it gets closer.
But as it turns out, gradient descent is kind of slow. Really slow, actually. Earlier I used the analogy of the weights “rolling” down the gradient to get to the bottom, but that doesn’t actually make any sense — it should pick up speed as it gets to the bottom, not slow down! Another iterative optimization algorithm, known as momentum, does just that. As the weights begin to “roll” down the slope, they pick up speed. When they get closer to the solution, the momentum that they picked up carries them closer to the optima while gradient descent would simply stop. As a result, training with momentum updates is both faster and can provide better results.
Here’s what the update rule looks like for momentum:
As we train, we accumulate a “velocity” value V. At each training step, we update V with the gradient at the current position (once again scaled by the learning rate). Also notice that, with each time step, we decay velocity V by a factor mu (usually somewhere around .9), so that over time we lose momentum instead of bouncing around by the minimum forever. We then update our weight in the direction of the velocity, and repeat the process again. Over the first few training iterations, V will grow as our weights “pick up speed” and take successively bigger leaps. As we approach the minimum, our velocity stops accumulating as quickly, and eventually begins to decay, until we’ve essentially reached the minimum. An important thing to note is that we accumulate a velocity independently for each weight — just because one weight is changing particularly clearly doesn’t mean any of the other weights need to be.
There are lots of other iterative optimization algorithms that are commonly used with neural networks, but I won’t go into all of them here (if you’re curious, some of the more popular ones include Adagrad and Adam). The basic principle remains the same throughout — gradually update the weights to get them closer to the minimum. But regardless of which optimization algorithm you use, we still need to be able to compute the gradient of the cost function w.r.t. each weight. But our cost function isn’t a simple parabola anymore — it’s a complicated, many-dimensional function with countless local optima that we need to watch out for. That’s where backpropagation comes in.
The backpropagation algorithm was a major milestone in machine learning because, before it was discovered, optimization methods were extremely unsatisfactory. One popular method was to perturb (adjust) the weights in a random, uninformed direction (ie. increase or decrease) and see if the performance of the ANN increased. If it did not, one would attempt to either a) go in the other direction b) reduce the perturbation size or c) a combination of both. Another attempt was to use Genetic Algorithms (which became popular in AI at the same time) to evolve a high-performance neural network. In both cases, without (analytically) being informed on the correct direction, results and efficiency were suboptimal. This is where the backpropagation algorithm comes into play.
Recall that, for any given supervised machine learning problem, we (aim to) select weights that provide the optimal estimation of a function that models our training data. In other words, we want to find a set of weights W that minimizes on the output of J(W). We discussed the gradient descent algorithm — one where we update each weight by some negative, scalar reduction of the error derivative with respect to that weight. If we do choose to use gradient descent (or almost any other convex optimization algorithm), we need to find said derivatives in numerical form.
For other machine learning algorithms like logistic regression or linear regression, computing the derivatives is an elementary application of differentiation. This is because the outputs of these models are just the inputs multiplied by some chosen weights, and at most fed through a single activation function (the sigmoid function in logistic regression). The same, however, cannot be said for neural networks. To demonstrate this, here is a diagram of a double-layered neural network:
As you can see, each neuron is a function of the previous one connected to it. In other words, if one were to change the value of w1, both “hidden 1” and “hidden 2” (and ultimately the output) neurons would change. Because of this notion of functional dependencies, we can mathematically formulate the output as an extensive composite function:
And thus:
Here, the output is a composite function of the weights, inputs, and activation function(s). It is important to realize that the hidden units/nodes are simply intermediary computations that, in actuality, can be reduced down to computations of the input layer.
If we were to then take the derivative of said function with respect to some arbitrary weight (for example w1), we would iteratively apply the chain rule (which I’m sure you all remember from your calculus classes). The result would look similar to the following:
Now, let’s attach a black box to the tail of our neural network. This black box will compute and return the error — using the cost function — from our output:
All we’ve done is add another functional dependency; our error is now a function of the output and hence a function of the input, weights, and activation function. If we were to compute the derivative of the error with any arbitrary weight (again, we’ll choose w1), the result would be:
Each of these derivatives can be simplified once we choose an activation and error function, such that the entire result would represent a numerical value. At that point, any abstraction has been removed, and the error derivative can be used in gradient descent (as discussed earlier) to iteratively improve upon the weight. We compute the error derivatives w.r.t. every other weight in the network and apply gradient descent in the same way. This is backpropagation — simply the computation of derivatives that are fed to a convex optimization algorithm. We call it “backpropagation” because it almost seems as if we are traversing from the output error to the weights, taking iterative steps using chain the rule until we “reach” our weight.
When I first truly understood the backprop algorithm (just a couple of weeks ago), I was taken aback by how simple it was. Sure, the actual arithmetic/computations can be difficult, but this process is handled by our computers. In reality, backpropagation is just a rather tedious (but again, for a generalized implementation, computers will handle this) application of the chain rule. Since neural networks are convoluted multilayer machine learning model structures (at least relative to other ones), each weight “contributes” to the overall error in a more complex manner, and hence the actual derivatives require a lot of effort to produce. However, once we get past the calculus, backpropagation of neural nets is equivalent to typical gradient descent for logistic/linear regression.
Thus far, I’ve walked through a very abstract form of backprop for a simple neural network. However, it is unlikely that you will ever use a single-layered ANN in applications. So, now, let’s make our black boxes — the activation and error functions — more concrete such that we can perform backprop on a multilayer neural net.
Recall that our error function J(W) will compute the “error” of our neural network based on the output predictions it produces vs. the correct a priori outputs we know in our training set. More formally, if we denote our predicted output estimations as vector p, and our actual output as vector a, then we can use:
This is just one example of a possible cost function (the log-likelihood is also a popular one), and we use it because of its mathematical convenience (this is a notion one will frequently encounter in machine learning): the squared expression exaggerates poor solutions and ensures each discrepancy is positive. It will soon become clear why we multiply the expression by half.
The derivative of the error w.r.t. the output was the first term in the error w.r.t. weight derivative expression we formulated earlier. Let’s now compute it!
Our result is simply our predictions take away our actual outputs.
Now, let’s move on to the activation function. The activation function used depends on the context of the neural network. If we aren’t in a classification context, ReLU (Rectified Linear Unit, which is zero if input is negative, and the identity function when the input is positive) is commonly used today.
If we’re in a classification context (that is, predicting on a discrete state with a probability ie. if an email is spam), we can use the sigmoid or tanh (hyperbolic tangent) function such that we can “squeeze” any value into the range 0 to 1. These are used instead of a typical step function because their “smoothness” properties allows for the derivatives to be non-zero. The derivative of the step function before and after the origin is zero. This will pose issues when we try to update our weights (nothing much will happen!).
Now, let’s say we’re in a classification context and we choose to use the sigmoid function, which is of the following equation:
As per usual, we’ll compute the derivative using differentiation rules as:
EDIT: On the 2nd line, the denominator should be raised to +2, not -2. Thanks to a reader for pointing this out.
Sidenote: ReLU activation functions are also commonly used in classification contexts. There are downsides to using the sigmoid function — particularly the “vanishing gradient” problem — which you can read more about here.
The sigmoid function is mathematically convenient (there it is again!) because we can represent its derivative in terms of the output of the function. Isn’t that cool‽
We are now in a good place to perform backpropagation on a multilayer neural network. Let me introduce you to the net we are going to work with:
This net is still not as complex as one you may use in your programming, but its architecture allows us to nevertheless get a good grasp on backprop. In this net, we have 3 input neurons and one output neuron. There are four layers in total: one input, one output, and two hidden layers. There are 3 neurons in each hidden layer, too (which, by the way, need not be the case). The network is fully connected; there are no missing connections. Each neuron/node (save the inputs, which are usually pre-processed anyways) is an activity; it is the weighted sum of the previous neurons’ activities applied to the sigmoid activation function.
To perform backprop by hand, we need to introduce the different variables/states at each point (layer-wise) in the neural network:
It is important to note that every variable you see here is a generalization on the entire layer at that point. For example, when I say x_i, I am referring to the input to any input neuron (arbitrary value of i). I chose to place it in the middle of the layer for visibility purposes, but that does not mean that x_i refers to the middle neuron. I’ll demonstrate and discuss the implications of this later on.
x refers to the input layer, y refers to hidden layer 1, z refers to hidden layer 2, and p refers to the prediction/output layer (which fits in nicely with the notation used in our cost function). If a variable has the subscript i, it means that the variable is the input to the relevant neuron at that layer. If a variable has the subscript j, it means that the variable is the output of the relevant neuron at that layer. For example, x_i refers to any input value we enter into the network. x_j is actually equal to x_i, but this is only because we choose not to use an activation function — or rather, we use the identity activation function — in the input layer’s activities. We only include these two separate variables to retain consistency. y_i is the input to any neuron in the first hidden layer; it is the weighted sum of all previous neurons (each neuron in the input layer multiplied by the corresponding connecting weights). y_j is the output of any neuron at the hidden layer, so it is equal to activation_function(y_i) = sigmoid(y_i) = sigmoid(weighted_sum_of_x_j). We can apply the same logic for z and p. Ultimately, p_j is the sigmoid output of p_i and hence is the output of the entire neural network that we pass to the error/cost function.
The weights are organized into three separate variables: W1, W2, and W3. Each W is a matrix (if you are not comfortable with Linear Algebra, think of a 2D array) of all the weights at the given layer. For example, W1 are the weights that connect the input layer to the hidden layer 1. Wlayer_ij refers to any arbitrary, single weight at a given layer. To get an intuition of ij (which is really i, j), Wlayer_i are all the weights that connect arbitrary neuron i at a given layer to the next layer. Wlayer_ij (adding the j component) is the weight that connects arbitrary neuron i at a given layer to an arbitrary neuron j at the next layer. Essentially, Wlayer is a vector of Wlayer_is, which is a vector of real-valued Wlayer_ijs.
NOTE: Please note that the i’s and j’s in the weights and other variables are completely different. These indices do not correspond in any way. In fact, for x/y/z/p, i and j do not represent tensor indices at all, they simply represent the input and output of a neuron. Wlayer_ij represents an arbitrary weight at an index in a weight matrix, and x_j/y_j/z_j/p_j represent an arbitrary input/output point of a neuron unit.
That last part about weights was tedious! It’s crucial to understand how we’re separating the neural network here, especially the notion of generalizing on an entire layer, before moving forward.
To acquire a comprehensive intuition of backpropagation, we’re going to backprop this neural net as discussed before. More specifically, we’re going to find the derivative of the error w.r.t. an arbitrary weight in the input layer (W1_ij). We could find the derivative of the error w.r.t. an arbitrary weight in the first or second hidden layer, but let’s go as far back as we can; the more backprop, the better!
So, mathematically, we are trying to obtain (to perform our iterative optimization algorithm with):
We can express this graphically/visually, using the same principles as earlier (chain rule), like so:
In two layers, we have three red lines pointing in three different directions, instead of just one. This is a reinforcement of (and why it is important to understand) the fact that variable j is just a generalization/represents any point in the layer. So, when we differentiate p_i with respect to the layer before that, there are three different weights, as I hope you can see, in W3_ij that contribute to the value p_i. There also happen to be three weights in W3 in total, but this isn’t the case for the layers before; it is only the case because layer p has one neuron — the output — in it. We stop backprop at the input layer and so we just point to the single weight we are looking for.
Wonderful! Now let’s work out all this great stuff mathematically. Immediately, we know:
We have already established the left hand side, so now we just need to use the chain rule to simplify it further. The derivative of the error w.r.t. the weight can be written as the derivative of the error w.r.t. the output prediction multiplied by the derivative of the output prediction w.r.t. the weight. At this point, we’ve traversed one red line back. We know this because
is reducible to a numerical value. Specifically, the derivative of the error w.r.t. the output prediction is:
Hence:
Going one more layer backwards, we can determine that:
In other words, the derivative of the output prediction w.r.t. the weight is the derivative of the output w.r.t. the input to the output layer (p_i) multiplied by the derivative of that value w.r.t. the weight. This represents our second red line. We can solve the first term like so:
This corresponds with the derivative of the sigmoid function we solved earlier, which was equal to the output multiplied by one minus the output. In this case, p_j is the output of the sigmoid function. Now, we have:
Let’s move on to the third red line(s). This one is interesting because we begin to “spread” out. Since there are multiple different weights that contribute to the value of p_i, we need to take into account their individual “pull” factors into our derivative:
If you’re a mathematician, this notation may irk you slightly; sorry if that’s the case! In computer science, we tend to stray from the notion of completely legal mathematical expressions. This is yet again again another reason why it’s key to understand the role of layer generalization; z_j here is not just referring to the middle neuron, it’s referring to an arbitrary neuron. The actual value of j in the summation is not changing (it’s not even an index or a value in the first place), and we don’t really consider it. It’s less of a mathematical expression and more of a statement that we will iterate through each generalized neuron z_j and use it. In other words, we iterate over the derivative terms and sum them up using z_1, z_2, and z_3. Before, we could write p_j as any single value because the output layer just contains one node; there is just one p_j. But we see here that this is no longer the case. We have multiple z_j values, and p_i is functionally dependent on each of these z_j values. So, when we traverse from p_j to the preceding layer, we need to consider each contribution from layer z to p_j separately and add them up to create one total contribution. There’s no upper bound to the summation; we just assume that we start at zero and end at our maximum value for the number of neurons in the layer. Please again note that the same changes are not reflected in W1_ij, where j refers to an entirely different thing. Instead, we’re just stating that we will use the different z_j neurons in layer z.
Since p_i is a summation of each weight multiplied by each z_j (weighted sum), if we were to take the derivative of p_i with any arbitrary z_j, the result would be the connecting weight since said weight would be the coefficient of the term (derivative of m*x w.r.t. x is just m):
W3_ij is loosely defined here. ij still refers to any arbitrary weight — where ij are still separate from the j used in p_i/z_j — but again, as computer scientists and not mathematicians, we need not be pedantic about the legality and intricacy of expressions; we just need an intuition of what the expressions imply/mean. It’s almost a succinct form of psuedo-code! So, even though this defines an arbitrary weight, we know it means the connecting weight. We can also see this from the diagram: when we walk from p_j to an arbitrary z_j, we walk along the connecting weight. So now, we have:
At this point, I like to continue playing the “reduction test”. The reduction test states that, if we can further simplify a derivative term, we still have more backprop to do. Since we can’t yet quite put the derivative of z_j w.r.t. W1_ij into a numerical term, let’s keep going (and fast-forward a bit). Using chain rule, we follow the fourth line back to determine that:
Since z_j is the sigmoid of z_i, we use the same logic as the previous layer and apply the sigmoid derivative. The derivative of z_i w.r.t. W1_ij, demonstrated by the fifth line(s) back, requires the same idea of “spreading out” and summation of contributions:
Briefly, since z_i is the weighted sum of each y_j in y, we sum over the derivatives which, similar to before, simplifies to the relevant connecting weights in the preceding layer (W2 in this case).
We’re almost there, let’s go further; there’s still more reduction to do:
We have, of course, another sigmoid activation function to deal with. This is the sixth red line. Notice, now, that we have just one line remaining. In fact, our last derivative term here passes (or rather, fails) the reduction test! The last line traverses from the input at y_i to x_j, walking along W1_ij. Wait a second — is this not what we are attempting to backprop to? Yes, it is! Since we are, for the first time, directly deriving y_i w.r.t. the weight W1_ij, we can think of the coefficient of W1_ij as being x_j in our weighted sum (instead of the vice versa as used previously). Hence, the simplification follows:
Of course, since each x_j in layer x contributes to the weighted sum y_i, we sum over the effects. And that’s it! We can’t reduce any further from here. Now, let’s tie all these individual expressions together:
EDIT: The denominator on the left hand side should say dW1ij instead of “layer”.
With no more partial derivative terms left, our work is complete! This gives us the derivative of the error w.r.t. any arbitrary weight in the input layer/W1. That was a lot of work — maybe now we can sympathize with the poor computers!
Something you should notice is that values such as p_j, a, z_j, y_j, x_j etc. are the values of the network at the different points. But where do they come from? Actually, we would need to perform a feed-forward of the neural network first and then capture these variables.
Our task is to now perform Gradient Descent to train the neural net:
We perform gradient descent on each weight in each layer. Notice that the resulting gradient should change each time because the weight itself changes, (and as a result, the performance and output of the entire net should change) even if it’s a small perturbation. This means that, at each update, we need to do a feed-forward of the neural net. Not just once before, but once each iteration.
These are then the steps to train an entire neural network:
It’s important to note that one must not initialize the weights to zero, similar to what may be done in other machine learning algorithms. If weights are initialized to zero, after each update, the outgoing weights of each neuron will be identical, because the gradients will be identical (which can be proved). Because of this, the proceeding hidden units will remain the same value and will continue to follow each other. Ultimately, this means that our training will become extremely constrained (due to the “symmetry”), and we won’t be able to build interesting functions. Also, neural networks may get stuck at local optima (places where the gradient is zero but are not the global minima), so random weight initialization allows one to hopefully have a chance of circumventing that by starting at many different random values.
3. Perform one feed-forward using the training data
4. Perform backpropagation to get the error derivatives w.r.t. each and every weight in the neural network
5. Perform gradient descent to update each weight by the negative scalar reduction (w.r.t. some learning rate alpha) of the respective error derivative. Increment the number of iterations.
6. If we have converged (in reality, though, we just stop when we have reached the number of maximum iterations) training is complete. Else, repeat starting at step 3.
If we initialize our weights randomly (and not to zero) and then perform gradient descent with derivatives computed from backpropagation, we should expect to train a neural network in no time! I hope this example brought clarity to how backprop works and the intuition behind it. If you didn’t understand the intricacies of the example but understand and appreciate the concept of backprop as a whole, you’re still in a great place! Next we’ll go ahead and explain backprop code that works on any generalized architecture of a neural network using the ReLU activation function.
Now that we’ve developed the math and intuition behind backpropagation, let’s try to implement it. We’ll divide our implementation into three distinct steps:
Let’s start off by defining what the API we’re implementing looks like. We’ll define our network as a series of Layer instances that our data passes through — this means that instead of modeling each individual neuron, we group neurons from a single layer together. This makes it a bit easier to reason about larger networks, but also makes the actual computations faster (as we’ll see shortly). Also — we’re going to write the code in Python.
Each layer will have the following API:
(This isn’t great API design — ideally, we would decouple the backprop and weight update from the rest of the object, so the specific algorithm we use for updating weights isn’t tied to the layer itself. But that’s not the point, so we’ll stick with this design for the purposes of explaining how backpropagation works in a real-life scenario. Also: we’ll be using numpy throughout the implementation. It’s an awesome tool for mathematical operations in Python (especially tensor based ones), but we don’t have the time to get into how it works — if you want a good introduction, here ya’ go.)
We can start by implementing the weight initialization. As it turns out, how you initialize your weights is actually kind of a big deal for both network performance and convergence rates. Here’s how we’ll initialize our weights:
This initializes a weight matrix of the appropriate dimensions with random values sampled from a normal distribution. We then scale it rad(2/self.size_in), giving us a variance of 2/self.size_in (derivation here).
And that’s all we need for layer initialization! Let’s move on to implementing our first objective — feed-forward. This is actually pretty simple — a dot product of our input activations with the weight matrix, followed by our activation function, will give us the activations we need. The dot product part should make intuitive sense; if it doesn’t, you should sit down and try to work through it on a piece of paper. This is where the performance gains of grouping neurons into layers comes from: instead of keeping an individual weight vector for each neuron, and performing a series of vector dot products, we can just do a single matrix operation (which, thanks to the wonders of modern processors, is significantly faster). In fact, we can compute all of the activations from a layer in just two lines:
Simple enough. Let’s move on to backpropagation.
This one’s a bit more involved. First, we compute the derivative of the output w.r.t. the weights, then the derivative of the cost w.r.t. the output, followed by chain rule to get the derivative of the cost w.r.t. the weights.
Let’s start with the first part — the derivative of the output w.r.t. the weights. That should be simple enough; because you’re multiplying the weight by the corresponding input activation, the derivative will just be the corresponding input activation.
Except, because we’re using the ReLU activation function, the weights have no effect if the corresponding output is < 0 (because it gets capped anyway). This should take care of that hiccup:
(More formally, you’re multiplying by the derivative of the activation function, which is 0 when the activation is < 0 and 1 elsewhere.)
Let’s take a brief detour to talk about the out_grad parameter that our backward method gets. Let’s say we have a network with two layers: the first has m neurons, and the second has n. Each of the m neurons produces an activation, and each of the n neurons looks at each of the m activations. The out_grad parameter is an m x n matrix of how each m affects each of the n neurons it feeds into.
Now, we need the derivative of the cost w.r.t. each of the outputs — which is essentially the out_grad parameter we’re given! We just need to sum up each row of the matrix we’re given, as per the backpropagation formula.
Finally, we end up with something like this:
Now, we need to compute the derivative of our inputs to pass along to the next layer. We can perform a similar chain rule — derivative of the output w.r.t. the inputs times the derivative of the cost w.r.t. the outputs.
And that’s it for the backpropagation step.
The final step is the weight update. Assuming we’re sticking with gradient descent for this example, this can be a simple one-liner:
To actually train our network, we take one of our training samples and call forward on each layer consecutively, passing the output of the previous layer as the input of the following layer. We compute dJ, passing that as the out_grad parameter to the last layer’s backward method. We call backward on each of the layers in reverse order, this time passing the output of the further layer as out_grad to the previous layer. Finally, we call update on each of our layers and repeat.
There’s one last detail that we should include, which is the concept of a bias (akin to that of a constant term in any given equation). Notice that, with our current implementation, the activation of a neuron is determined solely based on the activations of the previous layer. There’s no bias term that can shift the activation up or down independent of the inputs. A bias term isn’t strictly necessary — in fact, if you train your network as-is, it would probably still work fine. But if you do need a bias term, the code stays almost the same — the only difference is that you need to add a column of 1s to the incoming activations, and update your weight matrix accordingly, so one of your weights gets treated as a bias term. The only other difference is that, when returning cost_wrt_inputs, you can cut out the first row — nobody cares about the gradients associated with the bias term because the previous layer has no say in the activation of the bias neuron.
Implementing backpropagation can be kind of tricky, so it’s often a good idea to check your implementation. You can do so by computing the gradient numerically (by literally perturbing the weight and calculating the difference in your cost function) and comparing it to your backpropagation-computed gradient. This gradient check doesn’t need to be run once you’ve verified your implementation, but it could save a lot of time tracking down potential problems with your network.
Nowadays, you often don’t even need to implement a neural network on your own, as libraries such as Caffe, Torch, or TensorFlow will have implementations ready to go. That being said, it’s often a good idea to try implementing it on your own to get a better grasp of how everything works under the hood.
Intrigued? Looking to learn more about neural networks? Here are some great online classes to get you started:
Stanford’s CS231n. Although it’s technically about convolutional neural networks, the class provides an excellent introduction to and survey of neural networks in general. Class videos, notes, and assignments are all posted here, and if you have the patience for it I would strongly recommend walking through the assignments so you can really get to know what you’re learning.
MIT 6.034. This class, taught by Prof. Patrick Henry Winston, explores many different algorithms and disciplines in Artificial Intelligence. There’s a great lecture on backprop that I actually used as a stepping stone to getting setup writing this article. I also learned genetic algorithms from Prof. Winston — he’s a great teacher!
We hope that, if you visited this article without knowing how the backpropagation algorithm works, you are reading this with an (at least rudimentary) mathematical or conceptual intuition of it. Writing and conveying such a complex algorithm to a supposed beginner has proven to be an extremely difficult task for us, but it’s helped us truly understand what we’ve been learning about. With greater knowledge in a fundamental area of machine learning, we are now excited to take a look at new, interesting algorithms and disciplines in the field. We are looking forward to continue documenting these endeavors together.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
rohankapur.com
Our ongoing effort to make the mathematics, science, linguistics, and philosophy of artificial intelligence fun and simple.
",rohans last post talked evaluating plugging holes knowledge machine learning thus far backpropagation algorithm process training neural network glaring one us particular together embarked mastering backprop great online lectures professors mit stanford attempting programming implementations hand solutions felt equipped write article ayoai together today well best explain backpropagation neural networks beginning elementary understanding differential calculus perhaps intuition machine learning hope come blog post acute existent nonetheless understanding neural networks train let us know succeeded lets start quick introduction concept neural networks fundamentally neural networks nothing really good function approximators give trained network input vector performs series operations produces output vector train network estimate unknown function give collection data points denote training set network learn generalize make future inferences neural networks structured series layers composed one neurons depicted neuron produces output activation based outputs previous layer set weights using neural network approximate function data forwarded network layerbylayer reaches final layer final layers activations predictions network actually makes probably seems kind magical actually works key finding right set weights connections make right decisions happens process known training thats post going training network often convenient metric good bad call metric cost function generally speaking cost function looks function network inferred uses estimate values data points training set discrepancies outputs estimations training set data points principle values cost function training network goal get value cost function low possible well see bit focus intuition cost function good generally speaking cost function less convex like reality impossible network cost function truly convex however well soon see local minima may big deal long still general trend us follow get bottom also notice cost function parameterized networks weights control loss function changing weights one last thing keep mind loss function doesnt capture correctly network estimates specify objective needs optimized example generally want penalize larger weights could lead overfitting case simply adding regularization term cost function expresses big weights mean process training network look solution best estimates possible preventing overfitting lets take look actually minimize cost function training process find set weights work best objective weve developed metric scoring network well denote jw need find weights make score low possible think back precalculus days first instinct might set derivative cost function zero solve would give us locations every minimummaximum function unfortunately problems approach especially size networks begins scale solving weights directly becomes increasingly infeasible instead look different class algorithms called iterative optimization algorithms progressively work way towards optimal solution basic algorithms gradient descent recall cost function essentially convex want get close possible global minimum instead solving analytically gradient descent follows derivatives essentially roll slope finds way center lets take example singleweight neural network whose cost function depicted start initializing weight randomly puts us red dot diagram taking derivative see slope point pretty big positive number want move closer center naturally take pretty big step opposite direction slope repeat process enough soon find nearly bottom curve much closer optimal weight configuration network formally gradient descent looks something like lets dissect every time want update weights subtract derivative cost function wrt weight scaled learning rate thats youll see gets closer closer center derivative term gets smaller smaller converging zero approaches solution process applies networks tens hundreds thousands parameters compute gradient cost function wrt weights update weights accordingly want say words learning rate one important hyperparameters settings neural network control learning rate high could jump far direction never get minimum youre searching set low network take ages find right weights get stuck local minimum theres magic number use comes learning rate usually best try several pick one works best individual network dataset practice many choose anneal learning rate time starts high furthest solution decays gets closer turns gradient descent kind slow really slow actually earlier used analogy weights rolling gradient get bottom doesnt actually make sense pick speed gets bottom slow another iterative optimization algorithm known momentum weights begin roll slope pick speed get closer solution momentum picked carries closer optima gradient descent would simply stop result training momentum updates faster provide better results heres update rule looks like momentum train accumulate velocity value v training step update v gradient current position scaled learning rate also notice time step decay velocity v factor mu usually somewhere around 9 time lose momentum instead bouncing around minimum forever update weight direction velocity repeat process first training iterations v grow weights pick speed take successively bigger leaps approach minimum velocity stops accumulating quickly eventually begins decay weve essentially reached minimum important thing note accumulate velocity independently weight one weight changing particularly clearly doesnt mean weights need lots iterative optimization algorithms commonly used neural networks wont go youre curious popular ones include adagrad adam basic principle remains throughout gradually update weights get closer minimum regardless optimization algorithm use still need able compute gradient cost function wrt weight cost function isnt simple parabola anymore complicated manydimensional function countless local optima need watch thats backpropagation comes backpropagation algorithm major milestone machine learning discovered optimization methods extremely unsatisfactory one popular method perturb adjust weights random uninformed direction ie increase decrease see performance ann increased one would attempt either go direction b reduce perturbation size c combination another attempt use genetic algorithms became popular ai time evolve highperformance neural network cases without analytically informed correct direction results efficiency suboptimal backpropagation algorithm comes play recall given supervised machine learning problem aim select weights provide optimal estimation function models training data words want find set weights w minimizes output jw discussed gradient descent algorithm one update weight negative scalar reduction error derivative respect weight choose use gradient descent almost convex optimization algorithm need find said derivatives numerical form machine learning algorithms like logistic regression linear regression computing derivatives elementary application differentiation outputs models inputs multiplied chosen weights fed single activation function sigmoid function logistic regression however cannot said neural networks demonstrate diagram doublelayered neural network see neuron function previous one connected words one change value w1 hidden 1 hidden 2 ultimately output neurons would change notion functional dependencies mathematically formulate output extensive composite function thus output composite function weights inputs activation functions important realize hidden unitsnodes simply intermediary computations actuality reduced computations input layer take derivative said function respect arbitrary weight example w1 would iteratively apply chain rule im sure remember calculus classes result would look similar following lets attach black box tail neural network black box compute return error using cost function output weve done add another functional dependency error function output hence function input weights activation function compute derivative error arbitrary weight well choose w1 result would derivatives simplified choose activation error function entire result would represent numerical value point abstraction removed error derivative used gradient descent discussed earlier iteratively improve upon weight compute error derivatives wrt every weight network apply gradient descent way backpropagation simply computation derivatives fed convex optimization algorithm call backpropagation almost seems traversing output error weights taking iterative steps using chain rule reach weight first truly understood backprop algorithm couple weeks ago taken aback simple sure actual arithmeticcomputations difficult process handled computers reality backpropagation rather tedious generalized implementation computers handle application chain rule since neural networks convoluted multilayer machine learning model structures least relative ones weight contributes overall error complex manner hence actual derivatives require lot effort produce however get past calculus backpropagation neural nets equivalent typical gradient descent logisticlinear regression thus far ive walked abstract form backprop simple neural network however unlikely ever use singlelayered ann applications lets make black boxes activation error functions concrete perform backprop multilayer neural net recall error function jw compute error neural network based output predictions produces vs correct priori outputs know training set formally denote predicted output estimations vector p actual output vector use one example possible cost function loglikelihood also popular one use mathematical convenience notion one frequently encounter machine learning squared expression exaggerates poor solutions ensures discrepancy positive soon become clear multiply expression half derivative error wrt output first term error wrt weight derivative expression formulated earlier lets compute result simply predictions take away actual outputs lets move activation function activation function used depends context neural network arent classification context relu rectified linear unit zero input negative identity function input positive commonly used today classification context predicting discrete state probability ie email spam use sigmoid tanh hyperbolic tangent function squeeze value range 0 1 used instead typical step function smoothness properties allows derivatives nonzero derivative step function origin zero pose issues try update weights nothing much happen lets say classification context choose use sigmoid function following equation per usual well compute derivative using differentiation rules edit 2nd line denominator raised 2 2 thanks reader pointing sidenote relu activation functions also commonly used classification contexts downsides using sigmoid function particularly vanishing gradient problem read sigmoid function mathematically convenient represent derivative terms output function isnt cool good place perform backpropagation multilayer neural network let introduce net going work net still complex one may use programming architecture allows us nevertheless get good grasp backprop net 3 input neurons one output neuron four layers total one input one output two hidden layers 3 neurons hidden layer way need case network fully connected missing connections neuronnode save inputs usually preprocessed anyways activity weighted sum previous neurons activities applied sigmoid activation function perform backprop hand need introduce different variablesstates point layerwise neural network important note every variable see generalization entire layer point example say x_i referring input input neuron arbitrary value chose place middle layer visibility purposes mean x_i refers middle neuron ill demonstrate discuss implications later x refers input layer refers hidden layer 1 z refers hidden layer 2 p refers predictionoutput layer fits nicely notation used cost function variable subscript means variable input relevant neuron layer variable subscript j means variable output relevant neuron layer example x_i refers input value enter network x_j actually equal x_i choose use activation function rather use identity activation function input layers activities include two separate variables retain consistency y_i input neuron first hidden layer weighted sum previous neurons neuron input layer multiplied corresponding connecting weights y_j output neuron hidden layer equal activation_functiony_i sigmoidy_i sigmoidweighted_sum_of_x_j apply logic z p ultimately p_j sigmoid output p_i hence output entire neural network pass errorcost function weights organized three separate variables w1 w2 w3 w matrix comfortable linear algebra think 2d array weights given layer example w1 weights connect input layer hidden layer 1 wlayer_ij refers arbitrary single weight given layer get intuition ij really j wlayer_i weights connect arbitrary neuron given layer next layer wlayer_ij adding j component weight connects arbitrary neuron given layer arbitrary neuron j next layer essentially wlayer vector wlayer_is vector realvalued wlayer_ijs note please note js weights variables completely different indices correspond way fact xyzp j represent tensor indices simply represent input output neuron wlayer_ij represents arbitrary weight index weight matrix x_jy_jz_jp_j represent arbitrary inputoutput point neuron unit last part weights tedious crucial understand separating neural network especially notion generalizing entire layer moving forward acquire comprehensive intuition backpropagation going backprop neural net discussed specifically going find derivative error wrt arbitrary weight input layer w1_ij could find derivative error wrt arbitrary weight first second hidden layer lets go far back backprop better mathematically trying obtain perform iterative optimization algorithm express graphicallyvisually using principles earlier chain rule like two layers three red lines pointing three different directions instead one reinforcement important understand fact variable j generalizationrepresents point layer differentiate p_i respect layer three different weights hope see w3_ij contribute value p_i also happen three weights w3 total isnt case layers case layer p one neuron output stop backprop input layer point single weight looking wonderful lets work great stuff mathematically immediately know already established left hand side need use chain rule simplify derivative error wrt weight written derivative error wrt output prediction multiplied derivative output prediction wrt weight point weve traversed one red line back know reducible numerical value specifically derivative error wrt output prediction hence going one layer backwards determine words derivative output prediction wrt weight derivative output wrt input output layer p_i multiplied derivative value wrt weight represents second red line solve first term like corresponds derivative sigmoid function solved earlier equal output multiplied one minus output case p_j output sigmoid function lets move third red lines one interesting begin spread since multiple different weights contribute value p_i need take account individual pull factors derivative youre mathematician notation may irk slightly sorry thats case computer science tend stray notion completely legal mathematical expressions yet another reason key understand role layer generalization z_j referring middle neuron referring arbitrary neuron actual value j summation changing even index value first place dont really consider less mathematical expression statement iterate generalized neuron z_j use words iterate derivative terms sum using z_1 z_2 z_3 could write p_j single value output layer contains one node one p_j see longer case multiple z_j values p_i functionally dependent z_j values traverse p_j preceding layer need consider contribution layer z p_j separately add create one total contribution theres upper bound summation assume start zero end maximum value number neurons layer please note changes reflected w1_ij j refers entirely different thing instead stating use different z_j neurons layer z since p_i summation weight multiplied z_j weighted sum take derivative p_i arbitrary z_j result would connecting weight since said weight would coefficient term derivative mx wrt x w3_ij loosely defined ij still refers arbitrary weight ij still separate j used p_iz_j computer scientists mathematicians need pedantic legality intricacy expressions need intuition expressions implymean almost succinct form psuedocode even though defines arbitrary weight know means connecting weight also see diagram walk p_j arbitrary z_j walk along connecting weight point like continue playing reduction test reduction test states simplify derivative term still backprop since cant yet quite put derivative z_j wrt w1_ij numerical term lets keep going fastforward bit using chain rule follow fourth line back determine since z_j sigmoid z_i use logic previous layer apply sigmoid derivative derivative z_i wrt w1_ij demonstrated fifth lines back requires idea spreading summation contributions briefly since z_i weighted sum y_j sum derivatives similar simplifies relevant connecting weights preceding layer w2 case almost lets go theres still reduction course another sigmoid activation function deal sixth red line notice one line remaining fact last derivative term passes rather fails reduction test last line traverses input y_i x_j walking along w1_ij wait second attempting backprop yes since first time directly deriving y_i wrt weight w1_ij think coefficient w1_ij x_j weighted sum instead vice versa used previously hence simplification follows course since x_j layer x contributes weighted sum y_i sum effects thats cant reduce lets tie individual expressions together edit denominator left hand side say dw1ij instead layer partial derivative terms left work complete gives us derivative error wrt arbitrary weight input layerw1 lot work maybe sympathize poor computers something notice values p_j z_j y_j x_j etc values network different points come actually would need perform feedforward neural network first capture variables task perform gradient descent train neural net perform gradient descent weight layer notice resulting gradient change time weight changes result performance output entire net change even small perturbation means update need feedforward neural net iteration steps train entire neural network important note one must initialize weights zero similar may done machine learning algorithms weights initialized zero update outgoing weights neuron identical gradients identical proved proceeding hidden units remain value continue follow ultimately means training become extremely constrained due symmetry wont able build interesting functions also neural networks may get stuck local optima places gradient zero global minima random weight initialization allows one hopefully chance circumventing starting many different random values 3 perform one feedforward using training data 4 perform backpropagation get error derivatives wrt every weight neural network 5 perform gradient descent update weight negative scalar reduction wrt learning rate alpha respective error derivative increment number iterations 6 converged reality though stop reached number maximum iterations training complete else repeat starting step 3 initialize weights randomly zero perform gradient descent derivatives computed backpropagation expect train neural network time hope example brought clarity backprop works intuition behind didnt understand intricacies example understand appreciate concept backprop whole youre still great place next well go ahead explain backprop code works generalized architecture neural network using relu activation function weve developed math intuition behind backpropagation lets try implement well divide implementation three distinct steps lets start defining api implementing looks like well define network series layer instances data passes means instead modeling individual neuron group neurons single layer together makes bit easier reason larger networks also makes actual computations faster well see shortly also going write code python layer following api isnt great api design ideally would decouple backprop weight update rest object specific algorithm use updating weights isnt tied layer thats point well stick design purposes explaining backpropagation works reallife scenario also well using numpy throughout implementation awesome tool mathematical operations python especially tensor based ones dont time get works want good introduction ya go start implementing weight initialization turns initialize weights actually kind big deal network performance convergence rates heres well initialize weights initializes weight matrix appropriate dimensions random values sampled normal distribution scale rad2selfsize_in giving us variance 2selfsize_in derivation thats need layer initialization lets move implementing first objective feedforward actually pretty simple dot product input activations weight matrix followed activation function give us activations need dot product part make intuitive sense doesnt sit try work piece paper performance gains grouping neurons layers comes instead keeping individual weight vector neuron performing series vector dot products single matrix operation thanks wonders modern processors significantly faster fact compute activations layer two lines simple enough lets move backpropagation ones bit involved first compute derivative output wrt weights derivative cost wrt output followed chain rule get derivative cost wrt weights lets start first part derivative output wrt weights simple enough youre multiplying weight corresponding input activation derivative corresponding input activation except using relu activation function weights effect corresponding output 0 gets capped anyway take care hiccup formally youre multiplying derivative activation function 0 activation 0 1 elsewhere lets take brief detour talk out_grad parameter backward method gets lets say network two layers first neurons second n neurons produces activation n neurons looks activations out_grad parameter x n matrix affects n neurons feeds need derivative cost wrt outputs essentially out_grad parameter given need sum row matrix given per backpropagation formula finally end something like need compute derivative inputs pass along next layer perform similar chain rule derivative output wrt inputs times derivative cost wrt outputs thats backpropagation step final step weight update assuming sticking gradient descent example simple oneliner actually train network take one training samples call forward layer consecutively passing output previous layer input following layer compute dj passing out_grad parameter last layers backward method call backward layers reverse order time passing output layer out_grad previous layer finally call update layers repeat theres one last detail include concept bias akin constant term given equation notice current implementation activation neuron determined solely based activations previous layer theres bias term shift activation independent inputs bias term isnt strictly necessary fact train network asis would probably still work fine need bias term code stays almost difference need add column 1s incoming activations update weight matrix accordingly one weights gets treated bias term difference returning cost_wrt_inputs cut first row nobody cares gradients associated bias term previous layer say activation bias neuron implementing backpropagation kind tricky often good idea check implementation computing gradient numerically literally perturbing weight calculating difference cost function comparing backpropagationcomputed gradient gradient check doesnt need run youve verified implementation could save lot time tracking potential problems network nowadays often dont even need implement neural network libraries caffe torch tensorflow implementations ready go said often good idea try implementing get better grasp everything works hood intrigued looking learn neural networks great online classes get started stanfords cs231n although technically convolutional neural networks class provides excellent introduction survey neural networks general class videos notes assignments posted patience would strongly recommend walking assignments really get know youre learning mit 6034 class taught prof patrick henry winston explores many different algorithms disciplines artificial intelligence theres great lecture backprop actually used stepping stone getting setup writing article also learned genetic algorithms prof winston hes great teacher hope visited article without knowing backpropagation algorithm works reading least rudimentary mathematical conceptual intuition writing conveying complex algorithm supposed beginner proven extremely difficult task us helped us truly understand weve learning greater knowledge fundamental area machine learning excited take look new interesting algorithms disciplines field looking forward continue documenting endeavors together quick cheer standing ovation clap show much enjoyed story rohankapurcom ongoing effort make mathematics science linguistics philosophy artificial intelligence fun simple,en,"['MIT & Stanford', 'AYOAI', 'algorithm', 'ANN', 'convex', 'fed', 'ReLU (Rectified Linear Unit', 'x_i', 'Linear Algebra', 'neuron', 'z_3', 'Gradient Descent', 'out_grad', 'Caffe, Torch', 'TensorFlow', 'Stanford', 'CS231n', 'MIT', 'Artificial Intelligence']"
197,Per Harald Borgen,1300,Learning How To Code Neural Networks – Learning New Stuff – Medium,"This is the second post in a series of me trying to learn something new over a short period of time. The first time consisted of learning how to do machine learning in a week.
This time I’ve tried to learn neural networks. While I didn’t manage to do it within a week, due to various reasons, I did get a basic understanding of it throughout the summer and autumn of 2015.
By basic understanding, I mean that I finally know how to code simple neural networks from scratch on my own.
In this post, I’ll give a few explanations and guide you to the resources I’ve used, in case you’re interested in doing this yourself.
So what is a neural network? Let’s wait with the network part and start off with one single neuron.
The circle below illustrates an artificial neuron. Its input is 5 and its output is 1. The input is the sum of the three synapses connecting to the neuron (the three arrows at the left).
At the far left we see two input values plus a bias value. The input values are 1 and 0 (the green numbers), while the bias holds a value of -2 (the brown number).
The two inputs are then multiplied by their so called weights, which are 7 and 3 (the blue numbers).
Finally we add it up with the bias and end up with a number, in this case: 5 (the red number). This is the input for our artificial neuron.
The neuron then performs some kind of computation on this number — in our case the Sigmoid function, and then spits out an output. This happens to be 1, as Sigmoid of 5 equals to 1, if we round the number up (more info on the Sigmoid function follows later).
If you connect a network of these neurons together, you have a neural network, which propagates forward — from input output, via neurons which are connected to each other through synapses, like on the image to the left.
I can strongly recommend the Welch Labs videos on YouTube for getting a better intuitive explanation of this process.
After you’ve seen the Welch Labs videos, its a good idea to spend some time watching Week 4 of the Coursera’s Machine Learning course, which covers neural networks, as it’ll give you more intuition of how they work.
The course is fairly mathematical, and its based around Octave, while I prefer Python. Because of this, I did not do the programming exercises. Instead, I used the videos to help me understand what I needed to learn.
The first thing I realized I needed to investigate further was the Sigmoid function, as this seemed to be a critical part of many neural networks. I knew a little bit about the function, as it was also covered in Week 3 of the same course. So I went back and watched these videos again.
But watching videos won’t get you all the way. To really understand it, I felt I needed to code it from the ground up.
So I started to code a logistic regression algorithm from scratch (which happened to use the Sigmoid function).
It took a whole day, and it’s probably not a very good implementation of logistic regression. But that doesn’t matter, as I finally understood how it works. Check the code here.
You don’t need to perform this entire exercise yourself, as it requires some knowledge about and cost functions and gradient descent, which you might not have at this point.
But make sure you understand how the Sigmoid function works.
Understanding how a neural network works from input to output isn’t that difficult to understand, at least conceptually.
More difficult though, is understanding how the neural network actually learns from looking at a set of data samples.
The concept is called backpropagation.
The weights were the blue numbers on our neuron in the beginning of the article.
This process happens backwards, because you start at the end of the network (observe how wrong the networks ‘guess’ is), and then move backwards through the network, while adjusting the weights on the way, until you finally reach the inputs.
To calculate this by hand requires some calculus, as it involves getting some derivatives of the networks’ weights. The Kahn Academy calculus courses seems like a good way to start, though I haven’t used them myself, as I took calculus on university.
The three best sources I found for understanding backpropagation are these:
You should definitely code along while you’re reading the articles, especially the two first ones. It’ll give you some sample code to look back at when you’re confused in the future.
Plus, I can’t really emphasize this enough:
The third article is also fantastic, but I’ve used this more as a wiki than a plain tutorial, as it’s actually an entire book. It contains thorough explanations all the important concepts in neural networks.
These articles will also help you understand important concepts as cost functions and gradient descent, which play equally important roles in neural networks.
In some articles and tutorials you’ll actually end up coding small neural networks. As soon as you’re comfortable with that, I recommend you to go all in on this strategy. It’s both fun and an extremely effective way of learning.
One of the articles I also learned a lot from was A Neural Network in 11 Lines Of Python by IAmTrask. It contains an extraordinary amount of compressed knowledge and concepts in just 11 lines.
After you’ve coded along with this example, you should do as the article states at the bottom, which is to implement it once again without looking at the tutorial. This forces you to really understand the concepts, and will likely reveal holes in your knowledge, which isn’t fun. However, when you finally manage it, you’ll feel like you’ve just acquired a new superpower.
When you’ve done this, you can continue with this Wild ML tutorial, by Denny Britz, which guides you through a little more robust neural network.
At this point, you could either try and code your own neural network from scratch or start playing around with some of the networks you have coded up already. It’s great fun to find a dataset that interests you and try to make some predictions with your neural nets.
To get a hold of a dataset, just visit my side project Datasets.co (← shameless self promotion) and find one you like.
Anyway, the point is that you’re now better off experimenting with stuff that interests you rather than following my advices.
Personally, I’m currently learning how to use Python libraries that makes it easier to code up neural networks, like Theano, Lasagne and nolearn. I’m using this to do challenges on Kaggle, which is both great fun and great learning.
Good luck!
And don’t forget to press the heart button if you liked the article :)
Thanks for reading! My name is Per, I’m a co-founder of Scrimba — a better way to teach and learn code.
If you’ve read this far, I’d recommend you to check out this demo!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder of Scrimba, the next-generation platform for teaching and learning code. https://scrimba.com.
A publication about improving your technical skills.
",second post series trying learn something new short period time first time consisted learning machine learning week time ive tried learn neural networks didnt manage within week due various reasons get basic understanding throughout summer autumn 2015 basic understanding mean finally know code simple neural networks scratch post ill give explanations guide resources ive used case youre interested neural network lets wait network part start one single neuron circle illustrates artificial neuron input 5 output 1 input sum three synapses connecting neuron three arrows left far left see two input values plus bias value input values 1 0 green numbers bias holds value 2 brown number two inputs multiplied called weights 7 3 blue numbers finally add bias end number case 5 red number input artificial neuron neuron performs kind computation number case sigmoid function spits output happens 1 sigmoid 5 equals 1 round number info sigmoid function follows later connect network neurons together neural network propagates forward input output via neurons connected synapses like image left strongly recommend welch labs videos youtube getting better intuitive explanation process youve seen welch labs videos good idea spend time watching week 4 courseras machine learning course covers neural networks itll give intuition work course fairly mathematical based around octave prefer python programming exercises instead used videos help understand needed learn first thing realized needed investigate sigmoid function seemed critical part many neural networks knew little bit function also covered week 3 course went back watched videos watching videos wont get way really understand felt needed code ground started code logistic regression algorithm scratch happened use sigmoid function took whole day probably good implementation logistic regression doesnt matter finally understood works check code dont need perform entire exercise requires knowledge cost functions gradient descent might point make sure understand sigmoid function works understanding neural network works input output isnt difficult understand least conceptually difficult though understanding neural network actually learns looking set data samples concept called backpropagation weights blue numbers neuron beginning article process happens backwards start end network observe wrong networks guess move backwards network adjusting weights way finally reach inputs calculate hand requires calculus involves getting derivatives networks weights kahn academy calculus courses seems like good way start though havent used took calculus university three best sources found understanding backpropagation definitely code along youre reading articles especially two first ones itll give sample code look back youre confused future plus cant really emphasize enough third article also fantastic ive used wiki plain tutorial actually entire book contains thorough explanations important concepts neural networks articles also help understand important concepts cost functions gradient descent play equally important roles neural networks articles tutorials youll actually end coding small neural networks soon youre comfortable recommend go strategy fun extremely effective way learning one articles also learned lot neural network 11 lines python iamtrask contains extraordinary amount compressed knowledge concepts 11 lines youve coded along example article states bottom implement without looking tutorial forces really understand concepts likely reveal holes knowledge isnt fun however finally manage youll feel like youve acquired new superpower youve done continue wild ml tutorial denny britz guides little robust neural network point could either try code neural network scratch start playing around networks coded already great fun find dataset interests try make predictions neural nets get hold dataset visit side project datasetsco shameless self promotion find one like anyway point youre better experimenting stuff interests rather following advices personally im currently learning use python libraries makes easier code neural networks like theano lasagne nolearn im using challenges kaggle great fun great learning good luck dont forget press heart button liked article thanks reading name per im cofounder scrimba better way teach learn code youve read far id recommend check demo quick cheer standing ovation clap show much enjoyed story cofounder scrimba nextgeneration platform teaching learning code httpsscrimbacom publication improving technical skills,en,"['Coursera', 'The Kahn Academy', 'A Neural Network', 'IAmTrask', 'Python', 'Kaggle']"
198,Shi Yan,4400,Understanding LSTM and its diagrams – ML Review – Medium,"I just want to reiterate what’s said here:
I’m not better at explaining LSTM, I want to write this down as a way to remember it myself. I think the above blog post written by Christopher Olah is the best LSTM material you would find. Please visit the original link if you want to learn LSTM. (But I did create some nice diagrams.)
Although we don’t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.
But when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.
I think it’s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.
The naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.
A lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.
Theoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.
Then later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.
At a first sight, this looks intimidating. Let’s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the “memory” of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.
Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.
The way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.
The first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.
The second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.
On the LSTM diagram, the top “pipe” is the memory pipe. The input is the old memory (a vector). The first cross ✖ it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.
Then the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the ✖ below the + sign.
After these two operations, you have the old memory C_t-1 changed to the new memory C_t.
Now lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it’s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.
Now the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.
The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.
These two ✖ signs are the forget valve and the new memory valve.
And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.
The above diagram is inspired by Christopher’s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn’t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing “Cell”.
I like the Christopher’s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can’t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.
The second reason I don’t like the following diagram is that the computation you perform within the unit should be ordered, but you can’t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.
The following diagram tries to represent this “delay” or “order” with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.
But these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:
This is the forget gate (valve) that shuts the old memory:
This is the new memory valve and the new memory:
These are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big “Cell”):
This is the output valve and output of the LSTM unit:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Software engineer & wantrepreneur. Interested in computer graphics, bitcoin and deep learning.
Highlights from Machine Learning Research, Projects and Learning Materials. From and For ML Scientists, Engineers an Enthusiasts.
",want reiterate whats said im better explaining lstm want write way remember think blog post written christopher olah best lstm material would find please visit original link want learn lstm create nice diagrams although dont know brain functions yet feeling must logic unit memory unit make decisions reasoning experience computers logic units cpus gpus also memories look neural network functions like black box feed inputs one side receive outputs side decision makes mostly based current inputs think unfair say neural network memory learnt weights kind memory training data memory static sometimes want remember input later use many examples situation stock market make good investment judgement least look stock data time window naive way let neural network accept time series data connecting several neural networks together neural networks handles one time step instead feeding data individual time step provide data time steps within window context neural network lot times need process data periodic patterns silly example suppose want predict christmas tree sales seasonal thing likely peak year good strategy predict christmas tree sale looking data exactly year back kind problems either need big context include ancient data points good memory know data valuable remember later use needs forgotten useless theoretically naively connected neural network called recurrent neural network work practice suffers two problems vanishing gradient exploding gradient make unusable later lstm long short term memory invented solve issue explicitly introducing memory unit called cell network diagram lstm building block first sight looks intimidating lets ignore internals look inputs outputs unit network takes three inputs x_t input current time step h_t1 output previous lstm unit c_t1 memory previous unit think important input outputs h_t output current network c_t memory current unit therefore single unit makes decision considering current input previous output previous memory generates new output alters memory way internal memory c_t changes pretty similar piping water pipe assuming memory water flows pipe want change memory flow along way change controlled two valves first valve called forget valve shut old memory kept fully open valve old memory pass second valve new memory valve new memory come shaped joint like merge old memory exactly much new memory come controlled second valve lstm diagram top pipe memory pipe input old memory vector first cross passes forget valve actually elementwise multiplication operation multiply old memory c_t1 vector close 0 means want forget old memory let old memory goes forget valve equals 1 second operation memory flow go operator operator means piecewise summation resembles shape joint pipe new memory old memory merge operation much new memory added old memory controlled another valve sign two operations old memory c_t1 changed new memory c_t lets look valves first one called forget valve controlled simple one layer neural network inputs neural network h_t1 output previous lstm block x_t input current lstm block c_t1 memory previous block finally bias vector b_0 neural network sigmoid function activation output vector forget valve applied old memory c_t1 elementwise multiplication second valve called new memory valve one layer simple neural network takes inputs forget valve valve controls much new memory influence old memory new memory however generated another neural network also one layer network uses tanh activation function output network elementwise multiple new memory valve add old memory form new memory two signs forget valve new memory valve finally need generate output lstm unit step output valve controlled new memory previous output h_t1 input x_t bias vector valve controls much new memory output next lstm unit diagram inspired christophers blog post time see diagram like major difference two variations following diagram doesnt treat memory unit c input unit instead treats internal thing cell like christophers diagram explicitly shows memory c gets passed previous unit next following image cant easily see c_t1 actually previous unit c_t part output second reason dont like following diagram computation perform within unit ordered cant see clearly following diagram example calculate output unit need c_t new memory ready therefore first step evaluating c_t following diagram tries represent delay order dash lines solid lines errors picture dash lines means old memory available beginning solid lines means new memory operations require new memory wait c_t available two diagrams essentially want use symbols colors first diagram redraw diagram forget gate valve shuts old memory new memory valve new memory two valves elementwise summation merge old memory new memory form c_t green flows back big cell output valve output lstm unit quick cheer standing ovation clap show much enjoyed story software engineer wantrepreneur interested computer graphics bitcoin deep learning highlights machine learning research projects learning materials ml scientists engineers enthusiasts,en,"['LSTM', 'Machine Learning Research', 'Learning Materials']"
199,Ross Goodwin,686,Adventures in Narrated Reality – Artists and Machine Intelligence – Medium,"By Ross Goodwin
In May 2015, Stanford PhD student Andrej Karpathy wrote a blog post entitled The Unreasonable Effectiveness of Recurrent Neural Networks and released a code repository called Char-RNN. Both received quite a lot of attention from the machine learning community in the months that followed, spurring commentary and a number of response posts from other researchers.
I remember reading these posts early last summer. Initially, I was somewhat underwhelmed—as at least one commentator pointed out, much of the generated text that Karpathy chose to highlight did not seem much better than results one might expect from high order character-level Markov chains.
Here is a snippet of Karpathy’s Char-RNN generated Shakespeare:
And here is a snippet of generated Shakespeare from a high order character-level Markov chain, via the post linked above:
So I was discouraged. And without access to affordable GPUs for training recurrent neural networks, I continued to experiment with Markov chains, generative grammars, template systems, and other ML-free solutions for generating text.
In December, New York University was kind enough to grant me access to their High Performance Computing facilities. I began to train my own recurrent neural networks using Karpathy’s code, and I finally discovered the quasi-magical capacities of these machines. Since then, I have been training a collection of recurrent neural network models for my thesis project at NYU, and exploring possibilities for devices that could enable such models to serve as expressive real-time narrators in our everyday lives.
At this point, since this is my very first Medium post, perhaps I should introduce myself: my name is Ross Goodwin, I’m a graduate student at NYU ITP in my final semester, and computational creative writing is my personal obsession.
Before I began my studies at ITP, I was a political ghostwriter. I graduated from MIT in 2009 with a B.S. degree in Economics, and during my undergraduate years I had worked on Barack Obama’s 2008 Presidential campaign. At the time, I wanted to be a political speechwriter, and my first job after graduation was a Presidential Writer position at the White House. In this role, I wrote Presidential Proclamations, which are statements of national days, weeks, and months of things—everything from Thanksgiving and African American History Month to lesser known observances like Safe Boating Week. It was a very strange job, but I thoroughly enjoyed it.
I left the White House in 2010 for a position at the U.S. Department of the Treasury, where I worked for two years, mostly putting together briefing binders for then-Secretary Timothy Geithner and Deputy Secretary Neal Wolin in the Department’s front office. I didn’t get many speechwriting opportunities, and pursuing a future in the financial world did not appeal to me, so I left to work as a freelance ghostwriter.
This was a rather dark time in my life, as I rapidly found myself writing for a variety of unsavory clients and causes in order to pay my rent every month. In completing these assignments, I began to integrate algorithms into my writing process to improve my productivity. (At the time, I didn’t think about these techniques as algorithmic, but it’s obvious in retrospect.) For example, if I had to write 12 letters, I’d write them in a spreadsheet with a paragraph in each cell. Each letter would exist in a column, and I would write across the rows—first I’d write all the first paragraphs as one group, then all the second paragraphs, then all the thirds, and so on. If I had to write a similar group of letters the next day for the same client, I would use an Excel macro to randomly shuffle the cells, then edit the paragraphs for cohesion and turn the results in as an entirely new batch of letters.
Writing this way, I found I could complete an 8-hour day of work in about 2 hours. I used the rest of my time to work on a novel that’s still not finished (but that’s a story for another time). With help from some friends, I turned the technique into a game we called The Diagonalization Argument after Georg Cantor’s 1891 mathematical proof of the same name.
In early 2014, a client asked me to write reviews of all the guides available online to learn the Python programming language. One guide stood out above all others, in the sheer number of times I saw users reference it on various online forums and in the countless glowing reviews it had earned across the Internet: Learn Python the Hard Way by Zed Shaw
So, to make my reviews better, I decided I might as well try to learn Python. My past attempts at learning to code had failed due to lack of commitment, lack of interest, or lack of a good project to get started. But this time was different somehow—Zed’s guide worked for me, and just like that I found myself completely and hopelessly addicted to programming.
As a writer, I gravitated immediately to the broad and expanding world of natural language processing and generation. My first few projects were simple poetry generators. And once I moved to New York City and started ITP, I discovered a local community of likeminded individuals leveraging computation to produce and enhance textual work. I hosted a Code Poetry Slam in November 2014 and began attending Todd Anderson’s monthly WordHack events at Babycastles.
In early 2015, I developed and launched word.camera, a web app and set of physical devices that use the Clarifai API to tag images with nouns, ConceptNet to find related words, and a template system to string the results together into descriptive (though often bizarre) prose poems related to the captured photographs. The project was about redefining the photographic experience, and it earned more attention than I expected [1,2,3]. In November, I was invited to exhibit this work at IDFA DocLab in Amsterdam.
At that point, it became obvious that word.camera (or some extension thereof) would become my ITP thesis project. And while searching for ways to improve its output, I began to experiment with training my own neural networks rather than using those others had trained via APIs.
As I mentioned above, I started using NYU’s High Performance Computing facilities in December. This supercomputing cluster includes a staggering array of computational resources — in particular, at least 32 Nvidia Tesla K80 GPUs, each with 24 GB of GPU memory. While GPUs aren’t strictly required to train deep neural networks, the massively parallel processes involved make them all but a necessity for training a larger model that will perform well in a reasonable amount of time.
Using two of Andrej Karpathy’s repositories, NeuralTalk2 and Char-RNN respectively, I trained an image captioning model and a number of models for generating text. As a result of having free access to the largest GPUs in the world, I was able to start training very large models right away.
NeuralTalk2 uses a convolutional neural network to classify images, then transfers that classification data to a recurrent neural network that generates a brief caption. For my first attempt at training a NeuralTalk2 model, I wanted to do something less traditional than simply captioning images.
In my opinion, the idea of machine “image captioning” is problematic because it’s so limited in scope. Fundamentally, a machine that can caption images is a machine that can describe or relate to what it sees in a highly intelligent way. I do understand that image captioning is an important benchmark for machine intelligence. However, I also believe that thinking such a machine’s primary use case will be to replace human image captioning represents a highly restrictive and narrow point of view.
So I tried training a model on frames and corresponding captions from every episode of the TV show The X-Files. My idea was to create a model that, if given an image, would generate a plausible line of dialogue from what it saw.
Unfortunately, it simply did not work—most likely due to the dialogue for a particular scene bearing no direct relationship to that scene’s imagery. Rather than generating a different line of dialogue for different images, the machine seemed to want to assign the same line to every image indiscriminately.
Strangely, these repetitive lines tended to say things like I don’t know, I’m not sure what you want, and I don’t know what to do. (One of my faculty advisors, Patrick Hebron, jokingly suggested this may be a sign of metacognition—needless to say, I was slightly creeped out but excited to continue these explorations.)
I tried two other less-than-traditional approaches with NeuralTalk2: training on Reddit image posts and corresponding comments, and training on pictures of recreational drugs and corresponding Erowid experience reports. Both worked better than my X-Files experiment, but neither produced particularly interesting results.
So I resigned myself to training a traditional image captioning model using the Microsoft Common Objects in Context (MSCOCO) caption set. In terms of objects represented, MSCOCO is far from exhaustive, but it does contain over 120,000 images with 5 captions each, which is more than I could’ve expected to produce on my own from any source. Furthermore, I figured I could always do something less traditional with such a model once trained.
I made just one adjustment to Karpathy’s default training parameters: decreased the word-frequency threshold from five to three. By default, NeuralTalk2 ignores any word that appears fewer than five times in the caption corpus it trains on. I guessed that reducing this threshold would result in some extra verbosity in the generated captions, possibly at the expense of accuracy, as a more verbose model might describe details that were not actually present in an image. However, after about five days of training, I had produced a model that exceeded 0.9 CIDEr in tests, which is about as good as Karpathy suggested the model could get in his documentation.
As opposed to NeuralTalk2, which is designed to caption images, Karpathy’s Char-RNN employs a character-level LSTM recurrent neural network simply for generating text. A recurrent neural network is fundamentally a linear pattern machine. Given a character (or set of characters) as a seed, a Char-RNN model will predict which character would come next based on what it has learned from its input corpus. By doing this again and again, the model can generate text in the same manner as a Markov chain, though its internal processes are far more sophisticated.
LSTM stands for Long Short-Term Memory, which remains a popular architecture for recurrent neural networks. Unlike a no-frills vanilla RNN, an LSTM protects its fragile underlying neural net with “gates” that determine which connections will persist in the machine’s weight matrices. (I’ve been told that others are using something called a GRU, but I have yet to investigate this architecture.)
I trained my first text generating LSTM on the same prose corpus I used for word.camera’s literary epitaphs. After about 18 hours, I was getting results like this:
This paragraph struck me as highly poetic, compared to what I’d seen in the past from a computer. The language wasn’t entirely sensical, but it certainly conjured imagery and employed relatively solid grammar. Furthermore, it was original. Originality has always been important to me in computer generated text—because what good is a generator if it just plagiarizes your input corpus? This is a major issue with high order Markov chains, but due to its more sophisticated internal mechanisms, the LSTM didn’t seem to have the same tendency.
Unfortunately, much of the prose-trained model output that contained less poetic language was also less interesting than the passage above. But given that I could produce poetic language with a prose-trained model, I wondered what results I could get from a poetry-trained model.
The output above comes from the first model I trained on poetry. I used the most readily available books I could find, mostly those of poets from the 19th century and earlier whose work had entered the public domain. The consistent line breaks and capitalization schemes were encouraging. But I still wasn’t satisfied with the language—due to the predominant age of the corpus, it seemed too ornate and formal. I wanted more modern-sounding poetic language, and so I knew I had to train a model on modern poetry.
I assembled a corpus of all the modern poetry books I could find online. It wasn’t nearly as easy as assembling the prior corpus—unfortunately, I can’t go into detail on how I got all the books for fear of being sued.
The results were much closer to what I was looking for in terms of language. But they were also inconsistent in quality. At the time, I believed this was because the corpus was too small, so I began to supplement my modern poetry corpus with select prose works to increase its size. It remains likely that this was the case. However, I had not yet discovered the seeding techniques I would later learn can dramatically improve LSTM output.
Another idea occurred to me: I could seed a poetic language LSTM model with a generated image caption to make a new, more poetic version of word.camera. Some of the initial results (see: left) were striking. I showed them to one of my mentors, Allison Parrish, who suggested that I find a way to integrate the caption throughout the poetic text, rather than just at the beginning. (I had showed her some longer examples, where the language had strayed quite far from the subject matter of the caption after a few lines.)
I thought about how to accomplish this, and settled on a technique of seeding the poetic language LSTM multiple times with the same image caption at different temperatures.
Temperature is a parameter, a number between zero and one, that controls the riskiness of a recurrent neural network’s character predictions. A low temperature value will result in text that’s repetitive but highly grammatical. Accordingly, high temperature results will be more innovative and surprising (the model may even invent its own words) while containing more mistakes. By iterating through temperature values with the same seed, the subject matter would remain consistent while the language varied, resulting in longer pieces that seemed more cohesive than anything I’d ever produced with a computer.
As I refined the aforementioned technique, I trained more LSTM models, attempting to discover the best training parameters. The performance of a neural network model is measured by its loss, which drops during training and eventually should be as close to zero as possible. A model’s loss is a statistical measurement indicating how well a model can predict the character sequences in its own corpus. During training, there are two loss figures to monitor: the training loss, which is defined by how well the model predicts the part of the corpus it’s actually training on, and the validation loss, which is defined by how well the model predicts an unknown validation sample that was removed from the corpus prior to training.
The goal of training a model is to reduce its validation loss as much as possible, because we want a model that accurately predicts unknown character sequences, not just those it’s already seen. To this end, there are a number of parameters to adjust, among which are:
The training process largely consists of monitoring the validation loss as it drops across model checkpoints, and monitoring the difference between training loss and validation loss. As Karpathy writes in his Char-RNN documentation:
In January, I released my code on GitHub along with a set of trained neural network models: an image captioning model and two poetic language LSTM models. In my GitHub README, I highlighted a few results I felt were particularly strong [1,2,3,4,5]. Unlike prior versions of word.camera that mostly relied on a strong connection between the image and the output, I found that I could still enjoy the result when the image caption was totally incorrect, and there often seemed to be some other accidental (or perhaps slightly-less-than-accidental) element connecting the image to the words.
I then shifted my focus to developing a new physical prototype. With the prior version of word.camera, I believed one of the most important parts of the experience was its portability. That’s why I developed a mobile web app first, and why I ensured all the physical prototypes I built were fully portable. For the new version, I started with a physical prototype rather than a mobile web application because developing an app initially seemed infeasible due to computational requirements, though I have since thought of some possible solutions.
Since this would be a rapid prototype, I decided to use a very small messenger bag as the case rather than fabricating my own. Also, my research suggested that some of Karpathy’s code may not run on the Raspberry Pi’s ARM architecture, so I needed a slightly larger computer that would require a larger power source.
I decided to use an Intel NUC that I powered with a backup battery for a laptop. I mounted an ELP wide angle camera to the strap, alongside a set of controls (a rotary potentiometer and a button) that communicated with the main computer via an Arduino.
Originally, I planned to dump the text output to a hacked Kindle, but ultimately decided the tactile nature of thermal printer paper would provide for a superior experience (and allow me to hand out the output on the street like I’d done with prior word.camera models). I found a large format thermal printer model with built-in batteries that uses 4""-wide paper (previous printers I’d used had taken paper half as wide), and I was able to pick up a couple of them on eBay for less than $50 each. Based on a suggestion from my friend Anthony Kesich, I decided to add an “ascii image” of the photo above the text.
In February, I was invited to speak at an art and machine learning symposium at Gray Area in San Francisco. In Amsterdam at IDFA in November, I had met Jessica Brillhart, who is a VR director on Google’s Cardboard team. In January, I began to collaborate with her and some other folks at Google on Deep Dream VR experiences with automated poetic voiceover. (If you’re unfamiliar with Deep Dream, check out this blog post from last summer along with the related GitHub repo and Wikipedia article.) We demonstrated these experiences at the event, which was also an auction to sell Deep Dream artwork to benefit the Gray Area Foundation.
Mike Tyka, an artist whose Deep Dream work was prominently featured in the auction, had asked me to use my poetic language LSTM to generate titles for his artwork. I had a lot of fun doing this, and I thought the titles came out well—they even earned a brief mention in the WIRED article about the show.
During my talk the day after the auction, I demonstrated my prototype. I walked onto the stage wearing my messenger bag, snapped a quick photo before I started speaking, and revealed the output at the end.
I would have been more nervous about sharing the machine’s poetic output in front of so many people, but the poetry had already passed what was, in my opinion, a more genuine test of its integrity: a small reading at a library in Brooklyn alongside traditional poets.
Earlier in February, I was invited to share some work at the Leonard Library in Williamsburg. The theme of the evening’s event was love and romance, so I generated several poems [1,2] from images I considered romantic. My reading was met with overwhelming approval from the other poets at the event, one of whom said that the poem I had generated from the iconic Times Square V-J Day kiss photograph by Alfred Eisenstaedt “messed [him] up” as it seemed to contain a plausible description of a flashback from the man’s perspective.
I had been worried because, as I once heard Allison Parrish say, so much commentary about computational creative writing focuses on computers replacing humans—but as anyone who has worked with computers and language knows, that perspective (which Allison summarized as “Now they’re even taking the poet’s job!”) is highly uninformed.
When we teach computers to write, the computers don’t replace us any more than pianos replace pianists—in a certain way, they become our pens, and we become more than writers. We become writers of writers.
Nietzsche, who was the first philosopher to use a typewriter, famously wrote “Our writing tools are also working on our thoughts,” which media theorist Friedrich Kittler analyzes in his book Gramophone, Film, Typewriter (p. 200):
If we employ machine intelligence to augment our writing activities, it’s worth asking how such technology would affect how we think about writing as well as how we think in the general sense. I’m inclined to believe that such a transformation would be positive, as it would enable us to reach beyond our native writing capacities and produce work that might better reflect our wordless internal thoughts and notions. (I hesitate to repeat the piano/pianist analogy for fear of stomping out its impact, but I think it applies here too.)
In producing fully automated writing machines, I am only attempting to demonstrate what is possible with a machine alone. In my research, I am ultimately striving to produce devices that allow humans to work in concert with machines to produce written work. My ambition is to augment our creativity, not to replace it.
Another ambition of mine is to promote a new framework that I’ve been calling Narrated Reality. We already have Virtual Reality (VR) and Augmented Reality (AR), so it only makes sense to provide another option (NR?)—perhaps one that’s less visual and more about supplementing existing experiences with expressive narration. That way, we can enjoy our experiences while we’re having them, then revisit them later in an augmented format.
For my ITP thesis, I had originally planned to produce one general-purpose device that used photographs, GPS coordinates (supplemented with Foursquare locations), and the time to narrate everyday experiences. However, after receiving some sage advice from Taeyoon Choi, I have decided to split that project into three devices: a camera, a compass, and a clock that respectively use image, location, and time to realize Narrated Reality.
Along with designing and building those devices, I am in the process of training a library of interchangeable LSTM models in order to experience a variety of options with each device in this new space.
After training a number of models on fiction and poetry, I decided to try something different: I trained a model on the Oxford English Dictionary.
The result was better than I ever could have anticipated: an automated Balderdash player that could generate plausible definitions for made up words. I made a Twitter bot so that people could submit their linguistic inventions, and a Tumblr blog for the complete, unabridged definitions.
I was amazed by the machine’s ability to take in and parrot back strings of arbitrary characters it had never seen before, and how it often seemed to understand them in the context of actual words.
The fictional definitions it created for real words were also frequently entertaining. My favorite of these was its definition for “love”—although a prior version of the model had defined love as “past tense of leave,” which I found equally amusing.
One particularly fascinating discovery I made with this bot concerned the importance of a certain seeding technique that Kyle McDonald taught me. As discussed above, when you generate text with a recurrent neural network, you can provide a seed to get the machine started. For example, if you wanted to know the machine’s feelings on the meaning of life, you might seed your LSTM with the following text:
And the machine would logically complete your sentence based on the patterns it had absorbed from its training corpus:
However, to get better and more consistent results, it makes sense to prepend the seed with a pre-seed (another paragraph of text) to push the LSTM into a desired state. In practice, it’s good to use a high quality sample of output from the model you’re seeding with length approximately equal to the sequence length (see above) you set during training.
This means the seed will now look something like this:
And the raw output will look like this (though usually I remove the pre-seed when I present the output):
The difference was more than apparent when I began using this technique with the dictionary model. Without the pre-seed, the bot would usually fail to repeat an unknown word within its generated definition. With the pre-seed, it would reliably parrot back whatever gibberish it had received.
In the end, the Oxford English Dictionary model trained to a significantly lower final validation loss (< 0.75) than any other model I had trained, or have trained since. One commenter on Hacker News noted:
After considering what to do next, I decided to try integrating dictionary definitions into the prose and poetry corpora I had been training before. Additionally, another Stanford PhD student named Justin Johnson released a new and improved version of Karpathy’s Char-RNN, Torch-RNN, which promised to use 7x less memory, which would in turn allow for me to train even larger models than I had been training before on the same GPUs.
It took me an evening to get Torch-RNN working on NYU’s supercomputing cluster, but once I had it running I was immediately able to start training models four times as large as those I’d trained on before. My initial models had 20–25 million parameters, and now I was training with 80–85 million, with some extra room to increase batch size and sequence length parameters.
The results I got from the first model were stunning—the corpus was about 45% poetry, 45% prose, and 10% dictionary definitions, and the output appeared more prose-like while remaining somewhat cohesive and painting vivid imagery.
Next, I decided to train a model on Noam Chomsky’s complete works. Most individuals have not produced enough publicly available text (25–100 MB raw text, or 50–200 novels) to train an LSTM this size. Noam Chomsky is an exception, and the corpus of his writing I was able to assemble weighs in at a hefty 41.2 MB. (This project was complicated by the fact that I worked for Noam Chomsky as an undergraduate at MIT, but that’s a story for another time.) Here is a sample of the output from that model:
Unfortunately, I’ve had trouble making it say anything interesting about language, as it prefers to rattle on and on about the U.S. and Israel and Palestine. Perhaps I’ll have to train the next model on academic papers alone and see what happens.
Most recently, I’ve been training machines on movie screenplays, and getting some interesting results. If you train an LSTM on continuous dialogue, you can ask the model questions and receive plausible responses.
I promised myself I wouldn’t write more than 5000 words for this article, and I’ve already passed that threshold. So, rather than attempting some sort of eloquent conclusion, I’ll leave you with this brief video.
There’s much more to come in the near future. Stay tuned.
Edit 6/9/16: Check out Part II!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
not a poet | new forms & interfaces for written language, narrated reality, &c.
AMI is a program at Google that brings together artists and engineers to realize projects using Machine Intelligence. Works are developed together alongside artists’ current practices and shown at galleries, biennials, festivals, or online.
",ross goodwin may 2015 stanford phd student andrej karpathy wrote blog post entitled unreasonable effectiveness recurrent neural networks released code repository called charrnn received quite lot attention machine learning community months followed spurring commentary number response posts researchers remember reading posts early last summer initially somewhat underwhelmedas least one commentator pointed much generated text karpathy chose highlight seem much better results one might expect high order characterlevel markov chains snippet karpathys charrnn generated shakespeare snippet generated shakespeare high order characterlevel markov chain via post linked discouraged without access affordable gpus training recurrent neural networks continued experiment markov chains generative grammars template systems mlfree solutions generating text december new york university kind enough grant access high performance computing facilities began train recurrent neural networks using karpathys code finally discovered quasimagical capacities machines since training collection recurrent neural network models thesis project nyu exploring possibilities devices could enable models serve expressive realtime narrators everyday lives point since first medium post perhaps introduce name ross goodwin im graduate student nyu itp final semester computational creative writing personal obsession began studies itp political ghostwriter graduated mit 2009 bs degree economics undergraduate years worked barack obamas 2008 presidential campaign time wanted political speechwriter first job graduation presidential writer position white house role wrote presidential proclamations statements national days weeks months thingseverything thanksgiving african american history month lesser known observances like safe boating week strange job thoroughly enjoyed left white house 2010 position us department treasury worked two years mostly putting together briefing binders thensecretary timothy geithner deputy secretary neal wolin departments front office didnt get many speechwriting opportunities pursuing future financial world appeal left work freelance ghostwriter rather dark time life rapidly found writing variety unsavory clients causes order pay rent every month completing assignments began integrate algorithms writing process improve productivity time didnt think techniques algorithmic obvious retrospect example write 12 letters id write spreadsheet paragraph cell letter would exist column would write across rowsfirst id write first paragraphs one group second paragraphs thirds write similar group letters next day client would use excel macro randomly shuffle cells edit paragraphs cohesion turn results entirely new batch letters writing way found could complete 8hour day work 2 hours used rest time work novel thats still finished thats story another time help friends turned technique game called diagonalization argument georg cantors 1891 mathematical proof name early 2014 client asked write reviews guides available online learn python programming language one guide stood others sheer number times saw users reference various online forums countless glowing reviews earned across internet learn python hard way zed shaw make reviews better decided might well try learn python past attempts learning code failed due lack commitment lack interest lack good project get started time different somehowzeds guide worked like found completely hopelessly addicted programming writer gravitated immediately broad expanding world natural language processing generation first projects simple poetry generators moved new york city started itp discovered local community likeminded individuals leveraging computation produce enhance textual work hosted code poetry slam november 2014 began attending todd andersons monthly wordhack events babycastles early 2015 developed launched wordcamera web app set physical devices use clarifai api tag images nouns conceptnet find related words template system string results together descriptive though often bizarre prose poems related captured photographs project redefining photographic experience earned attention expected 123 november invited exhibit work idfa doclab amsterdam point became obvious wordcamera extension thereof would become itp thesis project searching ways improve output began experiment training neural networks rather using others trained via apis mentioned started using nyus high performance computing facilities december supercomputing cluster includes staggering array computational resources particular least 32 nvidia tesla k80 gpus 24 gb gpu memory gpus arent strictly required train deep neural networks massively parallel processes involved make necessity training larger model perform well reasonable amount time using two andrej karpathys repositories neuraltalk2 charrnn respectively trained image captioning model number models generating text result free access largest gpus world able start training large models right away neuraltalk2 uses convolutional neural network classify images transfers classification data recurrent neural network generates brief caption first attempt training neuraltalk2 model wanted something less traditional simply captioning images opinion idea machine image captioning problematic limited scope fundamentally machine caption images machine describe relate sees highly intelligent way understand image captioning important benchmark machine intelligence however also believe thinking machines primary use case replace human image captioning represents highly restrictive narrow point view tried training model frames corresponding captions every episode tv show xfiles idea create model given image would generate plausible line dialogue saw unfortunately simply workmost likely due dialogue particular scene bearing direct relationship scenes imagery rather generating different line dialogue different images machine seemed want assign line every image indiscriminately strangely repetitive lines tended say things like dont know im sure want dont know one faculty advisors patrick hebron jokingly suggested may sign metacognitionneedless say slightly creeped excited continue explorations tried two lessthantraditional approaches neuraltalk2 training reddit image posts corresponding comments training pictures recreational drugs corresponding erowid experience reports worked better xfiles experiment neither produced particularly interesting results resigned training traditional image captioning model using microsoft common objects context mscoco caption set terms objects represented mscoco far exhaustive contain 120000 images 5 captions couldve expected produce source furthermore figured could always something less traditional model trained made one adjustment karpathys default training parameters decreased wordfrequency threshold five three default neuraltalk2 ignores word appears fewer five times caption corpus trains guessed reducing threshold would result extra verbosity generated captions possibly expense accuracy verbose model might describe details actually present image however five days training produced model exceeded 09 cider tests good karpathy suggested model could get documentation opposed neuraltalk2 designed caption images karpathys charrnn employs characterlevel lstm recurrent neural network simply generating text recurrent neural network fundamentally linear pattern machine given character set characters seed charrnn model predict character would come next based learned input corpus model generate text manner markov chain though internal processes far sophisticated lstm stands long shortterm memory remains popular architecture recurrent neural networks unlike nofrills vanilla rnn lstm protects fragile underlying neural net gates determine connections persist machines weight matrices ive told others using something called gru yet investigate architecture trained first text generating lstm prose corpus used wordcameras literary epitaphs 18 hours getting results like paragraph struck highly poetic compared id seen past computer language wasnt entirely sensical certainly conjured imagery employed relatively solid grammar furthermore original originality always important computer generated textbecause good generator plagiarizes input corpus major issue high order markov chains due sophisticated internal mechanisms lstm didnt seem tendency unfortunately much prosetrained model output contained less poetic language also less interesting passage given could produce poetic language prosetrained model wondered results could get poetrytrained model output comes first model trained poetry used readily available books could find mostly poets 19th century earlier whose work entered public domain consistent line breaks capitalization schemes encouraging still wasnt satisfied languagedue predominant age corpus seemed ornate formal wanted modernsounding poetic language knew train model modern poetry assembled corpus modern poetry books could find online wasnt nearly easy assembling prior corpusunfortunately cant go detail got books fear sued results much closer looking terms language also inconsistent quality time believed corpus small began supplement modern poetry corpus select prose works increase size remains likely case however yet discovered seeding techniques would later learn dramatically improve lstm output another idea occurred could seed poetic language lstm model generated image caption make new poetic version wordcamera initial results see left striking showed one mentors allison parrish suggested find way integrate caption throughout poetic text rather beginning showed longer examples language strayed quite far subject matter caption lines thought accomplish settled technique seeding poetic language lstm multiple times image caption different temperatures temperature parameter number zero one controls riskiness recurrent neural networks character predictions low temperature value result text thats repetitive highly grammatical accordingly high temperature results innovative surprising model may even invent words containing mistakes iterating temperature values seed subject matter would remain consistent language varied resulting longer pieces seemed cohesive anything id ever produced computer refined aforementioned technique trained lstm models attempting discover best training parameters performance neural network model measured loss drops training eventually close zero possible models loss statistical measurement indicating well model predict character sequences corpus training two loss figures monitor training loss defined well model predicts part corpus actually training validation loss defined well model predicts unknown validation sample removed corpus prior training goal training model reduce validation loss much possible want model accurately predicts unknown character sequences already seen end number parameters adjust among training process largely consists monitoring validation loss drops across model checkpoints monitoring difference training loss validation loss karpathy writes charrnn documentation january released code github along set trained neural network models image captioning model two poetic language lstm models github readme highlighted results felt particularly strong 12345 unlike prior versions wordcamera mostly relied strong connection image output found could still enjoy result image caption totally incorrect often seemed accidental perhaps slightlylessthanaccidental element connecting image words shifted focus developing new physical prototype prior version wordcamera believed one important parts experience portability thats developed mobile web app first ensured physical prototypes built fully portable new version started physical prototype rather mobile web application developing app initially seemed infeasible due computational requirements though since thought possible solutions since would rapid prototype decided use small messenger bag case rather fabricating also research suggested karpathys code may run raspberry pis arm architecture needed slightly larger computer would require larger power source decided use intel nuc powered backup battery laptop mounted elp wide angle camera strap alongside set controls rotary potentiometer button communicated main computer via arduino originally planned dump text output hacked kindle ultimately decided tactile nature thermal printer paper would provide superior experience allow hand output street like id done prior wordcamera models found large format thermal printer model builtin batteries uses 4wide paper previous printers id used taken paper half wide able pick couple ebay less 50 based suggestion friend anthony kesich decided add ascii image photo text february invited speak art machine learning symposium gray area san francisco amsterdam idfa november met jessica brillhart vr director googles cardboard team january began collaborate folks google deep dream vr experiences automated poetic voiceover youre unfamiliar deep dream check blog post last summer along related github repo wikipedia article demonstrated experiences event also auction sell deep dream artwork benefit gray area foundation mike tyka artist whose deep dream work prominently featured auction asked use poetic language lstm generate titles artwork lot fun thought titles came wellthey even earned brief mention wired article show talk day auction demonstrated prototype walked onto stage wearing messenger bag snapped quick photo started speaking revealed output end would nervous sharing machines poetic output front many people poetry already passed opinion genuine test integrity small reading library brooklyn alongside traditional poets earlier february invited share work leonard library williamsburg theme evenings event love romance generated several poems 12 images considered romantic reading met overwhelming approval poets event one said poem generated iconic times square vj day kiss photograph alfred eisenstaedt messed seemed contain plausible description flashback mans perspective worried heard allison parrish say much commentary computational creative writing focuses computers replacing humansbut anyone worked computers language knows perspective allison summarized theyre even taking poets job highly uninformed teach computers write computers dont replace us pianos replace pianistsin certain way become pens become writers become writers writers nietzsche first philosopher use typewriter famously wrote writing tools also working thoughts media theorist friedrich kittler analyzes book gramophone film typewriter p 200 employ machine intelligence augment writing activities worth asking technology would affect think writing well think general sense im inclined believe transformation would positive would enable us reach beyond native writing capacities produce work might better reflect wordless internal thoughts notions hesitate repeat pianopianist analogy fear stomping impact think applies producing fully automated writing machines attempting demonstrate possible machine alone research ultimately striving produce devices allow humans work concert machines produce written work ambition augment creativity replace another ambition mine promote new framework ive calling narrated reality already virtual reality vr augmented reality ar makes sense provide another option nrperhaps one thats less visual supplementing existing experiences expressive narration way enjoy experiences revisit later augmented format itp thesis originally planned produce one generalpurpose device used photographs gps coordinates supplemented foursquare locations time narrate everyday experiences however receiving sage advice taeyoon choi decided split project three devices camera compass clock respectively use image location time realize narrated reality along designing building devices process training library interchangeable lstm models order experience variety options device new space training number models fiction poetry decided try something different trained model oxford english dictionary result better ever could anticipated automated balderdash player could generate plausible definitions made words made twitter bot people could submit linguistic inventions tumblr blog complete unabridged definitions amazed machines ability take parrot back strings arbitrary characters never seen often seemed understand context actual words fictional definitions created real words also frequently entertaining favorite definition lovealthough prior version model defined love past tense leave found equally amusing one particularly fascinating discovery made bot concerned importance certain seeding technique kyle mcdonald taught discussed generate text recurrent neural network provide seed get machine started example wanted know machines feelings meaning life might seed lstm following text machine would logically complete sentence based patterns absorbed training corpus however get better consistent results makes sense prepend seed preseed another paragraph text push lstm desired state practice good use high quality sample output model youre seeding length approximately equal sequence length see set training means seed look something like raw output look like though usually remove preseed present output difference apparent began using technique dictionary model without preseed bot would usually fail repeat unknown word within generated definition preseed would reliably parrot back whatever gibberish received end oxford english dictionary model trained significantly lower final validation loss 075 model trained trained since one commenter hacker news noted considering next decided try integrating dictionary definitions prose poetry corpora training additionally another stanford phd student named justin johnson released new improved version karpathys charrnn torchrnn promised use 7x less memory would turn allow train even larger models training gpus took evening get torchrnn working nyus supercomputing cluster running immediately able start training models four times large id trained initial models 2025 million parameters training 8085 million extra room increase batch size sequence length parameters results got first model stunningthe corpus 45 poetry 45 prose 10 dictionary definitions output appeared proselike remaining somewhat cohesive painting vivid imagery next decided train model noam chomskys complete works individuals produced enough publicly available text 25100 mb raw text 50200 novels train lstm size noam chomsky exception corpus writing able assemble weighs hefty 412 mb project complicated fact worked noam chomsky undergraduate mit thats story another time sample output model unfortunately ive trouble making say anything interesting language prefers rattle us israel palestine perhaps ill train next model academic papers alone see happens recently ive training machines movie screenplays getting interesting results train lstm continuous dialogue ask model questions receive plausible responses promised wouldnt write 5000 words article ive already passed threshold rather attempting sort eloquent conclusion ill leave brief video theres much come near future stay tuned edit 6916 check part ii quick cheer standing ovation clap show much enjoyed story poet new forms interfaces written language narrated reality c ami program google brings together artists engineers realize projects using machine intelligence works developed together alongside artists current practices shown galleries biennials festivals online,en,"['Stanford PhD', 'Char-RNN', 'New York University', 'NYU', 'ITP', 'MIT', 'Presidential Proclamations', 'African American History Month', 'Safe Boating Week', 'the White House', 'the U.S. Department', 'Treasury', 'Department', 'The Diagonalization Argument', 'ConceptNet', 'Nvidia Tesla', 'NeuralTalk2', 'Microsoft', 'Long Short-Term Memory', 'GRU', 'GitHub', 'ARM', 'Intel', 'ELP', 'eBay', 'Google', 'Cardboard', 'Google on Deep Dream VR', 'Deep Dream', 'the Gray Area Foundation', 'Allison', 'Nietzsche', 'GPS', 'Foursquare', 'Balderdash', 'Tumblr', 'Hacker News', 'Stanford', 'Torch-RNN', 'AMI']"
200,Eric Elliott,947,How to Build a Neuron: Exploring AI in JavaScript Pt 1,"Years ago, I was working on a project that needed to be adaptive. Essentially, the software needed to learn and get better at a frequently repeated task over time.
I’d read about neural networks and some early success people had achieved with them, so I decided to try it out myself. That marked the beginning of a life-long fascination with AI.
AI is a really big deal. There are a small handful of technologies that will dramatically change the world over the course of the next 25 years. Three of the biggest disruptors rely deeply on AI:
Self driving cars alone will disrupt more than 10 million jobs in America, radically improve transportation and shipping efficiency, and may lead to a huge change in car ownership as we outsource transportation and the pains of car ownership and maintenance to apps like Uber.
You’ve probably heard about Google’s self driving cars, but Tesla, Mercedes, BMW and other car manufacturers are also making big bets on self driving technology.
Regulations, not technology, are the primary obstacles for drone-based commercial services such as Amazon air, and just a few days ago, the FAA relaxed restrictions on commercial drone flights. It’s still not legal for Amazon to deliver packages to your door with drones, but that will soon change, and when that happens, commerce will never be the same.
Of course half a million consumer drone sales over the last holiday season implies that drones are going to change a lot more than commerce. Expect to see a lot more of them hovering obnoxiously in every metro area in the world in the coming years.
Augmented and virtual reality will fundamentally transform what it means to be human. As our senses are augmented by virtual constructs mixed seamlessly with the real world, we’ll find new ways to work, new ways to play, and new ways to interact with each other, including AR assisted learning, telepresence, and radical new experiences we haven’t dreamed of, yet.
All of these technologies require our gadgets to have an awareness of the surrounding environment, and the ability to respond behaviorally to environmental inputs. Self driving cars need to see obstacles and make corrections to avoid them. Drones need to detect collision hazards, wind, and the ground to land on. Room scale VR needs to alert you of the room boundaries so you don’t wander into walls, and AR devices need to detect tables, chairs, and desks, and walls, and allow virtual elements and characters to interact with them.
Processing sensory inputs and figuring out what they mean is one of the most important jobs that our brain is responsible for.
How does the human brain deal with the complexity of that job? With neurons.
Taken alone, a single neuron doesn’t do anything particularly interesting, but when combined together, neural networks are responsible for our ability to recognize the world around us, solve problems, and interact with our environment and the people around us.
Neural networks are the mechanism that allows us to use language, build tools, catch balls, type, read this article, remember things, and basically do all the things we consider to be “thinking”.
Recently, scientists have been scanning sections of small animal brains on the road to whole brain emulation. For example, a molecular-level model of the 302 neurons in the C. elegans roundworm.
The blue brain project is an attempt to do the same thing with a human brain. The research uses microscopes to scan slices of living human brain tissue. It’s an ambitious project that is still in its infancy a decade after it launched, but nobody expects it to be finished tomorrow.
We are still a long way from whole brain emulation for anything but the simplest organisms, but eventually, we may be able to emulate a whole human brain on a computer at the molecular level.
Before we try to emulate even basic neuron functionality ourselves, we should learn more about how neurons work.
A neuron is a cell that collects input signals (electrical potentials) from synaptic terminals (typically from dendrites, but sometimes directly on the cell membrane). When those signals sum past a certain threshold potential at the axon hillock trigger zone, it triggers an output signal, called an action potential.
The action potential travels along the output nerve fiber, called an axon. The axon splits into collateral branches which can carry the output signal to different parts of the neural network. Each axon branch terminates by splitting into clusters of tiny terminal branches, which interface with other neurons through synapses.
Synapse is the word used to describe the transmission mechanism from one neuron to the next.
There are two kinds of synapse receptors on the postsynaptic terminal wall: ion channels and metabolic channels.
Ion channels are fast (tens of milliseconds), and can either excite or inhibit the potential in the postsynaptic neuron, by opening channels for positively or negatively charged ions to enter the cell, respectively.
In an ionotropic transmission, the neurotransmitter is released from the presynaptic neuron into the synaptic cleft — a tiny gap between the terminals of the presynaptic neuron and the postsynaptic neuron. It binds to receptors on the postsynaptic terminal wall, which causes them to open, allowing electrically charged ions to flow into the postsynaptic cell, causing a change to the cell’s potential.
Metabolic channels are slower and more controlled than ion channels. In chemical transmissions, the action potential triggers the release of chemical transmitters from the presynaptic terminal into the synaptic cleft.
Those chemical transmitters bind to metabolic receptors which do not have ion channels of their own. That binding triggers chemical reactions on the inside of the cell wall to release G-proteins which can open ion channels connected to different receptors. As the G-proteins must first diffuse and rebind to neighboring channels, this process naturally takes longer.
The duration of metabolic effect can vary from about 100ms to several minutes, depending on how long it takes for neurotransmitters to be absorbed, released, diffused, or recycled back into the presynaptic terminal.
Like ion channels, the signal can be either exciting or inhibitory to the postsynaptic neuron potential.
There is also another type of synapse, called an electrical synapse. Unlike the chemical synapses described above, which rely on chemical neurotransmitters and receptors at axon terminals, an electrical synapse connects dendrites from one cell directly to dendrites of another cell by a gap junction, which is a channel that allows ions and other small molecules to pass directly between the cells, effectively creating one large neuron with multiple axons.
Cells connected by electrical synapses almost always fire simultaneously. When any connected cell fires, all connected cells fire with it. However, some gap junctions are one way.
Among other things, electrical synapses connect cells that control muscle groups such as the heart, where it’s important that all related cells cooperate, creating simultaneous muscle contractions.
Different synapses can have different strengths (called weights). A synapse weight can change over time through a process known as synaptic plasticity.
It is believed that changes in synapse connection strength is how we form memory. In other words, in order to learn and form memories, our brain literally rewires itself.
An increase in synaptic weight is called Long Term Potentiation (LTP).
A decrease in synaptic weight is called Long Term Depression (LTD).
If the postsynaptic neuron tends to fire a lot when the presynaptic neuron fires, the synaptic weight increases. If the cells don’t tend to fire together often, the connection weakens. In other words:
The key to synaptic plasticity is hidden in a pair of 20ms windows:
If the presynaptic neuron fires before the postsynaptic neuron within 20ms, the weight increases (LTP).
If the presynaptic neuron fires after the postsynaptic neuron within 20ms, the weight decreases (LTD).
This process is called spike-timing-dependent plasticity.
Spike-timing-dependent plasticity was discovered in the 1990’s and is still being explored, but it is believed that action potential backpropagation from the cell’s axon to the dendrites is involved in the LTP process.
During a typical forward-propagating event, glutamate will be released from the presynaptic terminal, which binds to AMPA receptors in the postsynaptic terminal wall, allowing positively charged sodium ions (Na+) into the cell.
If a large enough depolarization event occurs inside the cell (perhaps a backpropagation potential from the axon trigger point), electrostatic repulsion will open a magnesium block in NMDA receptors, allowing even more sodium to flood the cell along with calcium (Ca2+). At the same time, potassium (K+) flows out of the cell. These events themselves only last tens of milliseconds, but they have indirect lasting effects.
An influx of calcium causes extra AMPA receptors to be inserted into the cell membrane, which will allow more sodium ions into the cell during future action potential events from the presynaptic neuron.
A similar process works in reverse to trigger LTD.
During LTP events, a special class of proteins called growth factors can also form, which can cause new synapses to grow, strengthening the bond between the two cells. The impact of new synapse growth can be permanent, assuming that the neurons continue to fire together frequently.
Many artificial neurons act less like neurons and more like transistors with two simple states: on or off. If enough upstream neurons are on rather than off, the neuron is on. Otherwise, it’s off. Other neural nets use input values from -1 to +1. The basic math looks a little like the following:
This is a good idea if you want to conserve CPU power so you can emulate a lot more neurons, and we’ve been able to use these basic principles to accomplish very simple pattern recognition tasks, such as optical character recognition (OCR) using pre-trained networks. However, there’s a problem.
As I’ve described above, real neurons don’t behave that way. Instead, synapses transmit fluctuating continuous value potentials over time through the soma (cell body) to the axon hillock trigger zone where the sum of the signal may or may not trigger an action potential at any given moment in time. If the potential in the soma remains high, pulses may continue as the cell triggers at high frequency (once every few milliseconds).
Lots of variables influence the process, the trigger frequencies, and the pattern of action potential bursts. With the model presented above, how would you determine whether or not triggers occurred within the LTP/LTD windows?
What critical element is our basic model missing? Time.
But that’s a story for a different article. Stay tuned for part 2.
Eric Elliott is the author of “Programming JavaScript Applications” (O’Reilly), and “Learn JavaScript with Eric Elliott”. He has contributed to software experiences for Adobe Systems, Zumba Fitness, The Wall Street Journal, ESPN, BBC, and top recording artists including Usher, Frank Ocean, Metallica, and many more.
He spends most of his time in the San Francisco Bay Area with the most beautiful woman in the world.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Make some magic. #JavaScript
To submit, DM your proposal to @JS_Cheerleader on Twitter
",years ago working project needed adaptive essentially software needed learn get better frequently repeated task time id read neural networks early success people achieved decided try marked beginning lifelong fascination ai ai really big deal small handful technologies dramatically change world course next 25 years three biggest disruptors rely deeply ai self driving cars alone disrupt 10 million jobs america radically improve transportation shipping efficiency may lead huge change car ownership outsource transportation pains car ownership maintenance apps like uber youve probably heard googles self driving cars tesla mercedes bmw car manufacturers also making big bets self driving technology regulations technology primary obstacles dronebased commercial services amazon air days ago faa relaxed restrictions commercial drone flights still legal amazon deliver packages door drones soon change happens commerce never course half million consumer drone sales last holiday season implies drones going change lot commerce expect see lot hovering obnoxiously every metro area world coming years augmented virtual reality fundamentally transform means human senses augmented virtual constructs mixed seamlessly real world well find new ways work new ways play new ways interact including ar assisted learning telepresence radical new experiences havent dreamed yet technologies require gadgets awareness surrounding environment ability respond behaviorally environmental inputs self driving cars need see obstacles make corrections avoid drones need detect collision hazards wind ground land room scale vr needs alert room boundaries dont wander walls ar devices need detect tables chairs desks walls allow virtual elements characters interact processing sensory inputs figuring mean one important jobs brain responsible human brain deal complexity job neurons taken alone single neuron doesnt anything particularly interesting combined together neural networks responsible ability recognize world around us solve problems interact environment people around us neural networks mechanism allows us use language build tools catch balls type read article remember things basically things consider thinking recently scientists scanning sections small animal brains road whole brain emulation example molecularlevel model 302 neurons c elegans roundworm blue brain project attempt thing human brain research uses microscopes scan slices living human brain tissue ambitious project still infancy decade launched nobody expects finished tomorrow still long way whole brain emulation anything simplest organisms eventually may able emulate whole human brain computer molecular level try emulate even basic neuron functionality learn neurons work neuron cell collects input signals electrical potentials synaptic terminals typically dendrites sometimes directly cell membrane signals sum past certain threshold potential axon hillock trigger zone triggers output signal called action potential action potential travels along output nerve fiber called axon axon splits collateral branches carry output signal different parts neural network axon branch terminates splitting clusters tiny terminal branches interface neurons synapses synapse word used describe transmission mechanism one neuron next two kinds synapse receptors postsynaptic terminal wall ion channels metabolic channels ion channels fast tens milliseconds either excite inhibit potential postsynaptic neuron opening channels positively negatively charged ions enter cell respectively ionotropic transmission neurotransmitter released presynaptic neuron synaptic cleft tiny gap terminals presynaptic neuron postsynaptic neuron binds receptors postsynaptic terminal wall causes open allowing electrically charged ions flow postsynaptic cell causing change cells potential metabolic channels slower controlled ion channels chemical transmissions action potential triggers release chemical transmitters presynaptic terminal synaptic cleft chemical transmitters bind metabolic receptors ion channels binding triggers chemical reactions inside cell wall release gproteins open ion channels connected different receptors gproteins must first diffuse rebind neighboring channels process naturally takes longer duration metabolic effect vary 100ms several minutes depending long takes neurotransmitters absorbed released diffused recycled back presynaptic terminal like ion channels signal either exciting inhibitory postsynaptic neuron potential also another type synapse called electrical synapse unlike chemical synapses described rely chemical neurotransmitters receptors axon terminals electrical synapse connects dendrites one cell directly dendrites another cell gap junction channel allows ions small molecules pass directly cells effectively creating one large neuron multiple axons cells connected electrical synapses almost always fire simultaneously connected cell fires connected cells fire however gap junctions one way among things electrical synapses connect cells control muscle groups heart important related cells cooperate creating simultaneous muscle contractions different synapses different strengths called weights synapse weight change time process known synaptic plasticity believed changes synapse connection strength form memory words order learn form memories brain literally rewires increase synaptic weight called long term potentiation ltp decrease synaptic weight called long term depression ltd postsynaptic neuron tends fire lot presynaptic neuron fires synaptic weight increases cells dont tend fire together often connection weakens words key synaptic plasticity hidden pair 20ms windows presynaptic neuron fires postsynaptic neuron within 20ms weight increases ltp presynaptic neuron fires postsynaptic neuron within 20ms weight decreases ltd process called spiketimingdependent plasticity spiketimingdependent plasticity discovered 1990s still explored believed action potential backpropagation cells axon dendrites involved ltp process typical forwardpropagating event glutamate released presynaptic terminal binds ampa receptors postsynaptic terminal wall allowing positively charged sodium ions na cell large enough depolarization event occurs inside cell perhaps backpropagation potential axon trigger point electrostatic repulsion open magnesium block nmda receptors allowing even sodium flood cell along calcium ca2 time potassium k flows cell events last tens milliseconds indirect lasting effects influx calcium causes extra ampa receptors inserted cell membrane allow sodium ions cell future action potential events presynaptic neuron similar process works reverse trigger ltd ltp events special class proteins called growth factors also form cause new synapses grow strengthening bond two cells impact new synapse growth permanent assuming neurons continue fire together frequently many artificial neurons act less like neurons like transistors two simple states enough upstream neurons rather neuron otherwise neural nets use input values 1 1 basic math looks little like following good idea want conserve cpu power emulate lot neurons weve able use basic principles accomplish simple pattern recognition tasks optical character recognition ocr using pretrained networks however theres problem ive described real neurons dont behave way instead synapses transmit fluctuating continuous value potentials time soma cell body axon hillock trigger zone sum signal may may trigger action potential given moment time potential soma remains high pulses may continue cell triggers high frequency every milliseconds lots variables influence process trigger frequencies pattern action potential bursts model presented would determine whether triggers occurred within ltpltd windows critical element basic model missing time thats story different article stay tuned part 2 eric elliott author programming javascript applications oreilly learn javascript eric elliott contributed software experiences adobe systems zumba fitness wall street journal espn bbc top recording artists including usher frank ocean metallica many spends time san francisco bay area beautiful woman world quick cheer standing ovation clap show much enjoyed story make magic javascript submit dm proposal js_cheerleader twitter,en,"['Google', 'Mercedes', 'BMW', 'Amazon', 'FAA', 'Metabolic', 'LTD', 'LTP', 'NMDA', 'K+', 'LTP/LTD', 'Adobe Systems', 'The Wall Street Journal', 'ESPN', 'BBC', 'DM']"
201,Dhruv Parthasarathy,665,Write an AI to win at Pong from scratch with Reinforcement Learning,"There’s a huge difference between reading about Reinforcement Learning and actually implementing it.
In this post, you’ll implement a Neural Network for Reinforcement Learning and see it learn more and more as it finally becomes good enough to beat the computer in Pong! You can play around with other such Atari games at the OpenAI Gym.
By the end of this post, you’ll be able to do the following:
The code and the idea are all tightly based on Andrej Karpathy’s blog post. The code in me_pong.py is intended to be a simpler to follow version of pong.py which was written by Dr. Karpathy.
To follow along, you’ll need to know the following:
If you want a deeper dive into the material at hand, read the blog post on which all of this is based. This post is meant to be a simpler introduction to that material.
Great! Let’s get started.
We are given the following:
Can we use these pieces to train our agent to beat the computer? Moreover, can we make our solution generic enough so it can be reused to win in games that aren’t pong?
Indeed, we can! Andrej does this by building a Neural Network that takes in each image and outputs a command to our AI to move up or down.
We can break this down a bit more into the following steps:
Our Neural Network, based heavily on Andrej’s solution, will do the following:
Ok now that we’ve described the problem and its solution, let’s get to writing some code!
We’re now going to follow the code in me_pong.py. Please keep it open and read along! The code starts here:
First, let’s use OpenAI Gym to make a game environment and get our very first image of the game.
Next, we set a bunch of parameters based off of Andrej’s blog post. We aren’t going to worry about tuning them but note that you can probably get better performance by doing so. The parameters we will use are:
Then, we set counters, initial values, and the initial weights in our Neural Network.
Weights are stored in matrices. Layer 1 of our Neural Network is a 200 x 6400 matrix representing the weights for our hidden layer. For layer 1, element w1_ij represents the weight of neuron i for input pixel j in layer 1.
Layer 2 is a 200 x 1 matrix representing the weights of the output of the hidden layer on our final output. For layer 2, element w2_i represents the weights we place on the activation of neuron i in the hidden layer.
We initialize each layer’s weights with random numbers for now. We divide by the square root of the number of the dimension size to normalize our weights.
Next, we set up the initial parameters for RMSProp (a method for updating weights that we will discuss later). Don’t worry too much about understanding what you see below. I’m mainly bringing it up here so we can continue to follow along the main code block.
We’ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result. The below sets up the arrays where we’ll collect all that information.
Ok we’re all done with the setup! If you were following, it should look something like this:
Phew. Now for the fun part!
The crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We’ll put everything in a while block for now but in reality you might set up a break condition to stop the process.
The first step to our algorithm is processing the image of the game that OpenAI Gym passed us. We really don’t care about the entire image - just certain details. We do this below:
Let’s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. The basic steps are:
Now that we’ve preprocessed the observations, let’s move on to actually sending the observations through our neural net to generate the probability of telling our AI to move up. Here are the steps we’ll take:
How exactly does apply_neural_nets take observations and weights and generate a probability of going up? This is just the forward pass of the Neural Network. Let’s look at the code below for more information:
As you can see, it’s not many steps at all! Let’s go step by step:
Let’s return to the main algorithm and continue on. Now that we have obtained a probability of going up, we need to now record the results for later learning and choose an action to tell our AI to implement:
We choose an action by flipping an imaginary coin that lands “up” with probability up_probability and down with 1 - up_probability. If it lands up, we choose tell our AI to go up and if not, we tell it to go down. We also
Having done that, we pass the action to OpenAI Gym via env.step(action).
Ok we’ve covered the first half of the solution! We know what action to tell our AI to take. If you’ve been following along, your code should look like this:
Now that we’ve made our move, it’s time to start learning so we figure out the right weights in our Neural Network!
Learning is all about seeing the result of the action (i.e. whether or not we won the round) and changing our weights accordingly. The first step to learning is asking the following question:
Mathematically, this is just the derivative of our result with respect to the outputs of our final layer. If L is the value of our result to us and f is the function that gives us the activations of our final layer, this derivative is just ∂L/∂f.
In a binary classification context (i.e. we just have to tell the AI one of two actions, up or down), this derivative turns out to be
Note that σ in the above equation represents the sigmoid function. Read the Attribute Classification section here for more information about how we get the above derivative. We simplify this further below:
After one action(moving the paddle up or down), we don’t really have an idea of whether or not this was the right action. So we’re going to cheat and treat the action we end up sampling from our probability as the correct action.
Our predicion for this round is going to be the probability of going up we calculated. Using that, we have that ∂L/∂f can be computed by
Awesome! We have the gradient per action.
The next step is to figure out how we learn after the end of an episode (i.e. when we or our opponent miss the ball and someone gets a point). We do this by computing the policy gradient of the network at the end of each episode. The intuition here is that if we won the round, we’d like our network to generate more of the actions that led to us winning. Alternatively, if we lose, we’re going to try and generate less of these actions.
OpenAI Gym provides us the handy done variable to tell us when an episode finishes (i.e. we missed the ball or our opponent missed the ball). When we notice we are done, the first thing we do is compile all our observations and gradient calculations for the episode. This allows us to apply our learnings over all the actions in the episode.
Next, we want to learn in such a way that actions taken towards the end of an episode more heavily influence our learning than actions taken at the beginning. This is called discounting.
Think about it this way - if you moved up at the first frame of the episode, it probably had very little impact on whether or not you win. However, closer to the end of the episode, your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball.
We’re going to take this weighting into account by discounting our rewards such that rewards from earlier frames are discounted a lot more than rewards for later frames. After this, we’re going to finally use backpropagation to compute the gradient (i.e. the direction we need to move our weights to improve).
Let’s dig in a bit into how the policy gradient for the episode is computed. This is one of the most important parts of Reinforcement Learning as it’s how our agent figures out how to improve over time.
To begin with, if you haven’t already, read this excerpt on backpropagation from Michael Nielsen’s excellent free book on Deep Learning.
As you’ll see in that excerpt, there are four fundamental equations of backpropogation, a technique for computing the gradient for our weights.
Our goal is to find ∂C/∂w1 (BP4), the derivative of the cost function with respect to the first layer’s weights, and ∂C/∂w2, the derivative of the cost function with respect to the second layer’s weights. These gradients will help us understand what direction to move our weights in for the greatest improvement.
To begin with, let’s start with ∂C/∂w2. If a^l2 is the activations of the hidden layer (layer 2), we see that the formula is:
Indeed, this is exactly what we do here:
Next, we need to calculate ∂C/∂w1. The formula for that is:
and we also know that a^l1 is just our observation_values.
So all we need now is δ^l2. Once we have that, we can calculate ∂C/∂w1 and return. We do just that below:
If you’ve been following along, your function should look like this:
With that, we’ve finished backpropagation and computed our gradients!
After we have finished batch_size episodes, we finally update our weights for our Neural Network and implement our learnings.
To update the weights, we simply apply RMSProp, an algorithm for updating weights described by Sebastian Reuder here.
We implement this below:
This is the step that tweaks our weights and allows us to get better over time.
This is basically it! Putting it altogether it should look like this.
You just coded a full Neural Network for playing Pong! Uncomment env.render() and run it for 3–4 days to see it finally beat the computer! You’ll need to do some pickling as done in Andrej Karpathy’s solution to be able to visualize your results when you win.
According to the blog post, this algorithm should take around 3 days of training on a Macbook to start beating the computer.
Consider tweaking the parameters or using Convolutional Neural Nets to boost the performance further.
If you want a further primer into Neural Networks and Reinforcement Learning, there are some great resources to learn more (I work at Udacity as the Director of Machine Learning programs):
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@dhruvp. VP Eng @Athelas. MIT Math and CS Undergrad ’13. MIT CS Masters ’14. Previously: Director of AI Programs @ Udacity.
",theres huge difference reading reinforcement learning actually implementing post youll implement neural network reinforcement learning see learn finally becomes good enough beat computer pong play around atari games openai gym end post youll able following code idea tightly based andrej karpathys blog post code me_pongpy intended simpler follow version pongpy written dr karpathy follow along youll need know following want deeper dive material hand read blog post based post meant simpler introduction material great lets get started given following use pieces train agent beat computer moreover make solution generic enough reused win games arent pong indeed andrej building neural network takes image outputs command ai move break bit following steps neural network based heavily andrejs solution following ok weve described problem solution lets get writing code going follow code me_pongpy please keep open read along code starts first lets use openai gym make game environment get first image game next set bunch parameters based andrejs blog post arent going worry tuning note probably get better performance parameters use set counters initial values initial weights neural network weights stored matrices layer 1 neural network 200 x 6400 matrix representing weights hidden layer layer 1 element w1_ij represents weight neuron input pixel j layer 1 layer 2 200 x 1 matrix representing weights output hidden layer final output layer 2 element w2_i represents weights place activation neuron hidden layer initialize layers weights random numbers divide square root number dimension size normalize weights next set initial parameters rmsprop method updating weights discuss later dont worry much understanding see im mainly bringing continue follow along main code block well need collect bunch observations intermediate values across episode use compute gradient end based result sets arrays well collect information ok done setup following look something like phew fun part crux algorithm going live loop continually make move learn based results move well put everything block reality might set break condition stop process first step algorithm processing image game openai gym passed us really dont care entire image certain details lets dive preprocess_observations see convert image openai gym gives us something use train neural network basic steps weve preprocessed observations lets move actually sending observations neural net generate probability telling ai move steps well take exactly apply_neural_nets take observations weights generate probability going forward pass neural network lets look code information see many steps lets go step step lets return main algorithm continue obtained probability going need record results later learning choose action tell ai implement choose action flipping imaginary coin lands probability up_probability 1 up_probability lands choose tell ai go tell go also done pass action openai gym via envstepaction ok weve covered first half solution know action tell ai take youve following along code look like weve made move time start learning figure right weights neural network learning seeing result action ie whether round changing weights accordingly first step learning asking following question mathematically derivative result respect outputs final layer l value result us f function gives us activations final layer derivative lf binary classification context ie tell ai one two actions derivative turns note equation represents sigmoid function read attribute classification section information get derivative simplify one actionmoving paddle dont really idea whether right action going cheat treat action end sampling probability correct action predicion round going probability going calculated using lf computed awesome gradient per action next step figure learn end episode ie opponent miss ball someone gets point computing policy gradient network end episode intuition round wed like network generate actions led us winning alternatively lose going try generate less actions openai gym provides us handy done variable tell us episode finishes ie missed ball opponent missed ball notice done first thing compile observations gradient calculations episode allows us apply learnings actions episode next want learn way actions taken towards end episode heavily influence learning actions taken beginning called discounting think way moved first frame episode probably little impact whether win however closer end episode actions probably much larger effect determine whether paddle reaches ball paddle hits ball going take weighting account discounting rewards rewards earlier frames discounted lot rewards later frames going finally use backpropagation compute gradient ie direction need move weights improve lets dig bit policy gradient episode computed one important parts reinforcement learning agent figures improve time begin havent already read excerpt backpropagation michael nielsens excellent free book deep learning youll see excerpt four fundamental equations backpropogation technique computing gradient weights goal find cw1 bp4 derivative cost function respect first layers weights cw2 derivative cost function respect second layers weights gradients help us understand direction move weights greatest improvement begin lets start cw2 al2 activations hidden layer layer 2 see formula indeed exactly next need calculate cw1 formula also know al1 observation_values need l2 calculate cw1 return youve following along function look like weve finished backpropagation computed gradients finished batch_size episodes finally update weights neural network implement learnings update weights simply apply rmsprop algorithm updating weights described sebastian reuder implement step tweaks weights allows us get better time basically putting altogether look like coded full neural network playing pong uncomment envrender run 34 days see finally beat computer youll need pickling done andrej karpathys solution able visualize results win according blog post algorithm take around 3 days training macbook start beating computer consider tweaking parameters using convolutional neural nets boost performance want primer neural networks reinforcement learning great resources learn work udacity director machine learning programs quick cheer standing ovation clap show much enjoyed story dhruvp vp eng athelas mit math cs undergrad 13 mit cs masters 14 previously director ai programs udacity,en,"['Neural Network for Reinforcement Learning', 'Atari', 'a Neural Network', 'Our Neural Network', 'RMSProp', 'algorithm', 'the Neural Network', 'Attribute Classification', 'Neural Network', 'Convolutional Neural Nets', 'Neural Networks and Reinforcement Learning', 'Udacity', 'VP Eng @Athelas', 'CS', 'MIT CS Masters', 'AI Programs @ Udacity']"
202,Waleed Abdulla,507,Traffic Sign Recognition with TensorFlow – Waleed Abdulla – Medium,"This is part 1 of a series about building a deep learning model to recognize traffic signs. It’s intended to be a learning experience, for myself and for anyone else who likes to follow along. There are a lot of resources that cover the theory and math of neural networks, so I’ll focus on the practical aspects instead. I’ll describe my own experience building this model and share the source code and relevant materials. This is suitable for those who know Python and the basics of machine learning already, but want hands on experience and to practice building a real application.
In this part, I’ll talk about image classification and I’ll keep the model as simple as possible. In later parts, I’ll cover convolutional networks, data augmentation, and object detection.
The source code is available in this Jupyter notebook. I’m using Python 3.5 and TensorFlow 0.12. If you prefer to run the code in Docker, you can use my Docker image that contains many popular deep learning tools. Run it with this command:
Note that my project directory is in ~/traffic and I’m mapping it to the /traffic directory in the Docker container. Modify this if you’re using a different directory.
My first challenge was finding a good training dataset. Traffic sign recognition is a well studied problem, so I figured I’ll find something online.
I started by googling “traffic sign dataset” and found several options. I picked the Belgian Traffic Sign Dataset because it was big enough to train on, and yet small enough to be easy to work with.
You can download the dataset from http://btsd.ethz.ch/shareddata/. There are a lot of datasets on that page, but you only need the two files listed under BelgiumTS for Classification (cropped images):
After expanding the files, this is my directory structure. Try to match it so you can run the code without having to change the paths:
Each of the two directories contain 62 subdirectories, named sequentially from 00000 to 00061. The directory names represent the labels, and the images inside each directory are samples of each label.
Or, if you prefer to sound more formal: do Exploratory Data Analysis. It’s tempting to skip this part, but I’ve found that the code I write to examine the data ends up being used a lot throughout the project. I usually do this in Jupyter notebooks and share them with the team. Knowing your data well from the start saves you a lot of time later.
The images in this dataset are in an old .ppm format. So old, in fact, that most tools don’t support it. Which meant that I couldn’t casually browse the folders to take a look at the images. Luckily, the Scikit Image library recognizes this format. This code will load the data and return two lists: images and labels.
This is a small dataset so I’m loading everything into RAM to keep it simple. For larger datasets, you’d want to load the data in batches.
After loading the images into Numpy arrays, I display a sample image of each label. See code in the notebook. This is our dataset:
Looks like a good training set. The image quality is great, and there are a variety of angles and lighting conditions. More importantly, the traffic signs occupy most of the area of each image, which allows me to focus on object classification and not have to worry about finding the location of the traffic sign in the image (object detection). I’ll get to object detection in a future post.
The first thing I noticed from the samples above is that images are square-ish, but have different aspect ratios. My neural network will take a fixed-size input, so I have some preprocessing to do. I’ll get to that soon, but first let’s pick one label and see more of its images. Here is an example of label 32:
It looks like the dataset considers all speed limit signs to be of the same class, regardless of the numbers on them. That’s fine, as long as we know about it beforehand and know what to expect. That’s why understanding your dataset is so important and can save you a lot of pain and confusion later.
I’ll leave exploring the other labels to you. Labels 26 and 27 are interesting to check. They also have numbers in red circles, so the model will have to get really good to differentiate between them.
Most image classification networks expect images of a fixed size, and our first model will do as well. So we need to resize all the images to the same size.
But since the images have different aspect ratios, then some of them will be stretched vertically or horizontally. Is that a problem? I think it’s not in this case, because the differences in aspect ratios are not that large. My own criteria is that if a person can recognize the images when they’re stretched then the model should be able to do so as well.
What are the sizes of the images anyway? Let’s print a few examples:
The sizes seem to hover around 128x128. I could use that size to preserve as much information as possible, but in early development I prefer to use a smaller size because it leads to faster training, which allows me to iterate faster. I experimented with 16x16 and 20x20, but they were too small. I ended up picking 32x32 which is easy to recognize (see below) and reduces the size of the model and training data by a factor of 16 compared to 128x128.
I’m also in the habit of printing the min() and max() values often. It’s a simple way to verify the range of the data and catch bugs early. This tells me that the image colors are the standard range of 0–255.
We’re getting to the interesting part! Continuing the theme of keeping it simple, I started with the simplest possible model: A one layer network that consists of one neuron per label.
This network has 62 neurons and each neuron takes the RGB values of all pixels as input. Effectively, each neuron receives 32*32*3=3072 inputs. This is a fully-connected layer because every neuron connects to every input value. You’re probably familiar with its equation:
I start with a simple model because it’s easy to explain, easy to debug, and fast to train. Once this works end to end, expanding on it is much easier than building something complex from the start.
TensorFlow encapsulates the architecture of a neural network in an execution graph. The graph consists of operations (Ops for short) such as Add, Multiply, Reshape, ...etc. These ops perform actions on data in tensors (multidimensional arrays).
I’ll go through the code to build the graph step by step below, but here is the full code if you prefer to scan it first:
First, I create the Graph object. TensorFlow has a default global graph, but I don’t recommend using it. Global variables are bad in general because they make it too easy to introduce bugs. I prefer to create the graph explicitly.
Then I define Placeholders for the images and labels. The placeholders are TensorFlow’s way of receiving input from the main program. Notice that I create the placeholders (and all other ops) inside the block of with graph.as_default(). This is so they become part of my graph object rather than the global graph.
The shape of the images_ph placeholder is [None, 32, 32, 3]. It stands for [batch size, height, width, channels] (often shortened as NHWC) . The None for batch size means that the batch size is flexible, which means that we can feed different batch sizes to the model without having to change the code. Pay attention to the order of your inputs because some models and frameworks might use a different arrangement, such as NCHW.
Next, I define the fully connected layer. Rather than implementing the raw equation, y = xW + b, I use a handy function that does that in one line and also applies the activation function. It expects input as a one-dimensional vector, though. So I flatten the images first.
I’m using the ReLU activation function here:
It simply converts all negative values to zeros. It’s been shown to work well in classification tasks and trains faster than sigmoid or tanh. For more background, check here and here.
The output of the fully connected layer is a logits vector of length 62 (technically, it’s [None, 62] because we’re dealing with a batch of logits vectors).
A row in the logits tensor might look like this: [0.3, 0, 0, 1.2, 2.1, .01, 0.4, ....., 0, 0]. The higher the value, the more likely that the image represents that label. Logits are not probabilities, though — They can have any value, and they don’t add up to 1. The actual absolute values of the logits are not important, just their values relative to each other. It’s easy to convert logits to probabilities using the softmax function if needed (it’s not needed here).
In this application, we just need the index of the largest value, which corresponds to the id of the label. The argmax op does that.
The argmax output will be integers in the range 0 to 61.
Choosing the right loss function is an area of research in and of itself, which I won’t delve into it here other than to say that cross-entropy is the most common function for classification tasks. If you’re not familiar with it, there is a really good explanation here and here.
Cross-entropy is a measure of difference between two vectors of probabilities. So we need to convert labels and the logits to probability vectors. The function sparse_softmax_cross_entropy_with_logits() simplifies that. It takes the generated logits and the groundtruth labels and does three things: converts the label indexes of shape [None] to logits of shape [None, 62] (one-hot vectors), then it runs softmax to convert both prediction logits and label logits to probabilities, and finally calculates the cross-entropy between the two. This generates a loss vector of shape [None] (1D of length = batch size), which we pass through reduce_mean() to get one single number that represents the loss value.
Choosing the optimization algorithm is another decision to make. I usually use the ADAM optimizer because it’s been shown to converge faster than simple gradient descent. This post does a great job comparing different gradient descent optimizers.
The last node in the graph is the initialization op, which simply sets the values of all variables to zeros (or to random values or whatever the variables are set to initialize to).
Notice that the code above doesn’t execute any of the ops yet. It’s just building the graph and describing its inputs. The variables we defined above, such as init, loss, predicted_labels don’t contain numerical values. They are references to ops that we’ll execute next.
This is where we iteratively train the model to minimize the loss function. Before we start training, though, we need to create a Session object.
I mentioned the Graph object earlier and how it holds all the Ops of the model. The Session, on the other hand, holds the values of all the variables. If a graph holds the equation y=xW+b then the session holds the actual values of these variables.
Usually the first thing to run after starting a session is the initialization op, init, to initialize the variables.
Then we start the training loop and run the train op repeatedly. While not necessary, it’s useful to run the loss op as well to print its values and monitor the progress of the training.
In case you’re wondering, I set the loop to 201 so that the i % 10 condition is satisfied in the last round and prints the last loss value. The output should look something like this:
Now we have a trained model in memory in the Session object. To use it, we call session.run() just like in the training code. The predicted_labels op returns the output of the argmax() function, so that’s what we need to run. Here I classify 10 random images and print both, the predictions and the groundtruth labels for comparison.
In the notebook, I include a function to visualize the results as well. It generates something like this:
The visualization shows that the model is working , but doesn’t quantify how accurate it is. And you might’ve noticed that it’s classifying the training images, so we don’t know yet if the model generalizes to images that it hasn’t seen before. Next, we calculate a better evaluation metric.
To properly measure how the model generalizes to data it hasn’t seen, I do the evaluation on test data that I didn’t use in training. The BelgiumTS dataset makes this easy by providing two separate sets, one for training and one for testing.
In the notebook I load the test set, resize the images to 32x32, and then calculate the accuracy. This is the relevant part of the code that calculates the accuracy.
The accuracy I get in each run ranges between 0.40 and 0.70 depending on whether the model lands on a local minimum or a global minimum. This is expected when running a simple model like this one. In a future post I’ll talk about ways to improve the consistency of the results.
Congratulations! We have a working simple neural network. Given how simple this neural network is, training takes just a minute on my laptop so I didn’t bother saving the trained model. In the next part, I’ll add code to save and load trained models and expand to use multiple layers, convolutional networks, and data augmentation. Stay tuned!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Startups, deep learning, computer vision.
",part 1 series building deep learning model recognize traffic signs intended learning experience anyone else likes follow along lot resources cover theory math neural networks ill focus practical aspects instead ill describe experience building model share source code relevant materials suitable know python basics machine learning already want hands experience practice building real application part ill talk image classification ill keep model simple possible later parts ill cover convolutional networks data augmentation object detection source code available jupyter notebook im using python 35 tensorflow 012 prefer run code docker use docker image contains many popular deep learning tools run command note project directory traffic im mapping traffic directory docker container modify youre using different directory first challenge finding good training dataset traffic sign recognition well studied problem figured ill find something online started googling traffic sign dataset found several options picked belgian traffic sign dataset big enough train yet small enough easy work download dataset httpbtsdethzchshareddata lot datasets page need two files listed belgiumts classification cropped images expanding files directory structure try match run code without change paths two directories contain 62 subdirectories named sequentially 00000 00061 directory names represent labels images inside directory samples label prefer sound formal exploratory data analysis tempting skip part ive found code write examine data ends used lot throughout project usually jupyter notebooks share team knowing data well start saves lot time later images dataset old ppm format old fact tools dont support meant couldnt casually browse folders take look images luckily scikit image library recognizes format code load data return two lists images labels small dataset im loading everything ram keep simple larger datasets youd want load data batches loading images numpy arrays display sample image label see code notebook dataset looks like good training set image quality great variety angles lighting conditions importantly traffic signs occupy area image allows focus object classification worry finding location traffic sign image object detection ill get object detection future post first thing noticed samples images squareish different aspect ratios neural network take fixedsize input preprocessing ill get soon first lets pick one label see images example label 32 looks like dataset considers speed limit signs class regardless numbers thats fine long know beforehand know expect thats understanding dataset important save lot pain confusion later ill leave exploring labels labels 26 27 interesting check also numbers red circles model get really good differentiate image classification networks expect images fixed size first model well need resize images size since images different aspect ratios stretched vertically horizontally problem think case differences aspect ratios large criteria person recognize images theyre stretched model able well sizes images anyway lets print examples sizes seem hover around 128x128 could use size preserve much information possible early development prefer use smaller size leads faster training allows iterate faster experimented 16x16 20x20 small ended picking 32x32 easy recognize see reduces size model training data factor 16 compared 128x128 im also habit printing min max values often simple way verify range data catch bugs early tells image colors standard range 0255 getting interesting part continuing theme keeping simple started simplest possible model one layer network consists one neuron per label network 62 neurons neuron takes rgb values pixels input effectively neuron receives 323233072 inputs fullyconnected layer every neuron connects every input value youre probably familiar equation start simple model easy explain easy debug fast train works end end expanding much easier building something complex start tensorflow encapsulates architecture neural network execution graph graph consists operations ops short add multiply reshape etc ops perform actions data tensors multidimensional arrays ill go code build graph step step full code prefer scan first first create graph object tensorflow default global graph dont recommend using global variables bad general make easy introduce bugs prefer create graph explicitly define placeholders images labels placeholders tensorflows way receiving input main program notice create placeholders ops inside block graphas_default become part graph object rather global graph shape images_ph placeholder none 32 32 3 stands batch size height width channels often shortened nhwc none batch size means batch size flexible means feed different batch sizes model without change code pay attention order inputs models frameworks might use different arrangement nchw next define fully connected layer rather implementing raw equation xw b use handy function one line also applies activation function expects input onedimensional vector though flatten images first im using relu activation function simply converts negative values zeros shown work well classification tasks trains faster sigmoid tanh background check output fully connected layer logits vector length 62 technically none 62 dealing batch logits vectors row logits tensor might look like 03 0 0 12 21 01 04 0 0 higher value likely image represents label logits probabilities though value dont add 1 actual absolute values logits important values relative easy convert logits probabilities using softmax function needed needed application need index largest value corresponds id label argmax op argmax output integers range 0 61 choosing right loss function area research wont delve say crossentropy common function classification tasks youre familiar really good explanation crossentropy measure difference two vectors probabilities need convert labels logits probability vectors function sparse_softmax_cross_entropy_with_logits simplifies takes generated logits groundtruth labels three things converts label indexes shape none logits shape none 62 onehot vectors runs softmax convert prediction logits label logits probabilities finally calculates crossentropy two generates loss vector shape none 1d length batch size pass reduce_mean get one single number represents loss value choosing optimization algorithm another decision make usually use adam optimizer shown converge faster simple gradient descent post great job comparing different gradient descent optimizers last node graph initialization op simply sets values variables zeros random values whatever variables set initialize notice code doesnt execute ops yet building graph describing inputs variables defined init loss predicted_labels dont contain numerical values references ops well execute next iteratively train model minimize loss function start training though need create session object mentioned graph object earlier holds ops model session hand holds values variables graph holds equation yxwb session holds actual values variables usually first thing run starting session initialization op init initialize variables start training loop run train op repeatedly necessary useful run loss op well print values monitor progress training case youre wondering set loop 201 10 condition satisfied last round prints last loss value output look something like trained model memory session object use call sessionrun like training code predicted_labels op returns output argmax function thats need run classify 10 random images print predictions groundtruth labels comparison notebook include function visualize results well generates something like visualization shows model working doesnt quantify accurate mightve noticed classifying training images dont know yet model generalizes images hasnt seen next calculate better evaluation metric properly measure model generalizes data hasnt seen evaluation test data didnt use training belgiumts dataset makes easy providing two separate sets one training one testing notebook load test set resize images 32x32 calculate accuracy relevant part code calculates accuracy accuracy get run ranges 040 070 depending whether model lands local minimum global minimum expected running simple model like one future post ill talk ways improve consistency results congratulations working simple neural network given simple neural network training takes minute laptop didnt bother saving trained model next part ill add code save load trained models expand use multiple layers convolutional networks data augmentation stay tuned quick cheer standing ovation clap show much enjoyed story startups deep learning computer vision,en,"['the Belgian Traffic Sign Dataset', 'Jupyter', 'Scikit Image', 'RAM', 'RGB', 'TensorFlow', 'NCHW', 'Cross', 'algorithm', 'ADAM', 'Session']"
203,gk_,1800,Text Classification using Neural Networks – Machine Learnings,"Understanding how chatbots work is important. A fundamental piece of machinery inside a chat-bot is the text classifier. Let’s look at the inner workings of an artificial neural network (ANN) for text classification.
We’ll use 2 layers of neurons (1 hidden layer) and a “bag of words” approach to organizing our training data. Text classification comes in 3 flavors: pattern matching, algorithms, neural nets. While the algorithmic approach using Multinomial Naive Bayes is surprisingly effective, it suffers from 3 fundamental flaws:
As with its ‘Naive’ counterpart, this classifier isn’t attempting to understand the meaning of a sentence, it’s trying to classify it. In fact so called “AI chat-bots” do not understand language, but that’s another story.
Let’s examine our text classifier one section at a time. We will take the following steps:
The code is here, we’re using iPython notebook which is a super productive way of working on data science projects. The code syntax is Python.
We begin by importing our natural language toolkit. We need a way to reliably tokenize sentences into words and a way to stem words.
And our training data, 12 sentences belonging to 3 classes (‘intents’).
We can now organize our data structures for documents, classes and words.
Notice that each word is stemmed and lower-cased. Stemming helps the machine equate words like “have” and “having”. We don’t care about case.
Our training data is transformed into “bag of words” for each sentence.
The above step is a classic in text classification: each training sentence is reduced to an array of 0’s and 1’s against the array of unique words in the corpus.
is stemmed:
then transformed to input: a 1 for each word in the bag (the ? is ignored)
and output: the first class
Note that a sentence could be given multiple classes, or none.
Make sure the above makes sense and play with the code until you grok it.
Next we have our core functions for our 2-layer neural network.
If you are new to artificial neural networks, here is how they work.
We use numpy because we want our matrix multiplication to be fast.
We use a sigmoid function to normalize values and its derivative to measure the error rate. Iterating and adjusting until our error rate is acceptably low.
Also below we implement our bag-of-words function, transforming an input sentence into an array of 0’s and 1’s. This matches precisely with our transform for training data, always crucial to get this right.
And now we code our neural network training function to create synaptic weights. Don’t get too excited, this is mostly matrix multiplication — from middle-school math class.
We are now ready to build our neural network model, we will save this as a json structure to represent our synaptic weights.
You should experiment with different ‘alpha’ (gradient descent parameter) and see how it affects the error rate. This parameter helps our error adjustment find the lowest error rate:
synapse_0 += alpha * synapse_0_weight_update
We use 20 neurons in our hidden layer, you can adjust this easily. These parameters will vary depending on the dimensions and shape of your training data, tune them down to ~10^-3 as a reasonable error rate.
The synapse.json file contains all of our synaptic weights, this is our model.
This classify() function is all that’s needed for the classification once synapse weights have been calculated: ~15 lines of code.
The catch: if there’s a change to the training data our model will need to be re-calculated. For a very large dataset this could take a non-insignificant amount of time.
We can now generate the probability of a sentence belonging to one (or more) of our classes. This is super fast because it’s dot-product calculation in our previously defined think() function.
Experiment with other sentences and different probabilities, you can then add training data and improve/expand the model. Notice the solid predictions with scant training data.
Some sentences will produce multiple predictions (above a threshold). You will need to establish the right threshold level for your application. Not all text classification scenarios are the same: some predictive situations require more confidence than others.
The last classification shows some internal details:
Notice the bag-of-words (bow) for the sentence, 2 words matched our corpus. The neural-net also learns from the 0’s, the non-matching words.
A low-probability classification is easily shown by providing a sentence where ‘a’ (common word) is the only match, for example:
Here you have a fundamental piece of machinery for building a chat-bot, capable of handling a large # of classes (‘intents’) and suitable for classes with limited or extensive training data (‘patterns’). Adding one or more responses to an intent is trivial.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Philosopher, Entrepreneur, Investor
Understand how machine learning and artificial intelligence will change your work & life.
",understanding chatbots work important fundamental piece machinery inside chatbot text classifier lets look inner workings artificial neural network ann text classification well use 2 layers neurons 1 hidden layer bag words approach organizing training data text classification comes 3 flavors pattern matching algorithms neural nets algorithmic approach using multinomial naive bayes surprisingly effective suffers 3 fundamental flaws naive counterpart classifier isnt attempting understand meaning sentence trying classify fact called ai chatbots understand language thats another story lets examine text classifier one section time take following steps code using ipython notebook super productive way working data science projects code syntax python begin importing natural language toolkit need way reliably tokenize sentences words way stem words training data 12 sentences belonging 3 classes intents organize data structures documents classes words notice word stemmed lowercased stemming helps machine equate words like dont care case training data transformed bag words sentence step classic text classification training sentence reduced array 0s 1s array unique words corpus stemmed transformed input 1 word bag ignored output first class note sentence could given multiple classes none make sure makes sense play code grok next core functions 2layer neural network new artificial neural networks work use numpy want matrix multiplication fast use sigmoid function normalize values derivative measure error rate iterating adjusting error rate acceptably low also implement bagofwords function transforming input sentence array 0s 1s matches precisely transform training data always crucial get right code neural network training function create synaptic weights dont get excited mostly matrix multiplication middleschool math class ready build neural network model save json structure represent synaptic weights experiment different alpha gradient descent parameter see affects error rate parameter helps error adjustment find lowest error rate synapse_0 alpha synapse_0_weight_update use 20 neurons hidden layer adjust easily parameters vary depending dimensions shape training data tune 103 reasonable error rate synapsejson file contains synaptic weights model classify function thats needed classification synapse weights calculated 15 lines code catch theres change training data model need recalculated large dataset could take noninsignificant amount time generate probability sentence belonging one classes super fast dotproduct calculation previously defined think function experiment sentences different probabilities add training data improveexpand model notice solid predictions scant training data sentences produce multiple predictions threshold need establish right threshold level application text classification scenarios predictive situations require confidence others last classification shows internal details notice bagofwords bow sentence 2 words matched corpus neuralnet also learns 0s nonmatching words lowprobability classification easily shown providing sentence common word match example fundamental piece machinery building chatbot capable handling large classes intents suitable classes limited extensive training data patterns adding one responses intent trivial quick cheer standing ovation clap show much enjoyed story philosopher entrepreneur investor understand machine learning artificial intelligence change work life,en,"['ANN', 'Multinomial Naive Bayes', 'iPython']"
204,nafrondel,1700,You requested someone with a degree in this? *Holds up hand*,"You requested someone with a degree in this? *Holds up hand*
So there are two main schools of Artificial Intelligence — Symbolic and non-symbolic.
Symbolic says the best way to make AI is to make an expert AI — e.g. if you want a doctor AI, you feed it medical text books and it answers questions by looking it up in the text book.
Non-symbolic says the best way to make AI is to decide that computers are better at understanding in computer, so give the information to the AI and let it turn that in to something it understands.
As a bit of an apt aside — consider the Chinese room thought experiment. Imagine you put someone in a room with shelves full of books. The books are filled with symbols and look up tables and the person inside is told “You will be given a sheet of paper with symbols on. Use the books in the room to look up the symbols to write in reply.” Then a person outside the room posts messages in to the room in Mandarin and gets messages back in Mandarin. The person inside the room doesn’t understand Mandarin, the knowledge is all in the books, but to the person outside the room it looks like they understand Mandarin.
That is how symbolic AI works. It has no inate knowledge of the subject mater, it just follows instructions. Even if some if those instructions are to update the books.
Non-symbolic AI says that it’d be better if the AI wrote the books itself. So looking back at the Chinese Room, this is like teaching the person in the room Mandarin, and the books are their study notes. The trouble is, teaching someone Mandarin takes time and effort as we’re starting with a blank slate here.
But consider that it takes decades to teach a child their first language, yet it takes only a little more effort to teach them a second language. So back to the AI — once we teach it one language, we want it to be like the child. We want it to be easy for it to learn a second language.
This is where Artificial Neural Networks come in. These are our blank slate children. They’re made up of three parts: Inputs, neurones, outputs. The neurones are where the magic happens — they’re modelled on brains. They’re a blob of neurones that can connect up to one another or cut links so they can join one bit of the brain up to another and let a signal go from one place to another. This is what joins the input up to the output. And in the pavlovian way, when something good happens, the brain remembers by strengthening the link between neurones. But just like a baby, these start out pretty much random so all you get out is baby babble. But we don’t want baby babble, we have to teach it how to get from dog to chien, not dog to goobababaa.
When teaching the ANN, you give it an input, and if the output is wrong, give it a tap on the nose and the neurones remember “whatever we just did was wrong, don’t do it again” by decreasing the value it has on the links between the neurones that led to the wrong answer and of it gets it right, give it a rub on the head and it does the opposite, it increases the numbers, meaning it’ll be more likely to take that path next time. This means that over time, it’ll join up the input Dog to the output Chien.
So how does this explain the article?
Well. ANNs work in both directions, we can give it outputs and it’ll give us back inputs by following the path of neurones back in the opposite direction. So by teaching it Dog means Chien, it also knows Chien could mean Dog. That also means we can teach it that Perro means Dog when we’re speaking Spanish. So when we teach it, the fastest way for it to go from Perro to Dog is to follow the same path that took Chien to Dog. Meaning over time it will pull the neurones linking Chien and Dog closer to Perro as well, which links Perro to Chien as well.
This three way link in the middle of Perro, Dog and Chien is the language the google AI is creating for itself.
Backing up a bit to our imaginary child learning a new language, when they learn their first language (e.g. English), they don’t write an English dictionary in their head, they hear the words and map them to an idea that the words represent. This is why people frequently misquote films, they remember what the quote meant, not what the words were. So when the child learns a second language, they hear Chien as being French, but map it to the idea of dog. Then when they hear Perro they hear it as Spanish but map that to the idea of dog too. This means the child only has to learn about the idea of a dog once, but can then link that idea up to many languages or synonyms for dog. And this is what the Google AI is doing. Instead of thinking if dog=chien, and chien=perro, perro must = dog, it thinks dog=0x3b chien =0x3b perro=0x3b. Where 0x3b is the idea of dog, meaning it can then turn 0x3b in to whichever language you ask for.
Tl;Dr: It wasn’t big news because Artificial Neural Networks have been doing this since they were invented in the 40s. And the entire non-symbolic branch of AI is all about having computers invent their own language to understand and learn things.
P.S. It really is smart enough to warrant that excitement! Most people have no idea how much they rely on AI. From the relatively simple AI that runs their washing machine, to the AI that reads the address hand written on mail and then figures out the best way to deliver it. These are real everyday machines making decisions for us. Even your computer mouse has AI in it to determine what you wanted to point at rather than what you actually pointed at (on a 1080p screen, there are 2 million points you could click on, it’s not by accident that it’s pretty easy to pick the correct one). Mobile phones constantly run AI to decide which phone tower to connect to, while the backbone of the internet is a huge interconnected AI deciding the fastest way to get data from one computer to another. Thinking, decision making AI is in our hands, beneath our feet, in our cars and almost every electronic device we have.
The robots have already taken over ;)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
",requested someone degree holds hand two main schools artificial intelligence symbolic nonsymbolic symbolic says best way make ai make expert ai eg want doctor ai feed medical text books answers questions looking text book nonsymbolic says best way make ai decide computers better understanding computer give information ai let turn something understands bit apt aside consider chinese room thought experiment imagine put someone room shelves full books books filled symbols look tables person inside told given sheet paper symbols use books room look symbols write reply person outside room posts messages room mandarin gets messages back mandarin person inside room doesnt understand mandarin knowledge books person outside room looks like understand mandarin symbolic ai works inate knowledge subject mater follows instructions even instructions update books nonsymbolic ai says itd better ai wrote books looking back chinese room like teaching person room mandarin books study notes trouble teaching someone mandarin takes time effort starting blank slate consider takes decades teach child first language yet takes little effort teach second language back ai teach one language want like child want easy learn second language artificial neural networks come blank slate children theyre made three parts inputs neurones outputs neurones magic happens theyre modelled brains theyre blob neurones connect one another cut links join one bit brain another let signal go one place another joins input output pavlovian way something good happens brain remembers strengthening link neurones like baby start pretty much random get baby babble dont want baby babble teach get dog chien dog goobababaa teaching ann give input output wrong give tap nose neurones remember whatever wrong dont decreasing value links neurones led wrong answer gets right give rub head opposite increases numbers meaning itll likely take path next time means time itll join input dog output chien explain article well anns work directions give outputs itll give us back inputs following path neurones back opposite direction teaching dog means chien also knows chien could mean dog also means teach perro means dog speaking spanish teach fastest way go perro dog follow path took chien dog meaning time pull neurones linking chien dog closer perro well links perro chien well three way link middle perro dog chien language google ai creating backing bit imaginary child learning new language learn first language eg english dont write english dictionary head hear words map idea words represent people frequently misquote films remember quote meant words child learns second language hear chien french map idea dog hear perro hear spanish map idea dog means child learn idea dog link idea many languages synonyms dog google ai instead thinking dogchien chienperro perro must dog thinks dog0x3b chien 0x3b perro0x3b 0x3b idea dog meaning turn 0x3b whichever language ask tldr wasnt big news artificial neural networks since invented 40s entire nonsymbolic branch ai computers invent language understand learn things ps really smart enough warrant excitement people idea much rely ai relatively simple ai runs washing machine ai reads address hand written mail figures best way deliver real everyday machines making decisions us even computer mouse ai determine wanted point rather actually pointed 1080p screen 2 million points could click accident pretty easy pick correct one mobile phones constantly run ai decide phone tower connect backbone internet huge interconnected ai deciding fastest way get data one computer another thinking decision making ai hands beneath feet cars almost every electronic device robots already taken quick cheer standing ovation clap show much enjoyed story,en,"['Artificial Intelligence', 'AI', 'Artificial Neural Networks', 'ANN', 'the Google AI']"
205,Neelabh Pant,2000,A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs),"The Statsbot team has already published the article about using time series analysis for anomaly detection. Today, we’d like to discuss time series prediction with a long short-term memory model (LSTMs). We asked a data scientist, Neelabh Pant, to tell you about his experience of forecasting exchange rates using recurrent neural networks.
As an Indian guy living in the US, I have a constant flow of money from home to me and vice versa. If the USD is stronger in the market, then the Indian rupee (INR) goes down, hence, a person from India buys a dollar for more rupees. If the dollar is weaker, you spend less rupees to buy the same dollar.
If one can predict how much a dollar will cost tomorrow, then this can guide one’s decision making and can be very important in minimizing risks and maximizing returns. Looking at the strengths of a neural network, especially a recurrent neural network, I came up with the idea of predicting the exchange rate between the USD and the INR.
There are a lot of methods of forecasting exchange rates such as:
In this article, we’ll tell you how to predict the future exchange rate behavior using time series analysis and by making use of machine learning with time series.
Let us begin by talking about sequence problems. The simplest machine learning problem involving a sequence is a one to one problem.
In this case, we have one data input or tensor to the model and the model generates a prediction with the given input. Linear regression, classification, and even image classification with convolutional network fall into this category. We can extend this formulation to allow for the model to make use of the pass values of the input and the output.
It is known as the one to many problem. The one to many problem starts like the one to one problem where we have an input to the model and the model generates one output. However, the output of the model is now fed back to the model as a new input. The model now can generate a new output and we can continue like this indefinitely. You can now see why these are known as recurrent neural networks.
A recurrent neural network deals with sequence problems because their connections form a directed cycle. In other words, they can retain state from one iteration to the next by using their own output as input for the next step. In programming terms this is like running a fixed program with certain inputs and some internal variables. The simplest recurrent neural network can be viewed as a fully connected neural network if we unroll the time axes.
In this univariate case only two weights are involved. The weight multiplying the current input xt, which is u, and the weight multiplying the previous output yt-1, which is w. This formula is like the exponential weighted moving average (EWMA) by making its pass values of the output with the current values of the input.
One can build a deep recurrent neural network by simply stacking units to one another. A simple recurrent neural network works well only for a short-term memory. We will see that it suffers from a fundamental problem if we have a longer time dependency.
As we have talked about, a simple recurrent network suffers from a fundamental problem of not being able to capture long-term dependencies in a sequence. This is a problem because we want our RNNs to analyze text and answer questions, which involves keeping track of long sequences of words.
In late ’90s, LSTM was proposed by Sepp Hochreiter and Jurgen Schmidhuber, which is relatively insensitive to gap length over alternatives RNNs, hidden markov models, and other sequence learning methods in numerous applications.
This model is organized in cells which include several operations. LSTM has an internal state variable, which is passed from one cell to another and modified by Operation Gates.
1. Forget Gate
It is a sigmoid layer that takes the output at t-1 and the current input at time t and concatenates them into a single tensor and applies a linear transformation followed by a sigmoid. Because of the sigmoid, the output of this gate is between 0 and 1. This number is multiplied with the internal state and that is why the gate is called a forget gate. If ft=0 then the previous internal state is completely forgotten, while if ft=1 it will be passed through unaltered.
2. Input Gate
The input gate takes the previous output and the new input and passes them through another sigmoid layer. This gate returns a value between 0 and 1. The value of the input gate is multiplied with the output of the candidate layer.
This layer applies a hyperbolic tangent to the mix of input and previous output, returning a candidate vector to be added to the internal state.
The internal state is updated with this rule:
.The previous state is multiplied by the forget gate and then added to the fraction of the new candidate allowed by the output gate.
3. Output Gate
This gate controls how much of the internal state is passed to the output and it works in a similar way to the other gates.
These three gates described above have independent weights and biases, hence the network will learn how much of the past output to keep, how much of the current input to keep, and how much of the internal state to send out to the output.
In a recurrent neural network, you not only give the network the data, but also the state of the network one moment before. For example, if I say “Hey! Something crazy happened to me when I was driving” there is a part of your brain that is flipping a switch that’s saying “Oh, this is a story Neelabh is telling me. It is a story where the main character is Neelabh and something happened on the road.” Now, you carry a little part of that one sentence I just told you. As you listen to all my other sentences you have to keep a bit of information from all past sentences around in order to understand the entire story.
Another example is video processing, where you would again need a recurrent neural network. What happens in the current frame is heavily dependent upon what was in the last frame of the movie most of the time. Over a period of time, a recurrent neural network tries to learn what to keep and how much to keep from the past, and how much information to keep from the present state, which makes it so powerful as compared to a simple feed forward neural network.
I was impressed with the strengths of a recurrent neural network and decided to use them to predict the exchange rate between the USD and the INR. The dataset used in this project is the exchange rate data between January 2, 1980 and August 10, 2017. Later, I’ll give you a link to download this dataset and experiment with it.
The dataset displays the value of $1 in rupees. We have a total of 13,730 records starting from January 2, 1980 to August 10, 2017.
Over the period, the price to buy $1 in rupees has been rising. One can see that there was a huge dip in the American economy during 2007–2008, which was hugely caused by the great recession during that period. It was a period of general economic decline observed in world markets during the late 2000s and early 2010s.
This period was not very good for the world’s developed economies, particularly in North America and Europe (including Russia), which fell into a definitive recession. Many of the newer developed economies suffered far less impact, particularly China and India, whose economies grew substantially during this period.
Now, to train the machine we need to divide the dataset into test and training sets. It is very important when you do time series to split train and test with respect to a certain date. So, you don’t want your test data to come before your training data.
In our experiment, we will define a date, say January 1, 2010, as our split date. The training data is the data between January 2, 1980 and December 31, 2009, which are about 11,000 training data points.
The test dataset is between January 1, 2010 and August 10, 2017, which are about 2,700 points.
The next thing to do is normalize the dataset. You only need to fit and transform your training data and just transform your test data. The reason you do that is you don’t want to assume that you know the scale of your test data.
Normalizing or transforming the data means that the new scale variables will be between zero and one.
A fully Connected Model is a simple neural network model which is built as a simple regression model that will take one input and will spit out one output. This basically takes the price from the previous day and forecasts the price of the next day.
As a loss function, we use mean squared error and stochastic gradient descent as an optimizer, which after enough numbers of epochs will try to look for a good local optimum. Below is the summary of the fully connected layer.
After training this model for 200 epochs or early_callbacks (whichever came first), the model tries to learn the pattern and the behavior of the data. Since we split the data into training and testing sets we can now predict the value of testing data and compare them with the ground truth.
As you can see, the model is not good. It essentially is repeating the previous values and there is a slight shift. The fully connected model is not able to predict the future from the single previous value. Let us now try using a recurrent neural network and see how well it does.
The recurrent model we have used is a one layer sequential model. We used 6 LSTM nodes in the layer to which we gave input of shape (1,1), which is one input given to the network with one value.
The last layer is a dense layer where the loss is mean squared error with stochastic gradient descent as an optimizer. We train this model for 200 epochs with early_stopping callback. The summary of the model is shown above.
This model has learned to reproduce the yearly shape of the data and doesn’t have the lag it used to have with a simple feed forward neural network. It is still underestimating some observations by certain amounts and there is definitely room for improvement in this model.
There can be a lot of changes to be made in this model to make it better. One can always try to change the configuration by changing the optimizer. Another important change I see is by using the Sliding Time Window method, which comes from the field of stream data management system.
This approach comes from the idea that only the most recent data are important. One can show the model data from a year and try to make a prediction for the first day of the next year. Sliding time window methods are very useful in terms of fetching important patterns in the dataset that are highly dependent on the past bulk of observations.
Try to make changes to this model as you like and see how the model reacts to those changes.
I made the dataset available on my github account under deep learning in python repository. Feel free to download the dataset and play with it.
I personally follow some of my favorite data scientists like Kirill Eremenko, Jose Portilla, Dan Van Boxel (better known as Dan Does Data), and many more. Most of them are available on different podcast stations where they talk about different current subjects like RNN, Convolutional Neural Networks, LSTM, and even the most recent technology, Neural Turing Machine.
Try to keep up with the news of different artificial intelligence conferences. By the way, if you are interested, then Kirill Eremenko is coming to San Diego this November with his amazing team to give talks on Machine Learning, Neural Networks, and Data Science.
LSTM models are powerful enough to learn the most important past behaviors and understand whether or not those past behaviors are important features in making future predictions. There are several applications where LSTMs are highly used. Applications like speech recognition, music composition, handwriting recognition, and even in my current research of human mobility and travel predictions.
According to me, LSTM is like a model which has its own memory and which can behave like an intelligent human in making decisions.
Thank you again and happy machine learning!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I love Data Science. Let’s build some intelligent bots together! ;)
Data stories on machine learning and analytics. From Statsbot’s makers.
",statsbot team already published article using time series analysis anomaly detection today wed like discuss time series prediction long shortterm memory model lstms asked data scientist neelabh pant tell experience forecasting exchange rates using recurrent neural networks indian guy living us constant flow money home vice versa usd stronger market indian rupee inr goes hence person india buys dollar rupees dollar weaker spend less rupees buy dollar one predict much dollar cost tomorrow guide ones decision making important minimizing risks maximizing returns looking strengths neural network especially recurrent neural network came idea predicting exchange rate usd inr lot methods forecasting exchange rates article well tell predict future exchange rate behavior using time series analysis making use machine learning time series let us begin talking sequence problems simplest machine learning problem involving sequence one one problem case one data input tensor model model generates prediction given input linear regression classification even image classification convolutional network fall category extend formulation allow model make use pass values input output known one many problem one many problem starts like one one problem input model model generates one output however output model fed back model new input model generate new output continue like indefinitely see known recurrent neural networks recurrent neural network deals sequence problems connections form directed cycle words retain state one iteration next using output input next step programming terms like running fixed program certain inputs internal variables simplest recurrent neural network viewed fully connected neural network unroll time axes univariate case two weights involved weight multiplying current input xt u weight multiplying previous output yt1 w formula like exponential weighted moving average ewma making pass values output current values input one build deep recurrent neural network simply stacking units one another simple recurrent neural network works well shortterm memory see suffers fundamental problem longer time dependency talked simple recurrent network suffers fundamental problem able capture longterm dependencies sequence problem want rnns analyze text answer questions involves keeping track long sequences words late 90s lstm proposed sepp hochreiter jurgen schmidhuber relatively insensitive gap length alternatives rnns hidden markov models sequence learning methods numerous applications model organized cells include several operations lstm internal state variable passed one cell another modified operation gates 1 forget gate sigmoid layer takes output t1 current input time concatenates single tensor applies linear transformation followed sigmoid sigmoid output gate 0 1 number multiplied internal state gate called forget gate ft0 previous internal state completely forgotten ft1 passed unaltered 2 input gate input gate takes previous output new input passes another sigmoid layer gate returns value 0 1 value input gate multiplied output candidate layer layer applies hyperbolic tangent mix input previous output returning candidate vector added internal state internal state updated rule previous state multiplied forget gate added fraction new candidate allowed output gate 3 output gate gate controls much internal state passed output works similar way gates three gates described independent weights biases hence network learn much past output keep much current input keep much internal state send output recurrent neural network give network data also state network one moment example say hey something crazy happened driving part brain flipping switch thats saying oh story neelabh telling story main character neelabh something happened road carry little part one sentence told listen sentences keep bit information past sentences around order understand entire story another example video processing would need recurrent neural network happens current frame heavily dependent upon last frame movie time period time recurrent neural network tries learn keep much keep past much information keep present state makes powerful compared simple feed forward neural network impressed strengths recurrent neural network decided use predict exchange rate usd inr dataset used project exchange rate data january 2 1980 august 10 2017 later ill give link download dataset experiment dataset displays value 1 rupees total 13730 records starting january 2 1980 august 10 2017 period price buy 1 rupees rising one see huge dip american economy 20072008 hugely caused great recession period period general economic decline observed world markets late 2000s early 2010s period good worlds developed economies particularly north america europe including russia fell definitive recession many newer developed economies suffered far less impact particularly china india whose economies grew substantially period train machine need divide dataset test training sets important time series split train test respect certain date dont want test data come training data experiment define date say january 1 2010 split date training data data january 2 1980 december 31 2009 11000 training data points test dataset january 1 2010 august 10 2017 2700 points next thing normalize dataset need fit transform training data transform test data reason dont want assume know scale test data normalizing transforming data means new scale variables zero one fully connected model simple neural network model built simple regression model take one input spit one output basically takes price previous day forecasts price next day loss function use mean squared error stochastic gradient descent optimizer enough numbers epochs try look good local optimum summary fully connected layer training model 200 epochs early_callbacks whichever came first model tries learn pattern behavior data since split data training testing sets predict value testing data compare ground truth see model good essentially repeating previous values slight shift fully connected model able predict future single previous value let us try using recurrent neural network see well recurrent model used one layer sequential model used 6 lstm nodes layer gave input shape 11 one input given network one value last layer dense layer loss mean squared error stochastic gradient descent optimizer train model 200 epochs early_stopping callback summary model shown model learned reproduce yearly shape data doesnt lag used simple feed forward neural network still underestimating observations certain amounts definitely room improvement model lot changes made model make better one always try change configuration changing optimizer another important change see using sliding time window method comes field stream data management system approach comes idea recent data important one show model data year try make prediction first day next year sliding time window methods useful terms fetching important patterns dataset highly dependent past bulk observations try make changes model like see model reacts changes made dataset available github account deep learning python repository feel free download dataset play personally follow favorite data scientists like kirill eremenko jose portilla dan van boxel better known dan data many available different podcast stations talk different current subjects like rnn convolutional neural networks lstm even recent technology neural turing machine try keep news different artificial intelligence conferences way interested kirill eremenko coming san diego november amazing team give talks machine learning neural networks data science lstm models powerful enough learn important past behaviors understand whether past behaviors important features making future predictions several applications lstms highly used applications like speech recognition music composition handwriting recognition even current research human mobility travel predictions according lstm like model memory behave like intelligent human making decisions thank happy machine learning quick cheer standing ovation clap show much enjoyed story love data science lets build intelligent bots together data stories machine learning analytics statsbots makers,en,"['USD', 'INR', 'Linear', 'fed', 'yt-1', 'LSTM', 'Operation Gates', 'Neural Turing Machine']"
206,Eugenio Culurciello,2200,Neural Network Architectures – Towards Data Science,"Deep neural networks and Deep Learning are powerful and popular algorithms. And a lot of their success lays in the careful design of the neural network architecture.
I wanted to revisit the history of neural network design in the last few years and in the context of Deep Learning.
For a more in-depth analysis and comparison of all the networks reported here, please see our recent article. One representative figure from this article is here:
Reporting top-1 one-crop accuracy versus amount of operations required for a single forward pass in multiple popular neural network architectures.
It is the year 1994, and this is one of the very first convolutional neural networks, and what propelled the field of Deep Learning. This pioneering work by Yann LeCun was named LeNet5 after many previous successful iterations since the year 1988!
The LeNet5 architecture was fundamental, in particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters. At the time there was no GPU to help training, and even CPUs were slow. Therefore being able to save parameters and computation was a key advantage. This is in contrast to using each pixel as a separate input of a large multi-layer neural network. LeNet5 explained that those should not be used in the first layer, because images are highly spatially correlated, and using individual pixel of the image as separate input features would not take advantage of these correlations.
LeNet5 features can be summarized as:
In overall this network was the origin of much of the recent architectures, and a true inspiration for many people in the field.
In the years from 1998 to 2010 neural network were in incubation. Most people did not notice their increasing power, while many other researchers slowly progressed. More and more data was available because of the rise of cell-phone cameras and cheap digital cameras. And computing power was on the rise, CPUs were becoming faster, and GPUs became a general-purpose computing tool. Both of these trends made neural network progress, albeit at a slow rate. Both data and computing power made the tasks that neural networks tackled more and more interesting. And then it became clear...
In 2010 Dan Claudiu Ciresan and Jurgen Schmidhuber published one of the very fist implementations of GPU Neural nets. This implementation had both forward and backward implemented on a a NVIDIA GTX 280 graphic processor of an up to 9 layers neural network.
In 2012, Alex Krizhevsky released AlexNet which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet competition.
AlexNet scaled the insights of LeNet into a much larger neural network that could be used to learn much more complex objects and object hierarchies. The contribution of this work were:
At the time GPU offered a much larger number of cores than CPUs, and allowed 10x faster training time, which in turn allowed to use larger datasets and also bigger images.
The success of AlexNet started a small revolution. Convolutional neural network were now the workhorse of Deep Learning, which became the new name for “large neural networks that can now solve useful tasks”.
In December 2013 the NYU lab from Yann LeCun came up with Overfeat, which is a derivative of AlexNet. The article also proposed learning bounding boxes, which later gave rise to many other papers on the same topic. I believe it is better to learn to segment objects rather than learn artificial bounding boxes.
The VGG networks from Oxford were the first to use much smaller 3×3 filters in each convolutional layers and also combined them as a sequence of convolutions.
This seems to be contrary to the principles of LeNet, where large convolutions were used to capture similar features in an image. Instead of the 9×9 or 11×11 filters of AlexNet, filters started to become smaller, too dangerously close to the infamous 1×1 convolutions that LeNet wanted to avoid, at least on the first layers of the network. But the great advantage of VGG was the insight that multiple 3×3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5×5 and 7×7. These ideas will be also used in more recent network architectures as Inception and ResNet.
The VGG networks uses multiple 3x3 convolutional layers to represent complex features. Notice blocks 3, 4, 5 of VGG-E: 256×256 and 512×512 3×3 filters are used multiple times in sequence to extract more complex features and the combination of such features. This is effectively like having large 512×512 classifiers with 3 layers, which are convolutional! This obviously amounts to a massive number of parameters, and also learning power. But training of these network was difficult, and had to be split into smaller networks with layers added one by one. All this because of the lack of strong ways to regularize the model, or to somehow restrict the massive search space promoted by the large amount of parameters.
VGG used large feature sizes in many layers and thus inference was quite costly at run-time. Reducing the number of features, as done in Inception bottlenecks, will save some of the computational cost.
Network-in-network (NiN) had the great and simple insight of using 1x1 convolutions to provide more combinational power to the features of a convolutional layers.
The NiN architecture used spatial MLP layers after each convolution, in order to better combine features before another layer. Again one can think the 1x1 convolutions are against the original principles of LeNet, but really they instead help to combine convolutional features in a better way, which is not possible by simply stacking more convolutional layers. This is different from using raw pixels as input to the next layer. Here 1×1 convolution are used to spatially combine features across features maps after convolution, so they effectively use very few parameters, shared across all pixels of these features!
The power of MLP can greatly increase the effectiveness of individual convolutional features by combining them into more complex groups. This idea will be later used in most recent architectures as ResNet and Inception and derivatives.
NiN also used an average pooling layer as part of the last classifier, another practice that will become common. This was done to average the response of the network to multiple are of the input image before classification.
Christian Szegedy from Google begun a quest aimed at reducing the computational burden of deep neural networks, and devised the GoogLeNet the first Inception architecture.
By now, Fall 2014, deep learning models were becoming extermely useful in categorizing the content of images and video frames. Most skeptics had given in that Deep Learning and neural nets came back to stay this time. Given the usefulness of these techniques, the internet giants like Google were very interested in efficient and large deployments of architectures on their server farms.
Christian thought a lot about ways to reduce the computational burden of deep neural nets while obtaining state-of-art performance (on ImageNet, for example). Or be able to keep the computational cost the same, while offering improved performance.
He and his team came up with the Inception module:
which at a first glance is basically the parallel combination of 1×1, 3×3, and 5×5 convolutional filters. But the great insight of the inception module was the use of 1×1 convolutional blocks (NiN) to reduce the number of features before the expensive parallel blocks. This is commonly referred as “bottleneck”. This deserves its own section to explain: see “bottleneck layer” section below.
GoogLeNet used a stem without inception modules as initial layers, and an average pooling plus softmax classifier similar to NiN. This classifier is also extremely low number of operations, compared to the ones of AlexNet and VGG. This also contributed to a very efficient network design.
Inspired by NiN, the bottleneck layer of Inception was reducing the number of features, and thus operations, at each layer, so the inference time could be kept low. Before passing data to the expensive convolution modules, the number of features was reduce by, say, 4 times. This led to large savings in computational cost, and the success of this architecture.
Let’s examine this in detail. Let’s say you have 256 features coming in, and 256 coming out, and let’s say the Inception layer only performs 3x3 convolutions. That is 256x256 x 3x3 convolutions that have to be performed (589,000s multiply-accumulate, or MAC operations). That may be more than the computational budget we have, say, to run this layer in 0.5 milli-seconds on a Google Server. Instead of doing this, we decide to reduce the number of features that will have to be convolved, say to 64 or 256/4. In this case, we first perform 256 -> 64 1×1 convolutions, then 64 convolution on all Inception branches, and then we use again a 1x1 convolution from 64 -> 256 features back again. The operations are now:
For a total of about 70,000 versus the almost 600,000 we had before. Almost 10x less operations!
And although we are doing less operations, we are not losing generality in this layer. In fact the bottleneck layers have been proven to perform at state-of-art on the ImageNet dataset, for example, and will be also used in later architectures such as ResNet.
The reason for the success is that the input features are correlated, and thus redundancy can be removed by combining them appropriately with the 1x1 convolutions. Then, after convolution with a smaller number of features, they can be expanded again into meaningful combination for the next layer.
Christian and his team are very efficient researchers. In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values. This corresponds to “whitening” the data, and thus making all the neural maps have responses in the same range, and with zero mean. This helps training as the next layer does not have to learn offsets in the input data, and can focus on how to best combine features.
In December 2015 they released a new version of the Inception modules and the corresponding architecture This article better explains the original GoogLeNet architecture, giving a lot more detail on the design choices. A list of the original ideas are:
Inception still uses a pooling layer plus softmax as final classifier.
The revolution then came in December 2015, at about the same time as Inception v3. ResNet have a simple ideas: feed the output of two successive convolutional layer AND also bypass the input to the next layers!
This is similar to older ideas like this one. But here they bypass TWO layers and are applied to large scales. Bypassing after 2 layers is a key intuition, as bypassing a single layer did not give much improvements. By 2 layers can be thought as a small classifier, or a Network-In-Network!
This is also the very first time that a network of > hundred, even 1000 layers was trained.
ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck:
This layer reduces the number of features at each layer by first using a 1x1 convolution with a smaller output (usually 1/4 of the input), and then a 3x3 layer, and then again a 1x1 convolution to a larger number of features. Like in the case of Inception modules, this allows to keep the computation low, while providing rich combination of features. See “bottleneck layer” section after “GoogLeNet and Inception”.
ResNet uses a fairly simple initial layers at the input (stem): a 7x7 conv layer followed with a pool of 2. Contrast this to more complex and less intuitive stems as in Inception V3, V4.
ResNet also uses a pooling layer plus softmax as final classifier.
Additional insights about the ResNet architecture are appearing every day:
And Christian and team are at it again with a new version of Inception.
The Inception module after the stem is rather similar to Inception V3:
They also combined the Inception module with the ResNet module:
This time though the solution is, in my opinion, less elegant and more complex, but also full of less transparent heuristics. It is hard to understand the choices and it is also hard for the authors to justify them.
In this regard the prize for a clean and simple network that can be easily understood and modified now goes to ResNet.
SqueezeNet has been recently released. It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.
Our team set up to combine all the features of the recent architectures into a very efficient and light-weight network that uses very few parameters and computation to achieve state-of-the-art results. This network architecture is dubbed ENet, and was designed by Adam Paszke. We have used it to perform pixel-wise labeling and scene-parsing. Here are some videos of ENet in action. These videos are not part of the training dataset.
The technical report on ENet is available here. ENet is a encoder plus decoder network. The encoder is a regular CNN design for categorization, while the decoder is a upsampling network designed to propagate the categories back into the original image size for segmentation. This worked used only neural networks, and no other algorithm to perform image segmentation.
As you can see in this figure ENet has the highest accuracy per parameter used of any neural network out there!
ENet was designed to use the minimum number of resources possible from the start. As such it achieves such a small footprint that both encoder and decoder network together only occupies 0.7 MB with fp16 precision. Even at this small size, ENet is similar or above other pure neural network solutions in accuracy of segmentation.
A systematic evaluation of CNN modules has been presented. The found out that is advantageous to use:
• use ELU non-linearity without batchnorm or ReLU with it.
• apply a learned colorspace transformation of RGB.
• use the linear learning rate decay policy.
• use a sum of the average and max pooling layers.
• use mini-batch size around 128 or 256. If this is too big for your GPU, decrease the learning rate proportionally to the batch size.
• use fully-connected layers as convolutional and average the predictions for the final decision.
• when investing in increasing training set size, check if a plateau has not been reach. • cleanliness of the data is more important then the size.
• if you cannot increase the input image size, reduce the stride in the con- sequent layers, it has roughly the same effect.
• if your network has a complex and highly optimized architecture, like e.g. GoogLeNet, be careful with modifications.
Xception improves on the inception module and architecture with a simple and more elegant architecture that is as effective as ResNet and Inception V4.
The Xception module is presented here:
This network can be anyone’s favorite given the simplicity and elegance of the architecture, presented here:
The architecture has 36 convolutional stages, making it close in similarity to a ResNet-34. But the model and code is as simple as ResNet and much more comprehensible than Inception V4.
A Torch7 implementation of this network is available here An implementation in Keras/TF is availble here.
It is interesting to note that the recent Xception architecture was also inspired by our work on separable convolutional filters.
A new MobileNets architecture is also available since April 2017. This architecture uses separable convolutions to reduce the number of parameters. The separate convolution is the same as Xception above. Now the claim of the paper is that there is a great reduction in parameters — about 1/2 in case of FaceNet, as reported in the paper. Here is the complete model architecture:
Unfortunately, we have tested this network in actual application and found it to be abysmally slow on a batch of 1 on a Titan Xp GPU. Look at a comparison here of inference time per image:
Clearly this is not a contender in fast inference! It may reduce the parameters and size of network on disk, but is not usable.
FractalNet uses a recursive architecture, that was not tested on ImageNet, and is a derivative or the more general ResNet.
We believe that crafting neural network architectures is of paramount importance for the progress of the Deep Learning field. Our group highly recommends reading carefully and understanding all the papers in this post.
But one could now wonder why we have to spend so much time in crafting architectures, and why instead we do not use data to tell us what to use, and how to combine modules. This would be nice, but now it is work in progress. Some initial interesting results are here.
Note also that here we mostly talked about architectures for computer vision. Similarly neural network architectures developed in other areas, and it is interesting to study the evolution of architectures for all other tasks also.
If you are interested in a comparison of neural network architecture and computational performance, see our recent paper.
This post was inspired by discussions with Abhishek Chaurasia, Adam Paszke, Sangpil Kim, Alfredo Canziani and others in our e-Lab at Purdue University.
I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more...
If you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I dream and build new technology
Sharing concepts, ideas, and codes.
",deep neural networks deep learning powerful popular algorithms lot success lays careful design neural network architecture wanted revisit history neural network design last years context deep learning indepth analysis comparison networks reported please see recent article one representative figure article reporting top1 onecrop accuracy versus amount operations required single forward pass multiple popular neural network architectures year 1994 one first convolutional neural networks propelled field deep learning pioneering work yann lecun named lenet5 many previous successful iterations since year 1988 lenet5 architecture fundamental particular insight image features distributed across entire image convolutions learnable parameters effective way extract similar features multiple location parameters time gpu help training even cpus slow therefore able save parameters computation key advantage contrast using pixel separate input large multilayer neural network lenet5 explained used first layer images highly spatially correlated using individual pixel image separate input features would take advantage correlations lenet5 features summarized overall network origin much recent architectures true inspiration many people field years 1998 2010 neural network incubation people notice increasing power many researchers slowly progressed data available rise cellphone cameras cheap digital cameras computing power rise cpus becoming faster gpus became generalpurpose computing tool trends made neural network progress albeit slow rate data computing power made tasks neural networks tackled interesting became clear 2010 dan claudiu ciresan jurgen schmidhuber published one fist implementations gpu neural nets implementation forward backward implemented nvidia gtx 280 graphic processor 9 layers neural network 2012 alex krizhevsky released alexnet deeper much wider version lenet large margin difficult imagenet competition alexnet scaled insights lenet much larger neural network could used learn much complex objects object hierarchies contribution work time gpu offered much larger number cores cpus allowed 10x faster training time turn allowed use larger datasets also bigger images success alexnet started small revolution convolutional neural network workhorse deep learning became new name large neural networks solve useful tasks december 2013 nyu lab yann lecun came overfeat derivative alexnet article also proposed learning bounding boxes later gave rise many papers topic believe better learn segment objects rather learn artificial bounding boxes vgg networks oxford first use much smaller 33 filters convolutional layers also combined sequence convolutions seems contrary principles lenet large convolutions used capture similar features image instead 99 1111 filters alexnet filters started become smaller dangerously close infamous 11 convolutions lenet wanted avoid least first layers network great advantage vgg insight multiple 33 convolution sequence emulate effect larger receptive fields examples 55 77 ideas also used recent network architectures inception resnet vgg networks uses multiple 3x3 convolutional layers represent complex features notice blocks 3 4 5 vgge 256256 512512 33 filters used multiple times sequence extract complex features combination features effectively like large 512512 classifiers 3 layers convolutional obviously amounts massive number parameters also learning power training network difficult split smaller networks layers added one one lack strong ways regularize model somehow restrict massive search space promoted large amount parameters vgg used large feature sizes many layers thus inference quite costly runtime reducing number features done inception bottlenecks save computational cost networkinnetwork nin great simple insight using 1x1 convolutions provide combinational power features convolutional layers nin architecture used spatial mlp layers convolution order better combine features another layer one think 1x1 convolutions original principles lenet really instead help combine convolutional features better way possible simply stacking convolutional layers different using raw pixels input next layer 11 convolution used spatially combine features across features maps convolution effectively use parameters shared across pixels features power mlp greatly increase effectiveness individual convolutional features combining complex groups idea later used recent architectures resnet inception derivatives nin also used average pooling layer part last classifier another practice become common done average response network multiple input image classification christian szegedy google begun quest aimed reducing computational burden deep neural networks devised googlenet first inception architecture fall 2014 deep learning models becoming extermely useful categorizing content images video frames skeptics given deep learning neural nets came back stay time given usefulness techniques internet giants like google interested efficient large deployments architectures server farms christian thought lot ways reduce computational burden deep neural nets obtaining stateofart performance imagenet example able keep computational cost offering improved performance team came inception module first glance basically parallel combination 11 33 55 convolutional filters great insight inception module use 11 convolutional blocks nin reduce number features expensive parallel blocks commonly referred bottleneck deserves section explain see bottleneck layer section googlenet used stem without inception modules initial layers average pooling plus softmax classifier similar nin classifier also extremely low number operations compared ones alexnet vgg also contributed efficient network design inspired nin bottleneck layer inception reducing number features thus operations layer inference time could kept low passing data expensive convolution modules number features reduce say 4 times led large savings computational cost success architecture lets examine detail lets say 256 features coming 256 coming lets say inception layer performs 3x3 convolutions 256x256 x 3x3 convolutions performed 589000s multiplyaccumulate mac operations may computational budget say run layer 05 milliseconds google server instead decide reduce number features convolved say 64 2564 case first perform 256 64 11 convolutions 64 convolution inception branches use 1x1 convolution 64 256 features back operations total 70000 versus almost 600000 almost 10x less operations although less operations losing generality layer fact bottleneck layers proven perform stateofart imagenet dataset example also used later architectures resnet reason success input features correlated thus redundancy removed combining appropriately 1x1 convolutions convolution smaller number features expanded meaningful combination next layer christian team efficient researchers february 2015 batchnormalized inception introduced inception v2 batchnormalization computes mean standarddeviation feature maps output layer normalizes responses values corresponds whitening data thus making neural maps responses range zero mean helps training next layer learn offsets input data focus best combine features december 2015 released new version inception modules corresponding architecture article better explains original googlenet architecture giving lot detail design choices list original ideas inception still uses pooling layer plus softmax final classifier revolution came december 2015 time inception v3 resnet simple ideas feed output two successive convolutional layer also bypass input next layers similar older ideas like one bypass two layers applied large scales bypassing 2 layers key intuition bypassing single layer give much improvements 2 layers thought small classifier networkinnetwork also first time network hundred even 1000 layers trained resnet large number layers started use bottleneck layer similar inception bottleneck layer reduces number features layer first using 1x1 convolution smaller output usually 14 input 3x3 layer 1x1 convolution larger number features like case inception modules allows keep computation low providing rich combination features see bottleneck layer section googlenet inception resnet uses fairly simple initial layers input stem 7x7 conv layer followed pool 2 contrast complex less intuitive stems inception v3 v4 resnet also uses pooling layer plus softmax final classifier additional insights resnet architecture appearing every day christian team new version inception inception module stem rather similar inception v3 also combined inception module resnet module time though solution opinion less elegant complex also full less transparent heuristics hard understand choices also hard authors justify regard prize clean simple network easily understood modified goes resnet squeezenet recently released rehash many concepts resnet inception show better design architecture deliver small network sizes parameters without needing complex compression algorithms team set combine features recent architectures efficient lightweight network uses parameters computation achieve stateoftheart results network architecture dubbed enet designed adam paszke used perform pixelwise labeling sceneparsing videos enet action videos part training dataset technical report enet available enet encoder plus decoder network encoder regular cnn design categorization decoder upsampling network designed propagate categories back original image size segmentation worked used neural networks algorithm perform image segmentation see figure enet highest accuracy per parameter used neural network enet designed use minimum number resources possible start achieves small footprint encoder decoder network together occupies 07 mb fp16 precision even small size enet similar pure neural network solutions accuracy segmentation systematic evaluation cnn modules presented found advantageous use use elu nonlinearity without batchnorm relu apply learned colorspace transformation rgb use linear learning rate decay policy use sum average max pooling layers use minibatch size around 128 256 big gpu decrease learning rate proportionally batch size use fullyconnected layers convolutional average predictions final decision investing increasing training set size check plateau reach cleanliness data important size cannot increase input image size reduce stride con sequent layers roughly effect network complex highly optimized architecture like eg googlenet careful modifications xception improves inception module architecture simple elegant architecture effective resnet inception v4 xception module presented network anyones favorite given simplicity elegance architecture presented architecture 36 convolutional stages making close similarity resnet34 model code simple resnet much comprehensible inception v4 torch7 implementation network available implementation kerastf availble interesting note recent xception architecture also inspired work separable convolutional filters new mobilenets architecture also available since april 2017 architecture uses separable convolutions reduce number parameters separate convolution xception claim paper great reduction parameters 12 case facenet reported paper complete model architecture unfortunately tested network actual application found abysmally slow batch 1 titan xp gpu look comparison inference time per image clearly contender fast inference may reduce parameters size network disk usable fractalnet uses recursive architecture tested imagenet derivative general resnet believe crafting neural network architectures paramount importance progress deep learning field group highly recommends reading carefully understanding papers post one could wonder spend much time crafting architectures instead use data tell us use combine modules would nice work progress initial interesting results note also mostly talked architectures computer vision similarly neural network architectures developed areas interesting study evolution architectures tasks also interested comparison neural network architecture computational performance see recent paper post inspired discussions abhishek chaurasia adam paszke sangpil kim alfredo canziani others elab purdue university almost 20 years experience neural networks hardware software rare combination see medium webpage scholar linkedin found article useful please consider donation support tutorials blogs contribution make difference quick cheer standing ovation clap show much enjoyed story dream build new technology sharing concepts ideas codes,en,"['GPU', 'Dan Claudiu Ciresan', 'GPU Neural', 'AlexNet', 'NYU', 'Overfeat', 'VGG', 'Oxford', 'NiN', 'MLP', '1x1', 'Google', 'ImageNet', 'MAC', 'GoogLeNet', 'Inception V3', 'V4', 'ResNet', 'SqueezeNet', 'ENet', 'CNN', 'RGB', 'ResNet and Inception V4', 'Inception V4', 'Keras/TF', 'MobileNets', 'FaceNet', 'FractalNet', 'Purdue University', 'LinkedIn']"
207,Favio Vázquez,3300,A “weird” introduction to Deep Learning – Towards Data Science,"There are amazing introductions, courses and blog posts on Deep Learning. I will name some of them in the resources sections, but this is a different kind of introduction.
But why weird? Maybe because it won’t follow the “normal” structure of a Deep Learning post, where you start with the math, then go into the papers, the implementation and then to applications.
It will be more close to the post I did before about “My journey into Deep Learning”, I think telling a story can be much more helpful than just throwing information and formulas everywhere. So let’s begin.
NOTE: There’s a companion webinar to this article. Find it here:
Sometimes is important to have a written backup of your thoughts. I tend to talk a lot, and be present in several presentations and conference, and this is my way of contributing with a little knowledge to everyone.
Deep Learning (DL)is such an important field for Data Science, AI, Technology and our lives right now, and it deserves all of the attention is getting. Please don’t say that deep learning is just adding a layer to a neural net, and that’s it, magic! Nope. I’m hoping that after reading this you have a different perspective of what DL is.
I just created this timeline based on several papers and other timelines with the purpose of everyone seeing that Deep Learning is much more than just Neural Networks. There has been really theoretical advances, software and hardware improvements that were necessary for us to get to this day. If you want it just ping me and I’ll send it to you. (Find my contact in the end of the article).
Deep Learning has been around for quite a while now. So why it became so relevant so fast the last 5–7 years?
As I said before, until the late 2000s, we were still missing a reliable way to train very deep neural networks. Nowadays, with the development of several simple but important theoretical and algorithmic improvements, the advances in hardware (mostly GPUs, now TPUs), and the exponential generation and accumulation of data, DL came naturally to fit this missing spot to transform the way we do machine learning.
Deep Learning is an active field of research too, nothing is settle or closed, we are still searching for the best models, topology of the networks, best ways to optimize their hyperparameters and more. Is very hard, as any other active field on science, to keep up to date with the investigation, but it’s not impossible.
A side note on topology and machine learning (Deep Learning with Topological Signatures by Hofer et al.):
Luckily for us, there are lots of people helping understand and digest all of this information through courses like the Andrew Ng one, blog posts and much more.
This for me is weird, or uncommon because normally you have to wait for sometime (sometime years) to be able to digest difficult and advance information in papers or research journals. Of course, most areas of science are now really fast too to get from a paper to a blog post that tells you what yo need to know, but in my opinion DL has a different feel.
We are working with something that is very exciting, most people in the field are saying that the last ideas in the papers of deep learning (specifically new topologies and configurations for NN or algorithms to improve their usage) are the best ideas in Machine Learning in decades (remember that DL is inside of ML).
I’ve used the word learning a lot in this article so far. But what is learning?
In the context of Machine Learning, the word “learning” describes an automatic search process for better representations of the data you are analyzing and studying (please have this in mind, is not making a computer learn).
This is a very important word for this field, REP-RE-SEN-TA-TION. Don’t forget about it. What is a representation? It’s a way to look at data.
Let me give you an example, let’s say I tell you I want you to drive a line that separates the blue circles from the green triangles for this plot:
So, if you want to use a line this is what the author says:
This is impossible if we remember the concept of a line:
So is the case lost? Actually no. If we find a way of representing this data in a different way, in a way we can draw a straight line to separate the types of data. This is somethinkg that math taught us hundreds of years ago. In this case what we need is a coordinate transformation, so we can plot or represent this data in a way we can draw this line. If we look the polar coordinate transformation, we have the solution:
And that’s it now we can draw a line:
So, in this simple example we found and chose the transformation to get a better representation by hand. But if we create a system, a program that can search for different representations (in this case a coordinate change), and then find a way of calculating the percentage of categories being classified correctly with this new approach, in that moment we are doing Machine Learning.
This is something very important to have in mind, deep learning is representation learning using different kinds of neural networks and optimize the hyperparameters of the net to get (learn)the best representation for our data.
This wouldn’t be possible without the amazing breakthroughs that led us to the current state of Deep Learning. Here I name some of them:
Learning representations by back-propagating errors by David E. Rumelhart, Geoffrey E. Hinton & Ronald J. Williams.
A theoretical framework for Back-Propagation by Yann Lecun.
2. Idea: Better initialization of the parameters of the nets. Something to remember: The initialization strategy should be selected according to the activation function used (next).
3. Idea: Better activation functions. This mean, better ways of approximating the functions faster leading to faster training process.
4. Idea: Dropout. Better ways of preventing overfitting and more.
Dropout: A Simple Way to Prevent Neural Networks from Overfitting, a great paper by Srivastava, Hinton and others.
5. Idea: Convolutional Neural Nets (CNNs).
Gradient based learning applied to document recognition by Lecun and others
ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky and others.
6. Idea: Residual Nets (ResNets).
7. Idea: Region Based CNNs. Used for object detection and more.
8. Idea: Recurrent Neural Networks (RNNs) and LSTMs.
BTW: It was shown by Liao and Poggio (2016) that ResNets == RNNs, arXiv:1604.03640v1.
9. Idea: Generative Adversarial Networks (GANs).
10. Idea: Capsule Networks.
And there are many others but I think those are really important theoretical and algorithmic breakthroughs that are changing the world, and that gave momentum for the DL revolution.
It’s not easy to get started but I’ll try my best to guide you through this process. Check out this resources, but remember, this is not only watching videos and reading papers, it’s about understanding, programming, coding, failing and then making it happen.
-1. Learn Python and R ;)
0. Andrew Ng and Coursera (you know, he doesn’t need an intro):
Siraj Raval: He’s amazing. He has the power to explain hard concepts in a fun and easy way. Follow him on his YouTube channel. Specifically this playlists:
— The Math of Intelligence:
— Intro to Deep Learning:
3. François Chollet’s book: Deep Learning with Python (and R):
3. IBM Cognitive Class:
5. DataCamp:
Deep Learning is one of the most important tools and theories a Data Scientist should learn. We are so lucky to see amazing people creating both research, software, tools and hardware specific for DL tasks.
DL is computationally expensive, and even though there’s been advances in theory, software and hardware, we need the developments in Big Data and Distributed Machine Learning to improve performance and efficiency. Great people and companies are making amazing efforts to join the distributed frameworks (Spark) and DL libraries (TF and Keras).
Here’s an overview:
2. Elephas: Distributed DL with Keras & PySpark:
3. Yahoo! Inc.: TensorFlowOnSpark:
4. CERN Distributed Keras (Keras + Spark) :
5. Qubole (tutorial Keras + Spark):
6. Intel Corporation: BigDL (Distributed Deep Learning Library for Apache Spark)
7. TensorFlow and Spark on Google Cloud:
As I’ve said before one of the most important moments for this field was the creation and open sourced of TensorFlow.
TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.
The things you are seeing in the image above are tensor manipulations working with the Riemann Tensor in General Relativity.
Tensors, defined mathematically, are simply arrays of numbers, or functions, that transform according to certain rules under a change of coordinates.
But in the scope of Machine Learning and Deep Learning a tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.
We use heavily tensors all the time in DL, but you don’t need to be an expert in them to use it. You may need to understand a little bit about them so here I list some good resources:
After you check that out, the breakthroughs I mentioned before and the programming frameworks like TensorFlow or Keras (for more on Keras go here), now I think you have an idea of what you need to understand and work with Deep Learning.
But what have we achieved so far with DL? To name a few (from François Chollet book on DL):
And much more. Here’s a list of 30 great and funny applications of DL:
Thinking about the future of Deep Learning (for programming or building applications), I’ll repeat what I said in other posts.
I really think GUIs and AutoML are the near future of getting things done with Deep Learning. Don’t get me wrong, I love coding, but I think the amount of code we will be writing next years will decay.
We cannot spend so many hours worldwide programming the same stuff over and over again, so I think these two features (GUIs and AutoML) will help Data Scientist on getting more productive and solving more problems.
On of the best free platforms for doing these tasks in a simple GUI is Deep Cognition. Their simple drag & drop interface helps you design deep learning models with ease. Deep Learning Studio can automatically design a deep learning model for your custom dataset thanks to their advance AutoML feature with nearly one click.
Here you can learn more about them:
Take a look at the prices :O, it’s freeeee :)
I mean, it’s amazing how fast the development in the area is right now, that we can have simple GUIs to interact with all the hard and interesting concepts I talked about in this post.
One of the things I like about that platform is that you can still code, interact with TensorFlow, Keras, Caffe, MXNet an much more with the command line or their Notebook without installing anything. You have both the notebook and the CLI!
I take my hat off to them and their contribution to society.
Other interesting applications of deep learning that you can try for free or for little cost are (some of them are on private betas):
Thanks for reading this weird introduction to Deep Learning. I hope it helped you getting started in this amazing area, or maybe just discover something new.
If you have questions just add me on LinkedIn and we’ll chat there:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data scientist, physicist and computer engineer. Love sharing ideas, thoughts and contributing to Open Source in Machine Learning and Deep Learning ;).
Sharing concepts, ideas, and codes.
",amazing introductions courses blog posts deep learning name resources sections different kind introduction weird maybe wont follow normal structure deep learning post start math go papers implementation applications close post journey deep learning think telling story much helpful throwing information formulas everywhere lets begin note theres companion webinar article find sometimes important written backup thoughts tend talk lot present several presentations conference way contributing little knowledge everyone deep learning dlis important field data science ai technology lives right deserves attention getting please dont say deep learning adding layer neural net thats magic nope im hoping reading different perspective dl created timeline based several papers timelines purpose everyone seeing deep learning much neural networks really theoretical advances software hardware improvements necessary us get day want ping ill send find contact end article deep learning around quite became relevant fast last 57 years said late 2000s still missing reliable way train deep neural networks nowadays development several simple important theoretical algorithmic improvements advances hardware mostly gpus tpus exponential generation accumulation data dl came naturally fit missing spot transform way machine learning deep learning active field research nothing settle closed still searching best models topology networks best ways optimize hyperparameters hard active field science keep date investigation impossible side note topology machine learning deep learning topological signatures hofer et al luckily us lots people helping understand digest information courses like andrew ng one blog posts much weird uncommon normally wait sometime sometime years able digest difficult advance information papers research journals course areas science really fast get paper blog post tells yo need know opinion dl different feel working something exciting people field saying last ideas papers deep learning specifically new topologies configurations nn algorithms improve usage best ideas machine learning decades remember dl inside ml ive used word learning lot article far learning context machine learning word learning describes automatic search process better representations data analyzing studying please mind making computer learn important word field representation dont forget representation way look data let give example lets say tell want drive line separates blue circles green triangles plot want use line author says impossible remember concept line case lost actually find way representing data different way way draw straight line separate types data somethinkg math taught us hundreds years ago case need coordinate transformation plot represent data way draw line look polar coordinate transformation solution thats draw line simple example found chose transformation get better representation hand create system program search different representations case coordinate change find way calculating percentage categories classified correctly new approach moment machine learning something important mind deep learning representation learning using different kinds neural networks optimize hyperparameters net get learnthe best representation data wouldnt possible without amazing breakthroughs led us current state deep learning name learning representations backpropagating errors david e rumelhart geoffrey e hinton ronald j williams theoretical framework backpropagation yann lecun 2 idea better initialization parameters nets something remember initialization strategy selected according activation function used next 3 idea better activation functions mean better ways approximating functions faster leading faster training process 4 idea dropout better ways preventing overfitting dropout simple way prevent neural networks overfitting great paper srivastava hinton others 5 idea convolutional neural nets cnns gradient based learning applied document recognition lecun others imagenet classification deep convolutional neural networks krizhevsky others 6 idea residual nets resnets 7 idea region based cnns used object detection 8 idea recurrent neural networks rnns lstms btw shown liao poggio 2016 resnets rnns arxiv160403640v1 9 idea generative adversarial networks gans 10 idea capsule networks many others think really important theoretical algorithmic breakthroughs changing world gave momentum dl revolution easy get started ill try best guide process check resources remember watching videos reading papers understanding programming coding failing making happen 1 learn python r 0 andrew ng coursera know doesnt need intro siraj raval hes amazing power explain hard concepts fun easy way follow youtube channel specifically playlists math intelligence intro deep learning 3 francois chollets book deep learning python r 3 ibm cognitive class 5 datacamp deep learning one important tools theories data scientist learn lucky see amazing people creating research software tools hardware specific dl tasks dl computationally expensive even though theres advances theory software hardware need developments big data distributed machine learning improve performance efficiency great people companies making amazing efforts join distributed frameworks spark dl libraries tf keras heres overview 2 elephas distributed dl keras pyspark 3 yahoo inc tensorflowonspark 4 cern distributed keras keras spark 5 qubole tutorial keras spark 6 intel corporation bigdl distributed deep learning library apache spark 7 tensorflow spark google cloud ive said one important moments field creation open sourced tensorflow tensorflow open source software library numerical computation using data flow graphs nodes graph represent mathematical operations graph edges represent multidimensional data arrays tensors communicated things seeing image tensor manipulations working riemann tensor general relativity tensors defined mathematically simply arrays numbers functions transform according certain rules change coordinates scope machine learning deep learning tensor generalization vectors matrices potentially higher dimensions internally tensorflow represents tensors ndimensional arrays base datatypes use heavily tensors time dl dont need expert use may need understand little bit list good resources check breakthroughs mentioned programming frameworks like tensorflow keras keras go think idea need understand work deep learning achieved far dl name francois chollet book dl much heres list 30 great funny applications dl thinking future deep learning programming building applications ill repeat said posts really think guis automl near future getting things done deep learning dont get wrong love coding think amount code writing next years decay cannot spend many hours worldwide programming stuff think two features guis automl help data scientist getting productive solving problems best free platforms tasks simple gui deep cognition simple drag drop interface helps design deep learning models ease deep learning studio automatically design deep learning model custom dataset thanks advance automl feature nearly one click learn take look prices freeeee mean amazing fast development area right simple guis interact hard interesting concepts talked post one things like platform still code interact tensorflow keras caffe mxnet much command line notebook without installing anything notebook cli take hat contribution society interesting applications deep learning try free little cost private betas thanks reading weird introduction deep learning hope helped getting started amazing area maybe discover something new questions add linkedin well chat quick cheer standing ovation clap show much enjoyed story data scientist physicist computer engineer love sharing ideas thoughts contributing open source machine learning deep learning sharing concepts ideas codes,en,"['Deep Learning', 'REP-RE-SEN-TA-TION', 'Geoffrey E. Hinton & Ronald J. Williams', 'Srivastava', 'Hinton', 'Convolutional Neural Nets', 'Lecun', 'ImageNet Classification with Deep Convolutional Neural Networks', 'Krizhevsky', 'ResNets', 'IBM', 'Big Data', 'Distributed Machine Learning', 'TF', 'Keras', 'Keras & PySpark', 'Yahoo', 'Intel Corporation', 'TensorFlow', 'Google', 'Deep Cognition', 'MXNet', 'CLI', 'LinkedIn', 'Data']"
208,Oleksandr Savsunenko,5500,The New Neural Internet is Coming – Hacker Noon,"How it all began / The Landscape
Think of the typical and well-studied neural networks (such as image classifier) as a left hemisphere of the neural network technology. With this in mind, it is easy to understand what is Generative Adversarial Network. It is a kind of right hemisphere — the one that is claimed to be responsible for creativity.
The Generative Adversarial Networks (GANs) are the first step of neural networks technology learning creativity. Typical GAN is a neural network trained to generate images on the certain topic using an image dataset and some random noise as a seed. Up until now images created by GANs were of low quality and limited in resolution. Recent advances by NVIDIA showed that it is within a reach to generate photorealistic images in high-resolution and they published the technology itself in open-access.
There is a plethora of GANs types of various complexity, architectures, and strange acronyms. We are mostly interested here in conditional GANs and variational autoencoders. Conditional GANs are capable of not just mimicking the broad type of images as “bedroom”, “face”, “dog” but also dive into more specific categories. For example, the Text2Image network is capable of translation textual image description into the image itself.
By varying random seed that is concatenated to the “meanings” vector we are able to produce an infinite number of birds image, matching description.
Let’s just close your eyes and see the world in 2 years. Companies like NVIDIA will push GAN technology to industry-ready level, same as they did with celebrities faces generation. This means, that a GAN will be able to generate any image, on-demand, on-the-fly based on textual (for example) description. This will render obsolete a number of photography and design related industries. Here’s how this will work.
Again, the network is able to generate an infinite number of images by varying random seed.
And here’s the scary part. Such a network can receive not only description of the target object it needs to generate, but also a vector describing you — the ad consumer. This ad can have a very deep description of your personality, web browsing history, recent transactions, and geolocation, so the GAN will generate one-time, unique and, that fits you perfectly. CTR is going sky high.
By measuring your reactions the network will adapt and make ads targeting you more and more precisely, hitting your soft spots.
So, at the end of the day, we are going to see a fully personalized content everywhere on the Internet.
Everyone will see fully custom versions of all content, that is adapted to the consumer based on his lifestyle, opinions, and history. We all witnessed arousal of this Bubble pattern after latest USA elections and it’s gonna be getting worse. GANs will able to target content precisely to you with no limitations of the medium — starting from image ads and up to complex opinions, tread and publications, generated by machines. This will create a constant feedback loop, improving based on your interactions. And there is going to be a competition of different GANs between each other. Kind of a fully automated war of phycological manipulations, having humanity as a battlefield. The driving force behind this trend is extremely simple — profits.
And this is not a scary doomsday scenario, this actually is happening today.
I have no idea. But surely we need few things: broad public discussions about this technology inevitable arrival and a backup plan to stop it. So, it’s better to start thinking now — how we can fight this process and benefit from it at the same time.
We are not there yet due to some technical limitation. Up until recently images generated by GANs were just of bad quality and easily spotted as fake. NVIDIA showed that it is actually doable to generate 1024x1024 extremely real faces. To move things forward we would need faster and bigger GPUs, more theoretical studies on GAN, more smart hacks around GAN training, more labeled datasets, etc.
Please, notice — we don’t need new power sources, quantum processors (but they can help), general AI to reach this point or some other purely theoretical new cool things. All we need is within a reach of few years and likely big corp already have this kind of resources available.
Also, we will need smarter neural networks. I am definitely looking for progress in capsules approach by Hinton et al. And of course, we will be the first to implement this in super-resolution technology, that should heavily benefit from GAN progress.
Let me know what you think.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Machine learning engineer, doer/maker/dreamer, father
how hackers start their afternoons.
",began landscape think typical wellstudied neural networks image classifier left hemisphere neural network technology mind easy understand generative adversarial network kind right hemisphere one claimed responsible creativity generative adversarial networks gans first step neural networks technology learning creativity typical gan neural network trained generate images certain topic using image dataset random noise seed images created gans low quality limited resolution recent advances nvidia showed within reach generate photorealistic images highresolution published technology openaccess plethora gans types various complexity architectures strange acronyms mostly interested conditional gans variational autoencoders conditional gans capable mimicking broad type images bedroom face dog also dive specific categories example text2image network capable translation textual image description image varying random seed concatenated meanings vector able produce infinite number birds image matching description lets close eyes see world 2 years companies like nvidia push gan technology industryready level celebrities faces generation means gan able generate image ondemand onthefly based textual example description render obsolete number photography design related industries heres work network able generate infinite number images varying random seed heres scary part network receive description target object needs generate also vector describing ad consumer ad deep description personality web browsing history recent transactions geolocation gan generate onetime unique fits perfectly ctr going sky high measuring reactions network adapt make ads targeting precisely hitting soft spots end day going see fully personalized content everywhere internet everyone see fully custom versions content adapted consumer based lifestyle opinions history witnessed arousal bubble pattern latest usa elections gonna getting worse gans able target content precisely limitations medium starting image ads complex opinions tread publications generated machines create constant feedback loop improving based interactions going competition different gans kind fully automated war phycological manipulations humanity battlefield driving force behind trend extremely simple profits scary doomsday scenario actually happening today idea surely need things broad public discussions technology inevitable arrival backup plan stop better start thinking fight process benefit time yet due technical limitation recently images generated gans bad quality easily spotted fake nvidia showed actually doable generate 1024x1024 extremely real faces move things forward would need faster bigger gpus theoretical studies gan smart hacks around gan training labeled datasets etc please notice dont need new power sources quantum processors help general ai reach point purely theoretical new cool things need within reach years likely big corp already kind resources available also need smarter neural networks definitely looking progress capsules approach hinton et al course first implement superresolution technology heavily benefit gan progress let know think quick cheer standing ovation clap show much enjoyed story machine learning engineer doermakerdreamer father hackers start afternoons,en,"['Generative Adversarial Network', 'GAN', 'NVIDIA', 'big corp']"
209,Max Pechyonkin,3400,Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning,"In this article, I will discuss two interesting recent papers that provide an easy way to improve performance of any given neural network by using a smart way to ensemble. They are:
Additional prerequisite reading that will make context of this post much more easy to understand:
Traditional ensembling combines several different models and makes them predict on the same input. Then some way of averaging is used to determine the final prediction of the ensemble. It can be simple voting, an average or even another model that learns to predict correct value or label based on the inputs of models in the ensemble. Ridge regression is one particular way of combining several predictions which is used by Kaggle-winning machine learning practitioners.
When applied in deep learning, ensembling can be used to combine predictions of several neural networks to produce one final prediction. Usually it is a good idea to use neural networks of different architectures in an ensemble, because they will likely make mistakes on different training samples and therefore the benefit of ensembling will be larger.
However, you can also ensemble models with the same architecture and it will give surprisingly good results. One very cool trick exploiting this approach was proposed in the snapshot ensembling paper. The authors take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.
You can refer to this awesome post for more details. If you aren’t yet using cyclical learning rates, then you definitely should, as it becomes the standard state-of-the art training technique that is very simple, not computationally heavy and provides significant gains at almost no additional cost.
All of the examples above are ensembles in the model space, because they combine several models and then use models’ predictions to produce the final prediction.
In the paper that I am discussing in this post, however, the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:
Let’s see how it works. But first we need to understand some important facts about loss surfaces and generalizable solutions.
The first important insight is that a trained network is a point in multidimensional weight space. For a given architecture, each distinct combination of network weights produces a separate model. Since there are infinitely many combinations of weights for any given architecture, there will be infinitely many solutions. The goal of training of a neural network is to find a particular solution (point in the weight space) that will provide low value of the loss function both on training and testing data sets.
During training, by changing weights, training algorithm changes the network and travel in the weight space. Gradient descent algorithm travels on a loss plane in this space where plane elevation is given by the value of the loss function.
It is very hard to visualize and understand the geometry of multidimensional weight space. At the same time, it is very important to understand it because stochastic gradient descent essentially traverses a loss surface in this highly multidimensional space during training and tries to find a good solution — a “point” on the loss surface where loss value is low. It is known that such surfaces have many local optima. But it turns out that not all of them are equally good.
One metric that can distinguish a good solution from a bad one is its flatness. The idea being that training data set and testing data set will produce similar but not exactly the same loss surfaces. You can imagine that a test surface will be shifted a bit relative to the train surface. For a narrow solution, during test time, a point that gave low loss can have a large loss because of this shift. This means that this “narrow” solution did not generalize well — training loss is low, while testing loss is large. On the other hand, for a “wide” and flat solution, this shift will lead to training and testing loss being close to each other.
I explained the difference between narrow and wide solutions because the new method which is the focus of this post leads to nice and wide solutions.
Initially, SGD will make a big jump in the weight space. Then, as the learning rate gets smaller due to cosine annealing, SGD will converge to some local solution and the algorithm will take a “snapshot” of the model by adding it to the ensemble. Then the rate is reset to high value again and SGD takes a large jump again before converging to some different local solution.
Cycle length in the snapshot ensembling approach is 20 to 40 epochs. The idea of long learning rate cycles is to be able to find sufficiently different models in the weight space. If the models are too similar, then predictions of the separate networks in the ensemble will be too close and the benefit of ensembling will be negligible.
Snapshot ensembling works really well and improves model performance, but Fast Geometric Ensembling works even better.
Fast geometric ensembling is very similar to snapshot ensembling, but is has some distinguishing features. It uses linear piecewise cyclical learning rate schedule instead of cosine. Secondly, the cycle length in FGE is much shorter — only 2 to 4 epochs per cycle. At first intuition, the short cycle is wrong because the models at the end of each cycle will be close to each other and therefore ensembling them will not give any benefits. However, as the authors discovered, because there exist connected paths of low loss between sufficiently different models, it is possible to travel along those paths in small steps and the models encountered along will be different enough to allow ensembling them with good results. Thus, FGE shows improvement compared to snapshot ensembles and it takes smaller steps to find the model (which makes it faster to train).
To benefit from both snapshot ensembling or FGE, one needs to store multiple models and then make predictions for all of them before averaging for the final prediction. Thus, for additional performance of the ensemble, one needs to pay with higher amount of computation. So there is no free lunch there. Or is there? This is where the new paper with stochastic weight averaging comes in.
Stochastic weight averaging closely approximates fast geometric ensembling but at a fraction of computational loss. SWA can be applied to any architecture and data set and shows good result in all of them. The paper suggests that SWA leads to wider minima, the benefits of which I discussed above. SWA is not an ensemble in its classical understanding. At the end of training you get one model, but it’s performance beats snapshot ensembles and approaches FGE.
Intuition for SWA comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low (points W1, W2 and W3 are at the border of the red area of low loss in the left panel of figure above). By taking the average of several such points, it is possible to achieve a wide, generalizable solution with even lower loss (Wswa in the left panel of the figure above).
Here is how it works. Instead of an ensemble of many models, you only need two models:
At the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). By following this approach, you only need to train one model, and store only two models in memory during training. For prediction, you only need the running average model and predicting on it is much faster than using ensemble described above, where you use many models to predict and then average results.
Authors of the paper provide their own implementation in PyTorch.
Also, SWA is implemented in the awesome fast.ai library that everyone should be using. And if you haven’t yet seen their course, then follow the links.
You can follow me on Twitter. Let’s also connect on LinkedIn.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Deep Learning
Sharing concepts, ideas, and codes.
",article discuss two interesting recent papers provide easy way improve performance given neural network using smart way ensemble additional prerequisite reading make context post much easy understand traditional ensembling combines several different models makes predict input way averaging used determine final prediction ensemble simple voting average even another model learns predict correct value label based inputs models ensemble ridge regression one particular way combining several predictions used kagglewinning machine learning practitioners applied deep learning ensembling used combine predictions several neural networks produce one final prediction usually good idea use neural networks different architectures ensemble likely make mistakes different training samples therefore benefit ensembling larger however also ensemble models architecture give surprisingly good results one cool trick exploiting approach proposed snapshot ensembling paper authors take weights snapshot training network training create ensemble nets architecture different weights allows improve test performance cheap way train one model saving weights time time refer awesome post details arent yet using cyclical learning rates definitely becomes standard stateofthe art training technique simple computationally heavy provides significant gains almost additional cost examples ensembles model space combine several models use models predictions produce final prediction paper discussing post however authors propose use novel ensembling weights space method produces ensemble combining weights network different stages training uses model combined weights make predictions 2 benefits approach lets see works first need understand important facts loss surfaces generalizable solutions first important insight trained network point multidimensional weight space given architecture distinct combination network weights produces separate model since infinitely many combinations weights given architecture infinitely many solutions goal training neural network find particular solution point weight space provide low value loss function training testing data sets training changing weights training algorithm changes network travel weight space gradient descent algorithm travels loss plane space plane elevation given value loss function hard visualize understand geometry multidimensional weight space time important understand stochastic gradient descent essentially traverses loss surface highly multidimensional space training tries find good solution point loss surface loss value low known surfaces many local optima turns equally good one metric distinguish good solution bad one flatness idea training data set testing data set produce similar exactly loss surfaces imagine test surface shifted bit relative train surface narrow solution test time point gave low loss large loss shift means narrow solution generalize well training loss low testing loss large hand wide flat solution shift lead training testing loss close explained difference narrow wide solutions new method focus post leads nice wide solutions initially sgd make big jump weight space learning rate gets smaller due cosine annealing sgd converge local solution algorithm take snapshot model adding ensemble rate reset high value sgd takes large jump converging different local solution cycle length snapshot ensembling approach 20 40 epochs idea long learning rate cycles able find sufficiently different models weight space models similar predictions separate networks ensemble close benefit ensembling negligible snapshot ensembling works really well improves model performance fast geometric ensembling works even better fast geometric ensembling similar snapshot ensembling distinguishing features uses linear piecewise cyclical learning rate schedule instead cosine secondly cycle length fge much shorter 2 4 epochs per cycle first intuition short cycle wrong models end cycle close therefore ensembling give benefits however authors discovered exist connected paths low loss sufficiently different models possible travel along paths small steps models encountered along different enough allow ensembling good results thus fge shows improvement compared snapshot ensembles takes smaller steps find model makes faster train benefit snapshot ensembling fge one needs store multiple models make predictions averaging final prediction thus additional performance ensemble one needs pay higher amount computation free lunch new paper stochastic weight averaging comes stochastic weight averaging closely approximates fast geometric ensembling fraction computational loss swa applied architecture data set shows good result paper suggests swa leads wider minima benefits discussed swa ensemble classical understanding end training get one model performance beats snapshot ensembles approaches fge intuition swa comes empirical observation local minima end learning rate cycle tend accumulate border areas loss surface loss value low points w1 w2 w3 border red area low loss left panel figure taking average several points possible achieve wide generalizable solution even lower loss wswa left panel figure works instead ensemble many models need two models end learning rate cycle current weights second model used update weight running average model taking weighted mean old running average weights new set weights second model formula provided figure left following approach need train one model store two models memory training prediction need running average model predicting much faster using ensemble described use many models predict average results authors paper provide implementation pytorch also swa implemented awesome fastai library everyone using havent yet seen course follow links follow twitter lets also connect linkedin quick cheer standing ovation clap show much enjoyed story deep learning sharing concepts ideas codes,en,"['Kaggle', 'algorithm', 'SGD', 'FGE', 'SWA', 'PyTorch', 'LinkedIn']"
210,Eugenio Culurciello,2800,"Artificial Intelligence, AI in 2018 and beyond – Towards Data Science","These are my opinions on where deep neural network and machine learning is headed in the larger field of artificial intelligence, and how we can get more and more sophisticated machines that can help us in our daily routines.
Please note that these are not predictions of forecasts, but more a detailed analysis of the trajectory of the fields, the trends and the technical needs we have to achieve useful artificial intelligence.
Not all machine learning is targeting artificial intelligences, and there are low-hanging fruits, which we will examine here also.
The goal of the field is to achieve human and super-human abilities in machines that can help us in every-day lives. Autonomous vehicles, smart homes, artificial assistants, security cameras are a first target. Home cooking and cleaning robots are a second target, together with surveillance drones and robots. Another one is assistants on mobile devices or always-on assistants. Another is full-time companion assistants that can hear and see what we experience in our life. One ultimate goal is a fully autonomous synthetic entity that can behave at or beyond human level performance in everyday tasks.
See more about these goals here, and here, and here.
Software is defined here as neural networks architectures trained with an optimization algorithm to solve a specific task.
Today neural networks are the de-facto tool for learning to solve tasks that involve learning supervised to categorize from a large dataset.
But this is not artificial intelligence, which requires acting in the real world often learning without supervision and from experiences never seen before, often combining previous knowledge in disparate circumstances to solve the current challenge.
Neural network architectures — when the field boomed, a few years back, we often said it had the advantage to learn the parameters of an algorithms automatically from data, and as such was superior to hand-crafted features. But we conveniently forgot to mention one little detail... the neural network architecture that is at the foundation of training to solve a specific task is not learned from data! In fact it is still designed by hand. Hand-crafted from experience, and it is currently one of the major limitations of the field. There is research in this direction: here and here (for example), but much more is needed. Neural network architectures are the fundamental core of learning algorithms. Even if our learning algorithms are capable of mastering a new task, if the neural network is not correct, they will not be able to. The problem on learning neural network architecture from data is that it currently takes too long to experiment with multiple architectures on a large dataset. One has to try training multiple architectures from scratch and see which one works best. Well this is exactly the time-consuming trial-and-error procedure we are using today! We ought to overcome this limitation and put more brain-power on this very important issue.
Unsupervised learning —we cannot always be there for our neural networks, guiding them at every stop of their lives and every experience. We cannot afford to correct them at every instance, and provide feedback on their performance. We have our lives to live! But that is exactly what we do today with supervised neural networks: we offer help at every instance to make them perform correctly. Instead humans learn from just a handful of examples, and can self-correct and learn more complex data in a continuous fashion. We have talked about unsupervised learning extensively here.
Predictive neural networks — A major limitation of current neural networks is that they do not possess one of the most important features of human brains: their predictive power. One major theory about how the human brain work is by constantly making predictions: predictive coding. If you think about it, we experience it every day. As you lift an object that you thought was light but turned out heavy. It surprises you, because as you approached to pick it up, you have predicted how it was going to affect you and your body, or your environment in overall.
Prediction allows not only to understand the world, but also to know when we do not, and when we should learn. In fact we save information about things we do not know and surprise us, so next time they will not! And cognitive abilities are clearly linked to our attention mechanism in the brain: our innate ability to forego of 99.9% of our sensory inputs, only to focus on the very important data for our survival — where is the threat and where do we run to to avoid it. Or, in the modern world, where is my cell-phone as we walk out the door in a rush.
Building predictive neural networks is at the core of interacting with the real world, and acting in a complex environment. As such this is the core network for any work in reinforcement learning. See more below.
We have talked extensively about the topic of predictive neural networks, and were one of the pioneering groups to study them and create them. For more details on predictive neural networks, see here, and here, and here.
Limitations of current neural networks — We have talked about before on the limitation of neural networks as they are today. Cannot predict, reason on content, and have temporal instabilities — we need a new kind of neural networks that you can about read here.
Neural Network Capsules are one approach to solve the limitation of current neural networks. We reviewed them here. We argue here that Capsules have to be extended with a few additional features:
Continuous learning — this is important because neural networks need to continue to learn new data-points continuously for their life. Current neural networks are not able to learn new data without being re-trained from scratch at every instance. Neural networks need to be able to self-assess the need of new training and the fact that they do know something. This is also needed to perform in real-life and for reinforcement learning tasks, where we want to teach machines to do new tasks without forgetting older ones.
For more detail, see this excellent blog post by Vincenzo Lomonaco.
Transfer learning — or how do we have these algorithms learn on their own by watching videos, just like we do when we want to learn how to cook something new? That is an ability that requires all the components we listed above, and also is important for reinforcement learning. Now you can really train your machine to do what you want by just giving an example, the same way we humans do every!
Reinforcement learning — this is the holy grail of deep neural network research: teach machines how to learn to act in an environment, the real world! This requires self-learning, continuous learning, predictive power, and a lot more we do not know. There is much work in the field of reinforcement learning, but to the author it is really only scratching the surface of the problem, still millions of miles away from it. We already talked about this here.
Reinforcement learning is often referred as the “cherry on the cake”, meaning that it is just minor training on top of a plastic synthetic brain. But how can we get a “generic” brain that then solve all problems easily? It is a chicken-in-the-egg problem! Today to solve reinforcement learning problems, one by one, we use standard neural networks:
Both these components are obvious solutions to the problem, and currently are clearly wrong, but that is what everyone uses because they are some of the available building blocks. As such results are unimpressive: yes we can learn to play video-games from scratch, and master fully-observable games like chess and go, but I do not need to tell you that is nothing compared to solving problems in a complex world. Imagine an AI that can play Horizon Zero Dawn better than humans... I want to see that!
But this is what we want. Machine that can operate like us.
Our proposal for reinforcement learning work is detailed here. It uses a predictive neural network that can operate continuously and an associative memory to store recent experiences.
No more recurrent neural networks — recurrent neural network (RNN) have their days counted. RNN are particularly bad at parallelizing for training and also slow even on special custom machines, due to their very high memory bandwidth usage — as such they are memory-bandwidth-bound, rather than computation-bound, see here for more details. Attention based neural network are more efficient and faster to train and deploy, and they suffer much less from scalability in training and deployment. Attention in neural network has the potential to really revolutionize a lot of architectures, yet it has not been as recognized as it should. The combination of associative memories and attention is at the heart of the next wave of neural network advancements.
Attention has already showed to be able to learn sequences as well as RNNs and at up to 100x less computation! Who can ignore that?
We recognize that attention based neural network are going to slowly supplant speech recognition based on RNN, and also find their ways in reinforcement learning architecture and AI in general.
Localization of information in categorization neural networks — We have talked about how we can localize and detect key-points in images and video extensively here. This is practically a solved problem, that will be embedded in future neural network architectures.
Hardware for deep learning is at the core of progress. Let us now forget that the rapid expansion of deep learning in 2008–2012 and in the recent years is mainly due to hardware:
And we have talked about hardware extensively before. But we need to give you a recent update! Last 1–2 years saw a boom in the are of machine learning hardware, and in particular on the one targeting deep neural networks. We have significant experience here, and we are FWDNXT, the makers of SnowFlake: deep neural network accelerator.
There are several companies working in this space: NVIDIA (obviously), Intel, Nervana, Movidius, Bitmain, Cambricon, Cerebras, DeePhi, Google, Graphcore, Groq, Huawei, ARM, Wave Computing. All are developing custom high-performance micro-chips that will be able to train and run deep neural networks.
The key is to provide the lowest power and the highest measured performance while computing recent useful neural networks operations, not raw theoretical operations per seconds — as many claim to do.
But few people in the field understand how hardware can really change machine learning, neural networks and AI in general. And few understand what is important in micro-chips and how to develop them.
Here is our list:
About neuromorphic neural networks hardware, please see here.
We talked briefly about applications in the Goals section above, but we really need to go into details here. How is AI and neural network going to get into our daily life?
Here is our list:
I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more...
If you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!
For interesting additional reading, please see:
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
I dream and build new technology
Sharing concepts, ideas, and codes.
",opinions deep neural network machine learning headed larger field artificial intelligence get sophisticated machines help us daily routines please note predictions forecasts detailed analysis trajectory fields trends technical needs achieve useful artificial intelligence machine learning targeting artificial intelligences lowhanging fruits examine also goal field achieve human superhuman abilities machines help us everyday lives autonomous vehicles smart homes artificial assistants security cameras first target home cooking cleaning robots second target together surveillance drones robots another one assistants mobile devices alwayson assistants another fulltime companion assistants hear see experience life one ultimate goal fully autonomous synthetic entity behave beyond human level performance everyday tasks see goals software defined neural networks architectures trained optimization algorithm solve specific task today neural networks defacto tool learning solve tasks involve learning supervised categorize large dataset artificial intelligence requires acting real world often learning without supervision experiences never seen often combining previous knowledge disparate circumstances solve current challenge neural network architectures field boomed years back often said advantage learn parameters algorithms automatically data superior handcrafted features conveniently forgot mention one little detail neural network architecture foundation training solve specific task learned data fact still designed hand handcrafted experience currently one major limitations field research direction example much needed neural network architectures fundamental core learning algorithms even learning algorithms capable mastering new task neural network correct able problem learning neural network architecture data currently takes long experiment multiple architectures large dataset one try training multiple architectures scratch see one works best well exactly timeconsuming trialanderror procedure using today ought overcome limitation put brainpower important issue unsupervised learning cannot always neural networks guiding every stop lives every experience cannot afford correct every instance provide feedback performance lives live exactly today supervised neural networks offer help every instance make perform correctly instead humans learn handful examples selfcorrect learn complex data continuous fashion talked unsupervised learning extensively predictive neural networks major limitation current neural networks possess one important features human brains predictive power one major theory human brain work constantly making predictions predictive coding think experience every day lift object thought light turned heavy surprises approached pick predicted going affect body environment overall prediction allows understand world also know learn fact save information things know surprise us next time cognitive abilities clearly linked attention mechanism brain innate ability forego 999 sensory inputs focus important data survival threat run avoid modern world cellphone walk door rush building predictive neural networks core interacting real world acting complex environment core network work reinforcement learning see talked extensively topic predictive neural networks one pioneering groups study create details predictive neural networks see limitations current neural networks talked limitation neural networks today cannot predict reason content temporal instabilities need new kind neural networks read neural network capsules one approach solve limitation current neural networks reviewed argue capsules extended additional features continuous learning important neural networks need continue learn new datapoints continuously life current neural networks able learn new data without retrained scratch every instance neural networks need able selfassess need new training fact know something also needed perform reallife reinforcement learning tasks want teach machines new tasks without forgetting older ones detail see excellent blog post vincenzo lomonaco transfer learning algorithms learn watching videos like want learn cook something new ability requires components listed also important reinforcement learning really train machine want giving example way humans every reinforcement learning holy grail deep neural network research teach machines learn act environment real world requires selflearning continuous learning predictive power lot know much work field reinforcement learning author really scratching surface problem still millions miles away already talked reinforcement learning often referred cherry cake meaning minor training top plastic synthetic brain get generic brain solve problems easily chickenintheegg problem today solve reinforcement learning problems one one use standard neural networks components obvious solutions problem currently clearly wrong everyone uses available building blocks results unimpressive yes learn play videogames scratch master fullyobservable games like chess go need tell nothing compared solving problems complex world imagine ai play horizon zero dawn better humans want see want machine operate like us proposal reinforcement learning work detailed uses predictive neural network operate continuously associative memory store recent experiences recurrent neural networks recurrent neural network rnn days counted rnn particularly bad parallelizing training also slow even special custom machines due high memory bandwidth usage memorybandwidthbound rather computationbound see details attention based neural network efficient faster train deploy suffer much less scalability training deployment attention neural network potential really revolutionize lot architectures yet recognized combination associative memories attention heart next wave neural network advancements attention already showed able learn sequences well rnns 100x less computation ignore recognize attention based neural network going slowly supplant speech recognition based rnn also find ways reinforcement learning architecture ai general localization information categorization neural networks talked localize detect keypoints images video extensively practically solved problem embedded future neural network architectures hardware deep learning core progress let us forget rapid expansion deep learning 20082012 recent years mainly due hardware talked hardware extensively need give recent update last 12 years saw boom machine learning hardware particular one targeting deep neural networks significant experience fwdnxt makers snowflake deep neural network accelerator several companies working space nvidia obviously intel nervana movidius bitmain cambricon cerebras deephi google graphcore groq huawei arm wave computing developing custom highperformance microchips able train run deep neural networks key provide lowest power highest measured performance computing recent useful neural networks operations raw theoretical operations per seconds many claim people field understand hardware really change machine learning neural networks ai general understand important microchips develop list neuromorphic neural networks hardware please see talked briefly applications goals section really need go details ai neural network going get daily life list almost 20 years experience neural networks hardware software rare combination see medium webpage scholar linkedin found article useful please consider donation support tutorials blogs contribution make difference interesting additional reading please see quick cheer standing ovation clap show much enjoyed story dream build new technology sharing concepts ideas codes,en,"['smart homes', 'the de-facto tool', 'Attention', 'FWDNXT', 'SnowFlake', 'Intel', 'DeePhi', 'Google', 'Huawei', 'ARM', 'LinkedIn']"
211,Devin Soni,5800,"Spiking Neural Networks, the Next Generation of Machine Learning","Everyone who has been remotely tuned in to recent progress in machine learning has heard of the current 2nd generation artificial neural networks used for machine learning. These are generally fully connected, take in continuous values, and output continuous values. Although they have allowed us to make breakthrough progress in many fields, they are biologically inn-accurate and do not actually mimic the actual mechanisms of our brain’s neurons.
The 3rd generation of neural networks, spiking neural networks, aims to bridge the gap between neuroscience and machine learning, using biologically-realistic models of neurons to carry out computation. A spiking neural network (SNN) is fundamentally different from the neural networks that the machine learning community knows. SNNs operate using spikes, which are discrete events that take place at points in time, rather than continuous values. The occurrence of a spike is determined by differential equations that represent various biological processes, the most important of which is the membrane potential of the neuron. Essentially, once a neuron reaches a certain potential, it spikes, and the potential of that neuron is reset. The most common model for this is the Leaky integrate-and-fire (LIF) model. Additionally, SNNs are often sparsely connected and take advantage of specialized network topologies.
At first glance, this may seem like a step backwards. We have moved from continuous outputs to binary, and these spike trains are not very interpretable. However, spike trains offer us enhanced ability to process spatio-temporal data, or in other words, real-world sensory data. The spatial aspect refers to the fact that neurons are only connected to neurons local to them, so these inherently process chunks of the input separately (similar to how a CNN would using a filter). The temporal aspect refers to the fact that spike trains occur over time, so what we lose in binary encoding, we gain in the temporal information of the spikes. This allows us to naturally process temporal data without the extra complexity that RNNs add. It has been proven, in fact, that spiking neurons are fundamentally more powerful computational units than traditional artificial neurons.
Given that these SNNs are more powerful, in theory, than 2nd generation networks, it is natural to wonder why we do not see widespread use of them. The main issue that currently lies in practical use of SNNs is that of training. Although we have unsupervised biological learning methods such as Hebbian learning and STDP, there are no known effective supervised training methods for SNNs that offer higher performance than 2nd generation networks. Since spike trains are not differentiable, we cannot train SNNs using gradient descent without losing the precise temporal information in spike trains. Therefore, in order to properly use SNNs for real-world tasks, we would need to develop an effective supervised learning method. This is a very difficult task, as doing so would involve determining how the human brain actually learns, given the biological realism in these networks.
Another issue, that we are much closer to solving, is that simulating SNNs on normal hardware is very computationally-intensive since it requires simulating differential equations. However, neuromorphic hardware such as IBM’s TrueNorth aims to solve this by simulating neurons using specialized hardware that can take advantage of the discrete and sparse nature of neuronal spiking behavior.
The future of SNNs therefore remains unclear. On one hand, they are the natural successor of our current neural networks, but on the other, they are quite far from being practical tools for most tasks. There are some current real-world applications of SNNs in real-time image and audio processing, but the literature on practical applications remains sparse. Most papers on SNNs are either theoretical, or show performance under that of a simple fully-connected 2nd generation network. However, there are many teams working on developing SNN supervised learning rules, and I remain optimistic for the future of SNNs.
Make sure you give this post 50 claps and my blog a follow if you enjoyed this post and want to see more.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
crypto markets, data science ☞ twitter @devin_soni ☞ website https://100.github.io/
Sharing concepts, ideas, and codes.
",everyone remotely tuned recent progress machine learning heard current 2nd generation artificial neural networks used machine learning generally fully connected take continuous values output continuous values although allowed us make breakthrough progress many fields biologically innaccurate actually mimic actual mechanisms brains neurons 3rd generation neural networks spiking neural networks aims bridge gap neuroscience machine learning using biologicallyrealistic models neurons carry computation spiking neural network snn fundamentally different neural networks machine learning community knows snns operate using spikes discrete events take place points time rather continuous values occurrence spike determined differential equations represent various biological processes important membrane potential neuron essentially neuron reaches certain potential spikes potential neuron reset common model leaky integrateandfire lif model additionally snns often sparsely connected take advantage specialized network topologies first glance may seem like step backwards moved continuous outputs binary spike trains interpretable however spike trains offer us enhanced ability process spatiotemporal data words realworld sensory data spatial aspect refers fact neurons connected neurons local inherently process chunks input separately similar cnn would using filter temporal aspect refers fact spike trains occur time lose binary encoding gain temporal information spikes allows us naturally process temporal data without extra complexity rnns add proven fact spiking neurons fundamentally powerful computational units traditional artificial neurons given snns powerful theory 2nd generation networks natural wonder see widespread use main issue currently lies practical use snns training although unsupervised biological learning methods hebbian learning stdp known effective supervised training methods snns offer higher performance 2nd generation networks since spike trains differentiable cannot train snns using gradient descent without losing precise temporal information spike trains therefore order properly use snns realworld tasks would need develop effective supervised learning method difficult task would involve determining human brain actually learns given biological realism networks another issue much closer solving simulating snns normal hardware computationallyintensive since requires simulating differential equations however neuromorphic hardware ibms truenorth aims solve simulating neurons using specialized hardware take advantage discrete sparse nature neuronal spiking behavior future snns therefore remains unclear one hand natural successor current neural networks quite far practical tools tasks current realworld applications snns realtime image audio processing literature practical applications remains sparse papers snns either theoretical show performance simple fullyconnected 2nd generation network however many teams working developing snn supervised learning rules remain optimistic future snns make sure give post 50 claps blog follow enjoyed post want see quick cheer standing ovation clap show much enjoyed story crypto markets data science twitter devin_soni website https100githubio sharing concepts ideas codes,en,"['CNN', 'IBM', 'TrueNorth']"
212,Carlos E. Perez,3900,Surprise! Neurons are Now More Complex than We Thought!!,"One of the biggest misconceptions around is the idea that Deep Learning (DL) or Artificial Neural Networks (ANN) mimics biological neurons. At best, ANN mimics a cartoonish version of a 1957 model of a neuron. Anyone claiming Deep Learning is biologically inspired is doing so for marketing purposes or has never bother to read biological literature. Neurons in Deep Learning are essentially mathematical functions that perform a similarity function of its inputs against internal weights. The closer a match is made, the more likely an action is performed (i.e. not sending a signal to zero). There are exceptions to this model (see: Autoregressive networks) however it is general enough to include the perceptron, convolution networks and RNNs.
Neurons are very different from DL constructs. They don’t maintain continuous signals but rather exhibit spiking or event-driven behavior. So, when you hear about “neuromorphic” hardware, then these are inspired on “integrate and spike” neurons. These kinds of system, at best, get a lot of press (see: IBM TrueNorth), but have never been shown to be effective. However, there has been some research work that has shown some progress (see: https://arxiv.org/abs/1802.02627v1). If you ask me, if you truly want to build biologically inspired cognition, then you should at the very least explore systems are not continuous like DL. Biological systems, by nature, will use the least amount of energy to survive. DL systems, in stark contrast, are power hungry. That’s because DL is a brute-force method to achieve cognition. We know it works, we just don’t know how to scale it down.
Jeff Hawkins of Numenta has always lamented that a more biologically-inspired approach is needed. So, in his research in building cognitive machinery, he has architected systems that try to more closely mirror the structure of the neo-cortex. Numenta’s model of a neuron is considerably more elaborate than the Deep Learning model of a neuron as you can see in this graphic:
The team at Numenta is betting on this approach in the hopes of creating something that is more capable than Deep Learning. It hasn’t been proved to be anywhere near successful. They’ve been doing this long enough that the odds of them succeeding are diminishing overtime. Bycontrast, Deep Learning (despite its model of a cartoon neuron) has been shown to be unexpectedly effective in performing all kinds of mind-boggling feats of cognition. Deep Learning is doing something that is extraordinarily correct, we just don’t know exactly what that is!
Unfortunately, we have to throw in a new monkey wrench on all these research. New experiments on the nature of neurons have revealed that biological neurons are even more complex than we have imagined them to be:
In short, there is a lot more going on inside a single neuron than the simple idea of integrate-and-fire. Neurons may not be pure functions dependent on a single parameter (i.e. weight) but rather they are stateful machines. Alternatively, perhaps the weight may not even be single-valued and instead requires complex-valued or maybe higher dimensions. This is all behavior that research has yet to explore and thus we have little understanding to date.
If you think this throws a monkey wrench on our understanding, there’s an even newer discovery that reveals even greater complexity:
What this research reveals is that there is a mechanism for neurons to communicate with each other by sending packages of RNA code. To clarify, these are packages of instructions and not packages of data. There is a profound difference between sending codes and sending data. This implies that behavior from one neuron can change the behavior of another neuron; not through observation, but rather through injection of behavior.
This code exchange mechanism hints at the validity of my earlier conjecture: “Are biological brains made of only discrete logic?”
Experimental evidence reveals a new reality. Even at the smallest unit of our cognition, there is a kind of conversational cognition that is going on between individual neurons that modifies each other’s behavior. Thus, not only are neurons machines with state, they are also machines with an instruction set and a way to send code to each other. I’m sorry, but this is just another level of complexity.
There are two obvious ramifications of these experimental discoveries. The first is that our estimates of the computational capabilities of the human brain are likely to be at least an order of magnitude off. The second is that research will begin in earnest to explore DL architectures with more complex internal node (or neuron) structures.
If we were to make the rough argument that a single neuron performs a single operation, the total capacity of the human brain is measured at 38 peta operations per second. If were then to assume a DL model of operations being equal to floating point operations then a 38 petaflops system would be equivalent in capability. The top ranked supercomputer, Sunway Taihulight from China is estimated at 125 petaflops. However, let’s say the new results reveal 10x more computation, then the number should be 380 petaflops and we perhaps have breathing room until 2019. What is obvious, however, is that biological brains actually perform much more cognition with less computation.
The second consequences it that it’s now time to get back to the drawing board and begin to explore more complex kinds of neurons. The more complex kinds we’ve seen to date are the ones derived from LSTM. Here is the result of a brute force architectural search for LSTM-like neurons:
It’s not clear why these more complex LSTM are more effective. Only the architectural search algorithm knows but it can’t explain itself.
There is newly released paper that explores more complex hand-engineered LSTMs:
that reveals measurable improvements over standard LSTMs:
In summary, a research plan that explores more complex kinds of neurons may bear promising fruit. This is not unlike the research that explores the use of complex values in neural networks. In these complex-valued networks, performance improvements are noticed only on RNN networks. This should indicate that these internal neuron complexities may be necessary for capabilities beyond simple perception. I suspect that these complexities are necessary for advanced cognition that seems to evade current Deep Learning systems. These include robustness to adversarial features, learning to forget, learning what to ignore, learning abstraction and recognizing contextual switching.
I predict in the near future that we shall see more aggressive research in this area. After all, nature is already unequivocally telling us that neurons are individually more complex and therefore our own neuron models may also need to be more complex. Perhaps we need something as complicated as a Grassmann Algebra to make progress. ;-)
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Author of Artificial Intuition and the Deep Learning Playbook — Intuition Machine Inc.
Deep Learning Patterns, Methodology and Strategy
",one biggest misconceptions around idea deep learning dl artificial neural networks ann mimics biological neurons best ann mimics cartoonish version 1957 model neuron anyone claiming deep learning biologically inspired marketing purposes never bother read biological literature neurons deep learning essentially mathematical functions perform similarity function inputs internal weights closer match made likely action performed ie sending signal zero exceptions model see autoregressive networks however general enough include perceptron convolution networks rnns neurons different dl constructs dont maintain continuous signals rather exhibit spiking eventdriven behavior hear neuromorphic hardware inspired integrate spike neurons kinds system best get lot press see ibm truenorth never shown effective however research work shown progress see httpsarxivorgabs180202627v1 ask truly want build biologically inspired cognition least explore systems continuous like dl biological systems nature use least amount energy survive dl systems stark contrast power hungry thats dl bruteforce method achieve cognition know works dont know scale jeff hawkins numenta always lamented biologicallyinspired approach needed research building cognitive machinery architected systems try closely mirror structure neocortex numentas model neuron considerably elaborate deep learning model neuron see graphic team numenta betting approach hopes creating something capable deep learning hasnt proved anywhere near successful theyve long enough odds succeeding diminishing overtime bycontrast deep learning despite model cartoon neuron shown unexpectedly effective performing kinds mindboggling feats cognition deep learning something extraordinarily correct dont know exactly unfortunately throw new monkey wrench research new experiments nature neurons revealed biological neurons even complex imagined short lot going inside single neuron simple idea integrateandfire neurons may pure functions dependent single parameter ie weight rather stateful machines alternatively perhaps weight may even singlevalued instead requires complexvalued maybe higher dimensions behavior research yet explore thus little understanding date think throws monkey wrench understanding theres even newer discovery reveals even greater complexity research reveals mechanism neurons communicate sending packages rna code clarify packages instructions packages data profound difference sending codes sending data implies behavior one neuron change behavior another neuron observation rather injection behavior code exchange mechanism hints validity earlier conjecture biological brains made discrete logic experimental evidence reveals new reality even smallest unit cognition kind conversational cognition going individual neurons modifies others behavior thus neurons machines state also machines instruction set way send code im sorry another level complexity two obvious ramifications experimental discoveries first estimates computational capabilities human brain likely least order magnitude second research begin earnest explore dl architectures complex internal node neuron structures make rough argument single neuron performs single operation total capacity human brain measured 38 peta operations per second assume dl model operations equal floating point operations 38 petaflops system would equivalent capability top ranked supercomputer sunway taihulight china estimated 125 petaflops however lets say new results reveal 10x computation number 380 petaflops perhaps breathing room 2019 obvious however biological brains actually perform much cognition less computation second consequences time get back drawing board begin explore complex kinds neurons complex kinds weve seen date ones derived lstm result brute force architectural search lstmlike neurons clear complex lstm effective architectural search algorithm knows cant explain newly released paper explores complex handengineered lstms reveals measurable improvements standard lstms summary research plan explores complex kinds neurons may bear promising fruit unlike research explores use complex values neural networks complexvalued networks performance improvements noticed rnn networks indicate internal neuron complexities may necessary capabilities beyond simple perception suspect complexities necessary advanced cognition seems evade current deep learning systems include robustness adversarial features learning forget learning ignore learning abstraction recognizing contextual switching predict near future shall see aggressive research area nature already unequivocally telling us neurons individually complex therefore neuron models may also need complex perhaps need something complicated grassmann algebra make progress quick cheer standing ovation clap show much enjoyed story author artificial intuition deep learning playbook intuition machine inc deep learning patterns methodology strategy,en,"['Artificial Neural Networks', 'ANN', 'IBM TrueNorth', 'DL systems', 'Sunway Taihulight', 'algorithm', 'Deep Learning', 'Deep Learning Patterns']"
213,Nityesh Agarwal,2400,“WTH does a neural network even learn??” — a newcomer’s dilemma,"I believe, we all have that psychologist/philosopher in our brains that likes to ponder upon how thinking happens.
There.
A simple, clear bird’s eye view of what neural networks learn — they learn “increasingly more complex concepts”.
Doesn’t that feel familiar? Isn’t that how we learn anything at all?
For instance, let’s consider how we, as kids, probably learnt to recognise objects and animals —
See?
So, neural networks learn like we do!
It almost eases the mind to believe that we have this intangible sort of.. man-made “thing” that is analogous to the mind itself! It is especially appealing to someone who has just begun his/her Deep Learning journey.
But NO. A neural network’s learning is NOT ANALOGOUS to our own. Almost all the credible guides and ‘starters packs’ on the subject of deep learning come with a warning, something along the lines of:
..and that’s where all the confusion begins!
I think this was mostly because of the way in which most of the tutorials and beginner level books approach the subject.
Let’s see how Michael Nielsen describes what the hidden neurons are doing in his book — Neural Networks and Deep Learning:
He, like many others, uses the analogy between neural networks and the human mind to try to explain a neural networks. The way lines and edges make loops, which then help in recognising some digits is what we would think of doing. Many other tutorials try to use a similar analogy to explain what it means to build a hierarchy of knowledge.
I have to say that because of this analogy, I understand neural nets better.
But it is one of the paradoxes, that the very analogy that makes a difficult concept intelligible to the masses, can also create an illusion of knowledge among them.
Readers need to understand that it is just an analogy. Nothing more, nothing less. They need to understand that every simple analogy needs to be followed by more rigorous, seemingly difficult explanations.
Now don’t get me wrong. I am deeply thankful to Michael Nielsen for writing this book. It is one of the best books on the subject out there. He is careful in mentioning that this is “just for the sake of argument”.
But I took it to mean this — Maybe, the network won’t use the same exact pieces. Maybe, it will figure out some other pieces and join them in some other way to recognise the digits. But the essence will be the same. Right? I mean each of those pieces has to be some kind of an edge or a line or some loopy structure. After all, it doesn’t seem like there are other possibilities if you want to build a hierarchical structure to solve the problem of recognising digits.
As I gained a better intuition about them and how they work, I understood that this view is obviously wrong. It hit me..
Let’s consider loops —
Being able to identify a loop is essential for us humans to write digits- an 8 is two loops joined end-to-end, a 9 is loop with a tail under it and a 6 is loop with a tail up top. But when it comes to recognising digits in an image, features like loops seem difficult and infeasible for a neural network (Remember, I’m talking about your vanilla neutral networks or MLPs here).
I know its just a lot of “hand-wavy” reasoning but I think it is enough to convince. Probably, the edges and all the other hand-engineered features will face similar problems.
..and there’s the dilemma!
I had no clue about the answer or how to find it until 3blue1brown released a set of videos about neural networks. It was Grant Sanderson’s take at explaining the subject to newcomers. Maybe even he felt that there were some missing pieces in the explanation by other people and that he could address them in his tutorials.
And boy, did he!
Grant Sanderson of 3blue1brown, who uses a structure with 2 hidden layers, says —
The very loops and edges that we ruled out above.
They were not looking for loops or edges or anything even remotely close! They were looking for.. well something inexplicable.. some strange patterns that can be confused for random noise!
I found those weight matrix images (in the above screenshot) really fascinating. I thought of them as a Lego puzzle.
The weight matrix images were like the elementary Lego blocks and my task was to figure out a way to arrange them together so that I could create all 10 digits. This idea was inspired from the excerpt of Neural Networks and Deep Learning that I posted above. There we saw how we could assemble a 0 using hand-made features like edges and curves. So, I thought that, maybe, we could do the same with the features that the neural network actually found good.
All I needed was those weight matrix images that were used in 3blue1brown’s video. Now the problem was that Grant had put only 7 images in the video. So, I was gonna have to generate them on my own and create my very own set of Lego blocks!
I imported the code used in Michael Nielsen’s book to a Jupyter notebook. Then, I extended the Network class in there to include the methods that would help me visualise the weight matrices.
One pixel for every connection in the network. One image for each neuron showing how much it ‘likes’(colour: blue) or ‘dislikes’(colour: red) the previous layer neurons.
So, if I was to look at the image belonging to one of the neurons in the hidden layer, it would be like a heat map showing one feature, one basic Lego block that will be used to recognise digits. Blue pixels would represent connections that it “likes” whereas red ones would represent the connections that it “dislikes”.
I trained a neural network that had:
Notice that we will have 30 different types of basic Lego blocks for our Lego puzzle here because that’s the size of our hidden layer.
And.. here’s what they look like! —
These are the features that we were looking for! The ones that are better than loops and edges according to the network.
And here’s how it classifies all 10 digits:
And guess what?None of them make any sense!!
None of the features seem to capture any isolated distinguishable feature in the input image. All of them can be mistaken to be just randomly shaped blobs at randomly chosen places.
I mean, just look at how it identifies a ‘0':
This is the weight matrix image for the output neuron that recognizes ‘0's:
To be clear, the pixels in this image represent the weights connecting the hidden layer to the output neuron that recognises ‘0's.
We shall take only a handful of the most useful features for each digit into account. To do that, we can visually select the most intense blue pixels and the most intense red pixels. Here, the blue ones should give us the most useful features and the red ones should give us the most dreaded ones (think of it as the neuron saying — “The image will absolutely *not* match this prototype if it is a 0”).
Indices of the three most intense blue pixels: 3, 6, 26Indices of the three most intense red pixels: 5, 18, 22
Matrices 6 and 26 seem to capture something like a blue boundary of sorts that is surrounding inner red pixels — exactly what could actually help in identifying a ‘0’.
But what about matrix 3? It does not capture any feature we can even explain in words. The same goes for matrix 18. Why would the neuron not like it? It seems quite similar to matrix 3. And let’s not even go into the weird blue ‘S’ in 22.
Nonsensical, see!
Let’s do it for ‘1’:
Indices of the three most intense blue pixels: 0, 11, 16Indices of the top two most intense red pixels: 7, 20
I have no words for this one! I won’t even try to comment.
In what world can THOSE be used to identify 1’s !?
Now, the much anticipated ‘8’ (how will it represent the 2 loops in it??):
Top 3 most intense blue pixels: 1, 6, 14Top 3 most intense red pixels: 7, 24, 27
Nope, this isn’t any good either. There seem to be no loops like we were expecting it to have. But there is another interesting thing to notice in here — A majority of the pixels in the output layer neuron image (the one above the collage) are red. It seems like the network has figured out a way to recognise 8s using features that it does not like!
So, NO. I couldn’t put digits together using those features as Lego blocks. I failed real bad at the task.
But to be fair to myself, those features weren’t so much Lego-blocky either! Here’s why—
So, there it is. Neural networks can be said to learn like us if you consider the way they build hierarchies of features just like we do. But when you see the features themselves, they are nothing like what we would use. The networks give you almost no explanation for the features that they learn.
Neural networks are good function approximators. When we build and train one, we mostly just care about its accuracy — On what percentage of the test samples does it give positive results?
This works incredibly well for a lot of purposes because modern neural nets can have remarkably high accuracies — upward of 98% is not uncommon (meaning that the chances of failure are just 1 in a 100!)
But here’s the catch — When they are wrong, there’s no easy way to understand the reason why they are. They can’t be “debugged” in the traditional sense. For example, here’s an embarrassing incident that happened with Google because of this:
Understanding what neural networks learn is a subject of great importance. It is crucial to unleashing the true power of deep learning. It will help us in
A few weeks ago The New York Times Magazine ran a story about how neural networks were trained to predict the death of cancer patients with a remarkable accuracy.
Here’s what the writer, an oncologist, said:
I think I can strongly relate to this because of my little project. :-)
During the little project that I described earlier, I stumbled upon a few other results that I found really cool and worth sharing. So here they are —
Smaller networks:
I wanted to see how low I could make the hidden layer size while still getting a considerable accuracy across my test set. It turns out that with 10 neurons, the network was able to classify 9343 out of 10000 test images correctly. That’s 93.43% accuracy at classifying images that it has never seen with just 10 hidden neurons.
Just 10 different types of Lego blocks to recognise 10 digits!!
I find this incredibly fascinating.
Of course, these weights don’t make much sense either!
In case you are curious, I tried it with 5 neurons too and I got an accuracy of 86.65%; 4 neurons- accuracy 83.73%; below that it dropped very steeply — 3 neurons- 58.75%, 2 neurons- 22.80%.
Weight initialisation + regularisation makes a LOT of difference:
Just regularising your network and using good initialisations for the weights can have a huge effect on what your network learns.
Let me demonstrate.
I used the same network architecture, meaning same no. of layers and same no. of neurons in the layers. I then trained 2 Network objects- one without regularisation and using the same old np.random.randn() whereas in the other one I used regularisation along with np.random.randn()/sqrt(n). This is what I observed:
Yeah! I was shocked too!
(Note: I have shown the weight matrices associated with different index neurons in the above collage. This is because due to different initialisations, even the ones at the same index learn different features. So, I chose the ones that appear to make the effect most starking.)
To know more about weight initialisation techniques in neural networks I recommend that you start here.
If you want to discuss this article or any other project that you have in mind or really anything AI please feel free to comment below or drop me a message on LinkedIn, Facebook or Twitter. I have learnt a lot more about deep learning since I did the project in this article (like completing the Deep Learning Specialisation at Coursera!😄). Don’t hesitate to reach out if you think I could be of any help.
Thank you for reading! 😄 You can follow me on Twitter — https://twitter.com/nityeshaga; I won’t spam your feed. 😉
Originally published on the Zeolearn blog.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Reader, writer and a programmer.
Sharing concepts, ideas, and codes.
",believe psychologistphilosopher brains likes ponder upon thinking happens simple clear birds eye view neural networks learn learn increasingly complex concepts doesnt feel familiar isnt learn anything instance lets consider kids probably learnt recognise objects animals see neural networks learn like almost eases mind believe intangible sort manmade thing analogous mind especially appealing someone begun hisher deep learning journey neural networks learning analogous almost credible guides starters packs subject deep learning come warning something along lines thats confusion begins think mostly way tutorials beginner level books approach subject lets see michael nielsen describes hidden neurons book neural networks deep learning like many others uses analogy neural networks human mind try explain neural networks way lines edges make loops help recognising digits would think many tutorials try use similar analogy explain means build hierarchy knowledge say analogy understand neural nets better one paradoxes analogy makes difficult concept intelligible masses also create illusion knowledge among readers need understand analogy nothing nothing less need understand every simple analogy needs followed rigorous seemingly difficult explanations dont get wrong deeply thankful michael nielsen writing book one best books subject careful mentioning sake argument took mean maybe network wont use exact pieces maybe figure pieces join way recognise digits essence right mean pieces kind edge line loopy structure doesnt seem like possibilities want build hierarchical structure solve problem recognising digits gained better intuition work understood view obviously wrong hit lets consider loops able identify loop essential us humans write digits 8 two loops joined endtoend 9 loop tail 6 loop tail top comes recognising digits image features like loops seem difficult infeasible neural network remember im talking vanilla neutral networks mlps know lot handwavy reasoning think enough convince probably edges handengineered features face similar problems theres dilemma clue answer find 3blue1brown released set videos neural networks grant sandersons take explaining subject newcomers maybe even felt missing pieces explanation people could address tutorials boy grant sanderson 3blue1brown uses structure 2 hidden layers says loops edges ruled looking loops edges anything even remotely close looking well something inexplicable strange patterns confused random noise found weight matrix images screenshot really fascinating thought lego puzzle weight matrix images like elementary lego blocks task figure way arrange together could create 10 digits idea inspired excerpt neural networks deep learning posted saw could assemble 0 using handmade features like edges curves thought maybe could features neural network actually found good needed weight matrix images used 3blue1browns video problem grant put 7 images video gonna generate create set lego blocks imported code used michael nielsens book jupyter notebook extended network class include methods would help visualise weight matrices one pixel every connection network one image neuron showing much likescolour blue dislikescolour red previous layer neurons look image belonging one neurons hidden layer would like heat map showing one feature one basic lego block used recognise digits blue pixels would represent connections likes whereas red ones would represent connections dislikes trained neural network notice 30 different types basic lego blocks lego puzzle thats size hidden layer heres look like features looking ones better loops edges according network heres classifies 10 digits guess whatnone make sense none features seem capture isolated distinguishable feature input image mistaken randomly shaped blobs randomly chosen places mean look identifies 0 weight matrix image output neuron recognizes 0s clear pixels image represent weights connecting hidden layer output neuron recognises 0s shall take handful useful features digit account visually select intense blue pixels intense red pixels blue ones give us useful features red ones give us dreaded ones think neuron saying image absolutely match prototype 0 indices three intense blue pixels 3 6 26indices three intense red pixels 5 18 22 matrices 6 26 seem capture something like blue boundary sorts surrounding inner red pixels exactly could actually help identifying 0 matrix 3 capture feature even explain words goes matrix 18 would neuron like seems quite similar matrix 3 lets even go weird blue 22 nonsensical see lets 1 indices three intense blue pixels 0 11 16indices top two intense red pixels 7 20 words one wont even try comment world used identify 1s much anticipated 8 represent 2 loops top 3 intense blue pixels 1 6 14top 3 intense red pixels 7 24 27 nope isnt good either seem loops like expecting another interesting thing notice majority pixels output layer neuron image one collage red seems like network figured way recognise 8s using features like couldnt put digits together using features lego blocks failed real bad task fair features werent much legoblocky either heres neural networks said learn like us consider way build hierarchies features like see features nothing like would use networks give almost explanation features learn neural networks good function approximators build train one mostly care accuracy percentage test samples give positive results works incredibly well lot purposes modern neural nets remarkably high accuracies upward 98 uncommon meaning chances failure 1 100 heres catch wrong theres easy way understand reason cant debugged traditional sense example heres embarrassing incident happened google understanding neural networks learn subject great importance crucial unleashing true power deep learning help us weeks ago new york times magazine ran story neural networks trained predict death cancer patients remarkable accuracy heres writer oncologist said think strongly relate little project little project described earlier stumbled upon results found really cool worth sharing smaller networks wanted see low could make hidden layer size still getting considerable accuracy across test set turns 10 neurons network able classify 9343 10000 test images correctly thats 9343 accuracy classifying images never seen 10 hidden neurons 10 different types lego blocks recognise 10 digits find incredibly fascinating course weights dont make much sense either case curious tried 5 neurons got accuracy 8665 4 neurons accuracy 8373 dropped steeply 3 neurons 5875 2 neurons 2280 weight initialisation regularisation makes lot difference regularising network using good initialisations weights huge effect network learns let demonstrate used network architecture meaning layers neurons layers trained 2 network objects one without regularisation using old nprandomrandn whereas one used regularisation along nprandomrandnsqrtn observed yeah shocked note shown weight matrices associated different index neurons collage due different initialisations even ones index learn different features chose ones appear make effect starking know weight initialisation techniques neural networks recommend start want discuss article project mind really anything ai please feel free comment drop message linkedin facebook twitter learnt lot deep learning since project article like completing deep learning specialisation coursera dont hesitate reach think could help thank reading follow twitter httpstwittercomnityeshaga wont spam feed originally published zeolearn blog quick cheer standing ovation clap show much enjoyed story reader writer programmer sharing concepts ideas codes,en,"['Neural Networks', 'Nonsensical', 'Google', 'The New York Times Magazine', 'np.random.randn()/sqrt(n', 'LinkedIn', 'the Deep Learning Specialisation', 'Coursera']"
